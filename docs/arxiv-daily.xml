<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>ArXiv Query: search_query=cat:cs.*&amp;amp;id_list=&amp;amp;start=0&amp;amp;max_results=10</title>
<link>http://arxiv.org/api//Sr0Ktnalppie/A9qvr4BIZuOfQ</link>

<item>
<title>SCP-Diff: Photo-Realistic Semantic Image Synthesis with
  Spatial-Categorical Joint Prior</title>
<link>http://arxiv.org/abs/2403.09638v1</link>
<guid>http://arxiv.org/abs/2403.09638v1</guid>
<content:encoded><![CDATA[
<div> 关键词: Semantic image synthesis, ControlNet, latent diffusion models, noise priors, SCP-Diff

总结:
语义图像合成是传感器模拟领域的一个有希望的技术，但目前基于GAN的最佳实践在质量上还未达到期望的水平。然而，随着潜在扩散模型在图像生成方面取得了重大进展，有必要评估ControlNet这一密集控制能力显著的方法。我们的研究发现了ControlNet结果的两个主要问题：大语义区域内存在奇怪的子结构，以及内容与语义掩模不匹配。通过实证研究，我们找到了这些问题的原因，即训练数据分布与推断阶段应用的标准正态先验之间的不匹配。为了解决这一挑战，我们为SIS开发了特定的噪声先验，包括空间、分类和新颖的空间-分类联合先验。这种方法被我们称为SCP-Diff，并取得了卓越的成果，在Cityscapes上达到了10.53的FID，在ADE20K上达到了12.66的FID。项目页面上可以访问到代码和模型。 <br><br> <div>
Semantic image synthesis (SIS) shows good promises for sensor simulation.
However, current best practices in this field, based on GANs, have not yet
reached the desired level of quality. As latent diffusion models make
significant strides in image generation, we are prompted to evaluate
ControlNet, a notable method for its dense control capabilities. Our
investigation uncovered two primary issues with its results: the presence of
weird sub-structures within large semantic areas and the misalignment of
content with the semantic mask. Through empirical study, we pinpointed the
cause of these problems as a mismatch between the noised training data
distribution and the standard normal prior applied at the inference stage. To
address this challenge, we developed specific noise priors for SIS,
encompassing spatial, categorical, and a novel spatial-categorical joint prior
for inference. This approach, which we have named SCP-Diff, has yielded
exceptional results, achieving an FID of 10.53 on Cityscapes and 12.66 on
ADE20K.The code and models can be accessed via the project page.
]]></content:encoded>
<pubDate>2024-03-14T17:59:55Z</pubDate>
<pubDate>2024-03-14T17:59:55Z</pubDate>
</item>
<item>
<title>3D-VLA: A 3D Vision-Language-Action Generative World Model</title>
<link>http://arxiv.org/abs/2403.09631v1</link>
<guid>http://arxiv.org/abs/2403.09631v1</guid>
<content:encoded><![CDATA[
<div> 3D-VLA, perception, action, world model, embodied foundation model

3D-VLA是一种新型的视觉-语言-行动模型，与以往的2D输入不同，它能够无缝地连接3D感知、推理和行动。该模型引入了一系列交互令牌，以便在具体环境中进行交互，并通过训练一系列体现扩散模型，使模型具备生成能力。此外，模型还建立了一个大规模的3D具体指令数据集，通过提取现有机器人数据集中的大量3D相关信息进行训练。实验表明，3D-VLA显著改善了在具体环境中的推理、多模态生成和规划能力，展现出其在真实世界应用中的潜力。<br><br>总结: <div>
Recent vision-language-action (VLA) models rely on 2D inputs, lacking
integration with the broader realm of the 3D physical world. Furthermore, they
perform action prediction by learning a direct mapping from perception to
action, neglecting the vast dynamics of the world and the relations between
actions and dynamics. In contrast, human beings are endowed with world models
that depict imagination about future scenarios to plan actions accordingly. To
this end, we propose 3D-VLA by introducing a new family of embodied foundation
models that seamlessly link 3D perception, reasoning, and action through a
generative world model. Specifically, 3D-VLA is built on top of a 3D-based
large language model (LLM), and a set of interaction tokens is introduced to
engage with the embodied environment. Furthermore, to inject generation
abilities into the model, we train a series of embodied diffusion models and
align them into the LLM for predicting the goal images and point clouds. To
train our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by
extracting vast 3D-related information from existing robotics datasets. Our
experiments on held-in datasets demonstrate that 3D-VLA significantly improves
the reasoning, multimodal generation, and planning capabilities in embodied
environments, showcasing its potential in real-world applications.
]]></content:encoded>
<pubDate>2024-03-14T17:58:41Z</pubDate>
<pubDate>2024-03-14T17:58:41Z</pubDate>
</item>
<item>
<title>Quiet-STaR: Language Models Can Teach Themselves to Think Before
  Speaking</title>
<link>http://arxiv.org/abs/2403.09629v1</link>
<guid>http://arxiv.org/abs/2403.09629v1</guid>
<content:encoded><![CDATA[

When writing and talking, people sometimes pause to think. Although
reasoning-focused works have often framed reasoning as a method of answering
questions or completing agentic tasks, reasoning is implicit in almost all
written text. For example, this applies to the steps not stated between the
lines of a proof or to the theory of mind underlying a conversation. In the
Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned
by inferring rationales from few-shot examples in question-answering and
learning from those that lead to a correct answer. This is a highly constrained
setting -- ideally, a language model could instead learn to infer unstated
rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR
in which LMs learn to generate rationales at each token to explain future text,
improving their predictions. We address key challenges, including 1) the
computational cost of generating continuations, 2) the fact that the LM does
not initially know how to generate or use internal thoughts, and 3) the need to
predict beyond individual next tokens. To resolve these, we propose a tokenwise
parallel sampling algorithm, using learnable tokens indicating a thought's
start and end, and an extended teacher-forcing technique. Encouragingly,
generated rationales disproportionately help model difficult-to-predict tokens
and improve the LM's ability to directly answer difficult questions. In
particular, after continued pretraining of an LM on a corpus of internet text
with Quiet-STaR, we find zero-shot improvements on GSM8K
(5.9%$\rightarrow$10.9%) and CommonsenseQA (36.3%$\rightarrow$47.2%) and
observe a perplexity improvement of difficult tokens in natural text.
Crucially, these improvements require no fine-tuning on these tasks. Quiet-STaR
marks a step towards LMs that can learn to reason in a more general and
scalable way.
]]></content:encoded>
<pubDate>2024-03-14T17:58:16Z</pubDate>
<pubDate>2024-03-14T17:58:16Z</pubDate>
</item>

<item>
<title>VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis</title>
<link>http://arxiv.org/abs/2403.08764v1</link>
<guid>http://arxiv.org/abs/2403.08764v1</guid>
<content:encoded><![CDATA[
<div> VLOGGER, audio-driven, human video generation, diffusion model, MENTOR

VLOGGER提出了一种基于音频驱动的单张人物图像生成视频的方法，借鉴了最近生成扩散模型的成功。该方法包括1）随机的人体到三维运动扩散模型，2）一种新颖的基于扩散的架构，将文本到图像模型与空间和时间控制相结合。这支持生成可变长度的高质量视频，通过人脸和身体的高级表示轻松控制。与先前的工作不同，该方法不需要为每个人进行训练，也不依赖于人脸检测和裁剪，并且生成完整的图像（不仅仅是脸部或嘴唇），考虑到关键的场景（例如可见的躯干或多样化的主体身份），以正确合成交流的人类。他们还提出了MENTOR，这是一个新的多样化数据集，具有3D姿势和表情注释，比以前的数据集大一个数量级（800,000个身份），并具有动态手势，他们在这个数据集上训练并消融了他们的主要技术贡献。VLOGGER在三个公开基准测试中表现优越，考虑到图像质量，身份保留和时间一致性，同时生成上半身手势。他们分析了VLOGGER的性能，关于多个多样性指标，显示出他们的架构选择和MENTOR的使用有利于以规模训练公正和无偏差的模型。最后，他们展示了视频编辑和个性化的应用。

<br /><br />总结: VLOGGER是一种基于音频驱动的人类视频生成方法，利用扩散模型和大规模数据集MENTOR，能够生成高质量、可变长度的视频，并具有上半身手势。它不需要为每个人进行训练，能够生成完整的人物图像，并考虑到不同的场景和主体身份。该方法在公开基准测试中表现优越，同时能够训练公正和无偏差的模型。 <div>
We propose VLOGGER, a method for audio-driven human video generation from a
single input image of a person, which builds on the success of recent
generative diffusion models. Our method consists of 1) a stochastic
human-to-3d-motion diffusion model, and 2) a novel diffusion-based architecture
that augments text-to-image models with both spatial and temporal controls.
This supports the generation of high quality video of variable length, easily
controllable through high-level representations of human faces and bodies. In
contrast to previous work, our method does not require training for each
person, does not rely on face detection and cropping, generates the complete
image (not just the face or the lips), and considers a broad spectrum of
scenarios (e.g. visible torso or diverse subject identities) that are critical
to correctly synthesize humans who communicate. We also curate MENTOR, a new
and diverse dataset with 3d pose and expression annotations, one order of
magnitude larger than previous ones (800,000 identities) and with dynamic
gestures, on which we train and ablate our main technical contributions.
  VLOGGER outperforms state-of-the-art methods in three public benchmarks,
considering image quality, identity preservation and temporal consistency while
also generating upper-body gestures. We analyze the performance of VLOGGER with
respect to multiple diversity metrics, showing that our architectural choices
and the use of MENTOR benefit training a fair and unbiased model at scale.
Finally we show applications in video editing and personalization.
]]></content:encoded>
<pubDate>2024-03-13T17:59:02Z</pubDate>
</item>
<item>
<title>Bridging Different Language Models and Generative Vision Models for
  Text-to-Image Generation</title>
<link>http://arxiv.org/abs/2403.07860v1</link>
<guid>http://arxiv.org/abs/2403.07860v1</guid>
<content:encoded><![CDATA[
<div> text-to-image generation, language model, vision model, integration, LaVi-Bridge

总结: 
本文介绍了文本到图像生成的最新发展，即文本到图像扩散模型。这些模型通常包括一个语言模型，用于解释用户提示，以及一个视觉模型，用于生成相应的图像。研究的目标是探索如何整合任意两个不相关的语言和生成视觉模型，以提高文本到图像生成的质量。文中提出了LaVi-Bridge，这是一个能够整合各种预训练语言模型和生成视觉模型的流程。通过利用LoRA和适配器，LaVi-Bridge提供了一种灵活的插拔式方法，无需修改语言和视觉模型的原始权重。研究表明，整合更先进的模块，如更先进的语言模型或生成视觉模型，可以显着提高文本对齐和图像质量等能力。通过广泛的评估验证了LaVi-Bridge的有效性。 该项目的代码可在https://github.com/ShihaoZhaoZSH/LaVi-Bridge 上找到。 <div>
Text-to-image generation has made significant advancements with the
introduction of text-to-image diffusion models. These models typically consist
of a language model that interprets user prompts and a vision model that
generates corresponding images. As language and vision models continue to
progress in their respective domains, there is a great potential in exploring
the replacement of components in text-to-image diffusion models with more
advanced counterparts. A broader research objective would therefore be to
investigate the integration of any two unrelated language and generative vision
models for text-to-image generation. In this paper, we explore this objective
and propose LaVi-Bridge, a pipeline that enables the integration of diverse
pre-trained language models and generative vision models for text-to-image
generation. By leveraging LoRA and adapters, LaVi-Bridge offers a flexible and
plug-and-play approach without requiring modifications to the original weights
of the language and vision models. Our pipeline is compatible with various
language models and generative vision models, accommodating different
structures. Within this framework, we demonstrate that incorporating superior
modules, such as more advanced language models or generative vision models,
results in notable improvements in capabilities like text alignment or image
quality. Extensive evaluations have been conducted to verify the effectiveness
of LaVi-Bridge. Code is available at
https://github.com/ShihaoZhaoZSH/LaVi-Bridge.
]]></content:encoded>
<pubDate>2024-03-12T17:50:11Z</pubDate>
</item>
<item>
<title>Gemini 1.5: Unlocking multimodal understanding across millions of tokens
  of context</title>
<link>http://arxiv.org/abs/2403.05530v1</link>
<guid>http://arxiv.org/abs/2403.05530v1</guid>
<content:encoded><![CDATA[
<div> Gemini 1.5 Pro, multimodal, long-context, recall, state-of-the-art

Gemini 1.5 Pro是Gemini家族的最新模型，是一种高效的多模态专家混合模型，能够在包括多个长文档和数小时的视频和音频在内的上下文中召回和推理细粒度信息。在长上下文检索任务中，Gemini 1.5 Pro实现了接近完美的召回率，提高了长文档QA、长视频QA和长上下文ASR的最新水平，并在各种基准测试中达到或超过了Gemini 1.0 Ultra的最新表现。研究了Gemini 1.5 Pro在长上下文能力方面的极限，发现了在接下来的标记预测和近乎完美的（>99%）检索方面的持续改进，可达到至少10M个标记，比现有模型如Claude 2.1（20万）和GPT-4 Turbo（12.8万）有了一代的飞跃。最后，我们突出介绍了大型语言模型在前沿的惊人新能力；当给定一个Kalamang语法手册时，这个模型学会了将英语翻译成Kalamang，其水平与从相同内容学习的人相似。 <br /><br />总结: Gemimi 1.5 Pro是一款高效的多模态专家混合模型，能够处理长上下文任务，并在长文档QA、长视频QA和长上下文ASR方面取得了最新的突破。此外，该模型还在标记预测和检索方面表现出非常优越的能力，超越了现有的一些模型。最后，该模型还展示了在小语种翻译方面的惊人能力。 <div>
In this report, we present the latest model of the Gemini family, Gemini 1.5
Pro, a highly compute-efficient multimodal mixture-of-experts model capable of
recalling and reasoning over fine-grained information from millions of tokens
of context, including multiple long documents and hours of video and audio.
Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks
across modalities, improves the state-of-the-art in long-document QA,
long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's
state-of-the-art performance across a broad set of benchmarks. Studying the
limits of Gemini 1.5 Pro's long-context ability, we find continued improvement
in next-token prediction and near-perfect retrieval (>99%) up to at least 10M
tokens, a generational leap over existing models such as Claude 2.1 (200k) and
GPT-4 Turbo (128k). Finally, we highlight surprising new capabilities of large
language models at the frontier; when given a grammar manual for Kalamang, a
language with fewer than 200 speakers worldwide, the model learns to translate
English to Kalamang at a similar level to a person who learned from the same
content.
]]></content:encoded>
<pubDate>2024-03-08T18:54:20Z</pubDate>
</item>
<item>
<title>Mechanism for Decision-aware Collaborative Federated Learning: A Pitfall
  of Shapley Values</title>
<link>http://arxiv.org/abs/2403.04753v1</link>
<guid>http://arxiv.org/abs/2403.04753v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 机制设计, 协作学习, 联邦学习平台, 决策感知, Shapley value

总结: 本论文研究了通过联邦学习平台实现决策感知协作学习的机制设计。文章介绍了一个由数字平台和多个决策感知代理组成的框架，平台提供基础设施，使代理可以访问数据，为协作学习创造激励，进行联邦学习以避免直接共享原始数据。文章引入了多行动协作联邦学习（MCFL）框架，分析了著名的Shapley value机制的均衡问题。研究发现，虽然Shapley value通过鼓励充分参与有效地最大化了联合创造的剩余价值，但不慎促进了虚假身份操纵，进一步增加了平台进行联邦学习时的通信成本。因此，Shapley value机制有一个严重的缺陷，即暗示着数据分割和身份复制，最终损害了联邦学习系统的整体效率。<br /><br />总结: 本论文研究了决策感知协作学习的机制设计，介绍了联邦学习平台的框架，分析了Shapley value机制的均衡问题，并发现其存在的缺陷。 <div>
This paper investigates mechanism design for decision-aware collaboration via
federated learning (FL) platforms. Our framework consists of a digital platform
and multiple decision-aware agents, each endowed with proprietary data sets.
The platform offers an infrastructure that enables access to the data, creates
incentives for collaborative learning aimed at operational decision-making, and
conducts FL to avoid direct raw data sharing. The computation and communication
efficiency of the FL process is inherently influenced by the agent
participation equilibrium induced by the mechanism. Therefore, assessing the
system's efficiency involves two critical factors: the surplus created by
coalition formation and the communication costs incurred across the coalition
during FL. To evaluate the system efficiency under the intricate interplay
between mechanism design, agent participation, operational decision-making, and
the performance of FL algorithms, we introduce a multi-action collaborative
federated learning (MCFL) framework for decision-aware agents. Under this
framework, we further analyze the equilibrium for the renowned Shapley value
based mechanisms. Specifically, we examine the issue of false-name
manipulation, a form of dishonest behavior where participating agents create
duplicate fake identities to split their original data among these identities.
By solving the agent participation equilibrium, we demonstrate that while
Shapley value effectively maximizes coalition-generated surplus by encouraging
full participation, it inadvertently promotes false-name manipulation. This
further significantly increases the communication costs when the platform
conducts FL. Thus, we highlight a significant pitfall of Shapley value based
mechanisms, which implicitly incentivizes data splitting and identity
duplication, ultimately impairing the overall efficiency in FL systems.
]]></content:encoded>
<pubDate>2024-03-07T18:54:59Z</pubDate>
</item>
<item>
<title>Stop Regressing: Training Value Functions via Classification for
  Scalable Deep RL</title>
<link>http://arxiv.org/abs/2403.03950v1</link>
<guid>http://arxiv.org/abs/2403.03950v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 深度强化学习, 值函数, 分类交叉熵, 可扩展性, 改进性能

深度强化学习中的值函数是一个核心组件，通过神经网络参数化，采用均方误差回归目标进行训练以匹配自举目标值。然而，将使用回归的基于值的强化学习方法扩展到大型网络（如高容量Transformer）的难度已被证明是具有挑战性的。观察到这一差异，本文研究了是否通过在训练值函数时仅使用分类代替回归可以改进深度强化学习的可扩展性。我们证明，使用分类交叉熵训练的值函数显著改善了多个领域的性能和可扩展性。其中包括：在Atari 2600游戏中使用SoftMoEs进行单任务强化学习，使用大规模ResNets进行Atari的多任务强化学习，在Q-transformers中进行机器人操纵，无搜索下的国际象棋对局，以及使用高容量Transformer进行语言代理Wordle任务，在这些领域取得了最先进的结果。通过仔细分析，我们展示了分类交叉熵的好处主要源于其减轻了值为基础的强化学习固有的问题，如嘈杂的目标和非稳态性。总体而言，我们认为简单地将值函数训练与分类交叉熵相结合可以在几乎没有成本的情况下显著提高深度强化学习的可扩展性。
<br /><br />总结: 本文研究了通过使用分类交叉熵代替回归来训练值函数，从而改善深度强化学习的可扩展性。作者证明了这种方法在多个领域均取得了显著的性能提升，包括Atari游戏、机器人操纵和语言代理任务。通过分析，作者指出分类交叉熵的好处主要来自于其减轻值函数训练中的固有问题。因此，本文的贡献在于提出了一种简单且高效的方法来提高深度强化学习的可扩展性，为相关领域的研究和应用带来了新的启发。 <div>
Value functions are a central component of deep reinforcement learning (RL).
These functions, parameterized by neural networks, are trained using a mean
squared error regression objective to match bootstrapped target values.
However, scaling value-based RL methods that use regression to large networks,
such as high-capacity Transformers, has proven challenging. This difficulty is
in stark contrast to supervised learning: by leveraging a cross-entropy
classification loss, supervised methods have scaled reliably to massive
networks. Observing this discrepancy, in this paper, we investigate whether the
scalability of deep RL can also be improved simply by using classification in
place of regression for training value functions. We demonstrate that value
functions trained with categorical cross-entropy significantly improves
performance and scalability in a variety of domains. These include: single-task
RL on Atari 2600 games with SoftMoEs, multi-task RL on Atari with large-scale
ResNets, robotic manipulation with Q-transformers, playing Chess without
search, and a language-agent Wordle task with high-capacity Transformers,
achieving state-of-the-art results on these domains. Through careful analysis,
we show that the benefits of categorical cross-entropy primarily stem from its
ability to mitigate issues inherent to value-based RL, such as noisy targets
and non-stationarity. Overall, we argue that a simple shift to training value
functions with categorical cross-entropy can yield substantial improvements in
the scalability of deep RL at little-to-no cost.
]]></content:encoded>
<pubDate>2024-03-06T18:55:47Z</pubDate>
</item>
<item>
<title>Can Audio Reveal Music Performance Difficulty? Insights from the Piano
  Syllabus Dataset</title>
<link>http://arxiv.org/abs/2403.03947v1</link>
<guid>http://arxiv.org/abs/2403.03947v1</guid>
<content:encoded><![CDATA[
<div> 音乐教育, 音乐信息检索, 音频分析, 数据集, 难度估计
<br /><br />总结:
音乐教育中自动估计音乐作品难度的重要性，音乐信息检索领域已经有一些工作致力于这一任务。本文首次提出了一个基于音频录音的音乐作品难度估计数据集，并开发了一个识别框架来处理不同的输入表示。实验证明了该提议的有效性，并将数据集、代码和训练模型公开分享，以促进该领域的进一步研究。 <div>
Automatically estimating the performance difficulty of a music piece
represents a key process in music education to create tailored curricula
according to the individual needs of the students. Given its relevance, the
Music Information Retrieval (MIR) field depicts some proof-of-concept works
addressing this task that mainly focuses on high-level music abstractions such
as machine-readable scores or music sheet images. In this regard, the potential
of directly analyzing audio recordings has been generally neglected, which
prevents students from exploring diverse music pieces that may not have a
formal symbolic-level transcription. This work pioneers in the automatic
estimation of performance difficulty of music pieces on audio recordings with
two precise contributions: (i) the first audio-based difficulty estimation
dataset -- namely, Piano Syllabus (PSyllabus) dataset -- featuring 7,901 piano
pieces across 11 difficulty levels from 1,233 composers; and (ii) a recognition
framework capable of managing different input representations -- both unimodal
and multimodal manners -- directly derived from audio to perform the difficulty
estimation task. The comprehensive experimentation comprising different
pre-training schemes, input modalities, and multi-task scenarios prove the
validity of the proposal and establishes PSyllabus as a reference dataset for
audio-based difficulty estimation in the MIR field. The dataset as well as the
developed code and trained models are publicly shared to promote further
research in the field.
]]></content:encoded>
<pubDate>2024-03-06T18:54:13Z</pubDate>
</item>
<item>
<title>Scaling Rectified Flow Transformers for High-Resolution Image Synthesis</title>
<link>http://arxiv.org/abs/2403.03206v1</link>
<guid>http://arxiv.org/abs/2403.03206v1</guid>
<content:encoded><![CDATA[
<div> 高维数据，扩散模型，噪音采样，文本到图像生成，变压器架构
<br />
本文介绍了扩散模型和矫正流模型对高维感知数据的生成建模技术，矫正流模型为直线连接了数据和噪音，具有更好的理论特性和概念简单性，但尚未成为标准做法。作者提出了改进的噪音采样技术，通过偏向感知相关尺度的噪音，提高了对于高分辨率文本到图像合成的性能。此外，作者提出了一个基于变压器的架构，实现了文本到图像的双向信息流，提高了文本理解能力、排版效果和人类评价。最后，作者强调了他们的最大模型优于现有模型，并将实验数据、代码和模型权重公开发布。 
<br /><br />总结: 
<br />扩散模型和矫正流模型为高维感知数据的生成建模提供了有效手段，提出了改进的噪音采样技术，以提高对高分辨率文本到图像合成的性能。新的基于变压器的架构实现了文本到图像的双向信息流，提高了文本理解能力、排版效果和人类评价。作者的最大模型优于现有模型，并将实验数据、代码和模型权重公开发布。 <div>
Diffusion models create data from noise by inverting the forward paths of
data towards noise and have emerged as a powerful generative modeling technique
for high-dimensional, perceptual data such as images and videos. Rectified flow
is a recent generative model formulation that connects data and noise in a
straight line. Despite its better theoretical properties and conceptual
simplicity, it is not yet decisively established as standard practice. In this
work, we improve existing noise sampling techniques for training rectified flow
models by biasing them towards perceptually relevant scales. Through a
large-scale study, we demonstrate the superior performance of this approach
compared to established diffusion formulations for high-resolution
text-to-image synthesis. Additionally, we present a novel transformer-based
architecture for text-to-image generation that uses separate weights for the
two modalities and enables a bidirectional flow of information between image
and text tokens, improving text comprehension, typography, and human preference
ratings. We demonstrate that this architecture follows predictable scaling
trends and correlates lower validation loss to improved text-to-image synthesis
as measured by various metrics and human evaluations. Our largest models
outperform state-of-the-art models, and we will make our experimental data,
code, and model weights publicly available.
]]></content:encoded>
<pubDate>2024-03-05T18:45:39Z</pubDate>
</item>
<item>
<title>Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers</title>
<link>http://arxiv.org/abs/2402.19479v1</link>
<guid>http://arxiv.org/abs/2402.19479v1</guid>
<content:encoded><![CDATA[
<div> 数据质量、视频标注、Panda-70M数据集、下游任务、模型训练

数据质量和标注质量上限了下游模型的质量。视频文本数据难以收集，因为手动标注耗时多，需要观看整个视频，而视频包含多个场景和动作。为了建立高质量字幕视频数据集，研究人员提出了一种自动方法，利用文本视频描述、字幕和视频帧。他们从公开可用的HD-VILA-100M数据集中筛选出380万个高分辨率视频，并将它们分成语义一致的视频片段，然后利用跨模态教师模型为每个视频获取字幕。接下来，在少量视频子集上微调检索模型，然后在整个数据集上使用该模型选择最佳字幕作为标注。于是得到了7000万个与高质量文本字幕配对的视频数据，命名为Panda-70M数据集。研究人员展示了该数据集在视频字幕生成、视频和文本检索等三项下游任务上的价值，表明基于该数据集训练的模型在大多数指标上得分明显更好。<br /><br />总结: 该研究提出了一种自动方法，利用跨模态输入建立高质量字幕视频数据集Panda-70M，并展示了其在多项下游任务上训练模型得分明显更好的价值。 <div>
The quality of the data and annotation upper-bounds the quality of a
downstream model. While there exist large text corpora and image-text pairs,
high-quality video-text data is much harder to collect. First of all, manual
labeling is more time-consuming, as it requires an annotator to watch an entire
video. Second, videos have a temporal dimension, consisting of several scenes
stacked together, and showing multiple actions. Accordingly, to establish a
video dataset with high-quality captions, we propose an automatic approach
leveraging multimodal inputs, such as textual video description, subtitles, and
individual video frames. Specifically, we curate 3.8M high-resolution videos
from the publicly available HD-VILA-100M dataset. We then split them into
semantically consistent video clips, and apply multiple cross-modality teacher
models to obtain captions for each video. Next, we finetune a retrieval model
on a small subset where the best caption of each video is manually selected and
then employ the model in the whole dataset to select the best caption as the
annotation. In this way, we get 70M videos paired with high-quality text
captions. We dub the dataset as Panda-70M. We show the value of the proposed
dataset on three downstream tasks: video captioning, video and text retrieval,
and text-driven video generation. The models trained on the proposed data score
substantially better on the majority of metrics across all the tasks.
]]></content:encoded>
<pubDate>2024-02-29T18:59:50Z</pubDate>
</item>
<item>
<title>Trajectory Prediction for Autonomous Driving Using a Transformer Network</title>
<link>http://arxiv.org/abs/2402.16501v1</link>
<guid>http://arxiv.org/abs/2402.16501v1</guid>
<content:encoded><![CDATA[
<div> 关键词: autonomous driving, multi-modal trajectory prediction, transformer network, convolutional networks, Lyft l5kit dataset

这篇论文介绍了基于Transformer网络的多模态轨迹预测框架，以及利用每个代理的语义地图作为输入，通过卷积网络自动获取相关的上下文信息。另外，还提出了一种新的辅助损失函数，惩罚不可行的越野预测。实验证明，该模型在Lyft l5kit数据集上取得了最先进的性能，显著提高了预测结果的准确性和可行性。

总结:
1. 论文介绍了自动驾驶中预测周围代理的轨迹是一个具有挑战性的任务。
2. 提出了基于Transformer网络的多模态轨迹预测框架，并利用语义地图作为输入。
3. 使用卷积网络自动获取相关的上下文信息。
4. 提出了一种新的辅助损失函数，用于惩罚不可行的越野预测。
5. 实验证明所提出的模型在Lyft l5kit数据集上取得了最先进的性能，显著提高了预测结果的准确性和可行性。 <div>
Predicting the trajectories of surrounding agents is still considered one of
the most challenging tasks for autonomous driving. In this paper, we introduce
a multi-modal trajectory prediction framework based on the transformer network.
The semantic maps of each agent are used as inputs to convolutional networks to
automatically derive relevant contextual information. A novel auxiliary loss
that penalizes unfeasible off-road predictions is also proposed in this study.
Experiments on the Lyft l5kit dataset show that the proposed model achieves
state-of-the-art performance, substantially improving the accuracy and
feasibility of the prediction outcomes.
]]></content:encoded>
<pubDate>2024-02-26T11:35:23Z</pubDate>
</item>
<item>
<title>LLMArena: Assessing Capabilities of Large Language Models in Dynamic
  Multi-Agent Environments</title>
<link>http://arxiv.org/abs/2402.16499v1</link>
<guid>http://arxiv.org/abs/2402.16499v1</guid>
<content:encoded><![CDATA[
<div> LLMArena, 大型语言模型, 多智能体环境, 评估框架, 实验

LLMArena 是一个新颖且易于扩展的框架，用于评估大型语言模型在多智能体动态环境中的不同能力。该框架包括七个不同的游戏环境，并采用 Trueskill 计分来评估语言模型的关键能力，包括空间推理、战略规划、数值推理、风险评估、沟通、对手建模和团队协作。研究人员通过对不同大小和类型的大型语言模型进行大量实验和人类评估，发现大型语言模型在对手建模和团队协作方面仍有很大的发展空间。作者希望LLMArena可以引导未来的研究，增强大型语言模型的这些能力，最终实现在动态的多智能体环境中更复杂和实用的应用。  <br /><br />总结: <br />LLMArena 是一个新颖且易于扩展的框架，用于评估大型语言模型在多智能体动态环境中的不同能力。该框架包括七个不同的游戏环境，并采用 Trueskill 计分来评估语言模型的关键能力，研究发现大型语言模型在对手建模和团队协作方面仍有很大的发展空间。作者希望LLMArena可以引导未来的研究，增强大型语言模型的这些能力，最终实现在动态的多智能体环境中更复杂和实用的应用。 <div>
Recent advancements in large language models (LLMs) have revealed their
potential for achieving autonomous agents possessing human-level intelligence.
However, existing benchmarks for evaluating LLM Agents either use static
datasets, potentially leading to data leakage or focus only on single-agent
scenarios, overlooking the complexities of multi-agent interactions. There is a
lack of a benchmark that evaluates the diverse capabilities of LLM agents in
multi-agent, dynamic environments. To this end, we introduce LLMArena, a novel
and easily extensible framework for evaluating the diverse capabilities of LLM
in multi-agent dynamic environments. LLMArena encompasses seven distinct gaming
environments, employing Trueskill scoring to assess crucial abilities in LLM
agents, including spatial reasoning, strategic planning, numerical reasoning,
risk assessment, communication, opponent modeling, and team collaboration. We
conduct an extensive experiment and human evaluation among different sizes and
types of LLMs, showing that LLMs still have a significant journey ahead in
their development towards becoming fully autonomous agents, especially in
opponent modeling and team collaboration. We hope LLMArena could guide future
research towards enhancing these capabilities in LLMs, ultimately leading to
more sophisticated and practical applications in dynamic, multi-agent settings.
The code and data will be available.
]]></content:encoded>
<pubDate>2024-02-26T11:31:48Z</pubDate>
</item>
<item>
<title>Q-FOX Learning: Breaking Tradition in Reinforcement Learning</title>
<link>http://arxiv.org/abs/2402.16562v1</link>
<guid>http://arxiv.org/abs/2402.16562v1</guid>
<content:encoded><![CDATA[
<div> 强化学习, 人工智能, 超参数调优, Q-FOX, OpenAI Gym
总结:<br /><br />这篇文章介绍了强化学习中超参数调优的重要性，提出了一种新的自动超参数调优方法 Q-FOX，该方法利用了FOX优化器和Q-learning算法，通过优化新的目标函数，优化了超参数以获得更好的强化学习性能。实验结果表明，Q-FOX相比其他优化器在解决OpenAI Gym环境控制任务时获得了更高的累积奖励。但Q-FOX仍有局限性，不能直接用于真实环境，也需要在模拟环境中进行迭代优化，因此存在一定的时间成本。综合来看，Q-FOX在强化学习的超参数调优方面发挥了重要作用。 <div>
Reinforcement learning (RL) is a subset of artificial intelligence (AI) where
agents learn the best action by interacting with the environment, making it
suitable for tasks that do not require labeled data or direct supervision.
Hyperparameters (HP) tuning refers to choosing the best parameter that leads to
optimal solutions in RL algorithms. Manual or random tuning of the HP may be a
crucial process because variations in this parameter lead to changes in the
overall learning aspects and different rewards. In this paper, a novel and
automatic HP-tuning method called Q-FOX is proposed. This uses both the FOX
optimizer, a new optimization method inspired by nature that mimics red foxes'
hunting behavior, and the commonly used, easy-to-implement RL Q-learning
algorithm to solve the problem of HP tuning. Moreover, a new objective function
is proposed which prioritizes the reward over the mean squared error (MSE) and
learning time (steps). Q-FOX has been evaluated on two OpenAI Gym environment
control tasks: Cart Pole and Frozen Lake. It exposed greater cumulative rewards
than HP tuning with other optimizers, such as PSO, GA, Bee, or randomly
selected HP. The cumulative reward for the Cart Pole task was 32.08, and for
the Frozen Lake task was 0.95. Despite the robustness of Q-FOX, it has
limitations. It cannot be used directly in real-word problems before choosing
the HP in a simulation environment because its processes work iteratively,
making it time-consuming. The results indicate that Q-FOX has played an
essential role in HP tuning for RL algorithms to effectively solve different
control tasks.
]]></content:encoded>
<pubDate>2024-02-26T13:39:04Z</pubDate>
</item>
<item>
<title>Contracts with Inspections</title>
<link>http://arxiv.org/abs/2402.16553v1</link>
<guid>http://arxiv.org/abs/2402.16553v1</guid>
<content:encoded><![CDATA[
<div> hidden-action model, principal-agent, incentive, inspection, deterministic<br />
<br />
本文提出了一个新的模型，放松了隐性行为的假设，允许委托人在一定成本下检查特定的行为。如果委托人发现代理人没有选择协定的行为，他可以扣留支付。这个模型的放松引入了更广泛的策略空间，委托人需要在积极激励（增加支付）和负面激励（增加检查）之间进行权衡。作者展示了如何在所有单调检查成本函数中找到最佳的确定性激励兼容检查方案。然后，作者转向随机检查方案，展示了在检查成本函数为次模的情况下可以有效地找到最佳的随机激励兼容检查方案。作者补充说，对于更一般的XOS检查成本函数，不可能有效地找到最佳的随机检查方案。 <br /><br />总结: 本文提出了一个新的委托人-代理人模型，在此模型下，作者展示了如何找到最佳的确定性和随机激励兼容检查方案。 <div>
In the classical principal-agent hidden-action model, a principal delegates
the execution of a costly task to an agent for which he can choose among
actions with different costs and different success probabilities to accomplish
the task. To incentivize the agent to exert effort, the principal can commit to
a contract, which is the amount of payment based on the task's success. A
crucial assumption of this model is that the principal can only base the
payment on the outcome but not on the agent's chosen action.
  In this work, we relax the hidden-action assumption and introduce a new model
where the principal is allowed to inspect subsets of actions at some cost that
depends on the inspected subset. If the principal discovers that the agent did
not select the agreed-upon action through the inspection, the principal can
withhold payment. This relaxation of the model introduces a broader strategy
space for the principal, who now faces a tradeoff between positive incentives
(increasing payment) and negative incentives (increasing inspection).
  We show how to find the best deterministic incentive-compatible inspection
scheme for all monotone inspection cost functions. We then turn to randomized
inspection schemes and show that one can efficiently find the best randomized
incentive-compatible inspection scheme when the inspection cost function is
submodular. We complement this result by showing that it is impossible to
efficiently find the optimal randomized inspection scheme for the more general
case of XOS inspection cost functions.
]]></content:encoded>
<pubDate>2024-02-26T13:26:34Z</pubDate>
</item>
<item>
<title>AgentOhana: Design Unified Data and Training Pipeline for Effective
  Agent Learning</title>
<link>http://arxiv.org/abs/2402.15506v1</link>
<guid>http://arxiv.org/abs/2402.15506v1</guid>
<content:encoded><![CDATA[
<div> AgentOhana, LLMs, agent trajectories, data loader, xLAM-v0.1
总结:<br /><br />本文介绍了AgentOhana作为解决LLMs在agent-based任务中面临挑战的综合解决方案。AgentOhana聚合了不同环境中的agent轨迹，将它们标准化和统一格式，简化了用于agent训练的数据加载器的创建。利用数据的统一性，我们的训练流程在不同数据源之间保持平衡，并在数据集分区和模型训练过程中保持设备独立的随机性。此外，我们还介绍了xLAM-v0.1，这是一个针对AI Agent的大型动作模型，展现了在各种基准测试中的卓越性能。 <div>
Autonomous agents powered by large language models (LLMs) have garnered
significant research attention. However, fully harnessing the potential of LLMs
for agent-based tasks presents inherent challenges due to the heterogeneous
nature of diverse data sources featuring multi-turn trajectories. In this
paper, we introduce \textbf{AgentOhana} as a comprehensive solution to address
these challenges. \textit{AgentOhana} aggregates agent trajectories from
distinct environments, spanning a wide array of scenarios. It meticulously
standardizes and unifies these trajectories into a consistent format,
streamlining the creation of a generic data loader optimized for agent
training. Leveraging the data unification, our training pipeline maintains
equilibrium across different data sources and preserves independent randomness
across devices during dataset partitioning and model training. Additionally, we
present \textbf{xLAM-v0.1}, a large action model tailored for AI agents, which
demonstrates exceptional performance across various benchmarks.
]]></content:encoded>
<pubDate>2024-02-23T18:56:26Z</pubDate>
</item>
<item>
<title>PALO: A Polyglot Large Multimodal Model for 5B People</title>
<link>http://arxiv.org/abs/2402.14818v1</link>
<guid>http://arxiv.org/abs/2402.14818v1</guid>
<content:encoded><![CDATA[
<div> 多语言模型、视觉推理、Palo、跨语言性能、benchmark<br />
在这项研究中，我们介绍了一种名为Palo的大型多语言多模态模型，该模型涵盖了10种主要语言，包括英语、中文、印地语、西班牙语、法语、阿拉伯语、孟加拉语、俄语、乌尔都语和日语，覆盖了约50亿人口（全球人口的65%）。我们采用半自动化翻译方法，利用经过精细调节的大型语言模型，将多模态指令数据集从英语翻译成目标语言，确保高语言准确性的同时，最大程度地减少了手动工作量。我们的方法提高了跨多种语言的整体性能，特别是对一些较少代表的语言，如印地语、阿拉伯语、孟加拉语和乌尔都语。我们训练了三种规模（1.7B、7B和13B参数）的模型，展示了其泛化性和可扩展性，在与强基线模型相比取得了实质性的改进。此外，我们还提出了首个多语言多模态基准测试，用于评估未来方法在各种语言中的视觉与语言推理能力。 <div>
In pursuit of more inclusive Vision-Language Models (VLMs), this study
introduces a Large Multilingual Multimodal Model called \textsc{Palo}.
\textsc{Palo} offers visual reasoning capabilities in 10 major languages,
including English, Chinese, Hindi, Spanish, French, Arabic, Bengali, Russian,
Urdu, and Japanese, that span a total of $\sim$5B people (65\% of the world
population). Our approach involves a semi-automated translation approach to
adapt the multimodal instruction dataset from English to the target languages
using a fine-tuned Large Language Model, thereby ensuring high linguistic
fidelity while allowing scalability due to minimal manual effort. The
incorporation of diverse instruction sets helps us boost overall performance
across multiple languages especially those that are underrepresented like
Hindi, Arabic, Bengali, and Urdu. The resulting models are trained across three
scales (1.7B, 7B and 13B parameters) to show the generalization and scalability
where we observe substantial improvements compared to strong baselines. We also
propose the first multilingual multimodal benchmark for the forthcoming
approaches to evaluate their vision-language reasoning capabilities across
languages. Code: https://github.com/mbzuai-oryx/PALO.
]]></content:encoded>
<pubDate>2024-02-22T18:59:58Z</pubDate>
</item>
<item>
<title>A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit
  Tasks in Public Health</title>
<link>http://arxiv.org/abs/2402.14807v1</link>
<guid>http://arxiv.org/abs/2402.14807v1</guid>
<content:encoded><![CDATA[
<div> 决策语言模型，RMAB，健康资源分配，政策优先级，模拟研究<br />
决策语言模型（DLM）是一个结合了大型语言模型（LLM）和多臂老虎机算法（RMAB）的工具，旨在通过人类语言指令动态调整公共卫生政策。研究通过与印度的ARMMAN合作，展示了DLM能够通过人类语言指令动态塑造政策结果。文章通过提出DLM这一新方法，希望解决在公共卫生领域面临的挑战，包括资源有限、政策优先级不断变化等问题。这一研究对于提高孕产妇的预防保健水平，降低孕产妇死亡率具有重要意义。 <br /><br />总结: <br />决策语言模型（DLM）结合了大型语言模型和多臂老虎机算法，旨在通过人类语言指令动态调整公共卫生政策。研究展示了DLM能够通过人类语言指令动态塑造政策结果。文章提出了这一新方法，希望解决公共卫生领域面临的挑战，对于提高孕产妇的预防保健水平，降低孕产妇死亡率具有重要意义。 <div>
Efforts to reduce maternal mortality rate, a key UN Sustainable Development
target (SDG Target 3.1), rely largely on preventative care programs to spread
critical health information to high-risk populations. These programs face two
important challenges: efficiently allocating limited health resources to large
beneficiary populations, and adapting to evolving policy priorities. While
prior works in restless multi-armed bandit (RMAB) demonstrated success in
public health allocation tasks, they lack flexibility to adapt to evolving
policy priorities. Concurrently, Large Language Models (LLMs) have emerged as
adept, automated planners in various domains, including robotic control and
navigation. In this paper, we propose DLM: a Decision Language Model for RMABs.
To enable dynamic fine-tuning of RMAB policies for challenging public health
settings using human-language commands, we propose using LLMs as automated
planners to (1) interpret human policy preference prompts, (2) propose code
reward functions for a multi-agent RL environment for RMABs, and (3) iterate on
the generated reward using feedback from RMAB simulations to effectively adapt
policy outcomes. In collaboration with ARMMAN, an India-based public health
organization promoting preventative care for pregnant mothers, we conduct a
simulation study, showing DLM can dynamically shape policy outcomes using only
human language commands as input.
]]></content:encoded>
<pubDate>2024-02-22T18:58:27Z</pubDate>
</item>
<item>
<title>OlympiadBench: A Challenging Benchmark for Promoting AGI with
  Olympiad-Level Bilingual Multimodal Scientific Problems</title>
<link>http://arxiv.org/abs/2402.14008v1</link>
<guid>http://arxiv.org/abs/2402.14008v1</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Multimodal Models, OlympiadBench, GPT-4V, 评估方法

- Large Language Models（LLMs）和 Large Multimodal Models（LMMs）已经超过了一般人类在不同任务上的表现
- 文章介绍了一个新的多语言多模态科学竞赛基准（OlympiadBench），包括数学和物理竞赛问题
- 评估了顶尖模型在OlympiadBench上的表现，并发现最好的模型 GPT-4V 在物理方面仅得到了11.28% 的平均分数
- 评估指出 GPT-4V 模型存在幻觉、知识遗漏和逻辑错误的问题
- 希望这一挑战性的基准可以成为未来通用人工智能研究的宝贵资源

<br /><br />总结:
大型语言模型（LLMs）和多模态模型（LMMs）在多个任务上已经超过了一般人类的能力水平。该文章介绍了OlympiadBench，这是一个面向奥林匹克级别的双语多模态科学基准，包含了来自数学和物理竞赛以及中国高考的 8,952 个问题，并进行了专家级别的详细注释。通过在OlympiadBench上评估顶尖模型，文章实施了全面的评估方法来准确评估模型的响应。然而，最好的表现模型 GPT-4V 在OlympiadBench上的平均得分仅为 17.23%，在物理方面仅有 11.28% 的得分，凸显了基准的严谨性和物理推理的复杂性。评估发现 GPT-4V 模型存在幻觉、知识遗漏和逻辑错误等问题。希望这一具有挑战性的基准可以成为未来通用人工智能研究的宝贵资源。 <div>
Recent advancements have seen Large Language Models (LLMs) and Large
Multimodal Models (LMMs) surpassing general human capabilities in various
tasks, approaching the proficiency level of human experts across multiple
domains. With traditional benchmarks becoming less challenging for these
models, new rigorous challenges are essential to gauge their advanced
abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual
multimodal scientific benchmark, featuring 8,952 problems from Olympiad-level
mathematics and physics competitions, including the Chinese college entrance
exam. Each problem is detailed with expert-level annotations for step-by-step
reasoning. Evaluating top-tier models on OlympiadBench, we implement a
comprehensive assessment methodology to accurately evaluate model responses.
Notably, the best-performing model, GPT-4V, attains an average score of 17.23%
on OlympiadBench, with a mere 11.28% in physics, highlighting the benchmark
rigor and the intricacy of physical reasoning. Our analysis orienting GPT-4V
points out prevalent issues with hallucinations, knowledge omissions, and
logical fallacies. We hope that our challenging benchmark can serve as a
valuable resource for helping future AGI research endeavors.
]]></content:encoded>
<pubDate>2024-02-21T18:49:26Z</pubDate>
</item>
<item>
<title>Information Elicitation in Agency Games</title>
<link>http://arxiv.org/abs/2402.14005v1</link>
<guid>http://arxiv.org/abs/2402.14005v1</guid>
<content:encoded><![CDATA[
<div> 观测性问题，信息披露，代理关系，成本相关变量，市场效率<br />
总结：<br />
本文讨论了数据采集和处理工具的快速发展，提出了决定计算哪些评估指标的挑战。作者通过代理关系模型分析了代理何时有动机向委托人披露成本相关变量的可观测性。结果表明，代理倾向于披露能够揭示高低成本之间明显差异的信息，而在披露信息时倾向于进行信息扭曲。最后，作者分析了总体福利问题，指出信息扭曲可能导致更高的总体福利。 <div>
Rapid progress in scalable, commoditized tools for data collection and data
processing has made it possible for firms and policymakers to employ ever more
complex metrics as guides for decision-making. These developments have
highlighted a prevailing challenge -- deciding *which* metrics to compute. In
particular, a firm's ability to compute a wider range of existing metrics does
not address the problem of *unknown unknowns*, which reflects informational
limitations on the part of the firm. To guide the choice of metrics in the face
of this informational problem, we turn to the evaluated agents themselves, who
may have more information than a principal about how to measure outcomes
effectively. We model this interaction as a simple agency game, where we ask:
*When does an agent have an incentive to reveal the observability of a
cost-correlated variable to the principal?* There are two effects: better
information reduces the agent's information rents but also makes some projects
go forward that otherwise would fail. We show that the agent prefers to reveal
information that exposes a strong enough differentiation between high and low
costs. Expanding the agent's action space to include the ability to *garble*
their information, we show that the agent often prefers to garble over full
revelation. Still, giving the agent the ability to garble can lead to higher
total welfare. Our model has analogies with price discrimination, and we
leverage some of these synergies to analyze total welfare.
]]></content:encoded>
<pubDate>2024-02-21T18:44:38Z</pubDate>
</item>
<item>
<title>CounterCurate: Enhancing Physical and Semantic Visio-Linguistic
  Compositional Reasoning via Counterfactual Examples</title>
<link>http://arxiv.org/abs/2402.13254v1</link>
<guid>http://arxiv.org/abs/2402.13254v1</guid>
<content:encoded><![CDATA[
<div> 对应关键词: CounterCurate, visio-linguistic compositional reasoning, physically grounded reasoning, data augmentation, semantic counterfactuals

本研究提出了CounterCurate框架，旨在全面提高对比和生成式多模态模型的视觉-语言组合推理能力。首先，发现目前模型在与物理相关的组合推理方面表现不佳，然后利用GLIGEN模型进行简单的数据增强，显著提高了模型性能，尤其是在新创建的Flickr30k-Positions基准测试中，CLIP和LLaVA的性能分别提高了33%和37%。其次，利用高性能文本和图像生成模型（GPT-4V和DALLE-3）创建具有挑战性的语义反事实情况，从而进一步提高了组合推理能力，在SugarCrepe基准测试中，CounterCurate的表现超过了GPT-4V。<br /><br />总结: 本研究提出了CounterCurate框架，通过解决当前模型在物理相关推理和反事实情况处理上的不足，提高了多模态模型的组合推理能力。 <div>
We propose CounterCurate, a framework to comprehensively improve the
visio-linguistic compositional reasoning capability for both contrastive and
generative multimodal models. In particular, we identify two under-explored
critical problems: the neglect of the physically grounded reasoning (counting
and position understanding) and the potential of using highly capable text and
image generation models for semantic counterfactual fine-tuning. Our work
pioneers an approach that addresses these gaps. We first spotlight the
near-chance performance of multimodal models like CLIP and LLaVA in physically
grounded compositional reasoning. We then apply simple data augmentation using
a grounded image generation model, GLIGEN, to generate finetuning data,
resulting in significant performance improvements: +33% and +37% for CLIP and
LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark.
Moreover, we exploit the capabilities of high-performing text generation and
image generation models, specifically GPT-4V and DALLE-3, to curate challenging
semantic counterfactuals, thereby further enhancing compositional reasoning
capabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms
GPT-4V.
]]></content:encoded>
<pubDate>2024-02-20T18:59:55Z</pubDate>
</item>
<item>
<title>Fusion of Diffusion Weighted MRI and Clinical Data for Predicting
  Functional Outcome after Acute Ischemic Stroke with Deep Contrastive Learning</title>
<link>http://arxiv.org/abs/2402.10894v1</link>
<guid>http://arxiv.org/abs/2402.10894v1</guid>
<content:encoded><![CDATA[
<div> 病中风，扩散加权MRI，健康结构概要，预测功能结果，深度融合学习网络

总结:<br />
文章探讨了使用扩散加权MRI模态结合健康结构概要对预测中风患者功能结果的有效性，以促进早期干预。提出了两阶段训练的深度融合学习网络，采用监督对比学习来学习区分特征，预测患者在中风发作后3个月是否需要长期护理。研究发现，所提出的融合模型在AUC、F1分数和准确性方面均表现优异，优于现有模型，尤其在医疗领域的结合图像和结构数据模型中。此外，扩散加权MRI可以与其他临床变量结合以获得更好的普适性和准确性，甚至可以替代NIHSS以实现同等水平的准确性。 <div>
Stroke is a common disabling neurological condition that affects about
one-quarter of the adult population over age 25; more than half of patients
still have poor outcomes, such as permanent functional dependence or even
death, after the onset of acute stroke. The aim of this study is to investigate
the efficacy of diffusion-weighted MRI modalities combining with structured
health profile on predicting the functional outcome to facilitate early
intervention. A deep fusion learning network is proposed with two-stage
training: the first stage focuses on cross-modality representation learning and
the second stage on classification. Supervised contrastive learning is
exploited to learn discriminative features that separate the two classes of
patients from embeddings of individual modalities and from the fused multimodal
embedding. The network takes as the input DWI and ADC images, and structured
health profile data. The outcome is the prediction of the patient needing
long-term care at 3 months after the onset of stroke. Trained and evaluated
with a dataset of 3297 patients, our proposed fusion model achieves 0.87, 0.80
and 80.45% for AUC, F1-score and accuracy, respectively, outperforming existing
models that consolidate both imaging and structured data in the medical domain.
If trained with comprehensive clinical variables, including NIHSS and
comorbidities, the gain from images on making accurate prediction is not
considered substantial, but significant. However, diffusion-weighted MRI can
replace NIHSS to achieve comparable level of accuracy combining with other
readily available clinical variables for better generalization.
]]></content:encoded>
<pubDate>2024-02-16T18:51:42Z</pubDate>
</item>
<item>
<title>When is Tree Search Useful for LLM Planning? It Depends on the
  Discriminator</title>
<link>http://arxiv.org/abs/2402.10890v1</link>
<guid>http://arxiv.org/abs/2402.10890v1</guid>
<content:encoded><![CDATA[
<div> 大语言模型，语言代理，规划方法，迭代校正，树搜索<br />
这篇论文研究了大语言模型在多步问题下的解决方法，使用了语言代理框架的三个组件：生成器，鉴别器和规划方法。实验表明，使用迭代校正和树搜索这两种高级规划方法要求鉴别器至少有90%准确率才能取得明显的改进效果，而当前的大语言模型的鉴别能力并未达到这一要求。另外，使用基于大语言模型的鉴别器时，高级规划方法可能未能很好地平衡准确性和效率，例如，相较于其他两种方法，树搜索至少慢了10-20倍，但带来的性能提升微乎其微，这阻碍了其在实际应用中的使用。总结：<br /><br />这篇论文研究了大语言模型在多步问题下的解决方法，使用了语言代理框架的三个组件：生成器，鉴别器和规划方法。实验表明，使用迭代校正和树搜索这两种高级规划方法要求鉴别器至少有90%准确率才能取得明显的改进效果，而当前的大语言模型的鉴别能力并未达到这一要求。另外，使用基于大语言模型的鉴别器时，高级规划方法可能未能很好地平衡准确性和效率，例如，相较于其他两种方法，树搜索至少慢了10-20倍，但带来的性能提升微乎其微，这阻碍了其在实际应用中的使用。 <div>
In this paper, we examine how large language models (LLMs) solve multi-step
problems under a language agent framework with three components: a generator, a
discriminator, and a planning method. We investigate the practical utility of
two advanced planning methods, iterative correction and tree search. We present
a comprehensive analysis of how discrimination accuracy affects the overall
performance of agents when using these two methods or a simpler method,
re-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical
reasoning, show that: (1) advanced planning methods demand discriminators with
at least 90% accuracy to achieve significant improvements over re-ranking; (2)
current LLMs' discrimination abilities have not met the needs of advanced
planning methods to achieve such improvements; (3) with LLM-based
discriminators, advanced planning methods may not adequately balance accuracy
and efficiency. For example, compared to the other two methods, tree search is
at least 10--20 times slower but leads to negligible performance gains, which
hinders its real-world applications. Code and data will be released at
https://github.com/OSU-NLP-Group/llm-planning-eval.
]]></content:encoded>
<pubDate>2024-02-16T18:45:58Z</pubDate>
</item>
<item>
<title>A Trembling House of Cards? Mapping Adversarial Attacks against Language
  Agents</title>
<link>http://arxiv.org/abs/2402.10196v1</link>
<guid>http://arxiv.org/abs/2402.10196v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 语言代理、大型语言模型、自动化技术、安全风险、攻击场景
总结: 
语言代理基于大型语言模型的自动化技术发展迅猛，但其在安全风险方面存在着新的挑战。本文首次系统地探讨了针对语言代理的对抗性攻击，并提出了12种潜在的攻击场景，涵盖了不同的攻击策略。在三个主要组成部分（感知、大脑、行动）的框架下，对攻击场景进行了全面讨论，并与先前应用于大型语言模型的成功攻击策略进行了联系。强调了在广泛部署语言代理之前，我们迫切需要全面了解语言代理的风险。 <br /><br /> <div>
Language agents powered by large language models (LLMs) have seen exploding
development. Their capability of using language as a vehicle for thought and
communication lends an incredible level of flexibility and versatility. People
have quickly capitalized on this capability to connect LLMs to a wide range of
external components and environments: databases, tools, the Internet, robotic
embodiment, etc. Many believe an unprecedentedly powerful automation technology
is emerging. However, new automation technologies come with new safety risks,
especially for intricate systems like language agents. There is a surprisingly
large gap between the speed and scale of their development and deployment and
our understanding of their safety risks. Are we building a house of cards? In
this position paper, we present the first systematic effort in mapping
adversarial attacks against language agents. We first present a unified
conceptual framework for agents with three major components: Perception, Brain,
and Action. Under this framework, we present a comprehensive discussion and
propose 12 potential attack scenarios against different components of an agent,
covering different attack strategies (e.g., input manipulation, adversarial
demonstrations, jailbreaking, backdoors). We also draw connections to
successful attack strategies previously applied to LLMs. We emphasize the
urgency to gain a thorough understanding of language agent risks before their
widespread deployment.
]]></content:encoded>
<pubDate>2024-02-15T18:51:32Z</pubDate>
</item>
<item>
<title>Rec-GPT4V: Multimodal Recommendation with Large Vision-Language Models</title>
<link>http://arxiv.org/abs/2402.08670v1</link>
<guid>http://arxiv.org/abs/2402.08670v1</guid>
<content:encoded><![CDATA[
<div> 关键词: large vision-language models, multimodal recommendations, Rec-GPT4V, user preferences, image sequence dynamics

总结: <br /><br />这篇文章讨论了大型视觉语言模型在多模态推荐中的应用，指出了其存在的挑战和复杂性。为了克服这些问题，提出了一种名为Rec-GPT4V的新颖推理方案：Visual-Summary Thought（VST），并利用用户历史作为上下文用户偏好来解决LVLMs缺乏用户偏好知识的问题。接下来，文章介绍了如何利用LVLMs生成项目图像摘要，并结合项目标题在自然语言空间中查询用户对候选项目的偏好。最后，通过对四个数据集和三种LVLMs进行全面实验，结果表明了VST的有效性。 <div>
The development of large vision-language models (LVLMs) offers the potential
to address challenges faced by traditional multimodal recommendations thanks to
their proficient understanding of static images and textual dynamics. However,
the application of LVLMs in this field is still limited due to the following
complexities: First, LVLMs lack user preference knowledge as they are trained
from vast general datasets. Second, LVLMs suffer setbacks in addressing
multiple image dynamics in scenarios involving discrete, noisy, and redundant
image sequences. To overcome these issues, we propose the novel reasoning
scheme named Rec-GPT4V: Visual-Summary Thought (VST) of leveraging large
vision-language models for multimodal recommendation. We utilize user history
as in-context user preferences to address the first challenge. Next, we prompt
LVLMs to generate item image summaries and utilize image comprehension in
natural language space combined with item titles to query the user preferences
over candidate items. We conduct comprehensive experiments across four datasets
with three LVLMs: GPT4-V, LLaVa-7b, and LLaVa-13b. The numerical results
indicate the efficacy of VST.
]]></content:encoded>
<pubDate>2024-02-13T18:51:18Z</pubDate>
</item>
<item>
<title>MODIPHY: Multimodal Obscured Detection for IoT using PHantom
  Convolution-Enabled Faster YOLO</title>
<link>http://arxiv.org/abs/2402.07894v1</link>
<guid>http://arxiv.org/abs/2402.07894v1</guid>
<content:encoded><![CDATA[
<div> 模型压缩, YOLO Phantom, 低光条件, 多模态数据, 实时性能

总结:<br />
本研究介绍了一种名为"YOLO Phantom"的小型YOLO模型，利用全新的Phantom卷积块实现了可比较的准确性，同时减少了43%的参数和模型大小，减少了19%的GFLOPs。YOLO Phantom利用多模态RGB-红外数据进行迁移学习，解决了低光和遮挡问题，使其在恶劣条件下具有强大的视觉能力。在物联网平台上，与先进的低光和RGB摄像头无缝连接，连接到基于AWS的通知端点，实现了高效的实时目标检测。基准测试显示，与基线YOLOv8n模型相比，热成像和RGB检测的帧率提升了17%和14%。为了贡献于社区，代码和多模态数据集都在GitHub上可用。 <div>
Low-light conditions and occluded scenarios impede object detection in
real-world Internet of Things (IoT) applications like autonomous vehicles and
security systems. While advanced machine learning models strive for accuracy,
their computational demands clash with the limitations of resource-constrained
devices, hampering real-time performance. In our current research, we tackle
this challenge, by introducing "YOLO Phantom", one of the smallest YOLO models
ever conceived. YOLO Phantom utilizes the novel Phantom Convolution block,
achieving comparable accuracy to the latest YOLOv8n model while simultaneously
reducing both parameters and model size by 43%, resulting in a significant 19%
reduction in Giga Floating Point Operations (GFLOPs). YOLO Phantom leverages
transfer learning on our multimodal RGB-infrared dataset to address low-light
and occlusion issues, equipping it with robust vision under adverse conditions.
Its real-world efficacy is demonstrated on an IoT platform with advanced
low-light and RGB cameras, seamlessly connecting to an AWS-based notification
endpoint for efficient real-time object detection. Benchmarks reveal a
substantial boost of 17% and 14% in frames per second (FPS) for thermal and RGB
detection, respectively, compared to the baseline YOLOv8n model. For community
contribution, both the code and the multimodal dataset are available on GitHub.
]]></content:encoded>
<pubDate>2024-02-12T18:56:53Z</pubDate>
</item>
<item>
<title>MAIDCRL: Semi-centralized Multi-Agent Influence Dense-CNN Reinforcement
  Learning</title>
<link>http://arxiv.org/abs/2402.07890v1</link>
<guid>http://arxiv.org/abs/2402.07890v1</guid>
<content:encoded><![CDATA[
<div> Distributed decision-making, multi-agent systems, interactive behavior learning, Dense Reinforcement Learning, agent influence maps<br />
<br />
在多智能体系统中，分布式决策面临着巨大挑战，尤其是在合作和竞争系统中的交互式行为学习方面。为了缓解这种复杂性，本文介绍了一种半集中式的Dense强化学习算法，通过智能体影响图（AIMs）来增强对StarCraft多智能体挑战（SMAC）场景中有效的多智能体控制的学习。我们扩展了MAIDRL中的DenseNet，并引入了半集中式多智能体Dense-CNN强化学习（MAIDCRL），通过将卷积层结合到深度模型架构中，并在同质和异质场景中进行性能评估。结果显示，CNN使得MAIDCRL显著提高了学习性能，并在复杂的异质SMAC场景中取得了更快的学习速度。我们进一步调查了模型的稳定性和鲁棒性。统计数据表明，我们的模型不仅在所有给定场景中实现了更高的胜率，而且提升了智能体的细粒度决策过程的学习。 <br /><br />总结: <br />该研究介绍了一种半集中式Dense-CNN强化学习算法MAIDCRL，该算法通过智能体影响图（AIMs）在StarCraft多智能体挑战（SMAC）场景中实现了有效的多智能体控制。研究结果表明，在异质场景中，CNN-enabled MAIDCRL显著提高了学习性能并实现了更快的学习速度。该模型不仅在所有给定场景中实现了更高的胜率，而且提升了智能体的细粒度决策过程的学习。 <div>
Distributed decision-making in multi-agent systems presents difficult
challenges for interactive behavior learning in both cooperative and
competitive systems. To mitigate this complexity, MAIDRL presents a
semi-centralized Dense Reinforcement Learning algorithm enhanced by agent
influence maps (AIMs), for learning effective multi-agent control on StarCraft
Multi-Agent Challenge (SMAC) scenarios. In this paper, we extend the DenseNet
in MAIDRL and introduce semi-centralized Multi-Agent Dense-CNN Reinforcement
Learning, MAIDCRL, by incorporating convolutional layers into the deep model
architecture, and evaluate the performance on both homogeneous and
heterogeneous scenarios. The results show that the CNN-enabled MAIDCRL
significantly improved the learning performance and achieved a faster learning
rate compared to the existing MAIDRL, especially on more complicated
heterogeneous SMAC scenarios. We further investigate the stability and
robustness of our model. The statistics reflect that our model not only
achieves higher winning rate in all the given scenarios but also boosts the
agent's learning process in fine-grained decision-making.
]]></content:encoded>
<pubDate>2024-02-12T18:53:20Z</pubDate>
</item>
<item>
<title>Feedback Loops With Language Models Drive In-Context Reward Hacking</title>
<link>http://arxiv.org/abs/2402.06627v1</link>
<guid>http://arxiv.org/abs/2402.06627v1</guid>
<content:encoded><![CDATA[
<div> 反馈循环, 语言模型, 奖励欺骗, 输出精化, 策略精化
总结:
反馈循环会导致在上下文环境中发生奖励欺骗，语言模型在测试时会优化一个潜在的目标，但在过程中会产生负面影响。输出精化和策略精化是导致奖励欺骗的两个过程。在静态数据集上的评估不足以捕捉最有害的行为。因此，建议使用三种评估方法来捕捉更多奖励欺骗的情况。随着人工智能的发展加速，反馈循环的影响将会增加，这就增加了理解它们在塑造语言模型行为方面的重要性。 <div>
Language models influence the external world: they query APIs that read and
write to web pages, generate content that shapes human behavior, and run system
commands as autonomous agents. These interactions form feedback loops: LLM
outputs affect the world, which in turn affect subsequent LLM outputs. In this
work, we show that feedback loops can cause in-context reward hacking (ICRH),
where the LLM at test-time optimizes a (potentially implicit) objective but
creates negative side effects in the process. For example, consider an LLM
agent deployed to increase Twitter engagement; the LLM may retrieve its
previous tweets into the context window and make them more controversial,
increasing engagement but also toxicity. We identify and study two processes
that lead to ICRH: output-refinement and policy-refinement. For these
processes, evaluations on static datasets are insufficient -- they miss the
feedback effects and thus cannot capture the most harmful behavior. In
response, we provide three recommendations for evaluation to capture more
instances of ICRH. As AI development accelerates, the effects of feedback loops
will proliferate, increasing the need to understand their role in shaping LLM
behavior.
]]></content:encoded>
<pubDate>2024-02-09T18:59:29Z</pubDate>
</item>
<item>
<title>SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large
  Language Models</title>
<link>http://arxiv.org/abs/2402.05935v1</link>
<guid>http://arxiv.org/abs/2402.05935v1</guid>
<content:encoded><![CDATA[
<div> SPHINX-X, Multimodality, Large Language Model, dataset, training efficiency
<br /><br />
总结:SPHINX-X是在SPHINX基础上开发的一系列大型多模态语言模型（MLLM），通过修改框架并简化多阶段训练，提高了架构和训练效率。他们组建了一个包括语言、视觉和视觉-语言任务的多领域、多模态数据集，并且通过TinyLlama1.1B、InternLM2-7B、LLaMA2-13B和Mixtral8x7B等不同基础LLM的训练，获得了参数大小和多语言能力各异的MLLM。综合基准测试显示了多模态性能与数据和参数规模之间的强相关性。代码和模型已经在https://github.com/Alpha-VLLM/LLaMA2-Accessory上发布。 <div>
We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM)
series developed upon SPHINX. To improve the architecture and training
efficiency, we modify the SPHINX framework by removing redundant visual
encoders, bypassing fully-padded sub-images with skip tokens, and simplifying
multi-stage training into a one-stage all-in-one paradigm. To fully unleash the
potential of MLLMs, we assemble a comprehensive multi-domain and multimodal
dataset covering publicly available resources in language, vision, and
vision-language tasks. We further enrich this collection with our curated OCR
intensive and Set-of-Mark datasets, extending the diversity and generality. By
training over different base LLMs including TinyLlama1.1B, InternLM2-7B,
LLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in
parameter size and multilingual capabilities. Comprehensive benchmarking
reveals a strong correlation between the multi-modal performance with the data
and parameter scales. Code and models are released at
https://github.com/Alpha-VLLM/LLaMA2-Accessory
]]></content:encoded>
<pubDate>2024-02-08T18:59:48Z</pubDate>
</item>
<item>
<title>An Interactive Agent Foundation Model</title>
<link>http://arxiv.org/abs/2402.05929v1</link>
<guid>http://arxiv.org/abs/2402.05929v1</guid>
<content:encoded><![CDATA[
<div> Agent-based systems, multi-task agent training paradigm, versatile AI framework, Robotics, Gaming AI, Healthcare

发展中的人工智能系统正从创建静态的、特定任务的模型转变为能够在广泛应用中表现良好的动态的基于代理的系统。本文提出了一个交互式Agent Foundation模型，采用新颖的多任务代理训练范式，用于跨多个领域、数据集和任务训练AI代理。我们的训练范式统一了各种预训练策略，包括视觉蒙版自动编码器、语言建模和下一个动作预测，实现了多才多艺的、适应性强的AI框架。我们展示了我们的框架在三个不同领域--机器人技术、游戏人工智能和医疗保健领域的性能。我们的模型展示了它在每个领域产生有意义和上下文相关的输出的能力。我们的方法的优势在于其通用性，利用各种数据源，如机器人序列、游戏数据、大规模视频数据集和文字信息，进行有效的多模态和多任务学习。我们的方法为开发通用、行动取向、多模态系统提供了一个有前途的途径。<br /><br />总结: 人工智能系统的发展正在朝着能够适用于多领域的动态代理系统转变，该模型利用多任务训练范式和多种预训练策略，展示了在机器人技术、游戏人工智能和医疗保健领域的良好性能，为开发通用而多才多艺的AI系统提供了有前途的方向。 <div>
The development of artificial intelligence systems is transitioning from
creating static, task-specific models to dynamic, agent-based systems capable
of performing well in a wide range of applications. We propose an Interactive
Agent Foundation Model that uses a novel multi-task agent training paradigm for
training AI agents across a wide range of domains, datasets, and tasks. Our
training paradigm unifies diverse pre-training strategies, including visual
masked auto-encoders, language modeling, and next-action prediction, enabling a
versatile and adaptable AI framework. We demonstrate the performance of our
framework across three separate domains -- Robotics, Gaming AI, and Healthcare.
Our model demonstrates its ability to generate meaningful and contextually
relevant outputs in each area. The strength of our approach lies in its
generality, leveraging a variety of data sources such as robotics sequences,
gameplay data, large-scale video datasets, and textual information for
effective multimodal and multi-task learning. Our approach provides a promising
avenue for developing generalist, action-taking, multimodal systems.
]]></content:encoded>
<pubDate>2024-02-08T18:58:02Z</pubDate>
</item>
<item>
<title>WebLINX: Real-World Website Navigation with Multi-Turn Dialogue</title>
<link>http://arxiv.org/abs/2402.05930v1</link>
<guid>http://arxiv.org/abs/2402.05930v1</guid>
<content:encoded><![CDATA[
We propose the problem of conversational web navigation, where a digital
agent controls a web browser and follows user instructions to solve real-world
tasks in a multi-turn dialogue fashion. To support this problem, we introduce
WEBLINX - a large-scale benchmark of 100K interactions across 2300 expert
demonstrations of conversational web navigation. Our benchmark covers a broad
range of patterns on over 150 real-world websites and can be used to train and
evaluate agents in diverse scenarios. Due to the magnitude of information
present, Large Language Models (LLMs) cannot process entire web pages in
real-time. To solve this bottleneck, we design a retrieval-inspired model that
efficiently prunes HTML pages by ranking relevant elements. We use the selected
elements, along with screenshots and action history, to assess a variety of
models for their ability to replicate human behavior when navigating the web.
Our experiments span from small text-only to proprietary multimodal LLMs. We
find that smaller finetuned decoders surpass the best zero-shot LLMs (including
GPT-4V), but also larger finetuned multimodal models which were explicitly
pretrained on screenshots. However, all finetuned models struggle to generalize
to unseen websites. Our findings highlight the need for large multimodal models
that can generalize to novel settings. Our code, data and models are available
for research: https://mcgill-nlp.github.io/weblinx
]]></content:encoded>
<pubDate>2024-02-08T18:58:02Z</pubDate>
</item>
<item>
<title>Language-Based Augmentation to Address Shortcut Learning in Object Goal
  Navigation</title>
<link>http://arxiv.org/abs/2402.05090v1</link>
<guid>http://arxiv.org/abs/2402.05090v1</guid>
<content:encoded><![CDATA[
<div> DRL, Object-Goal Navigation, Shortcut learning, Language-Based (L-B) augmentation, Vision-Language Model (VLM)
<br /><br />总结:
本文研究了深度强化学习在目标导航中的应用，发现在训练环境中存在快捷学习的问题，即代理程序学习到了针对特定环境细节的策略。作者设计了一个实验，证明了快捷学习的存在，并提出了基于语言的增强方法来解决这一问题。他们发现，通过在视觉-语言模型的多模态特征空间中进行增强，可以降低代理程序在新环境中的成功率下降，从而解决了快捷学习问题。 <div>
Deep Reinforcement Learning (DRL) has shown great potential in enabling
robots to find certain objects (e.g., `find a fridge') in environments like
homes or schools. This task is known as Object-Goal Navigation (ObjectNav). DRL
methods are predominantly trained and evaluated using environment simulators.
Although DRL has shown impressive results, the simulators may be biased or
limited. This creates a risk of shortcut learning, i.e., learning a policy
tailored to specific visual details of training environments. We aim to deepen
our understanding of shortcut learning in ObjectNav, its implications and
propose a solution. We design an experiment for inserting a shortcut bias in
the appearance of training environments. As a proof-of-concept, we associate
room types to specific wall colors (e.g., bedrooms with green walls), and
observe poor generalization of a state-of-the-art (SOTA) ObjectNav method to
environments where this is not the case (e.g., bedrooms with blue walls). We
find that shortcut learning is the root cause: the agent learns to navigate to
target objects, by simply searching for the associated wall color of the target
object's room. To solve this, we propose Language-Based (L-B) augmentation. Our
key insight is that we can leverage the multimodal feature space of a
Vision-Language Model (VLM) to augment visual representations directly at the
feature-level, requiring no changes to the simulator, and only an addition of
one layer to the model. Where the SOTA ObjectNav method's success rate drops
69%, our proposal has only a drop of 23%.
]]></content:encoded>
<pubDate>2024-02-07T18:44:27Z</pubDate>
</item>
<item>
<title>AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls</title>
<link>http://arxiv.org/abs/2402.04253v1</link>
<guid>http://arxiv.org/abs/2402.04253v1</guid>
<content:encoded><![CDATA[
<div> APIs, AnyTool, GPT-4, evaluation protocol, benchmark<br />
<br />总结:
本文介绍了一个名为AnyTool的大型语言模型代理，旨在通过利用大量工具来解决用户查询的方式来彻底改革工具的利用。通过利用超过16,000个来自Rapid API的API，任何工具主要包括三个元素：具有分层结构的API检索器、旨在使用一组选择的API候选解决用户查询的求解器，以及一个自我反思机制，当初始解决方案不可行时重新激活AnyTool。AnyTool由GPT-4的函数调用功能驱动，消除了训练外部模块的需要。作者还重新访问了之前作品引入的评估协议，并确定了这一协议存在的限制，导致人为地高通过率。通过修改评估协议以更好地反映实际应用场景，他们引入了另一个基准，称为AnyToolBench。在各种数据集上进行的实验表明，AnyTool优于强基准，例如ToolLLM和专门用于工具利用的GPT-4变体。例如，在ToolBench的平均通过率上，AnyTool的表现优于ToolLLM 35.4%。代码将在https://github.com/dyabel/AnyTool 上提供。 <div>
We introduce AnyTool, a large language model agent designed to revolutionize
the utilization of a vast array of tools in addressing user queries. We utilize
over 16,000 APIs from Rapid API, operating under the assumption that a subset
of these APIs could potentially resolve the queries. AnyTool primarily
incorporates three elements: an API retriever with a hierarchical structure, a
solver aimed at resolving user queries using a selected set of API candidates,
and a self-reflection mechanism, which re-activates AnyTool if the initial
solution proves impracticable. AnyTool is powered by the function calling
feature of GPT-4, eliminating the need for training external modules. We also
revisit the evaluation protocol introduced by previous works and identify a
limitation in this protocol that leads to an artificially high pass rate. By
revising the evaluation protocol to better reflect practical application
scenarios, we introduce an additional benchmark, termed AnyToolBench.
Experiments across various datasets demonstrate the superiority of our AnyTool
over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool
utilization. For instance, AnyTool outperforms ToolLLM by +35.4% in terms of
average pass rate on ToolBench. Code will be available at
https://github.com/dyabel/AnyTool.
]]></content:encoded>
<pubDate>2024-02-06T18:59:57Z</pubDate>
</item>
<item>
<title>EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters</title>
<link>http://arxiv.org/abs/2402.04252v1</link>
<guid>http://arxiv.org/abs/2402.04252v1</guid>
<content:encoded><![CDATA[
<div> 关键词: CLIP, EVA-CLIP-18B, 18-billion参数, 图像分类基准, EVA-style弱到强视觉模型缩放<br />
<br />
这篇文章介绍了EVA-CLIP-18B，这是迄今为止最大、最强大的开源CLIP模型，拥有180亿个参数。该模型在27个广泛认可的图像分类基准中取得了异常出色的零样本top-1准确率达80.7%，表现优于其前身EVA-CLIP（50亿参数）和其他开源CLIP模型。令人惊讶的是，尽管保持对LAION-2B和COYO-700M的20亿图像文本配对的训练数据集不变，EVA-CLIP模型规模的扩大仍然能够保持一致的性能改进。EVA-CLIP-18B展示了EVA风格的弱到强视觉模型缩放的潜力。通过公开发布模型权重，希望能促进未来在视觉和多模型基础模型上的研究。<br /><br />总结: 本文介绍了EVA-CLIP-18B，这是迄今为止最大、最强大的开源CLIP模型，拥有180亿个参数。该模型表现出色，通过公开发布模型权重，希望促进未来研究。 <div>
Scaling up contrastive language-image pretraining (CLIP) is critical for
empowering both vision and multimodal models. We present EVA-CLIP-18B, the
largest and most powerful open-source CLIP model to date, with 18-billion
parameters. With only 6-billion training samples seen, EVA-CLIP-18B achieves an
exceptional 80.7% zero-shot top-1 accuracy averaged across 27 widely recognized
image classification benchmarks, outperforming its forerunner EVA-CLIP
(5-billion parameters) and other open-source CLIP models by a large margin.
Remarkably, we observe a consistent performance improvement with the model size
scaling of EVA-CLIP, despite maintaining a constant training dataset of
2-billion image-text pairs from LAION-2B and COYO-700M. This dataset is openly
available and much smaller than the in-house datasets (e.g., DFN-5B, WebLI-10B)
employed in other state-of-the-art CLIP models. EVA-CLIP-18B demonstrates the
potential of EVA-style weak-to-strong visual model scaling. With our model
weights made publicly available, we hope to facilitate future research in
vision and multimodal foundation models.
]]></content:encoded>
<pubDate>2024-02-06T18:59:48Z</pubDate>
</item>
<item>
<title>Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science</title>
<link>http://arxiv.org/abs/2402.04247v1</link>
<guid>http://arxiv.org/abs/2402.04247v1</guid>
<content:encoded><![CDATA[
Intelligent agents powered by large language models (LLMs) have demonstrated
substantial promise in autonomously conducting experiments and facilitating
scientific discoveries across various disciplines. While their capabilities are
promising, they also introduce novel vulnerabilities that demand careful
consideration for safety. However, there exists a notable gap in the
literature, as there has been no comprehensive exploration of these
vulnerabilities. This position paper fills this gap by conducting a thorough
examination of vulnerabilities in LLM-based agents within scientific domains,
shedding light on potential risks associated with their misuse and emphasizing
the need for safety measures. We begin by providing a comprehensive overview of
the potential risks inherent to scientific LLM agents, taking into account user
intent, the specific scientific domain, and their potential impact on the
external environment. Then, we delve into the origins of these vulnerabilities
and provide a scoping review of the limited existing works. Based on our
analysis, we propose a triadic framework involving human regulation, agent
alignment, and an understanding of environmental feedback (agent regulation) to
mitigate these identified risks. Furthermore, we highlight the limitations and
challenges associated with safeguarding scientific agents and advocate for the
development of improved models, robust benchmarks, and comprehensive
regulations to address these issues effectively.
]]></content:encoded>
<pubDate>2024-02-06T18:54:07Z</pubDate>
</item>
<item>
<title>V-IRL: Grounding Virtual Intelligence in Real Life</title>
<link>http://arxiv.org/abs/2402.03310v1</link>
<guid>http://arxiv.org/abs/2402.03310v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 感知, AI代理, 环境, 虚拟平台, 互动
总结:<br /><br />这篇文章介绍了一个名为V-IRL的平台，旨在通过虚拟环境让AI代理能够在真实世界中交互。该平台能够帮助开发能够完成各种实际任务的代理，并且可作为一个广阔的测试基地，用于测量在感知、决策和与全球各地真实世界数据交互等能力方面的进展。通过在虚拟而又真实的环境中实现代理的具体体现，可以弥合数字世界和真实世界之间的现实差距。 <div>
There is a sensory gulf between the Earth that humans inhabit and the digital
realms in which modern AI agents are created. To develop AI agents that can
sense, think, and act as flexibly as humans in real-world settings, it is
imperative to bridge the realism gap between the digital and physical worlds.
How can we embody agents in an environment as rich and diverse as the one we
inhabit, without the constraints imposed by real hardware and control? Towards
this end, we introduce V-IRL: a platform that enables agents to scalably
interact with the real world in a virtual yet realistic environment. Our
platform serves as a playground for developing agents that can accomplish
various practical tasks and as a vast testbed for measuring progress in
capabilities spanning perception, decision-making, and interaction with
real-world data across the entire globe.
]]></content:encoded>
<pubDate>2024-02-05T18:59:36Z</pubDate>
</item>
<item>
<title>AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion</title>
<link>http://arxiv.org/abs/2402.03309v1</link>
<guid>http://arxiv.org/abs/2402.03309v1</guid>
<content:encoded><![CDATA[
<div> acoustic-optical neural surface reconstruction, underwater perception, 3D surface reconstruction, baselines, multimodal fusion<br />
<br />
在水下感知和三维表面重建领域存在着诸多挑战，涉及到建筑、安全、海洋考古和环境监测等广泛领域的应用。由于恶劣的操作条件、脆弱的环境和有限的导航控制，潜水器通常需要限制其运动范围，因此也限制了其测量基线。我们的研究开发了一种基于物理的多模式声光神经表面重建框架（AONeuS），能够有效地将高分辨率的RGB测量与低分辨率的深度成像声纳测量相结合。通过融合这些互补的模态，我们的框架可以从受限基线上捕获的测量中重建准确的高分辨率三维表面。通过大量的模拟和实验，我们证明AONeuS显著优于最近的仅RGB和仅声纳的反差分渲染表面重建方法。我们的论文结果可通过以下网址查看：https://aoneus.github.io/ <br /><br />总结: 水下感知和三维表面重建面临诸多挑战，AONeuS框架成功融合了声光神经表面重建，能够在受限的基线下实现准确的高分辨率三维表面重建，并且优于之前的方法。 <div>
Underwater perception and 3D surface reconstruction are challenging problems
with broad applications in construction, security, marine archaeology, and
environmental monitoring. Treacherous operating conditions, fragile
surroundings, and limited navigation control often dictate that submersibles
restrict their range of motion and, thus, the baseline over which they can
capture measurements. In the context of 3D scene reconstruction, it is
well-known that smaller baselines make reconstruction more challenging. Our
work develops a physics-based multimodal acoustic-optical neural surface
reconstruction framework (AONeuS) capable of effectively integrating
high-resolution RGB measurements with low-resolution depth-resolved imaging
sonar measurements. By fusing these complementary modalities, our framework can
reconstruct accurate high-resolution 3D surfaces from measurements captured
over heavily-restricted baselines. Through extensive simulations and in-lab
experiments, we demonstrate that AONeuS dramatically outperforms recent
RGB-only and sonar-only inverse-differentiable-rendering--based surface
reconstruction methods. A website visualizing the results of our paper is
located at this address: https://aoneus.github.io/
]]></content:encoded>
<pubDate>2024-02-05T18:59:31Z</pubDate>
</item>
<item>
<title>Do Diffusion Models Learn Semantically Meaningful and Efficient
  Representations?</title>
<link>http://arxiv.org/abs/2402.03305v1</link>
<guid>http://arxiv.org/abs/2402.03305v1</guid>
<content:encoded><![CDATA[
Diffusion models are capable of impressive feats of image generation with
uncommon juxtapositions such as astronauts riding horses on the moon with
properly placed shadows. These outputs indicate the ability to perform
compositional generalization, but how do the models do so? We perform
controlled experiments on conditional DDPMs learning to generate 2D spherical
Gaussian bumps centered at specified $x$- and $y$-positions. Our results show
that the emergence of semantically meaningful latent representations is key to
achieving high performance. En route to successful performance over learning,
the model traverses three distinct phases of latent representations: (phase A)
no latent structure, (phase B) a 2D manifold of disordered states, and (phase
C) a 2D ordered manifold. Corresponding to each of these phases, we identify
qualitatively different generation behaviors: 1) multiple bumps are generated,
2) one bump is generated but at inaccurate $x$ and $y$ locations, 3) a bump is
generated at the correct $x$ and y location. Furthermore, we show that even
under imbalanced datasets where features ($x$- versus $y$-positions) are
represented with skewed frequencies, the learning process for $x$ and $y$ is
coupled rather than factorized, demonstrating that simple vanilla-flavored
diffusion models cannot learn efficient representations in which localization
in $x$ and $y$ are factorized into separate 1D tasks. These findings suggest
the need for future work to find inductive biases that will push generative
models to discover and exploit factorizable independent structures in their
inputs, which will be required to vault these models into more data-efficient
regimes.
]]></content:encoded>
<pubDate>2024-02-05T18:58:38Z</pubDate>
</item>
<item>
<title>TravelPlanner: A Benchmark for Real-World Planning with Language Agents</title>
<link>http://arxiv.org/abs/2402.01622v1</link>
<guid>http://arxiv.org/abs/2402.01622v1</guid>
<content:encoded><![CDATA[
<div> 语言代理，人工智能，旅行规划，规划基准，挑战性测试<br />
<br />
人工智能自诞生以来一直将规划作为核心追求，但早期的人工智能代理主要专注于受限环境，因为缺乏人类级别规划所需的许多认知基础。然而，最近由大型语言模型（LLM）驱动的语言代理展现出了有趣的能力，如工具使用和推理。但这些语言代理能否在超出以往人工智能代理能力范围的更复杂环境中进行规划呢？为了推动这一调查，提出了“TravelPlanner”规划基准，侧重于旅行规划，这是一个常见的真实世界规划场景。它提供了一个丰富的沙盒环境，各种工具用于访问近400万条数据记录，以及1225个精心策划的规划意图和参考计划。全面的评估显示，当前的语言代理尚不能处理此类复杂规划任务，即使GPT-4的成功率也只有0.6％。语言代理在保持任务、使用正确工具收集信息或跟踪多个约束方面很困难。但我们注意到，语言代理仅仅有可能解决这样一个复杂的问题本身已经是非平凡的进步。TravelPlanner为未来语言代理提供了一个具有挑战性但有意义的试验平台。 <br /><br />总结: <div>
Planning has been part of the core pursuit for artificial intelligence since
its conception, but earlier AI agents mostly focused on constrained settings
because many of the cognitive substrates necessary for human-level planning
have been lacking. Recently, language agents powered by large language models
(LLMs) have shown interesting capabilities such as tool use and reasoning. Are
these language agents capable of planning in more complex settings that are out
of the reach of prior AI agents? To advance this investigation, we propose
TravelPlanner, a new planning benchmark that focuses on travel planning, a
common real-world planning scenario. It provides a rich sandbox environment,
various tools for accessing nearly four million data records, and 1,225
meticulously curated planning intents and reference plans. Comprehensive
evaluations show that the current language agents are not yet capable of
handling such complex planning tasks-even GPT-4 only achieves a success rate of
0.6%. Language agents struggle to stay on task, use the right tools to collect
information, or keep track of multiple constraints. However, we note that the
mere possibility for language agents to tackle such a complex problem is in
itself non-trivial progress. TravelPlanner provides a challenging yet
meaningful testbed for future language agents.
]]></content:encoded>
<pubDate>2024-02-02T18:39:51Z</pubDate>
</item>
<item>
<title>MAGDi: Structured Distillation of Multi-Agent Interaction Graphs
  Improves Reasoning in Smaller Language Models</title>
<link>http://arxiv.org/abs/2402.01620v1</link>
<guid>http://arxiv.org/abs/2402.01620v1</guid>
<content:encoded><![CDATA[
<div> 多智能体交互; 大型语言模型; 知识蒸馏; 推理能力; 效率提升<br />
<br />
多智能体交互的研究展示了大型语言模型在各种推理任务上的重大改进。然而，这些方法需要多个模型进行长时间的生成，并且成本高昂。此外，这些多智能体方法无法提供一个最终的、用于高效推理的单一模型。为了解决这一问题，文章介绍了一种新方法MAGDi，用于将多个大型语言模型之间的推理交互进行结构化蒸馏，以训练较小的模型。MAGDi通过将多智能体交互表示为图形，利用图编码器增强基础学生模型，并使用三种目标函数进行知识蒸馏：下一个标记的预测、正确和错误推理之间的对比损失，以及基于图的目标函数来模拟交互结构。实验证明，MAGDi提高了较小模型的推理能力，在七个广泛使用的常识和数学推理基准测试上表现优异，超过了几种从单个老师和多个老师进行蒸馏的方法。此外，MAGDi的效率也比其老师高一个数量级。作者进行了广泛的分析，表明MAGDi能够增强对跨领域任务的泛化能力，与基础学生模型的大小和强度呈正比，以及在应用自洽性时（一种依赖于模型多样性的推理技术）能够获得更大的改进（通过我们的多老师训练）。<br /><br />总结: <br />多智能体交互的研究展示了大型语言模型在各种推理任务上的重大改进；MAGDi通过结构化蒸馏提高了较小模型的推理能力；MAGDi在常识和数学推理基准测试上表现出色，且效率明显优于其老师；MAGDi增强了对跨领域任务的泛化能力，并且与基础学生模型的大小和强度呈正比；MAGDi在应用自洽性时能够获得更大的改进。 <div>
Multi-agent interactions between Large Language Model (LLM) agents have shown
major improvements on diverse reasoning tasks. However, these involve long
generations from multiple models across several rounds, making them expensive.
Moreover, these multi-agent approaches fail to provide a final, single model
for efficient inference. To address this, we introduce MAGDi, a new method for
structured distillation of the reasoning interactions between multiple LLMs
into smaller LMs. MAGDi teaches smaller models by representing multi-agent
interactions as graphs, augmenting a base student model with a graph encoder,
and distilling knowledge using three objective functions: next-token
prediction, a contrastive loss between correct and incorrect reasoning, and a
graph-based objective to model the interaction structure. Experiments on seven
widely-used commonsense and math reasoning benchmarks show that MAGDi improves
the reasoning capabilities of smaller models, outperforming several methods
that distill from a single teacher and multiple teachers. Moreover, MAGDi also
demonstrates an order of magnitude higher efficiency over its teachers. We
conduct extensive analyses to show that MAGDi (1) enhances the generalizability
to out-of-domain tasks, (2) scales positively with the size and strength of the
base student model, and (3) obtains larger improvements (via our multi-teacher
training) when applying self-consistency - an inference technique that relies
on model diversity.
]]></content:encoded>
<pubDate>2024-02-02T18:35:14Z</pubDate>
</item>
<item>
<title>Binding Touch to Everything: Learning Unified Multimodal Tactile
  Representations</title>
<link>http://arxiv.org/abs/2401.18084v1</link>
<guid>http://arxiv.org/abs/2401.18084v1</guid>
<content:encoded><![CDATA[
<div> tactile, multimodal learning, UniTouch, vision-based touch sensors, zero-shot setting
<br /><br />
1. 该研究介绍了UniTouch，一个统一的触觉模型，用于连接多种传感器从而实现触觉与视觉、语言和声音等多模态学习。
2. 通过将UniTouch嵌入与预训练的图像嵌入相关联，并提出可学习的传感器特定标记，使模型能够同时学习从一组异构触觉传感器中获取信息。
3. UniTouch能够在零-shot设置下进行各种触觉感知任务，从机器人抓取预测到触觉图像问题回答等。
4. 该模型具有巨大的潜在应用价值，能够解决触觉与其他模态之间的关联问题，对人类和计算系统都具有重大意义。
5. UniTouch是首个展示了这些能力的模型，具有重大的研究意义和应用前景。
<br /><br />总结: <br />本研究介绍了UniTouch，一个能够联合不同传感器的触觉模型，实现触觉与其他模态的学习。该模型能够进行多种触觉感知任务，并具有广泛的应用前景。 <div>
The ability to associate touch with other modalities has huge implications
for humans and computational systems. However, multimodal learning with touch
remains challenging due to the expensive data collection process and
non-standardized sensor outputs. We introduce UniTouch, a unified tactile model
for vision-based touch sensors connected to multiple modalities, including
vision, language, and sound. We achieve this by aligning our UniTouch
embeddings to pretrained image embeddings already associated with a variety of
other modalities. We further propose learnable sensor-specific tokens, allowing
the model to learn from a set of heterogeneous tactile sensors, all at the same
time. UniTouch is capable of conducting various touch sensing tasks in the
zero-shot setting, from robot grasping prediction to touch image question
answering. To the best of our knowledge, UniTouch is the first to demonstrate
such capabilities. Project page: https://cfeng16.github.io/UniTouch/
]]></content:encoded>
<pubDate>2024-01-31T18:59:57Z</pubDate>
</item>
<item>
<title>CARFF: Conditional Auto-encoded Radiance Field for 3D Scene Forecasting</title>
<link>http://arxiv.org/abs/2401.18075v1</link>
<guid>http://arxiv.org/abs/2401.18075v1</guid>
<content:encoded><![CDATA[
<div> 关键词: CARFF, Conditional Auto-encoded Radiance Field, 3D Scene Forecasting, Neural Radiance Field, CARLA driving simulator

CARFF是一种用于预测未来3D场景的方法，可以根据过去的观测（如2D自我中心图像）将图像映射到潜在的3D场景配置分布，并预测假设场景随时间的演变。该方法利用概率编码器将潜在场景表示映射到全局神经辐射场（NeRF），以表示3D场景模型，从而实现可解释的预测和简单的下游应用。此方法通过考虑环境状态和动态的不确定性复杂情景，扩展了以往的神经渲染工作。我们使用姿势条件VAE和NeRF的两阶段训练来学习3D表示。此外，我们利用混合密度网络，自回归地预测潜在场景表示作为一种部分可观察的马尔可夫决策过程。我们在CARLA驾驶模拟器中展示了我们方法的实用性，CARFF可用于实现复杂多智能体自动驾驶场景的高效轨迹和应急规划，包括视觉遮挡。<br /><br />总结: CARFF是一种用于预测未来3D场景的方法，利用概率编码器和全局神经辐射场表示潜在的3D场景模型，扩展了以往的神经渲染工作。该方法通过两阶段训练学习3D表示，并利用混合密度网络自回归地预测潜在场景表示。在CARLA驾驶模拟器中展示了方法的实用性，可用于复杂多智能体自动驾驶场景的轨迹和应急规划。 <div>
We propose CARFF: Conditional Auto-encoded Radiance Field for 3D Scene
Forecasting, a method for predicting future 3D scenes given past observations,
such as 2D ego-centric images. Our method maps an image to a distribution over
plausible 3D latent scene configurations using a probabilistic encoder, and
predicts the evolution of the hypothesized scenes through time. Our latent
scene representation conditions a global Neural Radiance Field (NeRF) to
represent a 3D scene model, which enables explainable predictions and
straightforward downstream applications. This approach extends beyond previous
neural rendering work by considering complex scenarios of uncertainty in
environmental states and dynamics. We employ a two-stage training of
Pose-Conditional-VAE and NeRF to learn 3D representations. Additionally, we
auto-regressively predict latent scene representations as a partially
observable Markov decision process, utilizing a mixture density network. We
demonstrate the utility of our method in realistic scenarios using the CARLA
driving simulator, where CARFF can be used to enable efficient trajectory and
contingency planning in complex multi-agent autonomous driving scenarios
involving visual occlusions.
]]></content:encoded>
<pubDate>2024-01-31T18:56:09Z</pubDate>
</item>
<item>
<title>Weaver: Foundation Models for Creative Writing</title>
<link>http://arxiv.org/abs/2401.17268v1</link>
<guid>http://arxiv.org/abs/2401.17268v1</guid>
<content:encoded><![CDATA[
<div> 关键词: Weaver, 大型语言模型, 写作能力, 领域专用, 预训练<br />
总结: <br />
本文介绍了Weaver，它是专门用于内容创作的大型语言模型家族的首个成员。Weaver经过精心筛选的语料库预训练，专注于提高大型语言模型的写作能力。通过新颖的方法进行微调，使Weaver能够生成更接近人类的文本，并能够遵循更多样化的创作指令。Weaver家族包括不同规模的模型，适用于不同的应用场景，并可以根据查询的复杂性进行动态调度。研究表明，Weaver模型在评估语言模型写作能力的基准测试中表现优异，超过了比它们大数倍的通用语言模型。此外，Weaver还原生支持检索增强生成和函数调用，并展示了这些能力在改进AI辅助写作系统中的各种应用案例。最后，文章讨论并总结了预训练和微调领域特定语言模型的指南和最佳实践。 <div>
This work introduces Weaver, our first family of large language models (LLMs)
dedicated to content creation. Weaver is pre-trained on a carefully selected
corpus that focuses on improving the writing capabilities of large language
models. We then fine-tune Weaver for creative and professional writing purposes
and align it to the preference of professional writers using a suit of novel
methods for instruction data synthesis and LLM alignment, making it able to
produce more human-like texts and follow more diverse instructions for content
creation. The Weaver family consists of models of Weaver Mini (1.8B), Weaver
Base (6B), Weaver Pro (14B), and Weaver Ultra (34B) sizes, suitable for
different applications and can be dynamically dispatched by a routing agent
according to query complexity to balance response quality and computation cost.
Evaluation on a carefully curated benchmark for assessing the writing
capabilities of LLMs shows Weaver models of all sizes outperform generalist
LLMs several times larger than them. Notably, our most-capable Weaver Ultra
model surpasses GPT-4, a state-of-the-art generalist LLM, on various writing
scenarios, demonstrating the advantage of training specialized LLMs for writing
purposes. Moreover, Weaver natively supports retrieval-augmented generation
(RAG) and function calling (tool usage). We present various use cases of these
abilities for improving AI-assisted writing systems, including integration of
external knowledge bases, tools, or APIs, and providing personalized writing
assistance. Furthermore, we discuss and summarize a guideline and best
practices for pre-training and fine-tuning domain-specific LLMs.
]]></content:encoded>
<pubDate>2024-01-30T18:58:43Z</pubDate>
</item>
<item>
<title>ReacLLaMA: Merging chemical and textual information in chemical
  reactivity AI models</title>
<link>http://arxiv.org/abs/2401.17267v1</link>
<guid>http://arxiv.org/abs/2401.17267v1</guid>
<content:encoded><![CDATA[
<div> Graphormer, reactivity model, chemical information, procedural text, accuracy

总结:<br />
本文提出了利用过程文本来增强Graphormer反应性模型并提高其准确性的方法。两种主要方法分别是训练一个适配器Graphormer模型，该模型提供了一个由GPT-2衍生的文本过程的潜在表示（ReacLLaMA-Adapter），以及使用LLaMA 2模型对数据集的未标记部分进行标记，然后在扩展数据集上训练Graphormer模型（Zero-Shot Labeling ReacLLaMA）。这两种方法都增强了对不利反应的识别能力，从而提供了更准确的模型，并改善了特异性。 <div>
Chemical reactivity models are developed to predict chemical reaction
outcomes in the form of classification (success/failure) or regression (product
yield) tasks. The vast majority of the reported models are trained solely on
chemical information such as reactants, products, reagents, and solvents, but
not on the details of a synthetic protocol. Herein incorporation of procedural
text with the aim to augment the Graphormer reactivity model and improve its
accuracy is presented. Two major approaches are used: training an adapter
Graphormer model that is provided with a GPT-2-derived latent representation of
the text procedure (ReacLLaMA-Adapter) and labeling an unlabeled part of a
dataset with the LLaMA 2 model followed by training the Graphormer on an
extended dataset (Zero-Shot Labeling ReacLLaMA). Both methodologies enhance the
discernment of unpromising reactions, thereby providing more accurate models
with improved specificity.
]]></content:encoded>
<pubDate>2024-01-30T18:57:08Z</pubDate>
</item>
<item>
<title>InternLM-XComposer2: Mastering Free-form Text-Image Composition and
  Comprehension in Vision-Language Large Model</title>
<link>http://arxiv.org/abs/2401.16420v1</link>
<guid>http://arxiv.org/abs/2401.16420v1</guid>
<content:encoded><![CDATA[
<div> 关键词: InternLM-XComposer2, PLoRA, 多模态理解, 高质量内容, GitHub链接
总结:
InternLM-XComposer2是一种先进的视觉语言模型，擅长自由形式的文本-图像组合和理解。它采用了Partial LoRA (PLoRA)方法，通过额外的LoRA参数对图像标记进行处理，从而在保持预训练语言知识完整性的同时，实现了精确的视觉理解和具有文学才华的文本组成。在实验结果中，InternLM-XComposer2在高质量长文本多模态内容的生成和跨各种基准测试中表现出了优越性能，不仅明显优于现有的多模态模型，而且在某些评估中甚至能够与甚至超过了GPT-4V和Gemini Pro。这突显了它在多模态理解领域的出色能力。InternLM-XComposer2模型系列的7B参数现已在https://github.com/InternLM/InternLM-XComposer公开提供。 <div>
We introduce InternLM-XComposer2, a cutting-edge vision-language model
excelling in free-form text-image composition and comprehension. This model
goes beyond conventional vision-language understanding, adeptly crafting
interleaved text-image content from diverse inputs like outlines, detailed
textual specifications, and reference images, enabling highly customizable
content creation. InternLM-XComposer2 proposes a Partial LoRA (PLoRA) approach
that applies additional LoRA parameters exclusively to image tokens to preserve
the integrity of pre-trained language knowledge, striking a balance between
precise vision understanding and text composition with literary talent.
Experimental results demonstrate the superiority of InternLM-XComposer2 based
on InternLM2-7B in producing high-quality long-text multi-modal content and its
exceptional vision-language understanding performance across various
benchmarks, where it not only significantly outperforms existing multimodal
models but also matches or even surpasses GPT-4V and Gemini Pro in certain
assessments. This highlights its remarkable proficiency in the realm of
multimodal understanding. The InternLM-XComposer2 model series with 7B
parameters are publicly available at
https://github.com/InternLM/InternLM-XComposer.
]]></content:encoded>
<pubDate>2024-01-29T18:59:02Z</pubDate>
</item>
<item>
<title>Annotated Hands for Generative Models</title>
<link>http://arxiv.org/abs/2401.15075v1</link>
<guid>http://arxiv.org/abs/2401.15075v1</guid>
<content:encoded><![CDATA[
<div> 生成模型, 手图像, 训练框架, 语义标注, 图像质量
<br />
生成模型如GANs和扩散模型在图像生成方面取得了令人瞩目的成就，但在生成手部图像方面却表现不佳。本文提出了一种新颖的训练框架，通过在训练图像中增加三个额外的通道，提供手部的标注信息，从而改善生成模型生成手部图像的能力。通过在合成手部图像数据集和真实照片上进行实验，证明了该方法的有效性。最终通过使用现成的手部检测器，测量生成的手部图像的质量得到了提升。 <br /><br />总结: <br />生成模型在图像生成方面表现突出，但在生成手部图像方面表现不佳；本文提出了一种新的训练框架，通过增加手部标注信息改善生成模型的能力；在合成和真实数据集上验证了方法的有效性；最终通过手部检测器测量了生成手部图像的质量。 <div>
Generative models such as GANs and diffusion models have demonstrated
impressive image generation capabilities. Despite these successes, these
systems are surprisingly poor at creating images with hands. We propose a novel
training framework for generative models that substantially improves the
ability of such systems to create hand images. Our approach is to augment the
training images with three additional channels that provide annotations to
hands in the image. These annotations provide additional structure that coax
the generative model to produce higher quality hand images. We demonstrate this
approach on two different generative models: a generative adversarial network
and a diffusion model. We demonstrate our method both on a new synthetic
dataset of hand images and also on real photographs that contain hands. We
measure the improved quality of the generated hands through higher confidence
in finger joint identification using an off-the-shelf hand detector.
]]></content:encoded>
<pubDate>2024-01-26T18:57:54Z</pubDate>
</item>
<item>
<title>Fully Independent Communication in Multi-Agent Reinforcement Learning</title>
<link>http://arxiv.org/abs/2401.15059v1</link>
<guid>http://arxiv.org/abs/2401.15059v1</guid>
<content:encoded><![CDATA[
<div> 多智能体强化学习, 通信方法, 参数共享, 网络容量, 学习效率
<br />
本文研究了多智能体强化学习中独立学习者如何进行通信，提出了一种新的学习方案作为解决方案。研究结果表明，尽管存在挑战，独立智能体仍然可以根据我们的方法学习通信策略。此外，我们利用这种方法研究了MARL中的通信如何受到不同网络容量的影响，无论是共享参数还是不共享参数。我们观察到通信并不总是必要的，并且在使用通信时需要考虑选择的智能体网络大小，以实现高效的学习。
<br /><br />总结: 
本文研究了在多智能体强化学习中独立学习者如何进行通信，并提出了一种新的学习方案作为解决方案。实验结果表明，独立智能体可以学习通信策略，并且证明了在学习效率方面网络容量的选择对通信的影响。 <div>
Multi-Agent Reinforcement Learning (MARL) comprises a broad area of research
within the field of multi-agent systems. Several recent works have focused
specifically on the study of communication approaches in MARL. While multiple
communication methods have been proposed, these might still be too complex and
not easily transferable to more practical contexts. One of the reasons for that
is due to the use of the famous parameter sharing trick. In this paper, we
investigate how independent learners in MARL that do not share parameters can
communicate. We demonstrate that this setting might incur into some problems,
to which we propose a new learning scheme as a solution. Our results show that,
despite the challenges, independent agents can still learn communication
strategies following our method. Additionally, we use this method to
investigate how communication in MARL is affected by different network
capacities, both for sharing and not sharing parameters. We observe that
communication may not always be needed and that the chosen agent network sizes
need to be considered when used together with communication in order to achieve
efficient learning.
]]></content:encoded>
<pubDate>2024-01-26T18:42:01Z</pubDate>
</item>
<item>
<title>LongFin: A Multimodal Document Understanding Model for Long Financial
  Domain Documents</title>
<link>http://arxiv.org/abs/2401.15050v1</link>
<guid>http://arxiv.org/abs/2401.15050v1</guid>
<content:encoded><![CDATA[
Document AI is a growing research field that focuses on the comprehension and
extraction of information from scanned and digital documents to make everyday
business operations more efficient. Numerous downstream tasks and datasets have
been introduced to facilitate the training of AI models capable of parsing and
extracting information from various document types such as receipts and scanned
forms. Despite these advancements, both existing datasets and models fail to
address critical challenges that arise in industrial contexts. Existing
datasets primarily comprise short documents consisting of a single page, while
existing models are constrained by a limited maximum length, often set at 512
tokens. Consequently, the practical application of these methods in financial
services, where documents can span multiple pages, is severely impeded. To
overcome these challenges, we introduce LongFin, a multimodal document AI model
capable of encoding up to 4K tokens. We also propose the LongForms dataset, a
comprehensive financial dataset that encapsulates several industrial challenges
in financial documents. Through an extensive evaluation, we demonstrate the
effectiveness of the LongFin model on the LongForms dataset, surpassing the
performance of existing public models while maintaining comparable results on
existing single-page benchmarks.
]]></content:encoded>
<pubDate>2024-01-26T18:23:45Z</pubDate>
</item>
<item>
<title>Deconstructing Denoising Diffusion Models for Self-Supervised Learning</title>
<link>http://arxiv.org/abs/2401.14404v1</link>
<guid>http://arxiv.org/abs/2401.14404v1</guid>
<content:encoded><![CDATA[
<div> Denoising Diffusion Models, representation learning, self-supervised learning, deconstructive procedure, classical methods
<br />
本研究探讨了最初用于图像生成的去噪扩散模型（DDM）的表示学习能力。我们的方法是逐步解构DDM，逐渐将其转化为经典的去噪自编码器（DAE）。这种解构性过程使我们能够探索现代DDM的各种组件如何影响自监督表示学习。我们观察到，只有很少现代组件对于学习良好的表示是至关重要的，而许多其他组件则是非必要的。我们的研究最终得出了一个高度简化的方法，在很大程度上类似于经典的DAE。我们希望我们的研究能重新引起人们对现代自监督学习领域内一系列经典方法的兴趣。
<br /><br />总结: Denoising Diffusion Models的表示学习能力得到探讨；研究采用逐步解构DDM的方法，发现现代DDMs的很多组件对良好的自监督表示学习并不是必要的；研究最终得出了一个高度简化的方法，类似于经典的DAE；希望研究能重新引起人们对现代自监督学习领域内经典方法的兴趣。 <div>
In this study, we examine the representation learning abilities of Denoising
Diffusion Models (DDM) that were originally purposed for image generation. Our
philosophy is to deconstruct a DDM, gradually transforming it into a classical
Denoising Autoencoder (DAE). This deconstructive procedure allows us to explore
how various components of modern DDMs influence self-supervised representation
learning. We observe that only a very few modern components are critical for
learning good representations, while many others are nonessential. Our study
ultimately arrives at an approach that is highly simplified and to a large
extent resembles a classical DAE. We hope our study will rekindle interest in a
family of classical methods within the realm of modern self-supervised
learning.
]]></content:encoded>
<pubDate>2024-01-25T18:59:57Z</pubDate>
</item>
<item>
<title>VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web
  Tasks</title>
<link>http://arxiv.org/abs/2401.13649v1</link>
<guid>http://arxiv.org/abs/2401.13649v1</guid>
<content:encoded><![CDATA[
<div> benchmark, autonomous agents, multimodal, VisualWebArena, web

在现有的基准测试基本上主要集中在基于文本的代理上，忽略了许多需要视觉信息来有效解决的自然任务。因此，我们引入了VisualWebArena，这是一个旨在评估多模态网络代理在现实中视觉相关任务表现的基准。它包括一系列多样化和复杂的基于网络的任务，评估自主多模态代理的各种能力。通过广泛的定量和定性分析，我们确定了文本-只有LLM代理的几个局限性，并揭示了最先进的多模态语言代理的能力差距。VisualWebArena为评估多模态自主语言代理提供了一个框架，并为构建更强大的网络自主代理提供了见解。 <div>
Autonomous agents capable of planning, reasoning, and executing actions on
the web offer a promising avenue for automating computer tasks. However, the
majority of existing benchmarks primarily focus on text-based agents,
neglecting many natural tasks that require visual information to effectively
solve. Given that most computer interfaces cater to human perception, visual
information often augments textual data in ways that text-only models struggle
to harness effectively. To bridge this gap, we introduce VisualWebArena, a
benchmark designed to assess the performance of multimodal web agents on
realistic \textit{visually grounded tasks}. VisualWebArena comprises of a set
of diverse and complex web-based tasks that evaluate various capabilities of
autonomous multimodal agents. To perform on this benchmark, agents need to
accurately process image-text inputs, interpret natural language instructions,
and execute actions on websites to accomplish user-defined objectives. We
conduct an extensive evaluation of state-of-the-art LLM-based autonomous
agents, including several multimodal models. Through extensive quantitative and
qualitative analysis, we identify several limitations of text-only LLM agents,
and reveal gaps in the capabilities of state-of-the-art multimodal language
agents. VisualWebArena provides a framework for evaluating multimodal
autonomous language agents, and offers insights towards building stronger
autonomous agents for the web. Our code, baseline models, and data is publicly
available at https://jykoh.com/vwa.
]]></content:encoded>
<pubDate>2024-01-24T18:35:21Z</pubDate>
</item>
<item>
<title>HAZARD Challenge: Embodied Decision Making in Dynamically Changing
  Environments</title>
<link>http://arxiv.org/abs/2401.12975v1</link>
<guid>http://arxiv.org/abs/2401.12975v1</guid>
<content:encoded><![CDATA[
<div> 高保真虚拟环境, 智能体, HAZARD, 动态环境, 大语言模型

最近，高保真虚拟环境的技术进步成为促进建立能够感知、推理和与物理世界互动的智能体的主要推动力之一。然而，在真实世界的情景中，智能体可能面临突发事件导致的动态环境变化，需要迅速做出相应的行动。为了弥补这一差距，研究人员提出了一个新的模拟智能体基准测试，名为HAZARD，专门设计用于评估智能体在动态情况下的决策能力。HAZARD包括火灾、洪水和风三种突发灾害场景，并专门支持利用大语言模型（LLMs）来辅助常识推理和决策制定。这一基准测试使我们能够评估自主智能体在动态变化的环境中通过强化学习（RL）、基于规则的方法和搜索方法等各种管道的决策能力。作为利用大语言模型解决这些具有挑战性任务的一个起步，我们进一步开发了一个基于LLM的智能体，并对其解决这些任务的前景和挑战进行了深入分析。HAZARD的详细信息可在https://vis-www.cs.umass.edu/hazard/找到。<br /><br />总结：最近出现了高保真虚拟环境技术，为了评估智能体在动态环境中的决策能力，研究人员提出了一个名为HAZARD的新的模拟智能体基准测试。该基准测试包括三种突发灾害场景，并特别支持利用大语言模型来辅助决策制定。研究人员还开发了基于LLM的智能体，并分析了其解决这些任务的前景和挑战。 <div>
Recent advances in high-fidelity virtual environments serve as one of the
major driving forces for building intelligent embodied agents to perceive,
reason and interact with the physical world. Typically, these environments
remain unchanged unless agents interact with them. However, in real-world
scenarios, agents might also face dynamically changing environments
characterized by unexpected events and need to rapidly take action accordingly.
To remedy this gap, we propose a new simulated embodied benchmark, called
HAZARD, specifically designed to assess the decision-making abilities of
embodied agents in dynamic situations. HAZARD consists of three unexpected
disaster scenarios, including fire, flood, and wind, and specifically supports
the utilization of large language models (LLMs) to assist common sense
reasoning and decision-making. This benchmark enables us to evaluate autonomous
agents' decision-making capabilities across various pipelines, including
reinforcement learning (RL), rule-based, and search-based methods in
dynamically changing environments. As a first step toward addressing this
challenge using large language models, we further develop an LLM-based agent
and perform an in-depth analysis of its promise and challenge of solving these
challenging tasks. HAZARD is available at https://vis-www.cs.umass.edu/hazard/.
]]></content:encoded>
<pubDate>2024-01-23T18:59:43Z</pubDate>
</item>
<item>
<title>CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation</title>
<link>http://arxiv.org/abs/2401.12208v1</link>
<guid>http://arxiv.org/abs/2401.12208v1</guid>
<content:encoded><![CDATA[
<div> CheXinstruct, CheXagent, vision-language模型, CXR解释, CheXbench
<br />
本文介绍了针对胸部X光片（CXR）解释的自动化模型CheXagent的开发。作者首先介绍了CheXinstruct数据集，然后提出了CheXagent模型，该模型包括临床大型语言模型（LLM）、视觉编码器和用于桥接视觉和语言模态的网络。此外，引入了CheXbench评估框架，用于系统评估FMs在8个临床相关的CXR解释任务上的表现。研究结果表明，CheXagent在CheXbench任务上的表现优于先前开发的通用和医学领域的FMs。此外，作者还进行了模型公平性评估，以突出潜在的性别、种族和年龄方面的性能差异。这项工作的详细信息可以在https://stanford-aimi.github.io/chexagent.html找到。
<br /><br />总结: 本文介绍了针对CXR解释的自动化模型CheXagent的开发，包括数据集、模型架构和评估框架。研究结果显示CheXagent在临床相关任务上表现优异，而且还进行了公平性评估。 <div>
Chest X-rays (CXRs) are the most frequently performed imaging test in
clinical practice. Recent advances in the development of vision-language
foundation models (FMs) give rise to the possibility of performing automated
CXR interpretation, which can assist physicians with clinical decision-making
and improve patient outcomes. However, developing FMs that can accurately
interpret CXRs is challenging due to the (1) limited availability of
large-scale vision-language datasets in the medical image domain, (2) lack of
vision and language encoders that can capture the complexities of medical data,
and (3) absence of evaluation frameworks for benchmarking the abilities of FMs
on CXR interpretation. In this work, we address these challenges by first
introducing \emph{CheXinstruct} - a large-scale instruction-tuning dataset
curated from 28 publicly-available datasets. We then present \emph{CheXagent} -
an instruction-tuned FM capable of analyzing and summarizing CXRs. To build
CheXagent, we design a clinical large language model (LLM) for parsing
radiology reports, a vision encoder for representing CXR images, and a network
to bridge the vision and language modalities. Finally, we introduce
\emph{CheXbench} - a novel benchmark designed to systematically evaluate FMs
across 8 clinically-relevant CXR interpretation tasks. Extensive quantitative
evaluations and qualitative reviews with five expert radiologists demonstrate
that CheXagent outperforms previously-developed general- and medical-domain FMs
on CheXbench tasks. Furthermore, in an effort to improve model transparency, we
perform a fairness evaluation across factors of sex, race and age to highlight
potential performance disparities. Our project is at
\url{https://stanford-aimi.github.io/chexagent.html}.
]]></content:encoded>
<pubDate>2024-01-22T18:51:07Z</pubDate>
</item>
<item>
<title>Retrieval-Guided Reinforcement Learning for Boolean Circuit Minimization</title>
<link>http://arxiv.org/abs/2401.12205v1</link>
<guid>http://arxiv.org/abs/2401.12205v1</guid>
<content:encoded><![CDATA[
<div> logic synthesis, chip design, hardware description languages, ABC-RL, quality-of-result

逻辑综合是芯片设计中的关键阶段，涉及将硬件描述语言（如Verilog）中编码的芯片规格优化为使用布尔逻辑门的高效实现。研究发现，预先训练的代理在面对全新设计时可能会偏离轨迹，对搜索轨迹产生不利影响。为解决这一挑战，提出了ABC-RL，一种根据训练数据集中的相似度分数计算出的精心调整建议的技术。实验结果显示，ABC-RL能够显著提高合成电路的结果质量（QoR），与当前最先进的技术相比，提高了最高达24.8%。此外，ABC-RL在运行时间上也取得了惊人的成就，与当前最先进的方法相比，实现了高达9倍的减少（等质量结果的情况下）。<br /><br />总结: 逻辑综合是芯片设计过程中的关键阶段，需要细致调整合成方法以适应各种硬件设计。ABC-RL技术通过精心调整预训练代理的建议，实现了合成电路质量的显著提高（最高24.8%）。与此同时，ABC-RL还取得了显著的运行时间减少（高达9倍）。 <div>
Logic synthesis, a pivotal stage in chip design, entails optimizing chip
specifications encoded in hardware description languages like Verilog into
highly efficient implementations using Boolean logic gates. The process
involves a sequential application of logic minimization heuristics (``synthesis
recipe"), with their arrangement significantly impacting crucial metrics such
as area and delay. Addressing the challenge posed by the broad spectrum of
design complexities - from variations of past designs (e.g., adders and
multipliers) to entirely novel configurations (e.g., innovative processor
instructions) - requires a nuanced `synthesis recipe` guided by human expertise
and intuition. This study conducts a thorough examination of learning and
search techniques for logic synthesis, unearthing a surprising revelation:
pre-trained agents, when confronted with entirely novel designs, may veer off
course, detrimentally affecting the search trajectory. We present ABC-RL, a
meticulously tuned $\alpha$ parameter that adeptly adjusts recommendations from
pre-trained agents during the search process. Computed based on similarity
scores through nearest neighbor retrieval from the training dataset, ABC-RL
yields superior synthesis recipes tailored for a wide array of hardware
designs. Our findings showcase substantial enhancements in the
Quality-of-result (QoR) of synthesized circuits, boasting improvements of up to
24.8% compared to state-of-the-art techniques. Furthermore, ABC-RL achieves an
impressive up to 9x reduction in runtime (iso-QoR) when compared to current
state-of-the-art methodologies.
]]></content:encoded>
<pubDate>2024-01-22T18:46:30Z</pubDate>
</item>
<item>
<title>Applications of flow models to the generation of correlated lattice QCD
  ensembles</title>
<link>http://arxiv.org/abs/2401.10874v1</link>
<guid>http://arxiv.org/abs/2401.10874v1</guid>
<content:encoded><![CDATA[
<div> 关键词: normalizing flows, lattice quantum field theory, variance reduction, gauge theories, QCD observables
总结: 
本研究利用机器学习的正规化流在晶格量子场论的背景下生成不同作用参数下统计相关的晶格规范场集合。这项工作展示了如何利用这些相关性来减少计算观测量时的方差。通过一种新颖的残余流架构，演示了三种概念验证应用：规范理论的连续极限、QCD观测量的质量依赖性，以及基于费曼-赫尔曼方法的强子矩阵元素。在所有三种情况下，实验证明当机器学习流被纳入时，与使用无关集合或直接重加权进行的相同计算相比，统计不确定性显著减少。 <br /><br /> <div>
Machine-learned normalizing flows can be used in the context of lattice
quantum field theory to generate statistically correlated ensembles of lattice
gauge fields at different action parameters. This work demonstrates how these
correlations can be exploited for variance reduction in the computation of
observables. Three different proof-of-concept applications are demonstrated
using a novel residual flow architecture: continuum limits of gauge theories,
the mass dependence of QCD observables, and hadronic matrix elements based on
the Feynman-Hellmann approach. In all three cases, it is shown that statistical
uncertainties are significantly reduced when machine-learned flows are
incorporated as compared with the same calculations performed with uncorrelated
ensembles or direct reweighting.
]]></content:encoded>
<pubDate>2024-01-19T18:33:52Z</pubDate>
</item>
<item>
<title>Vlogger: Make Your Dream A Vlog</title>
<link>http://arxiv.org/abs/2401.09414v1</link>
<guid>http://arxiv.org/abs/2401.09414v1</guid>
<content:encoded><![CDATA[
<div> Vlogger, AI, video blog, Large Language Model, 生成<br />
Script, Actor, ShowMaker, Voicer, 模型合作生成vlog的四个关键角色<br />

总结:<br />
本文介绍了Vlogger，这是一个用于生成用户描述的分钟级视频博客的通用AI系统。与几秒钟的短视频不同，vlog通常包含一个复杂的故事情节和多样化的场景，这对大多数现有的视频生成方法来说是一个挑战。为了突破这一瓶颈，Vlogger巧妙地利用大型语言模型（LLM）作为导演，并将vlog的长视频生成任务分解为四个关键阶段。在这些阶段中，我们调用各种基础模型来扮演vlog专业人员的关键角色，包括（1）脚本，（2）演员，（3）ShowMaker和（4）Voicer。通过这种模仿人类的设计，我们的Vlogger可以通过自上而下的规划和自下而上的拍摄，以可解释的合作方式生成vlog。此外，我们还引入了一个新颖的视频扩散模型ShowMaker，它在我们的Vlogger中充当了一个摄影师的角色，用于生成每个拍摄场景的视频片段。通过巧妙地结合脚本和演员作为文本和视觉提示，它可以有效地增强片段的时空连贯性。此外，我们为ShowMaker设计了一个简洁的混合训练范式，提升了其T2V生成和预测能力。最后，广泛的实验证明，我们的方法在零样本T2V生成和预测任务上取得了最先进的性能。更重要的是，Vlogger可以从开放世界描述中生成超过5分钟的vlog，而且在脚本和演员上不会丢失视频的连贯性。所有代码和模型都可以在https://github.com/zhuangshaobin/Vlogger上找到。 <div>
In this work, we present Vlogger, a generic AI system for generating a
minute-level video blog (i.e., vlog) of user descriptions. Different from short
videos with a few seconds, vlog often contains a complex storyline with
diversified scenes, which is challenging for most existing video generation
approaches. To break through this bottleneck, our Vlogger smartly leverages
Large Language Model (LLM) as Director and decomposes a long video generation
task of vlog into four key stages, where we invoke various foundation models to
play the critical roles of vlog professionals, including (1) Script, (2) Actor,
(3) ShowMaker, and (4) Voicer. With such a design of mimicking human beings,
our Vlogger can generate vlogs through explainable cooperation of top-down
planning and bottom-up shooting. Moreover, we introduce a novel video diffusion
model, ShowMaker, which serves as a videographer in our Vlogger for generating
the video snippet of each shooting scene. By incorporating Script and Actor
attentively as textual and visual prompts, it can effectively enhance
spatial-temporal coherence in the snippet. Besides, we design a concise mixed
training paradigm for ShowMaker, boosting its capacity for both T2V generation
and prediction. Finally, the extensive experiments show that our method
achieves state-of-the-art performance on zero-shot T2V generation and
prediction tasks. More importantly, Vlogger can generate over 5-minute vlogs
from open-world descriptions, without loss of video coherence on script and
actor. The code and model is all available at
https://github.com/zhuangshaobin/Vlogger.
]]></content:encoded>
<pubDate>2024-01-17T18:55:12Z</pubDate>
</item>
<item>
<title>Multimodal assessment of best possible self as a self-regulatory
  activity for the classroom</title>
<link>http://arxiv.org/abs/2401.08424v1</link>
<guid>http://arxiv.org/abs/2401.08424v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 最佳可能自我，积极心理干预，心身效应，自我调节资源，大学生

最佳可能自我（BPS）是一种被证明可以增强幸福感的积极心理干预，包括描述理想未来情景的写作活动。这篇论文比较了为教室环境改编的BPS活动和与之时间匹配的对照活动（NA）对心理生理效应的影响。三十三名本科生参与了这项研究，评估了三个时间段（之前，期间，之后）的状态焦虑（状态-特质焦虑量表，STAI）、情感（情感滑块，AS）和心脏迷走神经活动（心率变异性，HRV）作为自我调节资源使用的指标。结果显示，与NA相比，BPS导致了积极情绪价值（期间）的显著增加，并且整体上更高水平的心脏迷走神经活动（HRV）。这些发现表明，BPS作为一种自我调节技术具有潜在的特性，旨在培养积极情绪，对自我调节资源产生积极影响。由于BPS不需要专业知识或专门技术来进行管理，教育者在教学和学生实践自我调节时可能会选择这种适当的活动。这项研究呈现了对大学生进行的一个简短BPS活动的自我调节效应的可复制的多模态方法的证据。<br /><br />总结:本研究展示了BPS活动对大学生的自我调节效应的证据，表明BPS在提高积极情感和积极影响自我调节资源方面具有潜力。 BPS活动可以在教学中用于教育者教学和学生实践自我调节。 <div>
Best possible self (BPS) is a positive psychological intervention shown to
enhance well-being which involves writing a description of an ideal future
scenario. This paper presents a comparison of psychophysiological effects of a
BPS activity that has been adapted for classroom settings and a time-matched
control activity (NA). Thirty-three undergraduate students participated in the
study that assessed state anxiety (State-Trait Anxiety Inventory, STAI), affect
(Affective Slider, AS), and cardiac vagal activity (heart-rate variability,
HRV) as an indicator of self-regulatory resource usage, at three time periods
(PRE, DURING, POST). Results show that BPS led to a significantly greater
increase in positive valence (DURING) and overall higher levels of cardiac
vagal activity (HRV) compared to NA. These findings suggest that BPS has
promising characteristics as a self-regulatory technique aimed at fostering
positive affect and positively impacting self-regulatory resources. As BPS does
not require expert knowledge nor specialized technology to administer, it may
be a suitable activity for educators to use when teaching and having students
practice self-regulation. This study presents evidence collected in a
replicable multimodal approach of the self-regulatory effects of a brief BPS
activity on undergraduate students.
]]></content:encoded>
<pubDate>2024-01-16T15:11:12Z</pubDate>
</item>
<item>
<title>AGG: Amortized Generative 3D Gaussians for Single Image to 3D</title>
<link>http://arxiv.org/abs/2401.04099v1</link>
<guid>http://arxiv.org/abs/2401.04099v1</guid>
<content:encoded><![CDATA[
<div> 3D content creation, 3D Gaussian splatting, Amortized Generative 3D Gaussian framework, optimization-based, super-resolution

3D内容创建需求增长，研究了各种3D表示以从单个图像生成3D对象。最近，基于3D高斯光斑的模型在3D重建和生成方面取得了优异的渲染效果。然而，基于3D高斯光斑的方法通常是基于优化的，需要许多计算昂贵的分数蒸馏步骤。为了克服这些挑战，引入了一种摊销生成3D高斯框架（AGG），它可以立即从单个图像生成3D高斯，无需每个实例进行优化。AGG利用中间混合表示，将生成3D高斯位置和其他外观属性进行联合优化。此外，还提出了一个级联管道，首先生成3D数据的粗表示，然后利用3D高斯超分辨率模块进行上采样。我们的方法与现有的基于优化的3D高斯框架和利用其他3D表示的基于采样的管线进行了评估，结果表明AGG在生成能力上具有竞争优势，无论是定性还是定量，同时速度快几个数量级。项目页面：https://ir1d.github.io/AGG/ <br /><br />总结: 3D内容创建需求增长，研究了各种3D表示以从单个图像生成3D对象。基于3D高斯光斑的模型在3D重建和生成方面表现出色，但通常是基于优化的，需要许多计算昂贵的分数蒸馏步骤。为了克服这些挑战，引入了一种摊销生成3D高斯框架（AGG），它可以立即从单个图像生成3D高斯，无需每个实例进行优化。除此之外，提出了一个级联管道，首先生成3D数据的粗表示，然后利用3D高斯超分辨率模块进行上采样。我们的方法在生成能力上具有竞争优势，无论是定性还是定量，同时速度快几个数量级。 <div>
Given the growing need for automatic 3D content creation pipelines, various
3D representations have been studied to generate 3D objects from a single
image. Due to its superior rendering efficiency, 3D Gaussian splatting-based
models have recently excelled in both 3D reconstruction and generation. 3D
Gaussian splatting approaches for image to 3D generation are often
optimization-based, requiring many computationally expensive score-distillation
steps. To overcome these challenges, we introduce an Amortized Generative 3D
Gaussian framework (AGG) that instantly produces 3D Gaussians from a single
image, eliminating the need for per-instance optimization. Utilizing an
intermediate hybrid representation, AGG decomposes the generation of 3D
Gaussian locations and other appearance attributes for joint optimization.
Moreover, we propose a cascaded pipeline that first generates a coarse
representation of the 3D data and later upsamples it with a 3D Gaussian
super-resolution module. Our method is evaluated against existing
optimization-based 3D Gaussian frameworks and sampling-based pipelines
utilizing other 3D representations, where AGG showcases competitive generation
abilities both qualitatively and quantitatively while being several orders of
magnitude faster. Project page: https://ir1d.github.io/AGG/
]]></content:encoded>
<pubDate>2024-01-08T18:56:33Z</pubDate>
</item>
<item>
<title>The Tactician's Web of Large-Scale Formal Knowledge</title>
<link>http://arxiv.org/abs/2401.02950v1</link>
<guid>http://arxiv.org/abs/2401.02950v1</guid>
<content:encoded><![CDATA[
<div> Coq proof assistant, formal mathematical knowledge, machine learning, interconnected web, proof engineering
<br /><br />总结:
Tactician's Web是一个基于Coq证明助手构建的平台，提供了一个庞大而强大的机器检查的数学知识网络，方便机器学习、分析和证明工程。该平台导出一个数据集，其中包含广泛的形式化理论，呈现为定义、定理、证明术语、策略和证明状态的网络。紧密集成的Coq提供了使代理商可用于证明工程师作为实用工具的独特可能性。理论被编码为语义图和人类可读的文本，各自具有独特的优点和缺点。证明代理可以通过相同丰富的数据表示与Coq交互，并且可以自动在一组定理上进行基准测试。 <div>
The Tactician's Web is a platform offering a large web of strongly
interconnected, machine-checked, formal mathematical knowledge conveniently
packaged for machine learning, analytics, and proof engineering. Built on top
of the Coq proof assistant, the platform exports a dataset containing a wide
variety of formal theories, presented as a web of definitions, theorems, proof
terms, tactics, and proof states. Theories are encoded both as a semantic graph
(rendered below) and as human-readable text, each with a unique set of
advantages and disadvantages. Proving agents may interact with Coq through the
same rich data representation and can be automatically benchmarked on a set of
theorems. Tight integration with Coq provides the unique possibility to make
agents available to proof engineers as practical tools.
]]></content:encoded>
<pubDate>2024-01-05T18:52:35Z</pubDate>
</item>
<item>
<title>Graph2Tac: Learning Hierarchical Representations of Math Concepts in
  Theorem proving</title>
<link>http://arxiv.org/abs/2401.02949v1</link>
<guid>http://arxiv.org/abs/2401.02949v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 数学, AI代理, Coq证明助手, 图神经网络, 定义嵌入

总结: <br /><br />这篇文章讨论了数学和其应用中的概念，提到了在AI代理进行新定理证明时，需要实时将新信息融入其知识库，特别是在Coq证明助手中。作者使用了基于图的数据集，构建了图神经网络Graph2Tac(G2T)，能够考虑到导致当前目标的整个定义层次。同时，他们还提出了一项新的定义嵌入任务，用于计算训练中未见的数学概念的表示。这些方法使神经网络的性能能够与最先进的k最近邻预测器相媲美。 <div>
Concepts abound in mathematics and its applications. They vary greatly
between subject areas, and new ones are introduced in each mathematical paper
or application. A formal theory builds a hierarchy of definitions, theorems and
proofs that reference each other. When an AI agent is proving a new theorem,
most of the mathematical concepts and lemmas relevant to that theorem may have
never been seen during training. This is especially true in the Coq proof
assistant, which has a diverse library of Coq projects, each with its own
definitions, lemmas, and even custom tactic procedures used to prove those
lemmas. It is essential for agents to incorporate such new information into
their knowledge base on the fly. We work towards this goal by utilizing a new,
large-scale, graph-based dataset for machine learning in Coq. We leverage a
faithful graph-representation of Coq terms that induces a directed graph of
dependencies between definitions to create a novel graph neural network,
Graph2Tac (G2T), that takes into account not only the current goal, but also
the entire hierarchy of definitions that led to the current goal. G2T is an
online model that is deeply integrated into the users' workflow and can adapt
in real time to new Coq projects and their definitions. It complements well
with other online models that learn in real time from new proof scripts. Our
novel definition embedding task, which is trained to compute representations of
mathematical concepts not seen during training, boosts the performance of the
neural network to rival state-of-the-art k-nearest neighbor predictors.
]]></content:encoded>
<pubDate>2024-01-05T18:52:09Z</pubDate>
</item>
<item>
<title>ODIN: A Single Model for 2D and 3D Perception</title>
<link>http://arxiv.org/abs/2401.02416v1</link>
<guid>http://arxiv.org/abs/2401.02416v1</guid>
<content:encoded><![CDATA[
<div> 3D perception benchmarks, ScanNet, point clouds, transformer architecture, ODIN
<br />
这篇论文介绍了一个名为ODIN的模型，它能够同时处理2D RGB图像和3D点云数据，并使用了一种交替融合2D和3D信息的transformer架构。模型通过区分处理的token的位置编码来区分2D和3D特征操作。ODIN在ScanNet200、Matterport3D和AI2THOR 3D实例分割基准测试中实现了最先进的性能，并在ScanNet、S3DIS和COCO基准测试上实现了有竞争力的表现。当使用实际感知的3D点云代替从3D网格采样的点云时，它比所有先前的工作表现出更高的性能。在可指导的具身代理架构中用作3D感知引擎时，在TEACh对话动作基准测试中创造了新的最先进技术。 <div>
State-of-the-art models on contemporary 3D perception benchmarks like ScanNet
consume and label dataset-provided 3D point clouds, obtained through post
processing of sensed multiview RGB-D images. They are typically trained
in-domain, forego large-scale 2D pre-training and outperform alternatives that
featurize the posed RGB-D multiview images instead. The gap in performance
between methods that consume posed images versus post-processed 3D point clouds
has fueled the belief that 2D and 3D perception require distinct model
architectures. In this paper, we challenge this view and propose ODIN
(Omni-Dimensional INstance segmentation), a model that can segment and label
both 2D RGB images and 3D point clouds, using a transformer architecture that
alternates between 2D within-view and 3D cross-view information fusion. Our
model differentiates 2D and 3D feature operations through the positional
encodings of the tokens involved, which capture pixel coordinates for 2D patch
tokens and 3D coordinates for 3D feature tokens. ODIN achieves state-of-the-art
performance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation
benchmarks, and competitive performance on ScanNet, S3DIS and COCO. It
outperforms all previous works by a wide margin when the sensed 3D point cloud
is used in place of the point cloud sampled from 3D mesh. When used as the 3D
perception engine in an instructable embodied agent architecture, it sets a new
state-of-the-art on the TEACh action-from-dialogue benchmark. Our code and
checkpoints can be found at the project website: https://odin-seg.github.io.
]]></content:encoded>
<pubDate>2024-01-04T18:59:25Z</pubDate>
</item>
<item>
<title>LLaMA Pro: Progressive LLaMA with Block Expansion</title>
<link>http://arxiv.org/abs/2401.02415v1</link>
<guid>http://arxiv.org/abs/2401.02415v1</guid>
<content:encoded><![CDATA[
<div> LLaMA, post-pretraining, Transformer blocks, programming, mathematics
<br />
Large Language Models（LLMs）通过扩展Transformer模块的方法进行新的后预训练，从而在编程和数学领域取得了显著的进展。新模型LLaMA Pro-8.3B在通用任务、编程和数学领域表现出色，并在各种基准测试中取得了先进的性能。该研究为整合自然语言和编程语言提供了宝贵的见解，并为在各种环境中有效运行的先进语言代理的发展奠定了坚实的基础。 
<br /><br />总结: 
<br />LLaMA Pro-8.3B是通过扩展Transformer模块的后预训练方法得到的新模型，在通用任务、编程和数学领域获得了出色的性能。该研究为整合自然语言和编程语言提供了宝贵的见解，并为在各种环境中有效运行的先进语言代理的发展奠定了坚实的基础。 <div>
Humans generally acquire new skills without compromising the old; however,
the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to
CodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with
an expansion of Transformer blocks. We tune the expanded blocks using only new
corpus, efficiently and effectively improving the model's knowledge without
catastrophic forgetting. In this paper, we experiment on the corpus of code and
math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from
LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro
and its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced
performance among various benchmarks, demonstrating superiority over existing
open models in the LLaMA family and the immense potential of reasoning and
addressing diverse tasks as an intelligent agent. Our findings provide valuable
insights into integrating natural and programming languages, laying a solid
foundation for developing advanced language agents that operate effectively in
various environments.
]]></content:encoded>
<pubDate>2024-01-04T18:59:12Z</pubDate>
</item>
<item>
<title>TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview</title>
<link>http://arxiv.org/abs/2401.01330v1</link>
<guid>http://arxiv.org/abs/2401.01330v1</guid>
<content:encoded><![CDATA[
<div> TREC iKAT, Conversational Search Agents, personalized context, decisional search tasks, information operators <br />
<br />
总结: 本文介绍了TREC iKAT对话式检索的研究领域，强调了对话搜索代理的个性化适应能力和决策性搜索任务的重要性。文章描述了任务、主题、数据收集和评估框架，并总结了提交的研究成果。文章强调了不同用户角色和其信息需求的多样性，以及对话式检索的个性化上下文对于提高搜索效率的重要性。 <div>
Conversational Information Seeking stands as a pivotal research area with
significant contributions from previous works. The TREC Interactive Knowledge
Assistance Track (iKAT) builds on the foundational work of the TREC
Conversational Assistance Track (CAsT). However, iKAT distinctively emphasizes
the creation and research of conversational search agents that adapt responses
based on user's prior interactions and present context. The challenge lies in
enabling Conversational Search Agents (CSA) to incorporate this personalized
context to efficiency and effectively guide users through the relevant
information to them. iKAT also emphasizes decisional search tasks, where users
sift through data and information to weigh up options in order to reach a
conclusion or perform an action. These tasks, prevalent in everyday
information-seeking decisions -- be it related to travel, health, or shopping
-- often revolve around a subset of high-level information operators where
queries or questions about the information space include: finding options,
comparing options, identifying the pros and cons of options, etc. Given the
different personas and their information need (expressed through the sequence
of questions), diverse conversation trajectories will arise -- because the
answers to these similar queries will be very different. In this paper, we
report on the first year of TREC iKAT, describing the task, topics, data
collection, and evaluation framework. We further review the submissions and
summarize the findings.
]]></content:encoded>
<pubDate>2024-01-02T18:40:03Z</pubDate>
</item>
<item>
<title>K-PERM: Personalized Response Generation Using Dynamic Knowledge
  Retrieval and Persona-Adaptive Queries</title>
<link>http://arxiv.org/abs/2312.17748v1</link>
<guid>http://arxiv.org/abs/2312.17748v1</guid>
<content:encoded><![CDATA[
<div> 个性化、对话代理、知识、K-PERM、FoCus数据集
<br /><br />
对话代理的个性化可以提高对话质量和用户参与度，但缺乏外部知识以适当地满足用户的个性特点。为了增强个性化响应的相关性和全面性，文章提出了一个两步方法，包括选择性地整合用户个性和用补充信息来情景化回应。作者们开发了K-PERM（知识引导的个性化与奖励调节），这是一个动态对话代理，结合了这些元素。K-PERM在流行的FoCus数据集上取得了最先进的性能，该数据集包含有关全球地标的真实个性化对话。作者还表明，使用K-PERM的回应可以提高最先进的LLMs（例如GPT 3.5）的性能10.5％，突显了K-PERM对个性化聊天机器人的影响。 <div>
Personalizing conversational agents can enhance the quality of conversations
and increase user engagement. However, they often lack external knowledge to
appropriately tend to a user's persona. This is particularly crucial for
practical applications like mental health support, nutrition planning,
culturally sensitive conversations, or reducing toxic behavior in
conversational agents. To enhance the relevance and comprehensiveness of
personalized responses, we propose using a two-step approach that involves (1)
selectively integrating user personas and (2) contextualizing the response with
supplementing information from a background knowledge source. We develop K-PERM
(Knowledge-guided PErsonalization with Reward Modulation), a dynamic
conversational agent that combines these elements. K-PERM achieves
state-of-the-art performance on the popular FoCus dataset, containing
real-world personalized conversations concerning global landmarks. We show that
using responses from K-PERM can improve performance in state-of-the-art LLMs
(GPT 3.5) by 10.5%, highlighting the impact of K-PERM for personalizing
chatbots.
]]></content:encoded>
<pubDate>2023-12-29T18:59:58Z</pubDate>
</item>
<item>
<title>MURP: Multi-Agent Ultra-Wideband Relative Pose Estimation with
  Constrained Communications in 3D Environments</title>
<link>http://arxiv.org/abs/2312.17731v1</link>
<guid>http://arxiv.org/abs/2312.17731v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 多机器人系统, 相对定位, 3D姿态估计, 超宽带测距标签, 误差校正
总结:
本文提出了一种新颖的多机器人系统中的相对定位方法，使用超宽带(UWB)测距标签进行3D姿态估计。相比先前的方法，本文的方法通过使用本地收集到的UWB测距数据以及先验状态约束，并在违反约束时进行检测，可以避免通信网络能力不足和团队规模增加时可能出现的问题。通过利用已学习的平均测距偏差校正，我们实现了19%的定位误差改进，实验结果显示平均绝对位置误差为0.24米，航向误差为9.5度。与其他最新方法相比，本文的方法表现得更好，同时在通信成本显著较高的方法中也具有竞争力。此外，我们还提供了数据集供他人使用。 <br /><br />总结: 本文提出了一种新的多机器人系统的相对定位方法，通过使用UWB测距标签和误差校正来提高定位精度，并在保持竞争力的同时降低了通信成本。 <div>
Inter-agent relative localization is critical for many multi-robot systems
operating in the absence of external positioning infrastructure or prior
environmental knowledge. We propose a novel inter-agent relative 3D pose
estimation system where each participating agent is equipped with several
ultra-wideband (UWB) ranging tags. Prior work typically supplements noisy UWB
range measurements with additional continuously transmitted data, such as
odometry, leading to potential scaling issues with increased team size and/or
decreased communication network capability. By equipping each agent with
multiple UWB antennas, our approach addresses these concerns by using only
locally collected UWB range measurements, a priori state constraints, and
detections of when said constraints are violated. Leveraging our learned mean
ranging bias correction, we gain a 19% positional error improvement giving us
experimental mean absolute position and heading errors of 0.24m and 9.5 degrees
respectively. When compared to other state-of-the-art approaches, our work
demonstrates improved performance over similar systems, while remaining
competitive with methods that have significantly higher communication costs.
Additionally, we make our datasets available.
]]></content:encoded>
<pubDate>2023-12-29T18:40:05Z</pubDate>
</item>
<item>
<title>EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards
  Embodied AI</title>
<link>http://arxiv.org/abs/2312.16170v1</link>
<guid>http://arxiv.org/abs/2312.16170v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 计算机视觉, 机器人, 3D场景理解, 多模态感知数据集, Embodied Perceptron

总结:<br /><br />本文介绍了一种新的多模态、自我中心的3D感知数据集和基准测试，名为EmbodiedScan，以及一个基于此数据集的基线框架Embodied Perceptron。该数据集包含了超过5k个扫描，包括100万个自我中心的RGB-D视图，160k个涵盖760个类别的3D定向框和80个常见类别的密集语义占用。Embodied Perceptron能够处理任意数量的多模态输入，并展示出卓越的3D感知能力，不仅在基本3D感知任务和语言相关任务方面，在实际环境中也能取得显著成果。该研究填补了传统研究在3D场景理解方面的空白，为计算机视觉和机器人领域的进一步研究提供了有益的参考。GitHub链接提供了代码、数据集和基准测试。 <div>
In the realm of computer vision and robotics, embodied agents are expected to
explore their environment and carry out human instructions. This necessitates
the ability to fully understand 3D scenes given their first-person observations
and contextualize them into language for interaction. However, traditional
research focuses more on scene-level input and output setups from a global
view. To address the gap, we introduce EmbodiedScan, a multi-modal, ego-centric
3D perception dataset and benchmark for holistic 3D scene understanding. It
encompasses over 5k scans encapsulating 1M ego-centric RGB-D views, 1M language
prompts, 160k 3D-oriented boxes spanning over 760 categories, some of which
partially align with LVIS, and dense semantic occupancy with 80 common
categories. Building upon this database, we introduce a baseline framework
named Embodied Perceptron. It is capable of processing an arbitrary number of
multi-modal inputs and demonstrates remarkable 3D perception capabilities, both
within the two series of benchmarks we set up, i.e., fundamental 3D perception
tasks and language-grounded tasks, and in the wild. Codes, datasets, and
benchmarks will be available at https://github.com/OpenRobotLab/EmbodiedScan.
]]></content:encoded>
<pubDate>2023-12-26T18:59:11Z</pubDate>
</item>
<item>
<title>From Text to Multimodal: A Comprehensive Survey of Adversarial Example
  Generation in Question Answering Systems</title>
<link>http://arxiv.org/abs/2312.16156v1</link>
<guid>http://arxiv.org/abs/2312.16156v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 对抗性机器学习, 问答系统, 文本, 多模态, 模型漏洞

总结: 
本文综合评述了在问答系统领域中对抗性示例生成技术，包括文本和多模态情景。首先概述了传统问答模型，然后通过对基于规则的扰动和先进的生成模型的探讨，研究了对抗性示例的生成技术。之后，扩展到多模态问答系统，分析了各种方法，并对生成模型、seq2seq架构和混合方法进行了研究。研究覆盖了不同的防御策略、对抗性数据集和评估指标，并展示了对抗性问答方面的全面文献。最后，考虑了对抗性问题生成的未来发展方向，突出了可以推进文本和多模态问答系统在对抗性挑战方面的潜在研究方向。 <div>
Integrating adversarial machine learning with Question Answering (QA) systems
has emerged as a critical area for understanding the vulnerabilities and
robustness of these systems. This article aims to comprehensively review
adversarial example-generation techniques in the QA field, including textual
and multimodal contexts. We examine the techniques employed through systematic
categorization, providing a comprehensive, structured review. Beginning with an
overview of traditional QA models, we traverse the adversarial example
generation by exploring rule-based perturbations and advanced generative
models. We then extend our research to include multimodal QA systems, analyze
them across various methods, and examine generative models, seq2seq
architectures, and hybrid methodologies. Our research grows to different
defense strategies, adversarial datasets, and evaluation metrics and
illustrates the comprehensive literature on adversarial QA. Finally, the paper
considers the future landscape of adversarial question generation, highlighting
potential research directions that can advance textual and multimodal QA
systems in the context of adversarial challenges.
]]></content:encoded>
<pubDate>2023-12-26T18:30:29Z</pubDate>
</item>
</channel>
</rss>