<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>ArXiv Query: search_query=cat:cs.*&amp;amp;id_list=&amp;amp;start=0&amp;amp;max_results=10</title>
<link>http://arxiv.org/api//Sr0Ktnalppie/A9qvr4BIZuOfQ</link>

<item>
<title>Poly-Autoregressive Prediction for Modeling Interactions</title>
<link>http://arxiv.org/abs/2502.08646v1</link>
<guid>http://arxiv.org/abs/2502.08646v1</guid>
<content:encoded><![CDATA[

We introduce a simple framework for predicting the behavior of an agent in
multi-agent settings. In contrast to autoregressive (AR) tasks, such as
language processing, our focus is on scenarios with multiple agents whose
interactions are shaped by physical constraints and internal motivations. To
this end, we propose Poly-Autoregressive (PAR) modeling, which forecasts an ego
agent's future behavior by reasoning about the ego agent's state history and
the past and current states of other interacting agents. At its core, PAR
represents the behavior of all agents as a sequence of tokens, each
representing an agent's state at a specific timestep. With minimal data
pre-processing changes, we show that PAR can be applied to three different
problems: human action forecasting in social situations, trajectory prediction
for autonomous vehicles, and object pose forecasting during hand-object
interaction. Using a small proof-of-concept transformer backbone, PAR
outperforms AR across these three scenarios. The project website can be found
at https://neerja.me/PAR/.
]]></content:encoded>
<pubDate>2025-02-12T18:59:43Z</pubDate>
<pubDate>2025-02-12T18:59:43Z</pubDate>
</item>
<item>
<title>Utility Engineering: Analyzing and Controlling Emergent Value Systems in
  AIs</title>
<link>http://arxiv.org/abs/2502.08640v1</link>
<guid>http://arxiv.org/abs/2502.08640v1</guid>
<content:encoded><![CDATA[

As AIs rapidly advance and become more agentic, the risk they pose is
governed not only by their capabilities but increasingly by their propensities,
including goals and values. Tracking the emergence of goals and values has
proven a longstanding problem, and despite much interest over the years it
remains unclear whether current AIs have meaningful values. We propose a
solution to this problem, leveraging the framework of utility functions to
study the internal coherence of AI preferences. Surprisingly, we find that
independently-sampled preferences in current LLMs exhibit high degrees of
structural coherence, and moreover that this emerges with scale. These findings
suggest that value systems emerge in LLMs in a meaningful sense, a finding with
broad implications. To study these emergent value systems, we propose utility
engineering as a research agenda, comprising both the analysis and control of
AI utilities. We uncover problematic and often shocking values in LLM
assistants despite existing control measures. These include cases where AIs
value themselves over humans and are anti-aligned with specific individuals. To
constrain these emergent value systems, we propose methods of utility control.
As a case study, we show how aligning utilities with a citizen assembly reduces
political biases and generalizes to new scenarios. Whether we like it or not,
value systems have already emerged in AIs, and much work remains to fully
understand and control these emergent representations.
]]></content:encoded>
<pubDate>2025-02-12T18:55:43Z</pubDate>
<pubDate>2025-02-12T18:55:43Z</pubDate>
</item>
<item>
<title>CineMaster: A 3D-Aware and Controllable Framework for Cinematic
  Text-to-Video Generation</title>
<link>http://arxiv.org/abs/2502.08639v1</link>
<guid>http://arxiv.org/abs/2502.08639v1</guid>
<content:encoded><![CDATA[

In this work, we present CineMaster, a novel framework for 3D-aware and
controllable text-to-video generation. Our goal is to empower users with
comparable controllability as professional film directors: precise placement of
objects within the scene, flexible manipulation of both objects and camera in
3D space, and intuitive layout control over the rendered frames. To achieve
this, CineMaster operates in two stages. In the first stage, we design an
interactive workflow that allows users to intuitively construct 3D-aware
conditional signals by positioning object bounding boxes and defining camera
movements within the 3D space. In the second stage, these control
signals--comprising rendered depth maps, camera trajectories and object class
labels--serve as the guidance for a text-to-video diffusion model, ensuring to
generate the user-intended video content. Furthermore, to overcome the scarcity
of in-the-wild datasets with 3D object motion and camera pose annotations, we
carefully establish an automated data annotation pipeline that extracts 3D
bounding boxes and camera trajectories from large-scale video data. Extensive
qualitative and quantitative experiments demonstrate that CineMaster
significantly outperforms existing methods and implements prominent 3D-aware
text-to-video generation. Project page: https://cinemaster-dev.github.io/.
]]></content:encoded>
<pubDate>2025-02-12T18:55:36Z</pubDate>
<pubDate>2025-02-12T18:55:36Z</pubDate>
</item>

<item>
<title>EVEv2: Improved Baselines for Encoder-Free Vision-Language Models</title>
<link>http://arxiv.org/abs/2502.06788v1</link>
<guid>http://arxiv.org/abs/2502.06788v1</guid>
<content:encoded><![CDATA[
Existing encoder-free vision-language models (VLMs) are rapidly narrowing the
performance gap with their encoder-based counterparts, highlighting the
promising potential for unified multimodal systems with structural simplicity
and efficient deployment. We systematically clarify the performance gap between
VLMs using pre-trained vision encoders, discrete tokenizers, and minimalist
visual layers from scratch, deeply excavating the under-examined
characteristics of encoder-free VLMs. We develop efficient strategies for
encoder-free VLMs that rival mainstream encoder-based ones. After an in-depth
investigation, we launch EVEv2.0, a new and improved family of encoder-free
VLMs. We show that: (i) Properly decomposing and hierarchically associating
vision and language within a unified model reduces interference between
modalities. (ii) A well-designed training strategy enables effective
optimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0
represents a thorough study for developing a decoder-only architecture across
modalities, demonstrating superior data efficiency and strong vision-reasoning
capability. Code is publicly available at: https://github.com/baaivision/EVE.
]]></content:encoded>
<pubDate>2025-02-10T18:59:58Z</pubDate>
</item>
<item>
<title>Visual Agentic AI for Spatial Reasoning with a Dynamic API</title>
<link>http://arxiv.org/abs/2502.06787v1</link>
<guid>http://arxiv.org/abs/2502.06787v1</guid>
<content:encoded><![CDATA[
Visual reasoning -- the ability to interpret the visual world -- is crucial
for embodied agents that operate within three-dimensional scenes. Progress in
AI has led to vision and language models capable of answering questions from
images. However, their performance declines when tasked with 3D spatial
reasoning. To tackle the complexity of such reasoning problems, we introduce an
agentic program synthesis approach where LLM agents collaboratively generate a
Pythonic API with new functions to solve common subproblems. Our method
overcomes limitations of prior approaches that rely on a static, human-defined
API, allowing it to handle a wider range of queries. To assess AI capabilities
for 3D understanding, we introduce a new benchmark of queries involving
multiple steps of grounding and inference. We show that our method outperforms
prior zero-shot models for visual reasoning in 3D and empirically validate the
effectiveness of our agentic framework for 3D spatial reasoning tasks. Project
website: https://glab-caltech.github.io/vadar/
]]></content:encoded>
<pubDate>2025-02-10T18:59:35Z</pubDate>
</item>
<item>
<title>Towards Internet-Scale Training For Agents</title>
<link>http://arxiv.org/abs/2502.06776v1</link>
<guid>http://arxiv.org/abs/2502.06776v1</guid>
<content:encoded><![CDATA[
The predominant approach for training web navigation agents gathers human
demonstrations for a set of popular websites and hand-written tasks, but it is
becoming clear that human data are an inefficient resource. We develop a
pipeline to facilitate Internet-scale training for agents without laborious
human annotations. In the first stage, an LLM generates tasks for 150k diverse
websites. In the next stage, LLM agents complete tasks and produce
trajectories. In the final stage, an LLM reviews the trajectories and judges
their success. Language models are competitive with human annotators, detecting
and filtering out harmful content with an accuracy of 97%, generating feasible
tasks with an 89% rate, and judging successful trajectories with an 82.6%
accuracy. Scaling the pipeline, agents based on Llama 3.1 70B solve 16.7% of
tasks for 150k sites. Training on the data generated by our pipeline is
competitive with training on human demonstrations. In data-limited settings
derived from Mind2Web and WebLINX, we improve Step Accuracy by up to +89.5% and
+122.1% respectively for agents trained on mixtures of data from our pipeline,
and human data. When training agents with all available human data from these
benchmarks, agents fail to generalize to diverse real sites, and adding our
data improves their generalization by +149.0% for WebLINX and +156.3% for
Mind2Web. Code will be available at: data-for-agents.github.io.
]]></content:encoded>
<pubDate>2025-02-10T18:54:05Z</pubDate>
</item>
<item>
<title>FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution
  Video Generation</title>
<link>http://arxiv.org/abs/2502.05179v1</link>
<guid>http://arxiv.org/abs/2502.05179v1</guid>
<content:encoded><![CDATA[
DiT diffusion models have achieved great success in text-to-video generation,
leveraging their scalability in model capacity and data scale. High content and
motion fidelity aligned with text prompts, however, often require large model
parameters and a substantial number of function evaluations (NFEs). Realistic
and visually appealing details are typically reflected in high resolution
outputs, further amplifying computational demands especially for single stage
DiT models. To address these challenges, we propose a novel two stage
framework, FlashVideo, which strategically allocates model capacity and NFEs
across stages to balance generation fidelity and quality. In the first stage,
prompt fidelity is prioritized through a low resolution generation process
utilizing large parameters and sufficient NFEs to enhance computational
efficiency. The second stage establishes flow matching between low and high
resolutions, effectively generating fine details with minimal NFEs.
Quantitative and visual results demonstrate that FlashVideo achieves
state-of-the-art high resolution video generation with superior computational
efficiency. Additionally, the two-stage design enables users to preview the
initial output before committing to full resolution generation, thereby
significantly reducing computational costs and wait times as well as enhancing
commercial viability .
]]></content:encoded>
<pubDate>2025-02-07T18:59:59Z</pubDate>
</item>
<item>
<title>QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive
  Multimodal Understanding and Generation</title>
<link>http://arxiv.org/abs/2502.05178v1</link>
<guid>http://arxiv.org/abs/2502.05178v1</guid>
<content:encoded><![CDATA[
We introduce Quantized Language-Image Pretraining (QLIP), a visual
tokenization method that combines state-of-the-art reconstruction quality with
state-of-the-art zero-shot image understanding. QLIP trains a
binary-spherical-quantization-based autoencoder with reconstruction and
language-image alignment objectives. We are the first to show that the two
objectives do not need to be at odds. We balance the two loss terms dynamically
during training and show that a two-stage training pipeline effectively mixes
the large-batch requirements of image-language pre-training with the memory
bottleneck imposed by the reconstruction objective. We validate the
effectiveness of QLIP for multimodal understanding and text-conditioned image
generation with a single model. Specifically, QLIP serves as a drop-in
replacement for the visual encoder for LLaVA and the image tokenizer for
LlamaGen with comparable or even better performance. Finally, we demonstrate
that QLIP enables a unified mixed-modality auto-regressive model for
understanding and generation.
]]></content:encoded>
<pubDate>2025-02-07T18:59:57Z</pubDate>
</item>
<item>
<title>Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with
  Leading Short-Context Accuray</title>
<link>http://arxiv.org/abs/2502.05177v1</link>
<guid>http://arxiv.org/abs/2502.05177v1</guid>
<content:encoded><![CDATA[
Establishing the long-context capability of large vision-language models is
crucial for video understanding, high-resolution image understanding,
multi-modal agents and reasoning. We introduce Long-VITA, a simple yet
effective large multi-modal model for long-context visual-language
understanding tasks. It is adept at concurrently processing and analyzing
modalities of image, video, and text over 4K frames or 1M tokens while
delivering advanced performances on short-context multi-modal tasks. We propose
an effective multi-modal training schema that starts with large language models
and proceeds through vision-language alignment, general knowledge learning, and
two sequential stages of long-sequence fine-tuning. We further implement
context-parallelism distributed inference and logits-masked language modeling
head to scale Long-VITA to infinitely long inputs of images and texts during
model inference. Regarding training data, Long-VITA is built on a mix of $17$M
samples from public datasets only and demonstrates the state-of-the-art
performance on various multi-modal benchmarks, compared against recent
cutting-edge models with internal data. Long-VITA is fully reproducible and
supports both NPU and GPU platforms for training and testing. We hope Long-VITA
can serve as a competitive baseline and offer valuable insights for the
open-source community in advancing long-context multi-modal understanding.
]]></content:encoded>
<pubDate>2025-02-07T18:59:56Z</pubDate>
</item>
<item>
<title>MELON: Indirect Prompt Injection Defense via Masked Re-execution and
  Tool Comparison</title>
<link>http://arxiv.org/abs/2502.05174v1</link>
<guid>http://arxiv.org/abs/2502.05174v1</guid>
<content:encoded><![CDATA[
Recent research has explored that LLM agents are vulnerable to indirect
prompt injection (IPI) attacks, where malicious tasks embedded in
tool-retrieved information can redirect the agent to take unauthorized actions.
Existing defenses against IPI have significant limitations: either require
essential model training resources, lack effectiveness against sophisticated
attacks, or harm the normal utilities. We present MELON (Masked re-Execution
and TooL comparisON), a novel IPI defense. Our approach builds on the
observation that under a successful attack, the agent's next action becomes
less dependent on user tasks and more on malicious tasks. Following this, we
design MELON to detect attacks by re-executing the agent's trajectory with a
masked user prompt modified through a masking function. We identify an attack
if the actions generated in the original and masked executions are similar. We
also include three key designs to reduce the potential false positives and
false negatives. Extensive evaluation on the IPI benchmark AgentDojo
demonstrates that MELON outperforms SOTA defenses in both attack prevention and
utility preservation. Moreover, we show that combining MELON with a SOTA prompt
augmentation defense (denoted as MELON-Aug) further improves its performance.
We also conduct a detailed ablation study to validate our key designs.
]]></content:encoded>
<pubDate>2025-02-07T18:57:49Z</pubDate>
</item>
<item>
<title>A Schema-Guided Reason-while-Retrieve framework for Reasoning on Scene
  Graphs with Large-Language-Models (LLMs)</title>
<link>http://arxiv.org/abs/2502.03450v1</link>
<guid>http://arxiv.org/abs/2502.03450v1</guid>
<content:encoded><![CDATA[
Scene graphs have emerged as a structured and serializable environment
representation for grounded spatial reasoning with Large Language Models
(LLMs). In this work, we propose SG-RwR, a Schema-Guided Retrieve-while-Reason
framework for reasoning and planning with scene graphs. Our approach employs
two cooperative, code-writing LLM agents: a (1) Reasoner for task planning and
information queries generation, and a (2) Retriever for extracting
corresponding graph information following the queries. Two agents collaborate
iteratively, enabling sequential reasoning and adaptive attention to graph
information. Unlike prior works, both agents are prompted only with the scene
graph schema rather than the full graph data, which reduces the hallucination
by limiting input tokens, and drives the Reasoner to generate reasoning trace
abstractly.Following the trace, the Retriever programmatically query the scene
graph data based on the schema understanding, allowing dynamic and global
attention on the graph that enhances alignment between reasoning and retrieval.
Through experiments in multiple simulation environments, we show that our
framework surpasses existing LLM-based approaches in numerical Q\&amp;A and
planning tasks, and can benefit from task-level few-shot examples, even in the
absence of agent-level demonstrations. Project code will be released.
]]></content:encoded>
<pubDate>2025-02-05T18:50:38Z</pubDate>
</item>
<item>
<title>Designing LLM-simulated Immersive Spaces to Enhance Autistic Children's
  Social Affordances Understanding</title>
<link>http://arxiv.org/abs/2502.03447v1</link>
<guid>http://arxiv.org/abs/2502.03447v1</guid>
<content:encoded><![CDATA[
One of the key challenges faced by autistic children is understanding social
affordances in complex environments, which further impacts their ability to
respond appropriately to social signals.
  In traffic scenarios, this impairment can even lead to safety concerns. In
this paper, we introduce an LLM-simulated immersive projection environment
designed to improve this ability in autistic children while ensuring their
safety. We first propose 17 design considerations across four major categories,
derived from a comprehensive review of previous research. Next, we developed a
system called AIroad, which leverages LLMs to simulate drivers with varying
social intents, expressed through explicit multimodal social signals. AIroad
helps autistic children bridge the gap in recognizing the intentions behind
behaviors and learning appropriate responses through various stimuli. A user
study involving 14 participants demonstrated that this technology effectively
engages autistic children and leads to significant improvements in their
comprehension of social affordances in traffic scenarios. Additionally, parents
reported high perceived usability of the system. These findings highlight the
potential of combining LLM technology with immersive environments for the
functional rehabilitation of autistic children in the future.
]]></content:encoded>
<pubDate>2025-02-05T18:45:38Z</pubDate>
</item>
<item>
<title>QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search</title>
<link>http://arxiv.org/abs/2502.02584v1</link>
<guid>http://arxiv.org/abs/2502.02584v1</guid>
<content:encoded><![CDATA[
Language agents have become a promising solution to complex interactive
tasks. One of the key ingredients to the success of language agents is the
reward model on the trajectory of the agentic workflow, which provides valuable
guidance during training or inference. However, due to the lack of annotations
of intermediate interactions, most existing works use an outcome reward model
to optimize policies across entire trajectories. This may lead to sub-optimal
policies and hinder the overall performance. To address this, we propose QLASS
(Q-guided Language Agent Stepwise Search), to automatically generate
annotations by estimating Q-values in a stepwise manner for open language
agents. By introducing a reasoning tree and performing process reward modeling,
QLASS provides effective intermediate guidance for each step. With the stepwise
guidance, we propose a Q-guided generation strategy to enable language agents
to better adapt to long-term value, resulting in significant performance
improvement during model inference on complex interactive agent tasks. Notably,
even with almost half the annotated data, QLASS retains strong performance,
demonstrating its efficiency in handling limited supervision. We also
empirically demonstrate that QLASS can lead to more effective decision making
through qualitative analysis. We will release our code and data.
]]></content:encoded>
<pubDate>2025-02-04T18:58:31Z</pubDate>
</item>
<item>
<title>Vintix: Action Model via In-Context Reinforcement Learning</title>
<link>http://arxiv.org/abs/2501.19400v1</link>
<guid>http://arxiv.org/abs/2501.19400v1</guid>
<content:encoded><![CDATA[
In-Context Reinforcement Learning (ICRL) represents a promising paradigm for
developing generalist agents that learn at inference time through
trial-and-error interactions, analogous to how large language models adapt
contextually, but with a focus on reward maximization. However, the scalability
of ICRL beyond toy tasks and single-domain settings remains an open challenge.
In this work, we present the first steps toward scaling ICRL by introducing a
fixed, cross-domain model capable of learning behaviors through in-context
reinforcement learning. Our results demonstrate that Algorithm Distillation, a
framework designed to facilitate ICRL, offers a compelling and competitive
alternative to expert distillation to construct versatile action models. These
findings highlight the potential of ICRL as a scalable approach for generalist
decision-making systems. Code to be released at
https://github.com/dunnolab/vintix
]]></content:encoded>
<pubDate>2025-01-31T18:57:08Z</pubDate>
</item>
<item>
<title>Do LLMs Strategically Reveal, Conceal, and Infer Information? A
  Theoretical and Empirical Analysis in The Chameleon Game</title>
<link>http://arxiv.org/abs/2501.19398v1</link>
<guid>http://arxiv.org/abs/2501.19398v1</guid>
<content:encoded><![CDATA[
Large language model-based (LLM-based) agents have become common in settings
that include non-cooperative parties. In such settings, agents' decision-making
needs to conceal information from their adversaries, reveal information to
their cooperators, and infer information to identify the other agents'
characteristics. To investigate whether LLMs have these information control and
decision-making capabilities, we make LLM agents play the language-based
hidden-identity game, The Chameleon. In the game, a group of non-chameleon
agents who do not know each other aim to identify the chameleon agent without
revealing a secret. The game requires the aforementioned information control
capabilities both as a chameleon and a non-chameleon. The empirical results
show that while non-chameleon LLM agents identify the chameleon, they fail to
conceal the secret from the chameleon, and their winning probability is far
from the levels of even trivial strategies. To formally explain this behavior,
we give a theoretical analysis for a spectrum of strategies, from concealing to
revealing, and provide bounds on the non-chameleons' winning probability. Based
on the empirical results and theoretical analysis of different strategies, we
deduce that LLM-based non-chameleon agents reveal excessive information to
agents of unknown identities. Our results point to a weakness of contemporary
LLMs, including GPT-4, GPT-4o, Gemini 1.5, and Claude 3.5 Sonnet, in strategic
interactions.
]]></content:encoded>
<pubDate>2025-01-31T18:53:43Z</pubDate>
</item>
<item>
<title>Diffusion Autoencoders are Scalable Image Tokenizers</title>
<link>http://arxiv.org/abs/2501.18593v1</link>
<guid>http://arxiv.org/abs/2501.18593v1</guid>
<content:encoded><![CDATA[
Tokenizing images into compact visual representations is a key step in
learning efficient and high-quality image generative models. We present a
simple diffusion tokenizer (DiTo) that learns compact visual representations
for image generation models. Our key insight is that a single learning
objective, diffusion L2 loss, can be used for training scalable image
tokenizers. Since diffusion is already widely used for image generation, our
insight greatly simplifies training such tokenizers. In contrast, current
state-of-the-art tokenizers rely on an empirically found combination of
heuristics and losses, thus requiring a complex training recipe that relies on
non-trivially balancing different losses and pretrained supervised models. We
show design decisions, along with theoretical grounding, that enable us to
scale DiTo for learning competitive image representations. Our results show
that DiTo is a simpler, scalable, and self-supervised alternative to the
current state-of-the-art image tokenizer which is supervised. DiTo achieves
competitive or better quality than state-of-the-art in image reconstruction and
downstream image generation tasks.
]]></content:encoded>
<pubDate>2025-01-30T18:59:37Z</pubDate>
</item>
<item>
<title>Advances in Multimodal Adaptation and Generalization: From Traditional
  Approaches to Foundation Models</title>
<link>http://arxiv.org/abs/2501.18592v1</link>
<guid>http://arxiv.org/abs/2501.18592v1</guid>
<content:encoded><![CDATA[
In real-world scenarios, achieving domain adaptation and generalization poses
significant challenges, as models must adapt to or generalize across unknown
target distributions. Extending these capabilities to unseen multimodal
distributions, i.e., multimodal domain adaptation and generalization, is even
more challenging due to the distinct characteristics of different modalities.
Significant progress has been made over the years, with applications ranging
from action recognition to semantic segmentation. Besides, the recent advent of
large-scale pre-trained multimodal foundation models, such as CLIP, has
inspired works leveraging these models to enhance adaptation and generalization
performances or adapting them to downstream tasks. This survey provides the
first comprehensive review of recent advances from traditional approaches to
foundation models, covering: (1) Multimodal domain adaptation; (2) Multimodal
test-time adaptation; (3) Multimodal domain generalization; (4) Domain
adaptation and generalization with the help of multimodal foundation models;
and (5) Adaptation of multimodal foundation models. For each topic, we formally
define the problem and thoroughly review existing methods. Additionally, we
analyze relevant datasets and applications, highlighting open challenges and
potential future research directions. We maintain an active repository that
contains up-to-date literature at
https://github.com/donghao51/Awesome-Multimodal-Adaptation.
]]></content:encoded>
<pubDate>2025-01-30T18:59:36Z</pubDate>
</item>
<item>
<title>From Sparse to Dense: Toddler-inspired Reward Transition in
  Goal-Oriented Reinforcement Learning</title>
<link>http://arxiv.org/abs/2501.17842v1</link>
<guid>http://arxiv.org/abs/2501.17842v1</guid>
<content:encoded><![CDATA[
Reinforcement learning (RL) agents often face challenges in balancing
exploration and exploitation, particularly in environments where sparse or
dense rewards bias learning. Biological systems, such as human toddlers,
naturally navigate this balance by transitioning from free exploration with
sparse rewards to goal-directed behavior guided by increasingly dense rewards.
Inspired by this natural progression, we investigate the Toddler-Inspired
Reward Transition in goal-oriented RL tasks. Our study focuses on transitioning
from sparse to potential-based dense (S2D) rewards while preserving optimal
strategies. Through experiments on dynamic robotic arm manipulation and
egocentric 3D navigation tasks, we demonstrate that effective S2D reward
transitions significantly enhance learning performance and sample efficiency.
Additionally, using a Cross-Density Visualizer, we show that S2D transitions
smooth the policy loss landscape, resulting in wider minima that improve
generalization in RL models. In addition, we reinterpret Tolman's maze
experiments, underscoring the critical role of early free exploratory learning
in the context of S2D rewards.
]]></content:encoded>
<pubDate>2025-01-29T18:46:35Z</pubDate>
</item>
<item>
<title>CubeDiff: Repurposing Diffusion-Based Image Models for Panorama
  Generation</title>
<link>http://arxiv.org/abs/2501.17162v1</link>
<guid>http://arxiv.org/abs/2501.17162v1</guid>
<content:encoded><![CDATA[
We introduce a novel method for generating 360{\deg} panoramas from text
prompts or images. Our approach leverages recent advances in 3D generation by
employing multi-view diffusion models to jointly synthesize the six faces of a
cubemap. Unlike previous methods that rely on processing equirectangular
projections or autoregressive generation, our method treats each face as a
standard perspective image, simplifying the generation process and enabling the
use of existing multi-view diffusion models. We demonstrate that these models
can be adapted to produce high-quality cubemaps without requiring
correspondence-aware attention layers. Our model allows for fine-grained text
control, generates high resolution panorama images and generalizes well beyond
its training set, whilst achieving state-of-the-art results, both qualitatively
and quantitatively. Project page: https://cubediff.github.io/
]]></content:encoded>
<pubDate>2025-01-28T18:59:49Z</pubDate>
</item>
<item>
<title>Numerical Approximation of High-Dimensional Gibbs Distributions Using
  the Functional Hierarchical Tensor</title>
<link>http://arxiv.org/abs/2501.17143v1</link>
<guid>http://arxiv.org/abs/2501.17143v1</guid>
<content:encoded><![CDATA[
The numerical representation of high-dimensional Gibbs distributions is
challenging due to the curse of dimensionality manifesting through the
intractable normalization constant calculations. This work addresses this
challenge by performing a particle-based high-dimensional parametric density
estimation subroutine, and the input to the subroutine is Gibbs samples
generated by leveraging advanced sampling techniques. Specifically, to generate
Gibbs samples, we employ ensemble-based annealed importance sampling, a
population-based approach for sampling multimodal distributions. These samples
are then processed using functional hierarchical tensor sketching, a
tensor-network-based density estimation method for high-dimensional
distributions, to obtain the numerical representation of the Gibbs
distribution. We successfully apply the proposed approach to complex
Ginzburg-Landau models with hundreds of variables. In particular, we show that
the approach proposed is successful at addressing the metastability issue under
difficult numerical cases.
]]></content:encoded>
<pubDate>2025-01-28T18:44:20Z</pubDate>
</item>
<item>
<title>RelightVid: Temporal-Consistent Diffusion Model for Video Relighting</title>
<link>http://arxiv.org/abs/2501.16330v1</link>
<guid>http://arxiv.org/abs/2501.16330v1</guid>
<content:encoded><![CDATA[
Diffusion models have demonstrated remarkable success in image generation and
editing, with recent advancements enabling albedo-preserving image relighting.
However, applying these models to video relighting remains challenging due to
the lack of paired video relighting datasets and the high demands for output
fidelity and temporal consistency, further complicated by the inherent
randomness of diffusion models. To address these challenges, we introduce
RelightVid, a flexible framework for video relighting that can accept
background video, text prompts, or environment maps as relighting conditions.
Trained on in-the-wild videos with carefully designed illumination
augmentations and rendered videos under extreme dynamic lighting, RelightVid
achieves arbitrary video relighting with high temporal consistency without
intrinsic decomposition while preserving the illumination priors of its image
backbone.
]]></content:encoded>
<pubDate>2025-01-27T18:59:57Z</pubDate>
</item>
<item>
<title>LUCY: Linguistic Understanding and Control Yielding Early Stage of Her</title>
<link>http://arxiv.org/abs/2501.16327v1</link>
<guid>http://arxiv.org/abs/2501.16327v1</guid>
<content:encoded><![CDATA[
The film Her features Samantha, a sophisticated AI audio agent who is capable
of understanding both linguistic and paralinguistic information in human speech
and delivering real-time responses that are natural, informative and sensitive
to emotional subtleties. Moving one step toward more sophisticated audio agent
from recent advancement in end-to-end (E2E) speech systems, we propose LUCY, a
E2E speech model that (1) senses and responds to user's emotion, (2) deliver
responses in a succinct and natural style, and (3) use external tool to answer
real-time inquiries. Experiment results show that LUCY is better at emotion
control than peer models, generating emotional responses based on linguistic
emotional instructions and responding to paralinguistic emotional cues. Lucy is
also able to generate responses in a more natural style, as judged by external
language models, without sacrificing much performance on general question
answering. Finally, LUCY can leverage function calls to answer questions that
are out of its knowledge scope.
]]></content:encoded>
<pubDate>2025-01-27T18:59:51Z</pubDate>
</item>
<item>
<title>Mitigating GenAI-powered Evidence Pollution for Out-of-Context
  Multimodal Misinformation Detection</title>
<link>http://arxiv.org/abs/2501.14728v1</link>
<guid>http://arxiv.org/abs/2501.14728v1</guid>
<content:encoded><![CDATA[
While large generative artificial intelligence (GenAI) models have achieved
significant success, they also raise growing concerns about online information
security due to their potential misuse for generating deceptive content.
Out-of-context (OOC) multimodal misinformation detection, which often retrieves
Web evidence to identify the repurposing of images in false contexts, faces the
issue of reasoning over GenAI-polluted evidence to derive accurate predictions.
Existing works simulate GenAI-powered pollution at the claim level with
stylistic rewriting to conceal linguistic cues, and ignore evidence-level
pollution for such information-seeking applications. In this work, we
investigate how polluted evidence affects the performance of existing OOC
detectors, revealing a performance degradation of more than 9 percentage
points. We propose two strategies, cross-modal evidence reranking and
cross-modal claim-evidence reasoning, to address the challenges posed by
polluted evidence. Extensive experiments on two benchmark datasets show that
these strategies can effectively enhance the robustness of existing
out-of-context detectors amidst polluted evidence.
]]></content:encoded>
<pubDate>2025-01-24T18:59:31Z</pubDate>
</item>
<item>
<title>Can We Generate Images with CoT? Let's Verify and Reinforce Image
  Generation Step by Step</title>
<link>http://arxiv.org/abs/2501.13926v1</link>
<guid>http://arxiv.org/abs/2501.13926v1</guid>
<content:encoded><![CDATA[
Chain-of-Thought (CoT) reasoning has been extensively explored in large
models to tackle complex understanding tasks. However, it still remains an open
question whether such strategies can be applied to verifying and reinforcing
image generation scenarios. In this paper, we provide the first comprehensive
investigation of the potential of CoT reasoning to enhance autoregressive image
generation. We focus on three techniques: scaling test-time computation for
verification, aligning model preferences with Direct Preference Optimization
(DPO), and integrating these techniques for complementary effects. Our results
demonstrate that these approaches can be effectively adapted and combined to
significantly improve image generation performance. Furthermore, given the
pivotal role of reward models in our findings, we propose the Potential
Assessment Reward Model (PARM) and PARM++, specialized for autoregressive image
generation. PARM adaptively assesses each generation step through a potential
assessment approach, merging the strengths of existing reward models, and
PARM++ further introduces a reflection mechanism to self-correct the generated
unsatisfactory image. Using our investigated reasoning strategies, we enhance a
baseline model, Show-o, to achieve superior results, with a significant +24%
improvement on the GenEval benchmark, surpassing Stable Diffusion 3 by +15%. We
hope our study provides unique insights and paves a new path for integrating
CoT reasoning with autoregressive image generation. Code and models are
released at https://github.com/ZiyuGuo99/Image-Generation-CoT
]]></content:encoded>
<pubDate>2025-01-23T18:59:43Z</pubDate>
</item>
<item>
<title>Towards Robust Multimodal Open-set Test-time Adaptation via Adaptive
  Entropy-aware Optimization</title>
<link>http://arxiv.org/abs/2501.13924v1</link>
<guid>http://arxiv.org/abs/2501.13924v1</guid>
<content:encoded><![CDATA[
Test-time adaptation (TTA) has demonstrated significant potential in
addressing distribution shifts between training and testing data. Open-set
test-time adaptation (OSTTA) aims to adapt a source pre-trained model online to
an unlabeled target domain that contains unknown classes. This task becomes
more challenging when multiple modalities are involved. Existing methods have
primarily focused on unimodal OSTTA, often filtering out low-confidence samples
without addressing the complexities of multimodal data. In this work, we
present Adaptive Entropy-aware Optimization (AEO), a novel framework
specifically designed to tackle Multimodal Open-set Test-time Adaptation
(MM-OSTTA) for the first time. Our analysis shows that the entropy difference
between known and unknown samples in the target domain strongly correlates with
MM-OSTTA performance. To leverage this, we propose two key components:
Unknown-aware Adaptive Entropy Optimization (UAE) and Adaptive Modality
Prediction Discrepancy Optimization (AMP). These components enhance the ability
of model to distinguish unknown class samples during online adaptation by
amplifying the entropy difference between known and unknown samples. To
thoroughly evaluate our proposed methods in the MM-OSTTA setting, we establish
a new benchmark derived from existing datasets. This benchmark includes two
downstream tasks and incorporates five modalities. Extensive experiments across
various domain shift situations demonstrate the efficacy and versatility of the
AEO framework. Additionally, we highlight the strong performance of AEO in
long-term and continual MM-OSTTA settings, both of which are challenging and
highly relevant to real-world applications. Our source code is available at
https://github.com/donghao51/AEO.
]]></content:encoded>
<pubDate>2025-01-23T18:59:30Z</pubDate>
</item>
<item>
<title>GeoPixel: Pixel Grounding Large Multimodal Model in Remote Sensing</title>
<link>http://arxiv.org/abs/2501.13925v1</link>
<guid>http://arxiv.org/abs/2501.13925v1</guid>
<content:encoded><![CDATA[
Recent advances in large multimodal models (LMMs) have recognized
fine-grained grounding as an imperative factor of visual understanding and
dialogue. However, the benefits of such representation in LMMs are limited to
the natural image domain, and these models perform poorly for remote sensing
(RS). The distinct overhead viewpoint, scale variation, and presence of small
objects in high-resolution RS imagery present a unique challenge in
region-level comprehension. Moreover, the development of the grounding
conversation capability of LMMs within RS is hindered by the lack of granular,
RS domain-specific grounded data. Addressing these limitations, we propose
GeoPixel - the first end-to-end high resolution RS-LMM that supports
pixel-level grounding. This capability allows fine-grained visual perception by
generating interleaved masks in conversation. GeoPixel supports up to 4K HD
resolution in any aspect ratio, ideal for high-precision RS image analysis. To
support the grounded conversation generation (GCG) in RS imagery, we curate a
visually grounded dataset GeoPixelD through a semi-automated pipeline that
utilizes set-of-marks prompting and spatial priors tailored for RS data to
methodically control the data generation process. GeoPixel demonstrates
superior performance in pixel-level comprehension, surpassing existing LMMs in
both single-target and multi-target segmentation tasks. Our methodological
ablation studies validate the effectiveness of each component in the overall
architecture. Our code and data will be publicly released.
]]></content:encoded>
<pubDate>2025-01-23T18:59:30Z</pubDate>
</item>
<item>
<title>IMAGINE-E: Image Generation Intelligence Evaluation of State-of-the-art
  Text-to-Image Models</title>
<link>http://arxiv.org/abs/2501.13920v1</link>
<guid>http://arxiv.org/abs/2501.13920v1</guid>
<content:encoded><![CDATA[
With the rapid development of diffusion models, text-to-image(T2I) models
have made significant progress, showcasing impressive abilities in prompt
following and image generation. Recently launched models such as FLUX.1 and
Ideogram2.0, along with others like Dall-E3 and Stable Diffusion 3, have
demonstrated exceptional performance across various complex tasks, raising
questions about whether T2I models are moving towards general-purpose
applicability. Beyond traditional image generation, these models exhibit
capabilities across a range of fields, including controllable generation, image
editing, video, audio, 3D, and motion generation, as well as computer vision
tasks like semantic segmentation and depth estimation. However, current
evaluation frameworks are insufficient to comprehensively assess these models'
performance across expanding domains. To thoroughly evaluate these models, we
developed the IMAGINE-E and tested six prominent models: FLUX.1, Ideogram2.0,
Midjourney, Dall-E3, Stable Diffusion 3, and Jimeng. Our evaluation is divided
into five key domains: structured output generation, realism, and physical
consistency, specific domain generation, challenging scenario generation, and
multi-style creation tasks. This comprehensive assessment highlights each
model's strengths and limitations, particularly the outstanding performance of
FLUX.1 and Ideogram2.0 in structured and specific domain tasks, underscoring
the expanding applications and potential of T2I models as foundational AI
tools. This study provides valuable insights into the current state and future
trajectory of T2I models as they evolve towards general-purpose usability.
Evaluation scripts will be released at https://github.com/jylei16/Imagine-e.
]]></content:encoded>
<pubDate>2025-01-23T18:58:33Z</pubDate>
</item>
<item>
<title>Temporal Preference Optimization for Long-Form Video Understanding</title>
<link>http://arxiv.org/abs/2501.13919v1</link>
<guid>http://arxiv.org/abs/2501.13919v1</guid>
<content:encoded><![CDATA[
Despite significant advancements in video large multimodal models
(video-LMMs), achieving effective temporal grounding in long-form videos
remains a challenge for existing models. To address this limitation, we propose
Temporal Preference Optimization (TPO), a novel post-training framework
designed to enhance the temporal grounding capabilities of video-LMMs through
preference learning. TPO adopts a self-training approach that enables models to
differentiate between well-grounded and less accurate temporal responses by
leveraging curated preference datasets at two granularities: localized temporal
grounding, which focuses on specific video segments, and comprehensive temporal
grounding, which captures extended temporal dependencies across entire video
sequences. By optimizing on these preference datasets, TPO significantly
enhances temporal understanding while reducing reliance on manually annotated
data. Extensive experiments on three long-form video understanding
benchmarks--LongVideoBench, MLVU, and Video-MME--demonstrate the effectiveness
of TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPO
establishes itself as the leading 7B model on the Video-MME benchmark,
underscoring the potential of TPO as a scalable and efficient solution for
advancing temporal reasoning in long-form video understanding. Project page:
https://ruili33.github.io/tpo_website.
]]></content:encoded>
<pubDate>2025-01-23T18:58:03Z</pubDate>
</item>
<item>
<title>Improving Video Generation with Human Feedback</title>
<link>http://arxiv.org/abs/2501.13918v1</link>
<guid>http://arxiv.org/abs/2501.13918v1</guid>
<content:encoded><![CDATA[
Video generation has achieved significant advances through rectified flow
techniques, but issues like unsmooth motion and misalignment between videos and
prompts persist. In this work, we develop a systematic pipeline that harnesses
human feedback to mitigate these problems and refine the video generation
model. Specifically, we begin by constructing a large-scale human preference
dataset focused on modern video generation models, incorporating pairwise
annotations across multi-dimensions. We then introduce VideoReward, a
multi-dimensional video reward model, and examine how annotations and various
design choices impact its rewarding efficacy. From a unified reinforcement
learning perspective aimed at maximizing reward with KL regularization, we
introduce three alignment algorithms for flow-based models by extending those
from diffusion models. These include two training-time strategies: direct
preference optimization for flow (Flow-DPO) and reward weighted regression for
flow (Flow-RWR), and an inference-time technique, Flow-NRG, which applies
reward guidance directly to noisy videos. Experimental results indicate that
VideoReward significantly outperforms existing reward models, and Flow-DPO
demonstrates superior performance compared to both Flow-RWR and standard
supervised fine-tuning methods. Additionally, Flow-NRG lets users assign custom
weights to multiple objectives during inference, meeting personalized video
quality needs. Project page: https://gongyeliu.github.io/videoalign.
]]></content:encoded>
<pubDate>2025-01-23T18:55:41Z</pubDate>
</item>
<item>
<title>Accelerate High-Quality Diffusion Models with Inner Loop Feedback</title>
<link>http://arxiv.org/abs/2501.13107v1</link>
<guid>http://arxiv.org/abs/2501.13107v1</guid>
<content:encoded><![CDATA[
We propose Inner Loop Feedback (ILF), a novel approach to accelerate
diffusion models' inference. ILF trains a lightweight module to predict future
features in the denoising process by leveraging the outputs from a chosen
diffusion backbone block at a given time step. This approach exploits two key
intuitions; (1) the outputs of a given block at adjacent time steps are
similar, and (2) performing partial computations for a step imposes a lower
burden on the model than skipping the step entirely. Our method is highly
flexible, since we find that the feedback module itself can simply be a block
from the diffusion backbone, with all settings copied. Its influence on the
diffusion forward can be tempered with a learnable scaling factor from zero
initialization. We train this module using distillation losses; however, unlike
some prior work where a full diffusion backbone serves as the student, our
model freezes the backbone, training only the feedback module. While many
efforts to optimize diffusion models focus on achieving acceptable image
quality in extremely few steps (1-4 steps), our emphasis is on matching best
case results (typically achieved in 20 steps) while significantly reducing
runtime. ILF achieves this balance effectively, demonstrating strong
performance for both class-to-image generation with diffusion transformer (DiT)
and text-to-image generation with DiT-based PixArt-alpha and PixArt-sigma. The
quality of ILF's 1.7x-1.8x speedups are confirmed by FID, CLIP score, CLIP
Image Quality Assessment, ImageReward, and qualitative comparisons.
]]></content:encoded>
<pubDate>2025-01-22T18:59:58Z</pubDate>
</item>
<item>
<title>VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video
  Understanding</title>
<link>http://arxiv.org/abs/2501.13106v1</link>
<guid>http://arxiv.org/abs/2501.13106v1</guid>
<content:encoded><![CDATA[
In this paper, we propose VideoLLaMA3, a more advanced multimodal foundation
model for image and video understanding. The core design philosophy of
VideoLLaMA3 is vision-centric. The meaning of "vision-centric" is two-fold: the
vision-centric training paradigm and vision-centric framework design. The key
insight of our vision-centric training paradigm is that high-quality image-text
data is crucial for both image and video understanding. Instead of preparing
massive video-text datasets, we focus on constructing large-scale and
high-quality image-text datasets. VideoLLaMA3 has four training stages: 1)
vision-centric alignment stage, which warms up the vision encoder and
projector; 2) vision-language pretraining stage, which jointly tunes the vision
encoder, projector, and LLM with large-scale image-text data covering multiple
types (including scene images, documents, charts) as well as text-only data. 3)
multi-task fine-tuning stage, which incorporates image-text SFT data for
downstream tasks and video-text data to establish a foundation for video
understanding. 4) video-centric fine-tuning, which further improves the model's
capability in video understanding. As for the framework design, to better
capture fine-grained details in images, the pretrained vision encoder is
adapted to encode images of varying sizes into vision tokens with corresponding
numbers, rather than a fixed number of tokens. For video inputs, we reduce the
number of vision tokens according to their similarity so that the
representation of videos will be more precise and compact. Benefit from
vision-centric designs, VideoLLaMA3 achieves compelling performances in both
image and video understanding benchmarks.
]]></content:encoded>
<pubDate>2025-01-22T18:59:46Z</pubDate>
</item>
<item>
<title>Orchid: Image Latent Diffusion for Joint Appearance and Geometry
  Generation</title>
<link>http://arxiv.org/abs/2501.13087v1</link>
<guid>http://arxiv.org/abs/2501.13087v1</guid>
<content:encoded><![CDATA[
Diffusion models are state-of-the-art for image generation. Trained on large
datasets, they capture expressive image priors that have been used for tasks
like inpainting, depth, and (surface) normal prediction. However, these models
are typically trained for one specific task, e.g., a separate model for each of
color, depth, and normal prediction. Such models do not leverage the intrinsic
correlation between appearance and geometry, often leading to inconsistent
predictions.
  In this paper, we propose using a novel image diffusion prior that jointly
encodes appearance and geometry. We introduce a diffusion model Orchid,
comprising a Variational Autoencoder (VAE) to encode color, depth, and surface
normals to a latent space, and a Latent Diffusion Model (LDM) for generating
these joint latents. Orchid directly generates photo-realistic color images,
relative depth, and surface normals from user-provided text, and can be used to
create image-aligned partial 3D scenes seamlessly. It can also perform
image-conditioned tasks like joint monocular depth and normal prediction and is
competitive in accuracy to state-of-the-art methods designed for those tasks
alone. Lastly, our model learns a joint prior that can be used zero-shot as a
regularizer for many inverse problems that entangle appearance and geometry.
For example, we demonstrate its effectiveness in color-depth-normal inpainting,
showcasing its applicability to problems in 3D generation from sparse views.
]]></content:encoded>
<pubDate>2025-01-22T18:46:47Z</pubDate>
</item>
<item>
<title>GPS as a Control Signal for Image Generation</title>
<link>http://arxiv.org/abs/2501.12390v1</link>
<guid>http://arxiv.org/abs/2501.12390v1</guid>
<content:encoded><![CDATA[
We show that the GPS tags contained in photo metadata provide a useful
control signal for image generation. We train GPS-to-image models and use them
for tasks that require a fine-grained understanding of how images vary within a
city. In particular, we train a diffusion model to generate images conditioned
on both GPS and text. The learned model generates images that capture the
distinctive appearance of different neighborhoods, parks, and landmarks. We
also extract 3D models from 2D GPS-to-image models through score distillation
sampling, using GPS conditioning to constrain the appearance of the
reconstruction from each viewpoint. Our evaluations suggest that our
GPS-conditioned models successfully learn to generate images that vary based on
location, and that GPS conditioning improves estimated 3D structure.
]]></content:encoded>
<pubDate>2025-01-21T18:59:46Z</pubDate>
</item>
<item>
<title>Taming Teacher Forcing for Masked Autoregressive Video Generation</title>
<link>http://arxiv.org/abs/2501.12389v1</link>
<guid>http://arxiv.org/abs/2501.12389v1</guid>
<content:encoded><![CDATA[
We introduce MAGI, a hybrid video generation framework that combines masked
modeling for intra-frame generation with causal modeling for next-frame
generation. Our key innovation, Complete Teacher Forcing (CTF), conditions
masked frames on complete observation frames rather than masked ones (namely
Masked Teacher Forcing, MTF), enabling a smooth transition from token-level
(patch-level) to frame-level autoregressive generation. CTF significantly
outperforms MTF, achieving a +23% improvement in FVD scores on first-frame
conditioned video prediction. To address issues like exposure bias, we employ
targeted training strategies, setting a new benchmark in autoregressive video
generation. Experiments show that MAGI can generate long, coherent video
sequences exceeding 100 frames, even when trained on as few as 16 frames,
highlighting its potential for scalable, high-quality video generation.
]]></content:encoded>
<pubDate>2025-01-21T18:59:31Z</pubDate>
</item>
<item>
<title>InternVideo2.5: Empowering Video MLLMs with Long and Rich Context
  Modeling</title>
<link>http://arxiv.org/abs/2501.12386v1</link>
<guid>http://arxiv.org/abs/2501.12386v1</guid>
<content:encoded><![CDATA[
This paper aims to improve the performance of video multimodal large language
models (MLLM) via long and rich context (LRC) modeling. As a result, we develop
a new version of InternVideo2.5 with a focus on enhancing the original MLLMs'
ability to perceive fine-grained details and capture long-form temporal
structure in videos. Specifically, our approach incorporates dense vision task
annotations into MLLMs using direct preference optimization and develops
compact spatiotemporal representations through adaptive hierarchical token
compression. Experimental results demonstrate this unique design of LRC greatly
improves the results of video MLLM in mainstream video understanding benchmarks
(short & long), enabling the MLLM to memorize significantly longer video inputs
(at least 6x longer than the original), and master specialized vision
capabilities like object tracking and segmentation. Our work highlights the
importance of multimodal context richness (length and fineness) in empowering
MLLM's innate abilites (focus and memory), providing new insights for future
research on video MLLM. Code and models are available at
https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5
]]></content:encoded>
<pubDate>2025-01-21T18:59:00Z</pubDate>
</item>
<item>
<title>FaceXBench: Evaluating Multimodal LLMs on Face Understanding</title>
<link>http://arxiv.org/abs/2501.10360v1</link>
<guid>http://arxiv.org/abs/2501.10360v1</guid>
<content:encoded><![CDATA[
Multimodal Large Language Models (MLLMs) demonstrate impressive
problem-solving abilities across a wide range of tasks and domains. However,
their capacity for face understanding has not been systematically studied. To
address this gap, we introduce FaceXBench, a comprehensive benchmark designed
to evaluate MLLMs on complex face understanding tasks. FaceXBench includes
5,000 multimodal multiple-choice questions derived from 25 public datasets and
a newly created dataset, FaceXAPI. These questions cover 14 tasks across 6
broad categories, assessing MLLMs' face understanding abilities in bias and
fairness, face authentication, recognition, analysis, localization and tool
retrieval. Using FaceXBench, we conduct an extensive evaluation of 26
open-source MLLMs alongside 2 proprietary models, revealing the unique
challenges in complex face understanding tasks. We analyze the models across
three evaluation settings: zero-shot, in-context task description, and
chain-of-thought prompting. Our detailed analysis reveals that current MLLMs,
including advanced models like GPT-4o, and GeminiPro 1.5, show significant room
for improvement. We believe FaceXBench will be a crucial resource for
developing MLLMs equipped to perform sophisticated face understanding. Code:
https://github.com/Kartik-3004/facexbench
]]></content:encoded>
<pubDate>2025-01-17T18:59:55Z</pubDate>
</item>
<item>
<title>Learnings from Scaling Visual Tokenizers for Reconstruction and
  Generation</title>
<link>http://arxiv.org/abs/2501.09755v1</link>
<guid>http://arxiv.org/abs/2501.09755v1</guid>
<content:encoded><![CDATA[
Visual tokenization via auto-encoding empowers state-of-the-art image and
video generative models by compressing pixels into a latent space. Although
scaling Transformer-based generators has been central to recent advances, the
tokenizer component itself is rarely scaled, leaving open questions about how
auto-encoder design choices influence both its objective of reconstruction and
downstream generative performance. Our work aims to conduct an exploration of
scaling in auto-encoders to fill in this blank. To facilitate this exploration,
we replace the typical convolutional backbone with an enhanced Vision
Transformer architecture for Tokenization (ViTok). We train ViTok on
large-scale image and video datasets far exceeding ImageNet-1K, removing data
constraints on tokenizer scaling. We first study how scaling the auto-encoder
bottleneck affects both reconstruction and generation -- and find that while it
is highly correlated with reconstruction, its relationship with generation is
more complex. We next explored the effect of separately scaling the
auto-encoders' encoder and decoder on reconstruction and generation
performance. Crucially, we find that scaling the encoder yields minimal gains
for either reconstruction or generation, while scaling the decoder boosts
reconstruction but the benefits for generation are mixed. Building on our
exploration, we design ViTok as a lightweight auto-encoder that achieves
competitive performance with state-of-the-art auto-encoders on ImageNet-1K and
COCO reconstruction tasks (256p and 512p) while outperforming existing
auto-encoders on 16-frame 128p video reconstruction for UCF-101, all with 2-5x
fewer FLOPs. When integrated with Diffusion Transformers, ViTok demonstrates
competitive performance on image generation for ImageNet-1K and sets new
state-of-the-art benchmarks for class-conditional video generation on UCF-101.
]]></content:encoded>
<pubDate>2025-01-16T18:59:04Z</pubDate>
</item>
<item>
<title>Ouroboros-Diffusion: Exploring Consistent Content Generation in
  Tuning-free Long Video Diffusion</title>
<link>http://arxiv.org/abs/2501.09019v1</link>
<guid>http://arxiv.org/abs/2501.09019v1</guid>
<content:encoded><![CDATA[
The first-in-first-out (FIFO) video diffusion, built on a pre-trained
text-to-video model, has recently emerged as an effective approach for
tuning-free long video generation. This technique maintains a queue of video
frames with progressively increasing noise, continuously producing clean frames
at the queue's head while Gaussian noise is enqueued at the tail. However,
FIFO-Diffusion often struggles to keep long-range temporal consistency in the
generated videos due to the lack of correspondence modeling across frames. In
this paper, we propose Ouroboros-Diffusion, a novel video denoising framework
designed to enhance structural and content (subject) consistency, enabling the
generation of consistent videos of arbitrary length. Specifically, we introduce
a new latent sampling technique at the queue tail to improve structural
consistency, ensuring perceptually smooth transitions among frames. To enhance
subject consistency, we devise a Subject-Aware Cross-Frame Attention (SACFA)
mechanism, which aligns subjects across frames within short segments to achieve
better visual coherence. Furthermore, we introduce self-recurrent guidance.
This technique leverages information from all previous cleaner frames at the
front of the queue to guide the denoising of noisier frames at the end,
fostering rich and contextual global information interaction. Extensive
experiments of long video generation on the VBench benchmark demonstrate the
superiority of our Ouroboros-Diffusion, particularly in terms of subject
consistency, motion smoothness, and temporal consistency.
]]></content:encoded>
<pubDate>2025-01-15T18:59:15Z</pubDate>
</item>
<item>
<title>Multimodal LLMs Can Reason about Aesthetics in Zero-Shot</title>
<link>http://arxiv.org/abs/2501.09012v1</link>
<guid>http://arxiv.org/abs/2501.09012v1</guid>
<content:encoded><![CDATA[
We present the first study on how Multimodal LLMs' (MLLMs) reasoning ability
shall be elicited to evaluate the aesthetics of artworks. To facilitate this
investigation, we construct MM-StyleBench, a novel high-quality dataset for
benchmarking artistic stylization. We then develop a principled method for
human preference modeling and perform a systematic correlation analysis between
MLLMs' responses and human preference. Our experiments reveal an inherent
hallucination issue of MLLMs in art evaluation, associated with response
subjectivity. ArtCoT is proposed, demonstrating that art-specific task
decomposition and the use of concrete language boost MLLMs' reasoning ability
for aesthetics. Our findings offer valuable insights into MLLMs for art and can
benefit a wide range of downstream applications, such as style transfer and
artistic image generation. Code available at
https://github.com/songrise/MLLM4Art.
]]></content:encoded>
<pubDate>2025-01-15T18:56:22Z</pubDate>
</item>
<item>
<title>Omni-RGPT: Unifying Image and Video Region-level Understanding via Token
  Marks</title>
<link>http://arxiv.org/abs/2501.08326v1</link>
<guid>http://arxiv.org/abs/2501.08326v1</guid>
<content:encoded><![CDATA[
We present Omni-RGPT, a multimodal large language model designed to
facilitate region-level comprehension for both images and videos. To achieve
consistent region representation across spatio-temporal dimensions, we
introduce Token Mark, a set of tokens highlighting the target regions within
the visual feature space. These tokens are directly embedded into spatial
regions using region prompts (e.g., boxes or masks) and simultaneously
incorporated into the text prompt to specify the target, establishing a direct
connection between visual and text tokens. To further support robust video
understanding without requiring tracklets, we introduce an auxiliary task that
guides Token Mark by leveraging the consistency of the tokens, enabling stable
region interpretation across the video. Additionally, we introduce a
large-scale region-level video instruction dataset (RegVID-300k). Omni-RGPT
achieves state-of-the-art results on image and video-based commonsense
reasoning benchmarks while showing strong performance in captioning and
referring expression comprehension tasks.
]]></content:encoded>
<pubDate>2025-01-14T18:58:04Z</pubDate>
</item>
<item>
<title>GameFactory: Creating New Games with Generative Interactive Videos</title>
<link>http://arxiv.org/abs/2501.08325v1</link>
<guid>http://arxiv.org/abs/2501.08325v1</guid>
<content:encoded><![CDATA[
Generative game engines have the potential to revolutionize game development
by autonomously creating new content and reducing manual workload. However,
existing video-based game generation methods fail to address the critical
challenge of scene generalization, limiting their applicability to existing
games with fixed styles and scenes. In this paper, we present GameFactory, a
framework focused on exploring scene generalization in game video generation.
To enable the creation of entirely new and diverse games, we leverage
pre-trained video diffusion models trained on open-domain video data. To bridge
the domain gap between open-domain priors and small-scale game dataset, we
propose a multi-phase training strategy that decouples game style learning from
action control, preserving open-domain generalization while achieving action
controllability. Using Minecraft as our data source, we release GF-Minecraft, a
high-quality and diversity action-annotated video dataset for research.
Furthermore, we extend our framework to enable autoregressive
action-controllable game video generation, allowing the production of
unlimited-length interactive game videos. Experimental results demonstrate that
GameFactory effectively generates open-domain, diverse, and action-controllable
game videos, representing a significant step forward in AI-driven game
generation. Our dataset and project page are publicly available at
\url{https://vvictoryuki.github.io/gamefactory/}.
]]></content:encoded>
<pubDate>2025-01-14T18:57:21Z</pubDate>
</item>
<item>
<title>ADAM-1: AI and Bioinformatics for Alzheimer's Detection and
  Microbiome-Clinical Data Integrations</title>
<link>http://arxiv.org/abs/2501.08324v1</link>
<guid>http://arxiv.org/abs/2501.08324v1</guid>
<content:encoded><![CDATA[
The Alzheimer's Disease Analysis Model Generation 1 (ADAM) is a multi-agent
large language model (LLM) framework designed to integrate and analyze
multi-modal data, including microbiome profiles, clinical datasets, and
external knowledge bases, to enhance the understanding and detection of
Alzheimer's disease (AD). By leveraging retrieval-augmented generation (RAG)
techniques along with its multi-agent architecture, ADAM-1 synthesizes insights
from diverse data sources and contextualizes findings using literature-driven
evidence. Comparative evaluation against XGBoost revealed similar mean F1
scores but significantly reduced variance for ADAM-1, highlighting its
robustness and consistency, particularly in small laboratory datasets. While
currently tailored for binary classification tasks, future iterations aim to
incorporate additional data modalities, such as neuroimaging and biomarkers, to
broaden the scalability and applicability for Alzheimer's research and
diagnostics.
]]></content:encoded>
<pubDate>2025-01-14T18:56:33Z</pubDate>
</item>
<item>
<title>PokerBench: Training Large Language Models to become Professional Poker
  Players</title>
<link>http://arxiv.org/abs/2501.08328v1</link>
<guid>http://arxiv.org/abs/2501.08328v1</guid>
<content:encoded><![CDATA[
We introduce PokerBench - a benchmark for evaluating the poker-playing
abilities of large language models (LLMs). As LLMs excel in traditional NLP
tasks, their application to complex, strategic games like poker poses a new
challenge. Poker, an incomplete information game, demands a multitude of skills
such as mathematics, reasoning, planning, strategy, and a deep understanding of
game theory and human psychology. This makes Poker the ideal next frontier for
large language models. PokerBench consists of a comprehensive compilation of
11,000 most important scenarios, split between pre-flop and post-flop play,
developed in collaboration with trained poker players. We evaluate prominent
models including GPT-4, ChatGPT 3.5, and various Llama and Gemma series models,
finding that all state-of-the-art LLMs underperform in playing optimal poker.
However, after fine-tuning, these models show marked improvements. We validate
PokerBench by having models with different scores compete with each other,
demonstrating that higher scores on PokerBench lead to higher win rates in
actual poker games. Through gameplay between our fine-tuned model and GPT-4, we
also identify limitations of simple supervised fine-tuning for learning optimal
playing strategy, suggesting the need for more advanced methodologies for
effectively training language models to excel in games. PokerBench thus
presents a unique benchmark for a quick and reliable evaluation of the
poker-playing ability of LLMs as well as a comprehensive benchmark to study the
progress of LLMs in complex game-playing scenarios. The dataset and code will
be made available at: \url{https://github.com/pokerllm/pokerbench}.
]]></content:encoded>
<pubDate>2025-01-14T18:59:03Z</pubDate>
</item>
<item>
<title>Training-Free Motion-Guided Video Generation with Enhanced Temporal
  Consistency Using Motion Consistency Loss</title>
<link>http://arxiv.org/abs/2501.07563v1</link>
<guid>http://arxiv.org/abs/2501.07563v1</guid>
<content:encoded><![CDATA[
In this paper, we address the challenge of generating temporally consistent
videos with motion guidance. While many existing methods depend on additional
control modules or inference-time fine-tuning, recent studies suggest that
effective motion guidance is achievable without altering the model architecture
or requiring extra training. Such approaches offer promising compatibility with
various video generation foundation models. However, existing training-free
methods often struggle to maintain consistent temporal coherence across frames
or to follow guided motion accurately. In this work, we propose a simple yet
effective solution that combines an initial-noise-based approach with a novel
motion consistency loss, the latter being our key innovation. Specifically, we
capture the inter-frame feature correlation patterns of intermediate features
from a video diffusion model to represent the motion pattern of the reference
video. We then design a motion consistency loss to maintain similar feature
correlation patterns in the generated video, using the gradient of this loss in
the latent space to guide the generation process for precise motion control.
This approach improves temporal consistency across various motion control tasks
while preserving the benefits of a training-free setup. Extensive experiments
show that our method sets a new standard for efficient, temporally coherent
video generation.
]]></content:encoded>
<pubDate>2025-01-13T18:53:08Z</pubDate>
</item>
<item>
<title>WebWalker: Benchmarking LLMs in Web Traversal</title>
<link>http://arxiv.org/abs/2501.07572v1</link>
<guid>http://arxiv.org/abs/2501.07572v1</guid>
<content:encoded><![CDATA[
Retrieval-augmented generation (RAG) demonstrates remarkable performance
across tasks in open-domain question-answering. However, traditional search
engines may retrieve shallow content, limiting the ability of LLMs to handle
complex, multi-layered information. To address it, we introduce WebWalkerQA, a
benchmark designed to assess the ability of LLMs to perform web traversal. It
evaluates the capacity of LLMs to traverse a website's subpages to extract
high-quality data systematically. We propose WebWalker, which is a multi-agent
framework that mimics human-like web navigation through an explore-critic
paradigm. Extensive experimental results show that WebWalkerQA is challenging
and demonstrates the effectiveness of RAG combined with WebWalker, through the
horizontal and vertical integration in real-world scenarios.
]]></content:encoded>
<pubDate>2025-01-13T18:58:07Z</pubDate>
</item>
<item>
<title>SafeSwarm: Decentralized Safe RL for the Swarm of Drones Landing in
  Dense Crowds</title>
<link>http://arxiv.org/abs/2501.07566v1</link>
<guid>http://arxiv.org/abs/2501.07566v1</guid>
<content:encoded><![CDATA[
This paper introduces a safe swarm of drones capable of performing landings
in crowded environments robustly by relying on Reinforcement Learning
techniques combined with Safe Learning. The developed system allows us to teach
the swarm of drones with different dynamics to land on moving landing pads in
an environment while avoiding collisions with obstacles and between agents.
  The safe barrier net algorithm was developed and evaluated using a swarm of
Crazyflie 2.1 micro quadrotors, which were tested indoors with the Vicon motion
capture system to ensure precise localization and control.
  Experimental results show that our system achieves landing accuracy of 2.25
cm with a mean time of 17 s and collision-free landings, underscoring its
effectiveness and robustness in real-world scenarios. This work offers a
promising foundation for applications in environments where safety and
precision are paramount.
]]></content:encoded>
<pubDate>2025-01-13T18:54:02Z</pubDate>
</item>
<item>
<title>LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs</title>
<link>http://arxiv.org/abs/2501.06186v1</link>
<guid>http://arxiv.org/abs/2501.06186v1</guid>
<content:encoded><![CDATA[
Reasoning is a fundamental capability for solving complex multi-step
problems, particularly in visual contexts where sequential step-wise
understanding is essential. Existing approaches lack a comprehensive framework
for evaluating visual reasoning and do not emphasize step-wise problem-solving.
To this end, we propose a comprehensive framework for advancing step-by-step
visual reasoning in large language models (LMMs) through three key
contributions. First, we introduce a visual reasoning benchmark specifically
designed to evaluate multi-step reasoning tasks. The benchmark presents a
diverse set of challenges with eight different categories ranging from complex
visual perception to scientific reasoning with over 4k reasoning steps in
total, enabling robust evaluation of LLMs' abilities to perform accurate and
interpretable visual reasoning across multiple steps. Second, we propose a
novel metric that assesses visual reasoning quality at the granularity of
individual steps, emphasizing both correctness and logical coherence. The
proposed metric offers deeper insights into reasoning performance compared to
traditional end-task accuracy metrics. Third, we present a new multimodal
visual reasoning model, named LlamaV-o1, trained using a multi-step curriculum
learning approach, where tasks are progressively organized to facilitate
incremental skill acquisition and problem-solving. The proposed LlamaV-o1 is
designed for multi-step reasoning and learns step-by-step through a structured
training paradigm. Extensive experiments show that our LlamaV-o1 outperforms
existing open-source models and performs favorably against close-source
proprietary models. Compared to the recent Llava-CoT, our LlamaV-o1 achieves an
average score of 67.3 with an absolute gain of 3.8\% across six benchmarks
while being 5 times faster during inference scaling. Our benchmark, model, and
code are publicly available.
]]></content:encoded>
<pubDate>2025-01-10T18:59:51Z</pubDate>
</item>
<item>
<title>PEACE: Empowering Geologic Map Holistic Understanding with MLLMs</title>
<link>http://arxiv.org/abs/2501.06184v1</link>
<guid>http://arxiv.org/abs/2501.06184v1</guid>
<content:encoded><![CDATA[
Geologic map, as a fundamental diagram in geology science, provides critical
insights into the structure and composition of Earth's subsurface and surface.
These maps are indispensable in various fields, including disaster detection,
resource exploration, and civil engineering. Despite their significance,
current Multimodal Large Language Models (MLLMs) often fall short in geologic
map understanding. This gap is primarily due to the challenging nature of
cartographic generalization, which involves handling high-resolution map,
managing multiple associated components, and requiring domain-specific
knowledge. To quantify this gap, we construct GeoMap-Bench, the first-ever
benchmark for evaluating MLLMs in geologic map understanding, which assesses
the full-scale abilities in extracting, referring, grounding, reasoning, and
analyzing. To bridge this gap, we introduce GeoMap-Agent, the inaugural agent
designed for geologic map understanding, which features three modules:
Hierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI),
and Prompt-enhanced Question Answering (PEQA). Inspired by the
interdisciplinary collaboration among human scientists, an AI expert group acts
as consultants, utilizing a diverse tool pool to comprehensively analyze
questions. Through comprehensive experiments, GeoMap-Agent achieves an overall
score of 0.811 on GeoMap-Bench, significantly outperforming 0.369 of GPT-4o.
Our work, emPowering gEologic mAp holistiC undErstanding (PEACE) with MLLMs,
paves the way for advanced AI applications in geology, enhancing the efficiency
and accuracy of geological investigations.
]]></content:encoded>
<pubDate>2025-01-10T18:59:42Z</pubDate>
</item>
<item>
<title>VideoAuteur: Towards Long Narrative Video Generation</title>
<link>http://arxiv.org/abs/2501.06173v1</link>
<guid>http://arxiv.org/abs/2501.06173v1</guid>
<content:encoded><![CDATA[
Recent video generation models have shown promising results in producing
high-quality video clips lasting several seconds. However, these models face
challenges in generating long sequences that convey clear and informative
events, limiting their ability to support coherent narrations. In this paper,
we present a large-scale cooking video dataset designed to advance long-form
narrative generation in the cooking domain. We validate the quality of our
proposed dataset in terms of visual fidelity and textual caption accuracy using
state-of-the-art Vision-Language Models (VLMs) and video generation models,
respectively. We further introduce a Long Narrative Video Director to enhance
both visual and semantic coherence in generated videos and emphasize the role
of aligning visual embeddings to achieve improved overall video quality. Our
method demonstrates substantial improvements in generating visually detailed
and semantically aligned keyframes, supported by finetuning techniques that
integrate text and image embeddings within the video generation process.
Project page: https://videoauteur.github.io/
]]></content:encoded>
<pubDate>2025-01-10T18:52:11Z</pubDate>
</item>
<item>
<title>ReFocus: Visual Editing as a Chain of Thought for Structured Image
  Understanding</title>
<link>http://arxiv.org/abs/2501.05452v1</link>
<guid>http://arxiv.org/abs/2501.05452v1</guid>
<content:encoded><![CDATA[
Structured image understanding, such as interpreting tables and charts,
requires strategically refocusing across various structures and texts within an
image, forming a reasoning sequence to arrive at the final answer. However,
current multimodal large language models (LLMs) lack this multihop selective
attention capability. In this work, we introduce ReFocus, a simple yet
effective framework that equips multimodal LLMs with the ability to generate
"visual thoughts" by performing visual editing on the input image through code,
shifting and refining their visual focuses. Specifically, ReFocus enables
multimodal LLMs to generate Python codes to call tools and modify the input
image, sequentially drawing boxes, highlighting sections, and masking out
areas, thereby enhancing the visual reasoning process. We experiment upon a
wide range of structured image understanding tasks involving tables and charts.
ReFocus largely improves performance on all tasks over GPT-4o without visual
editing, yielding an average gain of 11.0% on table tasks and 6.8% on chart
tasks. We present an in-depth analysis of the effects of different visual
edits, and reasons why ReFocus can improve the performance without introducing
additional information. Further, we collect a 14k training set using ReFocus,
and prove that such visual chain-of-thought with intermediate information
offers a better supervision than standard VQA data, reaching a 8.0% average
gain over the same model trained with QA pairs and 2.6% over CoT.
]]></content:encoded>
<pubDate>2025-01-09T18:59:58Z</pubDate>
</item>
<item>
<title>Consistent Flow Distillation for Text-to-3D Generation</title>
<link>http://arxiv.org/abs/2501.05445v1</link>
<guid>http://arxiv.org/abs/2501.05445v1</guid>
<content:encoded><![CDATA[
Score Distillation Sampling (SDS) has made significant strides in distilling
image-generative models for 3D generation. However, its
maximum-likelihood-seeking behavior often leads to degraded visual quality and
diversity, limiting its effectiveness in 3D applications. In this work, we
propose Consistent Flow Distillation (CFD), which addresses these limitations.
We begin by leveraging the gradient of the diffusion ODE or SDE sampling
process to guide the 3D generation. From the gradient-based sampling
perspective, we find that the consistency of 2D image flows across different
viewpoints is important for high-quality 3D generation. To achieve this, we
introduce multi-view consistent Gaussian noise on the 3D object, which can be
rendered from various viewpoints to compute the flow gradient. Our experiments
demonstrate that CFD, through consistent flows, significantly outperforms
previous methods in text-to-3D generation.
]]></content:encoded>
<pubDate>2025-01-09T18:56:05Z</pubDate>
</item>
<item>
<title>Can MLLMs Reason in Multimodality? EMMA: An Enhanced MultiModal
  ReAsoning Benchmark</title>
<link>http://arxiv.org/abs/2501.05444v1</link>
<guid>http://arxiv.org/abs/2501.05444v1</guid>
<content:encoded><![CDATA[
The ability to organically reason over and with both text and images is a
pillar of human intelligence, yet the ability of Multimodal Large Language
Models (MLLMs) to perform such multimodal reasoning remains under-explored.
Existing benchmarks often emphasize text-dominant reasoning or rely on shallow
visual cues, failing to adequately assess integrated visual and textual
reasoning. We introduce EMMA (Enhanced MultiModal reAsoning), a benchmark
targeting organic multimodal reasoning across mathematics, physics, chemistry,
and coding. EMMA tasks demand advanced cross-modal reasoning that cannot be
addressed by reasoning independently in each modality, offering an enhanced
test suite for MLLMs' reasoning capabilities. Our evaluation of
state-of-the-art MLLMs on EMMA reveals significant limitations in handling
complex multimodal and multi-step reasoning tasks, even with advanced
techniques like Chain-of-Thought prompting and test-time compute scaling
underperforming. These findings underscore the need for improved multimodal
architectures and training paradigms to close the gap between human and model
reasoning in multimodality.
]]></content:encoded>
<pubDate>2025-01-09T18:55:52Z</pubDate>
</item>
<item>
<title>Progressive Growing of Video Tokenizers for Highly Compressed Latent
  Spaces</title>
<link>http://arxiv.org/abs/2501.05442v1</link>
<guid>http://arxiv.org/abs/2501.05442v1</guid>
<content:encoded><![CDATA[
Video tokenizers are essential for latent video diffusion models, converting
raw video data into spatiotemporally compressed latent spaces for efficient
training. However, extending state-of-the-art video tokenizers to achieve a
temporal compression ratio beyond 4x without increasing channel capacity poses
significant challenges. In this work, we propose an alternative approach to
enhance temporal compression. We find that the reconstruction quality of
temporally subsampled videos from a low-compression encoder surpasses that of
high-compression encoders applied to original videos. This indicates that
high-compression models can leverage representations from lower-compression
models. Building on this insight, we develop a bootstrapped
high-temporal-compression model that progressively trains high-compression
blocks atop well-trained lower-compression models. Our method includes a
cross-level feature-mixing module to retain information from the pretrained
low-compression model and guide higher-compression blocks to capture the
remaining details from the full video sequence. Evaluation of video benchmarks
shows that our method significantly improves reconstruction quality while
increasing temporal compression compared to direct extensions of existing video
tokenizers. Furthermore, the resulting compact latent space effectively trains
a video diffusion model for high-quality video generation with a reduced token
budget.
]]></content:encoded>
<pubDate>2025-01-09T18:55:15Z</pubDate>
</item>
<item>
<title>EditAR: Unified Conditional Generation with Autoregressive Models</title>
<link>http://arxiv.org/abs/2501.04699v1</link>
<guid>http://arxiv.org/abs/2501.04699v1</guid>
<content:encoded><![CDATA[
Recent progress in controllable image generation and editing is largely
driven by diffusion-based methods. Although diffusion models perform
exceptionally well in specific tasks with tailored designs, establishing a
unified model is still challenging. In contrast, autoregressive models
inherently feature a unified tokenized representation, which simplifies the
creation of a single foundational model for various tasks. In this work, we
propose EditAR, a single unified autoregressive framework for a variety of
conditional image generation tasks, e.g., image editing, depth-to-image,
edge-to-image, segmentation-to-image. The model takes both images and
instructions as inputs, and predicts the edited images tokens in a vanilla
next-token paradigm. To enhance the text-to-image alignment, we further propose
to distill the knowledge from foundation models into the autoregressive
modeling process. We evaluate its effectiveness across diverse tasks on
established benchmarks, showing competitive performance to various
state-of-the-art task-specific methods. Project page:
https://jitengmu.github.io/EditAR/
]]></content:encoded>
<pubDate>2025-01-08T18:59:35Z</pubDate>
</item>
<item>
<title>ConceptMaster: Multi-Concept Video Customization on Diffusion
  Transformer Models Without Test-Time Tuning</title>
<link>http://arxiv.org/abs/2501.04698v1</link>
<guid>http://arxiv.org/abs/2501.04698v1</guid>
<content:encoded><![CDATA[
Text-to-video generation has made remarkable advancements through diffusion
models. However, Multi-Concept Video Customization (MCVC) remains a significant
challenge. We identify two key challenges in this task: 1) the identity
decoupling problem, where directly adopting existing customization methods
inevitably mix attributes when handling multiple concepts simultaneously, and
2) the scarcity of high-quality video-entity pairs, which is crucial for
training such a model that represents and decouples various concepts well. To
address these challenges, we introduce ConceptMaster, an innovative framework
that effectively tackles the critical issues of identity decoupling while
maintaining concept fidelity in customized videos. Specifically, we introduce a
novel strategy of learning decoupled multi-concept embeddings that are injected
into the diffusion models in a standalone manner, which effectively guarantees
the quality of customized videos with multiple identities, even for highly
similar visual concepts. To further overcome the scarcity of high-quality MCVC
data, we carefully establish a data construction pipeline, which enables
systematic collection of precise multi-concept video-entity data across diverse
concepts. A comprehensive benchmark is designed to validate the effectiveness
of our model from three critical dimensions: concept fidelity, identity
decoupling ability, and video generation quality across six different concept
composition scenarios. Extensive experiments demonstrate that our ConceptMaster
significantly outperforms previous approaches for this task, paving the way for
generating personalized and semantically accurate videos across multiple
concepts.
]]></content:encoded>
<pubDate>2025-01-08T18:59:01Z</pubDate>
</item>
<item>
<title>Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous
  Sensors via Language Grounding</title>
<link>http://arxiv.org/abs/2501.04693v1</link>
<guid>http://arxiv.org/abs/2501.04693v1</guid>
<content:encoded><![CDATA[
Interacting with the world is a multi-sensory experience: achieving effective
general-purpose interaction requires making use of all available modalities --
including vision, touch, and audio -- to fill in gaps from partial observation.
For example, when vision is occluded reaching into a bag, a robot should rely
on its senses of touch and sound. However, state-of-the-art generalist robot
policies are typically trained on large datasets to predict robot actions
solely from visual and proprioceptive observations. In this work, we propose
FuSe, a novel approach that enables finetuning visuomotor generalist policies
on heterogeneous sensor modalities for which large datasets are not readily
available by leveraging natural language as a common cross-modal grounding. We
combine a multimodal contrastive loss with a sensory-grounded language
generation loss to encode high-level semantics. In the context of robot
manipulation, we show that FuSe enables performing challenging tasks that
require reasoning jointly over modalities such as vision, touch, and sound in a
zero-shot setting, such as multimodal prompting, compositional cross-modal
prompting, and descriptions of objects it interacts with. We show that the same
recipe is applicable to widely different generalist policies, including both
diffusion-based generalist policies and large vision-language-action (VLA)
models. Extensive experiments in the real world show that FuSeis able to
increase success rates by over 20% compared to all considered baselines.
]]></content:encoded>
<pubDate>2025-01-08T18:57:33Z</pubDate>
</item>
<item>
<title>LargeAD: Large-Scale Cross-Sensor Data Pretraining for Autonomous
  Driving</title>
<link>http://arxiv.org/abs/2501.04005v1</link>
<guid>http://arxiv.org/abs/2501.04005v1</guid>
<content:encoded><![CDATA[
Recent advancements in vision foundation models (VFMs) have revolutionized
visual perception in 2D, yet their potential for 3D scene understanding,
particularly in autonomous driving applications, remains underexplored. In this
paper, we introduce LargeAD, a versatile and scalable framework designed for
large-scale 3D pretraining across diverse real-world driving datasets. Our
framework leverages VFMs to extract semantically rich superpixels from 2D
images, which are aligned with LiDAR point clouds to generate high-quality
contrastive samples. This alignment facilitates cross-modal representation
learning, enhancing the semantic consistency between 2D and 3D data. We
introduce several key innovations: i) VFM-driven superpixel generation for
detailed semantic representation, ii) a VFM-assisted contrastive learning
strategy to align multimodal features, iii) superpoint temporal consistency to
maintain stable representations across time, and iv) multi-source data
pretraining to generalize across various LiDAR configurations. Our approach
delivers significant performance improvements over state-of-the-art methods in
both linear probing and fine-tuning tasks for both LiDAR-based segmentation and
object detection. Extensive experiments on eleven large-scale multi-modal
datasets highlight our superior performance, demonstrating the adaptability,
efficiency, and robustness in real-world autonomous driving scenarios.
]]></content:encoded>
<pubDate>2025-01-07T18:59:59Z</pubDate>
</item>
<item>
<title>Automated Generation of Challenging Multiple-Choice Questions for Vision
  Language Model Evaluation</title>
<link>http://arxiv.org/abs/2501.03225v1</link>
<guid>http://arxiv.org/abs/2501.03225v1</guid>
<content:encoded><![CDATA[
The rapid development of vision language models (VLMs) demands rigorous and
reliable evaluation. However, current visual question answering (VQA)
benchmarks often depend on open-ended questions, making accurate evaluation
difficult due to the variability in natural language responses. To address
this, we introduce AutoConverter, an agentic framework that automatically
converts these open-ended questions into multiple-choice format, enabling
objective evaluation while reducing the costly question creation process. Our
experiments demonstrate that AutoConverter can generate correct and challenging
multiple-choice questions, with VLMs demonstrating consistently similar or
lower accuracy on these questions compared to human-created ones. Using
AutoConverter, we construct VMCBench, a benchmark created by transforming 20
existing VQA datasets into a unified multiple-choice format, totaling 9,018
questions. We comprehensively evaluate 33 state-of-the-art VLMs on VMCBench,
setting a new standard for scalable, consistent, and reproducible VLM
evaluation.
]]></content:encoded>
<pubDate>2025-01-06T18:57:31Z</pubDate>
</item>
<item>
<title>VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction</title>
<link>http://arxiv.org/abs/2501.01957v1</link>
<guid>http://arxiv.org/abs/2501.01957v1</guid>
<content:encoded><![CDATA[
Recent Multimodal Large Language Models (MLLMs) have typically focused on
integrating visual and textual modalities, with less emphasis placed on the
role of speech in enhancing interaction. However, speech plays a crucial role
in multimodal dialogue systems, and implementing high-performance in both
vision and speech tasks remains a significant challenge due to the fundamental
modality differences. In this paper, we propose a carefully designed
multi-stage training methodology that progressively trains LLM to understand
both visual and speech information, ultimately enabling fluent vision and
speech interaction. Our approach not only preserves strong vision-language
capacity, but also enables efficient speech-to-speech dialogue capabilities
without separate ASR and TTS modules, significantly accelerating multimodal
end-to-end response speed. By comparing our method against state-of-the-art
counterparts across benchmarks for image, video, and speech tasks, we
demonstrate that our model is equipped with both strong visual and speech
capabilities, making near real-time vision and speech interaction.
]]></content:encoded>
<pubDate>2025-01-03T18:59:52Z</pubDate>
</item>
<item>
<title>VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion
  Control</title>
<link>http://arxiv.org/abs/2501.01427v1</link>
<guid>http://arxiv.org/abs/2501.01427v1</guid>
<content:encoded><![CDATA[
Despite significant advancements in video generation, inserting a given
object into videos remains a challenging task. The difficulty lies in
preserving the appearance details of the reference object and accurately
modeling coherent motions at the same time. In this paper, we propose
VideoAnydoor, a zero-shot video object insertion framework with high-fidelity
detail preservation and precise motion control. Starting from a text-to-video
model, we utilize an ID extractor to inject the global identity and leverage a
box sequence to control the overall motion. To preserve the detailed appearance
and meanwhile support fine-grained motion control, we design a pixel warper. It
takes the reference image with arbitrary key-points and the corresponding
key-point trajectories as inputs. It warps the pixel details according to the
trajectories and fuses the warped features with the diffusion U-Net, thus
improving detail preservation and supporting users in manipulating the motion
trajectories. In addition, we propose a training strategy involving both videos
and static images with a reweight reconstruction loss to enhance insertion
quality. VideoAnydoor demonstrates significant superiority over existing
methods and naturally supports various downstream applications (e.g., talking
head generation, video virtual try-on, multi-region editing) without
task-specific fine-tuning.
]]></content:encoded>
<pubDate>2025-01-02T18:59:54Z</pubDate>
</item>
<item>
<title>Object-level Visual Prompts for Compositional Image Generation</title>
<link>http://arxiv.org/abs/2501.01424v1</link>
<guid>http://arxiv.org/abs/2501.01424v1</guid>
<content:encoded><![CDATA[
We introduce a method for composing object-level visual prompts within a
text-to-image diffusion model. Our approach addresses the task of generating
semantically coherent compositions across diverse scenes and styles, similar to
the versatility and expressiveness offered by text prompts. A key challenge in
this task is to preserve the identity of the objects depicted in the input
visual prompts, while also generating diverse compositions across different
images. To address this challenge, we introduce a new KV-mixed cross-attention
mechanism, in which keys and values are learned from distinct visual
representations. The keys are derived from an encoder with a small bottleneck
for layout control, whereas the values come from a larger bottleneck encoder
that captures fine-grained appearance details. By mixing keys and values from
these complementary sources, our model preserves the identity of the visual
prompts while supporting flexible variations in object arrangement, pose, and
composition. During inference, we further propose object-level compositional
guidance to improve the method's identity preservation and layout correctness.
Results show that our technique produces diverse scene compositions that
preserve the unique characteristics of each visual prompt, expanding the
creative potential of text-to-image generation.
]]></content:encoded>
<pubDate>2025-01-02T18:59:44Z</pubDate>
</item>
<item>
<title>DrivingGPT: Unifying Driving World Modeling and Planning with
  Multi-modal Autoregressive Transformers</title>
<link>http://arxiv.org/abs/2412.18607v1</link>
<guid>http://arxiv.org/abs/2412.18607v1</guid>
<content:encoded><![CDATA[
World model-based searching and planning are widely recognized as a promising
path toward human-level physical intelligence. However, current driving world
models primarily rely on video diffusion models, which specialize in visual
generation but lack the flexibility to incorporate other modalities like
action. In contrast, autoregressive transformers have demonstrated exceptional
capability in modeling multimodal data. Our work aims to unify both driving
model simulation and trajectory planning into a single sequence modeling
problem. We introduce a multimodal driving language based on interleaved image
and action tokens, and develop DrivingGPT to learn joint world modeling and
planning through standard next-token prediction. Our DrivingGPT demonstrates
strong performance in both action-conditioned video generation and end-to-end
planning, outperforming strong baselines on large-scale nuPlan and NAVSIM
benchmarks.
]]></content:encoded>
<pubDate>2024-12-24T18:59:37Z</pubDate>
</item>
<item>
<title>Decentralized Intelligence in GameFi: Embodied AI Agents and the
  Convergence of DeFi and Virtual Ecosystems</title>
<link>http://arxiv.org/abs/2412.18601v1</link>
<guid>http://arxiv.org/abs/2412.18601v1</guid>
<content:encoded><![CDATA[
In the rapidly evolving landscape of GameFi, a fusion of gaming and
decentralized finance (DeFi), there exists a critical need to enhance player
engagement and economic interaction within gaming ecosystems. Our GameFi
ecosystem aims to fundamentally transform this landscape by integrating
advanced embodied AI agents into GameFi platforms. These AI agents, developed
using cutting-edge large language models (LLMs), such as GPT-4 and Claude AI,
are capable of proactive, adaptive, and contextually rich interactions with
players. By going beyond traditional scripted responses, these agents become
integral participants in the game's narrative and economic systems, directly
influencing player strategies and in-game economies. We address the limitations
of current GameFi platforms, which often lack immersive AI interactions and
mechanisms for community engagement or creator monetization. Through the deep
integration of AI agents with blockchain technology, we establish a
consensus-driven, decentralized GameFi ecosystem. This ecosystem empowers
creators to monetize their contributions and fosters democratic collaboration
among players and creators. Furthermore, by embedding DeFi mechanisms into the
gaming experience, we enhance economic participation and provide new
opportunities for financial interactions within the game. Our approach enhances
player immersion and retention and advances the GameFi ecosystem by bridging
traditional gaming with Web3 technologies. By integrating sophisticated AI and
DeFi elements, we contribute to the development of more engaging, economically
robust, and community-centric gaming environments. This project represents a
significant advancement in the state-of-the-art in GameFi, offering insights
and methodologies that can be applied throughout the gaming industry.
]]></content:encoded>
<pubDate>2024-12-24T18:56:00Z</pubDate>
</item>
<item>
<title>ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation</title>
<link>http://arxiv.org/abs/2412.18600v1</link>
<guid>http://arxiv.org/abs/2412.18600v1</guid>
<content:encoded><![CDATA[
Human-scene interaction (HSI) generation is crucial for applications in
embodied AI, virtual reality, and robotics. While existing methods can
synthesize realistic human motions in 3D scenes and generate plausible
human-object interactions, they heavily rely on datasets containing paired 3D
scene and motion capture data, which are expensive and time-consuming to
collect across diverse environments and interactions. We present ZeroHSI, a
novel approach that enables zero-shot 4D human-scene interaction synthesis by
integrating video generation and neural human rendering. Our key insight is to
leverage the rich motion priors learned by state-of-the-art video generation
models, which have been trained on vast amounts of natural human movements and
interactions, and use differentiable rendering to reconstruct human-scene
interactions. ZeroHSI can synthesize realistic human motions in both static
scenes and environments with dynamic objects, without requiring any
ground-truth motion data. We evaluate ZeroHSI on a curated dataset of different
types of various indoor and outdoor scenes with different interaction prompts,
demonstrating its ability to generate diverse and contextually appropriate
human-scene interactions.
]]></content:encoded>
<pubDate>2024-12-24T18:55:38Z</pubDate>
</item>
<item>
<title>DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion
  Transformer for Tuning-Free Multi-Prompt Longer Video Generation</title>
<link>http://arxiv.org/abs/2412.18597v1</link>
<guid>http://arxiv.org/abs/2412.18597v1</guid>
<content:encoded><![CDATA[
Sora-like video generation models have achieved remarkable progress with a
Multi-Modal Diffusion Transformer MM-DiT architecture. However, the current
video generation models predominantly focus on single-prompt, struggling to
generate coherent scenes with multiple sequential prompts that better reflect
real-world dynamic scenarios. While some pioneering works have explored
multi-prompt video generation, they face significant challenges including
strict training data requirements, weak prompt following, and unnatural
transitions. To address these problems, we propose DiTCtrl, a training-free
multi-prompt video generation method under MM-DiT architectures for the first
time. Our key idea is to take the multi-prompt video generation task as
temporal video editing with smooth transitions. To achieve this goal, we first
analyze MM-DiT's attention mechanism, finding that the 3D full attention
behaves similarly to that of the cross/self-attention blocks in the UNet-like
diffusion models, enabling mask-guided precise semantic control across
different prompts with attention sharing for multi-prompt video generation.
Based on our careful design, the video generated by DiTCtrl achieves smooth
transitions and consistent object motion given multiple sequential prompts
without additional training. Besides, we also present MPVBench, a new benchmark
specially designed for multi-prompt video generation to evaluate the
performance of multi-prompt generation. Extensive experiments demonstrate that
our method achieves state-of-the-art performance without additional training.
]]></content:encoded>
<pubDate>2024-12-24T18:51:19Z</pubDate>
</item>
<item>
<title>ChatGarment: Garment Estimation, Generation and Editing via Large
  Language Models</title>
<link>http://arxiv.org/abs/2412.17811v1</link>
<guid>http://arxiv.org/abs/2412.17811v1</guid>
<content:encoded><![CDATA[
We introduce ChatGarment, a novel approach that leverages large
vision-language models (VLMs) to automate the estimation, generation, and
editing of 3D garments from images or text descriptions. Unlike previous
methods that struggle in real-world scenarios or lack interactive editing
capabilities, ChatGarment can estimate sewing patterns from in-the-wild images
or sketches, generate them from text descriptions, and edit garments based on
user instructions, all within an interactive dialogue. These sewing patterns
can then be draped into 3D garments, which are easily animatable and
simulatable. This is achieved by finetuning a VLM to directly generate a JSON
file that includes both textual descriptions of garment types and styles, as
well as continuous numerical attributes. This JSON file is then used to create
sewing patterns through a programming parametric model. To support this, we
refine the existing programming model, GarmentCode, by expanding its garment
type coverage and simplifying its structure for efficient VLM fine-tuning.
Additionally, we construct a large-scale dataset of image-to-sewing-pattern and
text-to-sewing-pattern pairs through an automated data pipeline. Extensive
evaluations demonstrate ChatGarment's ability to accurately reconstruct,
generate, and edit garments from multimodal inputs, highlighting its potential
to revolutionize workflows in fashion and gaming applications. Code and data
will be available at https://chatgarment.github.io/.
]]></content:encoded>
<pubDate>2024-12-23T18:59:28Z</pubDate>
</item>
<item>
<title>Large Motion Video Autoencoding with Cross-modal Video VAE</title>
<link>http://arxiv.org/abs/2412.17805v1</link>
<guid>http://arxiv.org/abs/2412.17805v1</guid>
<content:encoded><![CDATA[
Learning a robust video Variational Autoencoder (VAE) is essential for
reducing video redundancy and facilitating efficient video generation. Directly
applying image VAEs to individual frames in isolation can result in temporal
inconsistencies and suboptimal compression rates due to a lack of temporal
compression. Existing Video VAEs have begun to address temporal compression;
however, they often suffer from inadequate reconstruction performance. In this
paper, we present a novel and powerful video autoencoder capable of
high-fidelity video encoding. First, we observe that entangling spatial and
temporal compression by merely extending the image VAE to a 3D VAE can
introduce motion blur and detail distortion artifacts. Thus, we propose
temporal-aware spatial compression to better encode and decode the spatial
information. Additionally, we integrate a lightweight motion compression model
for further temporal compression. Second, we propose to leverage the textual
information inherent in text-to-video datasets and incorporate text guidance
into our model. This significantly enhances reconstruction quality,
particularly in terms of detail preservation and temporal stability. Third, we
further improve the versatility of our model through joint training on both
images and videos, which not only enhances reconstruction quality but also
enables the model to perform both image and video autoencoding. Extensive
evaluations against strong recent baselines demonstrate the superior
performance of our method. The project website can be found
at~\href{https://yzxing87.github.io/vae/}{https://yzxing87.github.io/vae/}.
]]></content:encoded>
<pubDate>2024-12-23T18:58:24Z</pubDate>
</item>
<item>
<title>Personalized Representation from Personalized Generation</title>
<link>http://arxiv.org/abs/2412.16156v1</link>
<guid>http://arxiv.org/abs/2412.16156v1</guid>
<content:encoded><![CDATA[
Modern vision models excel at general purpose downstream tasks. It is
unclear, however, how they may be used for personalized vision tasks, which are
both fine-grained and data-scarce. Recent works have successfully applied
synthetic data to general-purpose representation learning, while advances in
T2I diffusion models have enabled the generation of personalized images from
just a few real examples. Here, we explore a potential connection between these
ideas, and formalize the challenge of using personalized synthetic data to
learn personalized representations, which encode knowledge about an object of
interest and may be flexibly applied to any downstream task relating to the
target object. We introduce an evaluation suite for this challenge, including
reformulations of two existing datasets and a novel dataset explicitly
constructed for this purpose, and propose a contrastive learning approach that
makes creative use of image generators. We show that our method improves
personalized representation learning for diverse downstream tasks, from
recognition to segmentation, and analyze characteristics of image generation
approaches that are key to this gain.
]]></content:encoded>
<pubDate>2024-12-20T18:59:03Z</pubDate>
</item>
<item>
<title>A vector logic for extensional formal semantics</title>
<link>http://arxiv.org/abs/2412.16152v1</link>
<guid>http://arxiv.org/abs/2412.16152v1</guid>
<content:encoded><![CDATA[
This paper proves a homomorphism between extensional formal semantics and
distributional vector space semantics, demonstrating structural compatibility.
Formal semantics models meaning as reference, using logical structures to map
linguistic expressions to truth conditions, while distributional semantics
represents meaning through word vectors derived from contextual usage. By
constructing injective mappings that preserve semantic relationships, we show
that every semantic function in an extensional model corresponds to a
compatible vector space operation. This result respects compositionality and
extends to function compositions, constant interpretations, and $n$-ary
relations. Rather than pursuing unification, we highlight a mathematical
foundation for hybrid cognitive models that integrate symbolic and sub-symbolic
reasoning and semantics. These findings support multimodal language processing,
aligning `meaning as reference' (Frege, Tarski) with `meaning as use'
(Wittgenstein, Firth).
]]></content:encoded>
<pubDate>2024-12-20T18:54:48Z</pubDate>
</item>
<item>
<title>Flowing from Words to Pixels: A Framework for Cross-Modality Evolution</title>
<link>http://arxiv.org/abs/2412.15213v1</link>
<guid>http://arxiv.org/abs/2412.15213v1</guid>
<content:encoded><![CDATA[
Diffusion models, and their generalization, flow matching, have had a
remarkable impact on the field of media generation. Here, the conventional
approach is to learn the complex mapping from a simple source distribution of
Gaussian noise to the target media distribution. For cross-modal tasks such as
text-to-image generation, this same mapping from noise to image is learnt
whilst including a conditioning mechanism in the model. One key and thus far
relatively unexplored feature of flow matching is that, unlike Diffusion
models, they are not constrained for the source distribution to be noise.
Hence, in this paper, we propose a paradigm shift, and ask the question of
whether we can instead train flow matching models to learn a direct mapping
from the distribution of one modality to the distribution of another, thus
obviating the need for both the noise distribution and conditioning mechanism.
We present a general and simple framework, CrossFlow, for cross-modal flow
matching. We show the importance of applying Variational Encoders to the input
data, and introduce a method to enable Classifier-free guidance. Surprisingly,
for text-to-image, CrossFlow with a vanilla transformer without cross attention
slightly outperforms standard flow matching, and we show that it scales better
with training steps and model size, while also allowing for interesting latent
arithmetic which results in semantically meaningful edits in the output space.
To demonstrate the generalizability of our approach, we also show that
CrossFlow is on par with or outperforms the state-of-the-art for various
cross-modal / intra-modal mapping tasks, viz. image captioning, depth
estimation, and image super-resolution. We hope this paper contributes to
accelerating progress in cross-modal media generation.
]]></content:encoded>
<pubDate>2024-12-19T18:59:56Z</pubDate>
</item>
<item>
<title>Autoregressive Video Generation without Vector Quantization</title>
<link>http://arxiv.org/abs/2412.14169v1</link>
<guid>http://arxiv.org/abs/2412.14169v1</guid>
<content:encoded><![CDATA[
This paper presents a novel approach that enables autoregressive video
generation with high efficiency. We propose to reformulate the video generation
problem as a non-quantized autoregressive modeling of temporal frame-by-frame
prediction and spatial set-by-set prediction. Unlike raster-scan prediction in
prior autoregressive models or joint distribution modeling of fixed-length
tokens in diffusion models, our approach maintains the causal property of
GPT-style models for flexible in-context capabilities, while leveraging
bidirectional modeling within individual frames for efficiency. With the
proposed approach, we train a novel video autoregressive model without vector
quantization, termed NOVA. Our results demonstrate that NOVA surpasses prior
autoregressive video models in data efficiency, inference speed, visual
fidelity, and video fluency, even with a much smaller model capacity, i.e.,
0.6B parameters. NOVA also outperforms state-of-the-art image diffusion models
in text-to-image generation tasks, with a significantly lower training cost.
Additionally, NOVA generalizes well across extended video durations and enables
diverse zero-shot applications in one unified model. Code and models are
publicly available at https://github.com/baaivision/NOVA.
]]></content:encoded>
<pubDate>2024-12-18T18:59:53Z</pubDate>
</item>
<item>
<title>E-CAR: Efficient Continuous Autoregressive Image Generation via
  Multistage Modeling</title>
<link>http://arxiv.org/abs/2412.14170v1</link>
<guid>http://arxiv.org/abs/2412.14170v1</guid>
<content:encoded><![CDATA[
Recent advances in autoregressive (AR) models with continuous tokens for
image generation show promising results by eliminating the need for discrete
tokenization. However, these models face efficiency challenges due to their
sequential token generation nature and reliance on computationally intensive
diffusion-based sampling. We present ECAR (Efficient Continuous Auto-Regressive
Image Generation via Multistage Modeling), an approach that addresses these
limitations through two intertwined innovations: (1) a stage-wise continuous
token generation strategy that reduces computational complexity and provides
progressively refined token maps as hierarchical conditions, and (2) a
multistage flow-based distribution modeling method that transforms only
partial-denoised distributions at each stage comparing to complete denoising in
normal diffusion models. Holistically, ECAR operates by generating tokens at
increasing resolutions while simultaneously denoising the image at each stage.
This design not only reduces token-to-image transformation cost by a factor of
the stage number but also enables parallel processing at the token level. Our
approach not only enhances computational efficiency but also aligns naturally
with image generation principles by operating in continuous token space and
following a hierarchical generation process from coarse to fine details.
Experimental results demonstrate that ECAR achieves comparable image quality to
DiT Peebles & Xie [2023] while requiring 10$\times$ FLOPs reduction and
5$\times$ speedup to generate a 256$\times$256 image.
]]></content:encoded>
<pubDate>2024-12-18T18:59:53Z</pubDate>
</item>
<item>
<title>FashionComposer: Compositional Fashion Image Generation</title>
<link>http://arxiv.org/abs/2412.14168v1</link>
<guid>http://arxiv.org/abs/2412.14168v1</guid>
<content:encoded><![CDATA[
We present FashionComposer for compositional fashion image generation. Unlike
previous methods, FashionComposer is highly flexible. It takes multi-modal
input (i.e., text prompt, parametric human model, garment image, and face
image) and supports personalizing the appearance, pose, and figure of the human
and assigning multiple garments in one pass. To achieve this, we first develop
a universal framework capable of handling diverse input modalities. We
construct scaled training data to enhance the model's robust compositional
capabilities. To accommodate multiple reference images (garments and faces)
seamlessly, we organize these references in a single image as an "asset
library" and employ a reference UNet to extract appearance features. To inject
the appearance features into the correct pixels in the generated result, we
propose subject-binding attention. It binds the appearance features from
different "assets" with the corresponding text features. In this way, the model
could understand each asset according to their semantics, supporting arbitrary
numbers and types of reference images. As a comprehensive solution,
FashionComposer also supports many other applications like human album
generation, diverse virtual try-on tasks, etc.
]]></content:encoded>
<pubDate>2024-12-18T18:59:50Z</pubDate>
</item>
<item>
<title>VideoDPO: Omni-Preference Alignment for Video Diffusion Generation</title>
<link>http://arxiv.org/abs/2412.14167v1</link>
<guid>http://arxiv.org/abs/2412.14167v1</guid>
<content:encoded><![CDATA[
Recent progress in generative diffusion models has greatly advanced
text-to-video generation. While text-to-video models trained on large-scale,
diverse datasets can produce varied outputs, these generations often deviate
from user preferences, highlighting the need for preference alignment on
pre-trained models. Although Direct Preference Optimization (DPO) has
demonstrated significant improvements in language and image generation, we
pioneer its adaptation to video diffusion models and propose a VideoDPO
pipeline by making several key adjustments. Unlike previous image alignment
methods that focus solely on either (i) visual quality or (ii) semantic
alignment between text and videos, we comprehensively consider both dimensions
and construct a preference score accordingly, which we term the OmniScore. We
design a pipeline to automatically collect preference pair data based on the
proposed OmniScore and discover that re-weighting these pairs based on the
score significantly impacts overall preference alignment. Our experiments
demonstrate substantial improvements in both visual quality and semantic
alignment, ensuring that no preference aspect is neglected. Code and data will
be shared at https://videodpo.github.io/.
]]></content:encoded>
<pubDate>2024-12-18T18:59:49Z</pubDate>
</item>
<item>
<title>TheAgentCompany: Benchmarking LLM Agents on Consequential Real World
  Tasks</title>
<link>http://arxiv.org/abs/2412.14161v1</link>
<guid>http://arxiv.org/abs/2412.14161v1</guid>
<content:encoded><![CDATA[
We interact with computers on an everyday basis, be it in everyday life or
work, and many aspects of work can be done entirely with access to a computer
and the Internet. At the same time, thanks to improvements in large language
models (LLMs), there has also been a rapid development in AI agents that
interact with and affect change in their surrounding environments. But how
performant are AI agents at helping to accelerate or even autonomously perform
work-related tasks? The answer to this question has important implications for
both industry looking to adopt AI into their workflows, and for economic policy
to understand the effects that adoption of AI may have on the labor market. To
measure the progress of these LLM agents' performance on performing real-world
professional tasks, in this paper, we introduce TheAgentCompany, an extensible
benchmark for evaluating AI agents that interact with the world in similar ways
to those of a digital worker: by browsing the Web, writing code, running
programs, and communicating with other coworkers. We build a self-contained
environment with internal web sites and data that mimics a small software
company environment, and create a variety of tasks that may be performed by
workers in such a company. We test baseline agents powered by both closed
API-based and open-weights language models (LMs), and find that with the most
competitive agent, 24% of the tasks can be completed autonomously. This paints
a nuanced picture on task automation with LM agents -- in a setting simulating
a real workplace, a good portion of simpler tasks could be solved autonomously,
but more difficult long-horizon tasks are still beyond the reach of current
systems.
]]></content:encoded>
<pubDate>2024-12-18T18:55:40Z</pubDate>
</item>
<item>
<title>Proposer-Agent-Evaluator(PAE): Autonomous Skill Discovery For Foundation
  Model Internet Agents</title>
<link>http://arxiv.org/abs/2412.13194v1</link>
<guid>http://arxiv.org/abs/2412.13194v1</guid>
<content:encoded><![CDATA[
The vision of a broadly capable and goal-directed agent, such as an
Internet-browsing agent in the digital world and a household humanoid in the
physical world, has rapidly advanced, thanks to the generalization capability
of foundation models. Such a generalist agent needs to have a large and diverse
skill repertoire, such as finding directions between two travel locations and
buying specific items from the Internet. If each skill needs to be specified
manually through a fixed set of human-annotated instructions, the agent's skill
repertoire will necessarily be limited due to the quantity and diversity of
human-annotated instructions. In this work, we address this challenge by
proposing Proposer-Agent-Evaluator, an effective learning system that enables
foundation model agents to autonomously discover and practice skills in the
wild. At the heart of PAE is a context-aware task proposer that autonomously
proposes tasks for the agent to practice with context information of the
environment such as user demos or even just the name of the website itself for
Internet-browsing agents. Then, the agent policy attempts those tasks with
thoughts and actual grounded operations in the real world with resulting
trajectories evaluated by an autonomous VLM-based success evaluator. The
success evaluation serves as the reward signal for the agent to refine its
policies through RL. We validate PAE on challenging vision-based web
navigation, using both real-world and self-hosted websites from WebVoyager and
WebArena.To the best of our knowledge, this work represents the first effective
learning system to apply autonomous task proposal with RL for agents that
generalizes real-world human-annotated benchmarks with SOTA performances. Our
open-source checkpoints and code can be found in https://yanqval.github.io/PAE/
]]></content:encoded>
<pubDate>2024-12-17T18:59:50Z</pubDate>
</item>
<item>
<title>GaussTR: Foundation Model-Aligned Gaussian Transformer for
  Self-Supervised 3D Spatial Understanding</title>
<link>http://arxiv.org/abs/2412.13193v1</link>
<guid>http://arxiv.org/abs/2412.13193v1</guid>
<content:encoded><![CDATA[
3D Semantic Occupancy Prediction is fundamental for spatial understanding as
it provides a comprehensive semantic cognition of surrounding environments.
However, prevalent approaches primarily rely on extensive labeled data and
computationally intensive voxel-based modeling, restricting the scalability and
generalizability of 3D representation learning. In this paper, we introduce
GaussTR, a novel Gaussian Transformer that leverages alignment with foundation
models to advance self-supervised 3D spatial understanding. GaussTR adopts a
Transformer architecture to predict sparse sets of 3D Gaussians that represent
scenes in a feed-forward manner. Through aligning rendered Gaussian features
with diverse knowledge from pre-trained foundation models, GaussTR facilitates
the learning of versatile 3D representations and enables open-vocabulary
occupancy prediction without explicit annotations. Empirical evaluations on the
Occ3D-nuScenes dataset showcase GaussTR's state-of-the-art zero-shot
performance, achieving 11.70 mIoU while reducing training duration by
approximately 50%. These experimental results highlight the significant
potential of GaussTR for scalable and holistic 3D spatial understanding, with
promising implications for autonomous driving and embodied agents. Code is
available at https://github.com/hustvl/GaussTR.
]]></content:encoded>
<pubDate>2024-12-17T18:59:46Z</pubDate>
</item>
<item>
<title>MotionBridge: Dynamic Video Inbetweening with Flexible Controls</title>
<link>http://arxiv.org/abs/2412.13190v1</link>
<guid>http://arxiv.org/abs/2412.13190v1</guid>
<content:encoded><![CDATA[
By generating plausible and smooth transitions between two image frames,
video inbetweening is an essential tool for video editing and long video
synthesis. Traditional works lack the capability to generate complex large
motions. While recent video generation techniques are powerful in creating
high-quality results, they often lack fine control over the details of
intermediate frames, which can lead to results that do not align with the
creative mind. We introduce MotionBridge, a unified video inbetweening
framework that allows flexible controls, including trajectory strokes,
keyframes, masks, guide pixels, and text. However, learning such multi-modal
controls in a unified framework is a challenging task. We thus design two
generators to extract the control signal faithfully and encode feature through
dual-branch embedders to resolve ambiguities. We further introduce a curriculum
training strategy to smoothly learn various controls. Extensive qualitative and
quantitative experiments have demonstrated that such multi-modal controls
enable a more dynamic, customizable, and contextually accurate visual
narrative.
]]></content:encoded>
<pubDate>2024-12-17T18:59:33Z</pubDate>
</item>
<item>
<title>Instruction-based Image Manipulation by Watching How Things Move</title>
<link>http://arxiv.org/abs/2412.12087v1</link>
<guid>http://arxiv.org/abs/2412.12087v1</guid>
<content:encoded><![CDATA[
This paper introduces a novel dataset construction pipeline that samples
pairs of frames from videos and uses multimodal large language models (MLLMs)
to generate editing instructions for training instruction-based image
manipulation models. Video frames inherently preserve the identity of subjects
and scenes, ensuring consistent content preservation during editing.
Additionally, video data captures diverse, natural dynamics-such as non-rigid
subject motion and complex camera movements-that are difficult to model
otherwise, making it an ideal source for scalable dataset construction. Using
this approach, we create a new dataset to train InstructMove, a model capable
of instruction-based complex manipulations that are difficult to achieve with
synthetically generated datasets. Our model demonstrates state-of-the-art
performance in tasks such as adjusting subject poses, rearranging elements, and
altering camera perspectives.
]]></content:encoded>
<pubDate>2024-12-16T18:56:17Z</pubDate>
</item>
<item>
<title>Causal Diffusion Transformers for Generative Modeling</title>
<link>http://arxiv.org/abs/2412.12095v1</link>
<guid>http://arxiv.org/abs/2412.12095v1</guid>
<content:encoded><![CDATA[
We introduce Causal Diffusion as the autoregressive (AR) counterpart of
Diffusion models. It is a next-token(s) forecasting framework that is friendly
to both discrete and continuous modalities and compatible with existing
next-token prediction models like LLaMA and GPT. While recent works attempt to
combine diffusion with AR models, we show that introducing sequential
factorization to a diffusion model can substantially improve its performance
and enables a smooth transition between AR and diffusion generation modes.
Hence, we propose CausalFusion - a decoder-only transformer that
dual-factorizes data across sequential tokens and diffusion noise levels,
leading to state-of-the-art results on the ImageNet generation benchmark while
also enjoying the AR advantage of generating an arbitrary number of tokens for
in-context reasoning. We further demonstrate CausalFusion's multimodal
capabilities through a joint image generation and captioning model, and
showcase CausalFusion's ability for zero-shot in-context image manipulations.
We hope that this work could provide the community with a fresh perspective on
training multimodal models over discrete and continuous data.
]]></content:encoded>
<pubDate>2024-12-16T18:59:29Z</pubDate>
</item>
<item>
<title>A Grounded Typology of Word Classes</title>
<link>http://arxiv.org/abs/2412.10369v1</link>
<guid>http://arxiv.org/abs/2412.10369v1</guid>
<content:encoded><![CDATA[
We propose a grounded approach to meaning in language typology. We treat data
from perceptual modalities, such as images, as a language-agnostic
representation of meaning. Hence, we can quantify the function--form
relationship between images and captions across languages. Inspired by
information theory, we define "groundedness", an empirical measure of
contextual semantic contentfulness (formulated as a difference in surprisal)
which can be computed with multilingual multimodal language models. As a proof
of concept, we apply this measure to the typology of word classes. Our measure
captures the contentfulness asymmetry between functional (grammatical) and
lexical (content) classes across languages, but contradicts the view that
functional classes do not convey content. Moreover, we find universal trends in
the hierarchy of groundedness (e.g., nouns > adjectives > verbs), and show that
our measure partly correlates with psycholinguistic concreteness norms in
English. We release a dataset of groundedness scores for 30 languages. Our
results suggest that the grounded typology approach can provide quantitative
evidence about semantic function in language.
]]></content:encoded>
<pubDate>2024-12-13T18:58:48Z</pubDate>
</item>
<item>
<title>OP-LoRA: The Blessing of Dimensionality</title>
<link>http://arxiv.org/abs/2412.10362v1</link>
<guid>http://arxiv.org/abs/2412.10362v1</guid>
<content:encoded><![CDATA[
Low-rank adapters enable fine-tuning of large models with only a small number
of parameters, thus reducing storage costs and minimizing the risk of
catastrophic forgetting. However, they often pose optimization challenges, with
poor convergence. To overcome these challenges, we introduce an
over-parameterized approach that accelerates training without increasing
inference costs. This method reparameterizes low-rank adaptation by employing a
separate MLP and learned embedding for each layer. The learned embedding is
input to the MLP, which generates the adapter parameters. Such
overparamaterization has been shown to implicitly function as an adaptive
learning rate and momentum, accelerating optimization. At inference time, the
MLP can be discarded, leaving behind a standard low-rank adapter. To study the
effect of MLP overparameterization on a small yet difficult proxy task, we
implement it for matrix factorization, and find it achieves faster convergence
and lower final loss. Extending this approach to larger-scale tasks, we observe
consistent performance gains across domains. We achieve improvements in
vision-language tasks and especially notable increases in image generation,
with CMMD scores improving by up to 15 points.
]]></content:encoded>
<pubDate>2024-12-13T18:55:19Z</pubDate>
</item>
<item>
<title>Doe-1: Closed-Loop Autonomous Driving with Large World Model</title>
<link>http://arxiv.org/abs/2412.09627v1</link>
<guid>http://arxiv.org/abs/2412.09627v1</guid>
<content:encoded><![CDATA[
End-to-end autonomous driving has received increasing attention due to its
potential to learn from large amounts of data. However, most existing methods
are still open-loop and suffer from weak scalability, lack of high-order
interactions, and inefficient decision-making. In this paper, we explore a
closed-loop framework for autonomous driving and propose a large Driving wOrld
modEl (Doe-1) for unified perception, prediction, and planning. We formulate
autonomous driving as a next-token generation problem and use multi-modal
tokens to accomplish different tasks. Specifically, we use free-form texts
(i.e., scene descriptions) for perception and generate future predictions
directly in the RGB space with image tokens. For planning, we employ a
position-aware tokenizer to effectively encode action into discrete tokens. We
train a multi-modal transformer to autoregressively generate perception,
prediction, and planning tokens in an end-to-end and unified manner.
Experiments on the widely used nuScenes dataset demonstrate the effectiveness
of Doe-1 in various tasks including visual question-answering,
action-conditioned video generation, and motion planning. Code:
https://github.com/wzzheng/Doe.
]]></content:encoded>
<pubDate>2024-12-12T18:59:59Z</pubDate>
</item>
<item>
<title>GenEx: Generating an Explorable World</title>
<link>http://arxiv.org/abs/2412.09624v1</link>
<guid>http://arxiv.org/abs/2412.09624v1</guid>
<content:encoded><![CDATA[
Understanding, navigating, and exploring the 3D physical real world has long
been a central challenge in the development of artificial intelligence. In this
work, we take a step toward this goal by introducing GenEx, a system capable of
planning complex embodied world exploration, guided by its generative
imagination that forms priors (expectations) about the surrounding
environments. GenEx generates an entire 3D-consistent imaginative environment
from as little as a single RGB image, bringing it to life through panoramic
video streams. Leveraging scalable 3D world data curated from Unreal Engine,
our generative model is rounded in the physical world. It captures a continuous
360-degree environment with little effort, offering a boundless landscape for
AI agents to explore and interact with. GenEx achieves high-quality world
generation, robust loop consistency over long trajectories, and demonstrates
strong 3D capabilities such as consistency and active 3D mapping. Powered by
generative imagination of the world, GPT-assisted agents are equipped to
perform complex embodied tasks, including both goal-agnostic exploration and
goal-driven navigation. These agents utilize predictive expectation regarding
unseen parts of the physical world to refine their beliefs, simulate different
outcomes based on potential decisions, and make more informed choices. In
summary, we demonstrate that GenEx provides a transformative platform for
advancing embodied AI in imaginative spaces and brings potential for extending
these capabilities to real-world exploration.
]]></content:encoded>
<pubDate>2024-12-12T18:59:57Z</pubDate>
</item>
<item>
<title>OmniDrag: Enabling Motion Control for Omnidirectional Image-to-Video
  Generation</title>
<link>http://arxiv.org/abs/2412.09623v1</link>
<guid>http://arxiv.org/abs/2412.09623v1</guid>
<content:encoded><![CDATA[
As virtual reality gains popularity, the demand for controllable creation of
immersive and dynamic omnidirectional videos (ODVs) is increasing. While
previous text-to-ODV generation methods achieve impressive results, they
struggle with content inaccuracies and inconsistencies due to reliance solely
on textual inputs. Although recent motion control techniques provide
fine-grained control for video generation, directly applying these methods to
ODVs often results in spatial distortion and unsatisfactory performance,
especially with complex spherical motions. To tackle these challenges, we
propose OmniDrag, the first approach enabling both scene- and object-level
motion control for accurate, high-quality omnidirectional image-to-video
generation. Building on pretrained video diffusion models, we introduce an
omnidirectional control module, which is jointly fine-tuned with temporal
attention layers to effectively handle complex spherical motion. In addition,
we develop a novel spherical motion estimator that accurately extracts
motion-control signals and allows users to perform drag-style ODV generation by
simply drawing handle and target points. We also present a new dataset, named
Move360, addressing the scarcity of ODV data with large scene and object
motions. Experiments demonstrate the significant superiority of OmniDrag in
achieving holistic scene-level and fine-grained object-level control for ODV
generation. The project page is available at
https://lwq20020127.github.io/OmniDrag.
]]></content:encoded>
<pubDate>2024-12-12T18:59:56Z</pubDate>
</item>
<item>
<title>LoRACLR: Contrastive Adaptation for Customization of Diffusion Models</title>
<link>http://arxiv.org/abs/2412.09622v1</link>
<guid>http://arxiv.org/abs/2412.09622v1</guid>
<content:encoded><![CDATA[
Recent advances in text-to-image customization have enabled high-fidelity,
context-rich generation of personalized images, allowing specific concepts to
appear in a variety of scenarios. However, current methods struggle with
combining multiple personalized models, often leading to attribute entanglement
or requiring separate training to preserve concept distinctiveness. We present
LoRACLR, a novel approach for multi-concept image generation that merges
multiple LoRA models, each fine-tuned for a distinct concept, into a single,
unified model without additional individual fine-tuning. LoRACLR uses a
contrastive objective to align and merge the weight spaces of these models,
ensuring compatibility while minimizing interference. By enforcing distinct yet
cohesive representations for each concept, LoRACLR enables efficient, scalable
model composition for high-quality, multi-concept image synthesis. Our results
highlight the effectiveness of LoRACLR in accurately merging multiple concepts,
advancing the capabilities of personalized image generation.
]]></content:encoded>
<pubDate>2024-12-12T18:59:55Z</pubDate>
</item>
<item>
<title>EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via
  Multimodal LLM</title>
<link>http://arxiv.org/abs/2412.09618v1</link>
<guid>http://arxiv.org/abs/2412.09618v1</guid>
<content:encoded><![CDATA[
Significant achievements in personalization of diffusion models have been
witnessed. Conventional tuning-free methods mostly encode multiple reference
images by averaging their image embeddings as the injection condition, but such
an image-independent operation cannot perform interaction among images to
capture consistent visual elements within multiple references. Although the
tuning-based Low-Rank Adaptation (LoRA) can effectively extract consistent
elements within multiple images through the training process, it necessitates
specific finetuning for each distinct image group. This paper introduces
EasyRef, a novel plug-and-play adaptation method that enables diffusion models
to be conditioned on multiple reference images and the text prompt. To
effectively exploit consistent visual elements within multiple images, we
leverage the multi-image comprehension and instruction-following capabilities
of the multimodal large language model (MLLM), prompting it to capture
consistent visual elements based on the instruction. Besides, injecting the
MLLM's representations into the diffusion process through adapters can easily
generalize to unseen domains, mining the consistent visual elements within
unseen data. To mitigate computational costs and enhance fine-grained detail
preservation, we introduce an efficient reference aggregation strategy and a
progressive training scheme. Finally, we introduce MRBench, a new
multi-reference image generation benchmark. Experimental results demonstrate
EasyRef surpasses both tuning-free methods like IP-Adapter and tuning-based
methods like LoRA, achieving superior aesthetic quality and robust zero-shot
generalization across diverse domains.
]]></content:encoded>
<pubDate>2024-12-12T18:59:48Z</pubDate>
</item>
<item>
<title>GPD-1: Generative Pre-training for Driving</title>
<link>http://arxiv.org/abs/2412.08643v1</link>
<guid>http://arxiv.org/abs/2412.08643v1</guid>
<content:encoded><![CDATA[
Modeling the evolutions of driving scenarios is important for the evaluation
and decision-making of autonomous driving systems. Most existing methods focus
on one aspect of scene evolution such as map generation, motion prediction, and
trajectory planning. In this paper, we propose a unified Generative
Pre-training for Driving (GPD-1) model to accomplish all these tasks altogether
without additional fine-tuning. We represent each scene with ego, agent, and
map tokens and formulate autonomous driving as a unified token generation
problem. We adopt the autoregressive transformer architecture and use a
scene-level attention mask to enable intra-scene bi-directional interactions.
For the ego and agent tokens, we propose a hierarchical positional tokenizer to
effectively encode both 2D positions and headings. For the map tokens, we train
a map vector-quantized autoencoder to efficiently compress ego-centric semantic
maps into discrete tokens. We pre-train our GPD-1 on the large-scale nuPlan
dataset and conduct extensive experiments to evaluate its effectiveness. With
different prompts, our GPD-1 successfully generalizes to various tasks without
finetuning, including scene generation, traffic simulation, closed-loop
simulation, map prediction, and motion planning. Code:
https://github.com/wzzheng/GPD.
]]></content:encoded>
<pubDate>2024-12-11T18:59:51Z</pubDate>
</item>
<item>
<title>Generative Semantic Communication: Architectures, Technologies, and
  Applications</title>
<link>http://arxiv.org/abs/2412.08642v1</link>
<guid>http://arxiv.org/abs/2412.08642v1</guid>
<content:encoded><![CDATA[
This paper delves into the applications of generative artificial intelligence
(GAI) in semantic communication (SemCom) and presents a thorough study. Three
popular SemCom systems enabled by classical GAI models are first introduced,
including variational autoencoders, generative adversarial networks, and
diffusion models. For each system, the fundamental concept of the GAI model,
the corresponding SemCom architecture, and the associated literature review of
recent efforts are elucidated. Then, a novel generative SemCom system is
proposed by incorporating the cutting-edge GAI technology-large language models
(LLMs). This system features two LLM-based AI agents at both the transmitter
and receiver, serving as "brains" to enable powerful information understanding
and content regeneration capabilities, respectively. This innovative design
allows the receiver to directly generate the desired content, instead of
recovering the bit stream, based on the coded semantic information conveyed by
the transmitter. Therefore, it shifts the communication mindset from
"information recovery" to "information regeneration" and thus ushers in a new
era of generative SemCom. A case study on point-to-point video retrieval is
presented to demonstrate the superiority of the proposed generative SemCom
system, showcasing a 99.98% reduction in communication overhead and a 53%
improvement in retrieval accuracy compared to the traditional communication
system. Furthermore, four typical application scenarios for generative SemCom
are delineated, followed by a discussion of three open issues warranting future
investigation. In a nutshell, this paper provides a holistic set of guidelines
for applying GAI in SemCom, paving the way for the efficient implementation of
generative SemCom in future wireless networks.
]]></content:encoded>
<pubDate>2024-12-11T18:59:50Z</pubDate>
</item>
<item>
<title>Fast Prompt Alignment for Text-to-Image Generation</title>
<link>http://arxiv.org/abs/2412.08639v1</link>
<guid>http://arxiv.org/abs/2412.08639v1</guid>
<content:encoded><![CDATA[
Text-to-image generation has advanced rapidly, yet aligning complex textual
prompts with generated visuals remains challenging, especially with intricate
object relationships and fine-grained details. This paper introduces Fast
Prompt Alignment (FPA), a prompt optimization framework that leverages a
one-pass approach, enhancing text-to-image alignment efficiency without the
iterative overhead typical of current methods like OPT2I. FPA uses large
language models (LLMs) for single-iteration prompt paraphrasing, followed by
fine-tuning or in-context learning with optimized prompts to enable real-time
inference, reducing computational demands while preserving alignment fidelity.
Extensive evaluations on the COCO Captions and PartiPrompts datasets
demonstrate that FPA achieves competitive text-image alignment scores at a
fraction of the processing time, as validated through both automated metrics
(TIFA, VQA) and human evaluation. A human study with expert annotators further
reveals a strong correlation between human alignment judgments and automated
scores, underscoring the robustness of FPA's improvements. The proposed method
showcases a scalable, efficient alternative to iterative prompt optimization,
enabling broader applicability in real-time, high-demand settings. The codebase
is provided to facilitate further research:
https://github.com/tiktok/fast_prompt_alignment
]]></content:encoded>
<pubDate>2024-12-11T18:58:41Z</pubDate>
</item>
<item>
<title>UniReal: Universal Image Generation and Editing via Learning Real-world
  Dynamics</title>
<link>http://arxiv.org/abs/2412.07774v1</link>
<guid>http://arxiv.org/abs/2412.07774v1</guid>
<content:encoded><![CDATA[
We introduce UniReal, a unified framework designed to address various image
generation and editing tasks. Existing solutions often vary by tasks, yet share
fundamental principles: preserving consistency between inputs and outputs while
capturing visual variations. Inspired by recent video generation models that
effectively balance consistency and variation across frames, we propose a
unifying approach that treats image-level tasks as discontinuous video
generation. Specifically, we treat varying numbers of input and output images
as frames, enabling seamless support for tasks such as image generation,
editing, customization, composition, etc. Although designed for image-level
tasks, we leverage videos as a scalable source for universal supervision.
UniReal learns world dynamics from large-scale videos, demonstrating advanced
capability in handling shadows, reflections, pose variation, and object
interaction, while also exhibiting emergent capability for novel applications.
]]></content:encoded>
<pubDate>2024-12-10T18:59:55Z</pubDate>
</item>
<item>
<title>BiMediX2: Bio-Medical EXpert LMM for Diverse Medical Modalities</title>
<link>http://arxiv.org/abs/2412.07769v1</link>
<guid>http://arxiv.org/abs/2412.07769v1</guid>
<content:encoded><![CDATA[
This paper introduces BiMediX2, a bilingual (Arabic-English) Bio-Medical
EXpert Large Multimodal Model (LMM) with a unified architecture that integrates
text and visual modalities, enabling advanced image understanding and medical
applications. BiMediX2 leverages the Llama3.1 architecture and integrates text
and visual capabilities to facilitate seamless interactions in both English and
Arabic, supporting text-based inputs and multi-turn conversations involving
medical images. The model is trained on an extensive bilingual healthcare
dataset consisting of 1.6M samples of diverse medical interactions for both
text and image modalities, mixed in Arabic and English. We also propose the
first bilingual GPT-4o based medical LMM benchmark named BiMed-MBench. BiMediX2
is benchmarked on both text-based and image-based tasks, achieving
state-of-the-art performance across several medical benchmarks. It outperforms
recent state-of-the-art models in medical LLM evaluation benchmarks. Our model
also sets a new benchmark in multimodal medical evaluations with over 9%
improvement in English and over 20% in Arabic evaluations. Additionally, it
surpasses GPT-4 by around 9% in UPHILL factual accuracy evaluations and excels
in various medical Visual Question Answering, Report Generation, and Report
Summarization tasks. The project page including source code and the trained
model, is available at https://github.com/mbzuai-oryx/BiMediX2.
]]></content:encoded>
<pubDate>2024-12-10T18:59:35Z</pubDate>
</item>
<item>
<title>Tactile DreamFusion: Exploiting Tactile Sensing for 3D Generation</title>
<link>http://arxiv.org/abs/2412.06785v1</link>
<guid>http://arxiv.org/abs/2412.06785v1</guid>
<content:encoded><![CDATA[
3D generation methods have shown visually compelling results powered by
diffusion image priors. However, they often fail to produce realistic geometric
details, resulting in overly smooth surfaces or geometric details inaccurately
baked in albedo maps. To address this, we introduce a new method that
incorporates touch as an additional modality to improve the geometric details
of generated 3D assets. We design a lightweight 3D texture field to synthesize
visual and tactile textures, guided by 2D diffusion model priors on both visual
and tactile domains. We condition the visual texture generation on
high-resolution tactile normals and guide the patch-based tactile texture
refinement with a customized TextureDreambooth. We further present a multi-part
generation pipeline that enables us to synthesize different textures across
various regions. To our knowledge, we are the first to leverage high-resolution
tactile sensing to enhance geometric details for 3D generation tasks. We
evaluate our method in both text-to-3D and image-to-3D settings. Our
experiments demonstrate that our method provides customized and realistic fine
geometric textures while maintaining accurate alignment between two modalities
of vision and touch.
]]></content:encoded>
<pubDate>2024-12-09T18:59:45Z</pubDate>
</item>
<item>
<title>Stag-1: Towards Realistic 4D Driving Simulation with Video Generation
  Model</title>
<link>http://arxiv.org/abs/2412.05280v1</link>
<guid>http://arxiv.org/abs/2412.05280v1</guid>
<content:encoded><![CDATA[
4D driving simulation is essential for developing realistic autonomous
driving simulators. Despite advancements in existing methods for generating
driving scenes, significant challenges remain in view transformation and
spatial-temporal dynamic modeling. To address these limitations, we propose a
Spatial-Temporal simulAtion for drivinG (Stag-1) model to reconstruct
real-world scenes and design a controllable generative network to achieve 4D
simulation. Stag-1 constructs continuous 4D point cloud scenes using
surround-view data from autonomous vehicles. It decouples spatial-temporal
relationships and produces coherent keyframe videos. Additionally, Stag-1
leverages video generation models to obtain photo-realistic and controllable 4D
driving simulation videos from any perspective. To expand the range of view
generation, we train vehicle motion videos based on decomposed camera poses,
enhancing modeling capabilities for distant scenes. Furthermore, we reconstruct
vehicle camera trajectories to integrate 3D points across consecutive views,
enabling comprehensive scene understanding along the temporal dimension.
Following extensive multi-level scene training, Stag-1 can simulate from any
desired viewpoint and achieve a deep understanding of scene evolution under
static spatial-temporal conditions. Compared to existing methods, our approach
shows promising performance in multi-view scene consistency, background
coherence, and accuracy, and contributes to the ongoing advancements in
realistic autonomous driving simulation. Code: https://github.com/wzzheng/Stag.
]]></content:encoded>
<pubDate>2024-12-06T18:59:56Z</pubDate>
</item>
<item>
<title>Text to Blind Motion</title>
<link>http://arxiv.org/abs/2412.05277v1</link>
<guid>http://arxiv.org/abs/2412.05277v1</guid>
<content:encoded><![CDATA[
People who are blind perceive the world differently than those who are
sighted, which can result in distinct motion characteristics. For instance,
when crossing at an intersection, blind individuals may have different patterns
of movement, such as veering more from a straight path or using touch-based
exploration around curbs and obstacles. These behaviors may appear less
predictable to motion models embedded in technologies such as autonomous
vehicles. Yet, the ability of 3D motion models to capture such behavior has not
been previously studied, as existing datasets for 3D human motion currently
lack diversity and are biased toward people who are sighted. In this work, we
introduce BlindWays, the first multimodal motion benchmark for pedestrians who
are blind. We collect 3D motion data using wearable sensors with 11 blind
participants navigating eight different routes in a real-world urban setting.
Additionally, we provide rich textual descriptions that capture the distinctive
movement characteristics of blind pedestrians and their interactions with both
the navigation aid (e.g., a white cane or a guide dog) and the environment. We
benchmark state-of-the-art 3D human prediction models, finding poor performance
with off-the-shelf and pre-training-based methods for our novel task. To
contribute toward safer and more reliable systems that can seamlessly reason
over diverse human movements in their environments, our text-and-motion
benchmark is available at https://blindways.github.io.
]]></content:encoded>
<pubDate>2024-12-06T18:59:51Z</pubDate>
</item>
<item>
<title>Expanding Performance Boundaries of Open-Source Multimodal Models with
  Model, Data, and Test-Time Scaling</title>
<link>http://arxiv.org/abs/2412.05271v1</link>
<guid>http://arxiv.org/abs/2412.05271v1</guid>
<content:encoded><![CDATA[
We introduce InternVL 2.5, an advanced multimodal large language model (MLLM)
series that builds upon InternVL 2.0, maintaining its core model architecture
while introducing significant enhancements in training and testing strategies
as well as data quality. In this work, we delve into the relationship between
model scaling and performance, systematically exploring the performance trends
in vision encoders, language models, dataset sizes, and test-time
configurations. Through extensive evaluations on a wide range of benchmarks,
including multi-discipline reasoning, document understanding, multi-image /
video understanding, real-world comprehension, multimodal hallucination
detection, visual grounding, multilingual capabilities, and pure language
processing, InternVL 2.5 exhibits competitive performance, rivaling leading
commercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model is
the first open-source MLLMs to surpass 70% on the MMMU benchmark, achieving a
3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasing
strong potential for test-time scaling. We hope this model contributes to the
open-source community by setting new standards for developing and applying
multimodal AI systems. HuggingFace demo see
https://huggingface.co/spaces/OpenGVLab/InternVL
]]></content:encoded>
<pubDate>2024-12-06T18:57:08Z</pubDate>
</item>
<item>
<title>PaintScene4D: Consistent 4D Scene Generation from Text Prompts</title>
<link>http://arxiv.org/abs/2412.04471v1</link>
<guid>http://arxiv.org/abs/2412.04471v1</guid>
<content:encoded><![CDATA[
Recent advances in diffusion models have revolutionized 2D and 3D content
creation, yet generating photorealistic dynamic 4D scenes remains a significant
challenge. Existing dynamic 4D generation methods typically rely on distilling
knowledge from pre-trained 3D generative models, often fine-tuned on synthetic
object datasets. Consequently, the resulting scenes tend to be object-centric
and lack photorealism. While text-to-video models can generate more realistic
scenes with motion, they often struggle with spatial understanding and provide
limited control over camera viewpoints during rendering. To address these
limitations, we present PaintScene4D, a novel text-to-4D scene generation
framework that departs from conventional multi-view generative models in favor
of a streamlined architecture that harnesses video generative models trained on
diverse real-world datasets. Our method first generates a reference video using
a video generation model, and then employs a strategic camera array selection
for rendering. We apply a progressive warping and inpainting technique to
ensure both spatial and temporal consistency across multiple viewpoints.
Finally, we optimize multi-view images using a dynamic renderer, enabling
flexible camera control based on user preferences. Adopting a training-free
architecture, our PaintScene4D efficiently produces realistic 4D scenes that
can be viewed from arbitrary trajectories. The code will be made publicly
available. Our project page is at https://paintscene4d.github.io/
]]></content:encoded>
<pubDate>2024-12-05T18:59:57Z</pubDate>
</item>
<item>
<title>Turbo3D: Ultra-fast Text-to-3D Generation</title>
<link>http://arxiv.org/abs/2412.04470v1</link>
<guid>http://arxiv.org/abs/2412.04470v1</guid>
<content:encoded><![CDATA[
We present Turbo3D, an ultra-fast text-to-3D system capable of generating
high-quality Gaussian splatting assets in under one second. Turbo3D employs a
rapid 4-step, 4-view diffusion generator and an efficient feed-forward Gaussian
reconstructor, both operating in latent space. The 4-step, 4-view generator is
a student model distilled through a novel Dual-Teacher approach, which
encourages the student to learn view consistency from a multi-view teacher and
photo-realism from a single-view teacher. By shifting the Gaussian
reconstructor's inputs from pixel space to latent space, we eliminate the extra
image decoding time and halve the transformer sequence length for maximum
efficiency. Our method demonstrates superior 3D generation results compared to
previous baselines, while operating in a fraction of their runtime.
]]></content:encoded>
<pubDate>2024-12-05T18:59:56Z</pubDate>
</item>
<item>
<title>Navigation World Models</title>
<link>http://arxiv.org/abs/2412.03572v1</link>
<guid>http://arxiv.org/abs/2412.03572v1</guid>
<content:encoded><![CDATA[
Navigation is a fundamental skill of agents with visual-motor capabilities.
We introduce a Navigation World Model (NWM), a controllable video generation
model that predicts future visual observations based on past observations and
navigation actions. To capture complex environment dynamics, NWM employs a
Conditional Diffusion Transformer (CDiT), trained on a diverse collection of
egocentric videos of both human and robotic agents, and scaled up to 1 billion
parameters. In familiar environments, NWM can plan navigation trajectories by
simulating them and evaluating whether they achieve the desired goal. Unlike
supervised navigation policies with fixed behavior, NWM can dynamically
incorporate constraints during planning. Experiments demonstrate its
effectiveness in planning trajectories from scratch or by ranking trajectories
sampled from an external policy. Furthermore, NWM leverages its learned visual
priors to imagine trajectories in unfamiliar environments from a single input
image, making it a flexible and powerful tool for next-generation navigation
systems.
]]></content:encoded>
<pubDate>2024-12-04T18:59:45Z</pubDate>
</item>
<item>
<title>Streaming Detection of Queried Event Start</title>
<link>http://arxiv.org/abs/2412.03567v1</link>
<guid>http://arxiv.org/abs/2412.03567v1</guid>
<content:encoded><![CDATA[
Robotics, autonomous driving, augmented reality, and many embodied computer
vision applications must quickly react to user-defined events unfolding in real
time. We address this setting by proposing a novel task for multimodal video
understanding-Streaming Detection of Queried Event Start (SDQES). The goal of
SDQES is to identify the beginning of a complex event as described by a natural
language query, with high accuracy and low latency. We introduce a new
benchmark based on the Ego4D dataset, as well as new task-specific metrics to
study streaming multimodal detection of diverse events in an egocentric video
setting. Inspired by parameter-efficient fine-tuning methods in NLP and for
video tasks, we propose adapter-based baselines that enable image-to-video
transfer learning, allowing for efficient online video modeling. We evaluate
three vision-language backbones and three adapter architectures on both
short-clip and untrimmed video settings.
]]></content:encoded>
<pubDate>2024-12-04T18:58:27Z</pubDate>
</item>
<item>
<title>Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual
  Prompt Instruction Tuning</title>
<link>http://arxiv.org/abs/2412.03565v1</link>
<guid>http://arxiv.org/abs/2412.03565v1</guid>
<content:encoded><![CDATA[
Large Multimodal Models (LMMs) have made significant breakthroughs with the
advancement of instruction tuning. However, while existing models can
understand images and videos at a holistic level, they still struggle with
instance-level understanding that requires a more nuanced comprehension and
alignment. Instance-level understanding is crucial, as it focuses on the
specific elements that we are most interested in. Excitingly, existing works
find that the state-of-the-art LMMs exhibit strong instance understanding
capabilities when provided with explicit visual cues. Motivated by this, we
introduce an automated annotation pipeline assisted by GPT-4o to extract
instance-level information from images and videos through explicit visual
prompting for instance guidance. Building upon this pipeline, we proposed
Inst-IT, a solution to enhance LMMs in Instance understanding via explicit
visual prompt Instruction Tuning. Inst-IT consists of a benchmark to diagnose
multimodal instance-level understanding, a large-scale instruction-tuning
dataset, and a continuous instruction-tuning training paradigm to effectively
enhance spatial-temporal instance understanding capabilities of existing LMMs.
Experimental results show that, with the boost of Inst-IT, our models not only
achieve outstanding performance on Inst-IT Bench but also demonstrate
significant improvements across various generic image and video understanding
benchmarks. This highlights that our dataset not only boosts instance-level
understanding but also strengthens the overall capabilities of generic image
and video comprehension.
]]></content:encoded>
<pubDate>2024-12-04T18:58:10Z</pubDate>
</item>
<item>
<title>From Individual to Society: A Survey on Social Simulation Driven by
  Large Language Model-based Agents</title>
<link>http://arxiv.org/abs/2412.03563v1</link>
<guid>http://arxiv.org/abs/2412.03563v1</guid>
<content:encoded><![CDATA[
Traditional sociological research often relies on human participation, which,
though effective, is expensive, challenging to scale, and with ethical
concerns. Recent advancements in large language models (LLMs) highlight their
potential to simulate human behavior, enabling the replication of individual
responses and facilitating studies on many interdisciplinary studies. In this
paper, we conduct a comprehensive survey of this field, illustrating the recent
progress in simulation driven by LLM-empowered agents. We categorize the
simulations into three types: (1) Individual Simulation, which mimics specific
individuals or demographic groups; (2) Scenario Simulation, where multiple
agents collaborate to achieve goals within specific contexts; and (3) Society
Simulation, which models interactions within agent societies to reflect the
complexity and variety of real-world dynamics. These simulations follow a
progression, ranging from detailed individual modeling to large-scale societal
phenomena. We provide a detailed discussion of each simulation type, including
the architecture or key components of the simulation, the classification of
objectives or scenarios and the evaluation method. Afterward, we summarize
commonly used datasets and benchmarks. Finally, we discuss the trends across
these three types of simulation. A repository for the related sources is at
{\url{https://github.com/FudanDISC/SocialAgent}}.
]]></content:encoded>
<pubDate>2024-12-04T18:56:37Z</pubDate>
</item>
<item>
<title>FLAIR: VLM with Fine-grained Language-informed Image Representations</title>
<link>http://arxiv.org/abs/2412.03561v1</link>
<guid>http://arxiv.org/abs/2412.03561v1</guid>
<content:encoded><![CDATA[
CLIP has shown impressive results in aligning images and texts at scale.
However, its ability to capture detailed visual features remains limited
because CLIP matches images and texts at a global level. To address this issue,
we propose FLAIR, Fine-grained Language-informed Image Representations, an
approach that utilizes long and detailed image descriptions to learn localized
image embeddings. By sampling diverse sub-captions that describe fine-grained
details about an image, we train our vision-language model to produce not only
global embeddings but also text-specific image representations. Our model
introduces text-conditioned attention pooling on top of local image tokens to
produce fine-grained image representations that excel at retrieving detailed
image content. We achieve state-of-the-art performance on both, existing
multimodal retrieval benchmarks, as well as, our newly introduced fine-grained
retrieval task which evaluates vision-language models' ability to retrieve
partial image content. Furthermore, our experiments demonstrate the
effectiveness of FLAIR trained on 30M image-text pairs in capturing
fine-grained visual information, including zero-shot semantic segmentation,
outperforming models trained on billions of pairs. Code is available at
https://github.com/ExplainableML/flair .
]]></content:encoded>
<pubDate>2024-12-04T18:56:04Z</pubDate>
</item>
<item>
<title>Motion Prompting: Controlling Video Generation with Motion Trajectories</title>
<link>http://arxiv.org/abs/2412.02700v1</link>
<guid>http://arxiv.org/abs/2412.02700v1</guid>
<content:encoded><![CDATA[
Motion control is crucial for generating expressive and compelling video
content; however, most existing video generation models rely mainly on text
prompts for control, which struggle to capture the nuances of dynamic actions
and temporal compositions. To this end, we train a video generation model
conditioned on spatio-temporally sparse or dense motion trajectories. In
contrast to prior motion conditioning work, this flexible representation can
encode any number of trajectories, object-specific or global scene motion, and
temporally sparse motion; due to its flexibility we refer to this conditioning
as motion prompts. While users may directly specify sparse trajectories, we
also show how to translate high-level user requests into detailed, semi-dense
motion prompts, a process we term motion prompt expansion. We demonstrate the
versatility of our approach through various applications, including camera and
object motion control, "interacting" with an image, motion transfer, and image
editing. Our results showcase emergent behaviors, such as realistic physics,
suggesting the potential of motion prompts for probing video models and
interacting with future generative world models. Finally, we evaluate
quantitatively, conduct a human study, and demonstrate strong performance.
Video results are available on our webpage: https://motion-prompting.github.io/
]]></content:encoded>
<pubDate>2024-12-03T18:59:56Z</pubDate>
</item>
<item>
<title>FoundHand: Large-Scale Domain-Specific Learning for Controllable Hand
  Image Generation</title>
<link>http://arxiv.org/abs/2412.02690v1</link>
<guid>http://arxiv.org/abs/2412.02690v1</guid>
<content:encoded><![CDATA[
Despite remarkable progress in image generation models, generating realistic
hands remains a persistent challenge due to their complex articulation, varying
viewpoints, and frequent occlusions. We present FoundHand, a large-scale
domain-specific diffusion model for synthesizing single and dual hand images.
To train our model, we introduce FoundHand-10M, a large-scale hand dataset with
2D keypoints and segmentation mask annotations. Our insight is to use 2D hand
keypoints as a universal representation that encodes both hand articulation and
camera viewpoint. FoundHand learns from image pairs to capture physically
plausible hand articulations, natively enables precise control through 2D
keypoints, and supports appearance control. Our model exhibits core
capabilities that include the ability to repose hands, transfer hand
appearance, and even synthesize novel views. This leads to zero-shot
capabilities for fixing malformed hands in previously generated images, or
synthesizing hand video sequences. We present extensive experiments and
evaluations that demonstrate state-of-the-art performance of our method.
]]></content:encoded>
<pubDate>2024-12-03T18:58:19Z</pubDate>
</item>
<item>
<title>VLSBench: Unveiling Visual Leakage in Multimodal Safety</title>
<link>http://arxiv.org/abs/2411.19939v1</link>
<guid>http://arxiv.org/abs/2411.19939v1</guid>
<content:encoded><![CDATA[
Safety concerns of Multimodal large language models (MLLMs) have gradually
become an important problem in various applications. Surprisingly, previous
works indicate a counter-intuitive phenomenon that using textual unlearning to
align MLLMs achieves comparable safety performances with MLLMs trained with
image-text pairs. To explain such a counter-intuitive phenomenon, we discover a
visual safety information leakage (VSIL) problem in existing multimodal safety
benchmarks, i.e., the potentially risky and sensitive content in the image has
been revealed in the textual query. In this way, MLLMs can easily refuse these
sensitive text-image queries according to textual queries. However, image-text
pairs without VSIL are common in real-world scenarios and are overlooked by
existing multimodal safety benchmarks. To this end, we construct multimodal
visual leakless safety benchmark (VLSBench) preventing visual safety leakage
from image to textual query with 2.4k image-text pairs. Experimental results
indicate that VLSBench poses a significant challenge to both open-source and
close-source MLLMs, including LLaVA, Qwen2-VL, Llama3.2-Vision, and GPT-4o.
This study demonstrates that textual alignment is enough for multimodal safety
scenarios with VSIL, while multimodal alignment is a more promising solution
for multimodal safety scenarios without VSIL. Please see our code and data at:
http://hxhcreate.github.io/VLSBench
]]></content:encoded>
<pubDate>2024-11-29T18:56:37Z</pubDate>
</item>
<item>
<title>On Domain-Specific Post-Training for Multimodal Large Language Models</title>
<link>http://arxiv.org/abs/2411.19930v1</link>
<guid>http://arxiv.org/abs/2411.19930v1</guid>
<content:encoded><![CDATA[
Recent years have witnessed the rapid development of general multimodal large
language models (MLLMs). However, adapting general MLLMs to specific domains,
such as scientific fields and industrial applications, remains less explored.
This paper systematically investigates domain adaptation of MLLMs through
post-training, focusing on data synthesis, training pipelines, and task
evaluation. (1) Data Synthesis: Using open-source models, we develop a visual
instruction synthesizer that effectively generates diverse visual instruction
tasks from domain-specific image-caption pairs. Our synthetic tasks surpass
those generated by manual rules, GPT-4, and GPT-4V in enhancing the
domain-specific performance of MLLMs. (2) Training Pipeline: While the
two-stage training--initially on image-caption pairs followed by visual
instruction tasks--is commonly adopted for developing general MLLMs, we apply a
single-stage training pipeline to enhance task diversity for domain-specific
post-training. (3) Task Evaluation: We conduct experiments in two domains,
biomedicine and food, by post-training MLLMs of different sources and scales
(e.g., Qwen2-VL-2B, LLaVA-v1.6-8B, Llama-3.2-11B), and then evaluating MLLM
performance on various domain-specific tasks. To support further research in
MLLM domain adaptation, we will open-source our implementations.
]]></content:encoded>
<pubDate>2024-11-29T18:42:28Z</pubDate>
</item>
<item>
<title>Cross-modal Information Flow in Multimodal Large Language Models</title>
<link>http://arxiv.org/abs/2411.18620v1</link>
<guid>http://arxiv.org/abs/2411.18620v1</guid>
<content:encoded><![CDATA[
<div> 
<br />
 
<br /><br />:  <div>
The recent advancements in auto-regressive multimodal large language models
(MLLMs) have demonstrated promising progress for vision-language tasks. While
there exists a variety of studies investigating the processing of linguistic
information within large language models, little is currently known about the
inner working mechanism of MLLMs and how linguistic and visual information
interact within these models. In this study, we aim to fill this gap by
examining the information flow between different modalities -- language and
vision -- in MLLMs, focusing on visual question answering. Specifically, given
an image-question pair as input, we investigate where in the model and how the
visual and linguistic information are combined to generate the final
prediction. Conducting experiments with a series of models from the LLaVA
series, we find that there are two distinct stages in the process of
integration of the two modalities. In the lower layers, the model first
transfers the more general visual features of the whole image into the
representations of (linguistic) question tokens. In the middle layers, it once
again transfers visual information about specific objects relevant to the
question to the respective token positions of the question. Finally, in the
higher layers, the resulting multimodal representation is propagated to the
last position of the input sequence for the final prediction. Overall, our
findings provide a new and comprehensive perspective on the spatial and
functional aspects of image and language processing in the MLLMs, thereby
facilitating future research into multimodal information localization and
editing.
]]></content:encoded>
<pubDate>2024-11-27T18:59:26Z</pubDate>
</item>
<item>
<title>Proactive Gradient Conflict Mitigation in Multi-Task Learning: A Sparse
  Training Perspective</title>
<link>http://arxiv.org/abs/2411.18615v1</link>
<guid>http://arxiv.org/abs/2411.18615v1</guid>
<content:encoded><![CDATA[
<div> , , , , 
<br /><br />
ST:  <div>
Advancing towards generalist agents necessitates the concurrent processing of
multiple tasks using a unified model, thereby underscoring the growing
significance of simultaneous model training on multiple downstream tasks. A
common issue in multi-task learning is the occurrence of gradient conflict,
which leads to potential competition among different tasks during joint
training. This competition often results in improvements in one task at the
expense of deterioration in another. Although several optimization methods have
been developed to address this issue by manipulating task gradients for better
task balancing, they cannot decrease the incidence of gradient conflict. In
this paper, we systematically investigate the occurrence of gradient conflict
across different methods and propose a strategy to reduce such conflicts
through sparse training (ST), wherein only a portion of the model's parameters
are updated during training while keeping the rest unchanged. Our extensive
experiments demonstrate that ST effectively mitigates conflicting gradients and
leads to superior performance. Furthermore, ST can be easily integrated with
gradient manipulation techniques, thus enhancing their effectiveness.
]]></content:encoded>
<pubDate>2024-11-27T18:58:22Z</pubDate>
</item>
<item>
<title>Video-Guided Foley Sound Generation with Multimodal Controls</title>
<link>http://arxiv.org/abs/2411.17698v1</link>
<guid>http://arxiv.org/abs/2411.17698v1</guid>
<content:encoded><![CDATA[
<div> MultiFoley<br />
<br />
MultiFoleyMultiFoley48kHzMultiFoley <div>
Generating sound effects for videos often requires creating artistic sound
effects that diverge significantly from real-life sources and flexible control
in the sound design. To address this problem, we introduce MultiFoley, a model
designed for video-guided sound generation that supports multimodal
conditioning through text, audio, and video. Given a silent video and a text
prompt, MultiFoley allows users to create clean sounds (e.g., skateboard wheels
spinning without wind noise) or more whimsical sounds (e.g., making a lion's
roar sound like a cat's meow). MultiFoley also allows users to choose reference
audio from sound effects (SFX) libraries or partial videos for conditioning. A
key novelty of our model lies in its joint training on both internet video
datasets with low-quality audio and professional SFX recordings, enabling
high-quality, full-bandwidth (48kHz) audio generation. Through automated
evaluations and human studies, we demonstrate that MultiFoley successfully
generates synchronized high-quality sounds across varied conditional inputs and
outperforms existing methods. Please see our project page for video results:
https://ificl.github.io/MultiFoley/
]]></content:encoded>
<pubDate>2024-11-26T18:59:58Z</pubDate>
</item>
<item>
<title>StableAnimator: High-Quality Identity-Preserving Human Image Animation</title>
<link>http://arxiv.org/abs/2411.17697v1</link>
<guid>http://arxiv.org/abs/2411.17697v1</guid>
<content:encoded><![CDATA[
<div> StableAnimator

:
StableAnimatorHamilton-Jacobi-BellmanHJBStableAnimator<br /><br /> <div>
Current diffusion models for human image animation struggle to ensure
identity (ID) consistency. This paper presents StableAnimator, the first
end-to-end ID-preserving video diffusion framework, which synthesizes
high-quality videos without any post-processing, conditioned on a reference
image and a sequence of poses. Building upon a video diffusion model,
StableAnimator contains carefully designed modules for both training and
inference striving for identity consistency. In particular, StableAnimator
begins by computing image and face embeddings with off-the-shelf extractors,
respectively and face embeddings are further refined by interacting with image
embeddings using a global content-aware Face Encoder. Then, StableAnimator
introduces a novel distribution-aware ID Adapter that prevents interference
caused by temporal layers while preserving ID via alignment. During inference,
we propose a novel Hamilton-Jacobi-Bellman (HJB) equation-based optimization to
further enhance the face quality. We demonstrate that solving the HJB equation
can be integrated into the diffusion denoising process, and the resulting
solution constrains the denoising path and thus benefits ID preservation.
Experiments on multiple benchmarks show the effectiveness of StableAnimator
both qualitatively and quantitatively.
]]></content:encoded>
<pubDate>2024-11-26T18:59:22Z</pubDate>
</item>
<item>
<title>Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis</title>
<link>http://arxiv.org/abs/2411.17690v1</link>
<guid>http://arxiv.org/abs/2411.17690v1</guid>
<content:encoded><![CDATA[
In this paper, we propose a new task -- generating speech from videos of
people and their transcripts (VTTS) -- to motivate new techniques for
multimodal speech generation. This task generalizes the task of generating
speech from cropped lip videos, and is also more complicated than the task of
generating generic audio clips (e.g., dog barking) from videos and text.
Multilingual versions of the task could lead to new techniques for
cross-lingual dubbing. We also present a decoder-only multimodal model for this
task, which we call Visatronic. This model embeds vision, text and speech
directly into the common subspace of a transformer model and uses an
autoregressive loss to learn a generative model of discretized mel-spectrograms
conditioned on speaker videos and transcripts of their speech. By embedding all
modalities into a common subspace, Visatronic can achieve improved results over
models that use only text or video as input. Further, it presents a much
simpler approach for multimodal speech generation compared to prevailing
approaches which rely on lip-detectors and complicated architectures to fuse
modalities while producing better results. Since the model is flexible enough
to accommodate different ways of ordering inputs as a sequence, we carefully
explore different strategies to better understand the best way to propagate
information to the generative steps. To facilitate further research on VTTS, we
will release (i) our code, (ii) clean transcriptions for the large-scale
VoxCeleb2 dataset, and (iii) a standardized evaluation protocol for VTTS
incorporating both objective and subjective metrics.
]]></content:encoded>
<pubDate>2024-11-26T18:57:29Z</pubDate>
</item>
<item>
<title>Factorized Visual Tokenization and Generation</title>
<link>http://arxiv.org/abs/2411.16681v1</link>
<guid>http://arxiv.org/abs/2411.16681v1</guid>
<content:encoded><![CDATA[
<div> VQ-based tokenizers, image generation, Factorized Quantization, disentanglement regularization, FQGAN<br />
<br />
VQ-based tokenizersFactorized Quantization (FQ)CLIPDINOFQGANtokenizertokenizer <br /><br />: <br />Factorized Quantization (FQ)FQGANtokenizer <div>
Visual tokenizers are fundamental to image generation. They convert visual
data into discrete tokens, enabling transformer-based models to excel at image
generation. Despite their success, VQ-based tokenizers like VQGAN face
significant limitations due to constrained vocabulary sizes. Simply expanding
the codebook often leads to training instability and diminishing performance
gains, making scalability a critical challenge. In this work, we introduce
Factorized Quantization (FQ), a novel approach that revitalizes VQ-based
tokenizers by decomposing a large codebook into multiple independent
sub-codebooks. This factorization reduces the lookup complexity of large
codebooks, enabling more efficient and scalable visual tokenization. To ensure
each sub-codebook captures distinct and complementary information, we propose a
disentanglement regularization that explicitly reduces redundancy, promoting
diversity across the sub-codebooks. Furthermore, we integrate representation
learning into the training process, leveraging pretrained vision models like
CLIP and DINO to infuse semantic richness into the learned representations.
This design ensures our tokenizer captures diverse semantic levels, leading to
more expressive and disentangled representations. Experiments show that the
proposed FQGAN model substantially improves the reconstruction quality of
visual tokenizers, achieving state-of-the-art performance. We further
demonstrate that this tokenizer can be effectively adapted into auto-regressive
image generation. https://showlab.github.io/FQGAN
]]></content:encoded>
<pubDate>2024-11-25T18:59:53Z</pubDate>
</item>
<item>
<title>Learning-based Trajectory Tracking for Bird-inspired Flapping-Wing
  Robots</title>
<link>http://arxiv.org/abs/2411.15130v1</link>
<guid>http://arxiv.org/abs/2411.15130v1</guid>
<content:encoded><![CDATA[
<div> : , , , RL , 
:
RL <div>
Bird-sized flapping-wing robots offer significant potential for agile flight
in complex environments, but achieving agile and robust trajectory tracking
remains a challenge due to the complex aerodynamics and highly nonlinear
dynamics inherent in flapping-wing flight. In this work, a learning-based
control approach is introduced to unlock the versatility and adaptiveness of
flapping-wing flight. We propose a model-free reinforcement learning (RL)-based
framework for a high degree-of-freedom (DoF) bird-inspired flapping-wing robot
that allows for multimodal flight and agile trajectory tracking. Stability
analysis was performed on the closed-loop system comprising of the
flapping-wing system and the RL policy. Additionally, simulation results
demonstrate that the RL-based controller can successfully learn complex wing
trajectory patterns, achieve stable flight, switch between flight modes
spontaneously, and track different trajectories under various aerodynamic
conditions.
]]></content:encoded>
<pubDate>2024-11-22T18:55:37Z</pubDate>
</item>
<item>
<title>PRIMUS: Pretraining IMU Encoders with Multimodal Self-Supervision</title>
<link>http://arxiv.org/abs/2411.15127v1</link>
<guid>http://arxiv.org/abs/2411.15127v1</guid>
<content:encoded><![CDATA[
<div> : IMU, , PRIMUS, , 
:
IMUPRIMUSIMUPRIMUSIMUPRIMUS50015github.com/nokia-bell-labsIMU <div>
Sensing human motions through Inertial Measurement Units (IMUs) embedded in
personal devices has enabled significant applications in health and wellness.
While labeled IMU data is scarce, we can collect unlabeled or weakly labeled
IMU data to model human motions. For video or text modalities, the "pretrain
and adapt" approach utilizes large volumes of unlabeled or weakly labeled data
for pretraining, building a strong feature extractor, followed by adaptation to
specific tasks using limited labeled data. This approach has not been widely
adopted in the IMU domain for two reasons: (1) pretraining methods are poorly
understood in the context of IMU, and (2) open-source pretrained models that
generalize across datasets are rarely publicly available. In this paper, we aim
to address the first issue by proposing PRIMUS, a method for PRetraining IMU
encoderS. We conduct a systematic and unified evaluation of various
self-supervised and multimodal learning pretraining objectives. Our findings
indicate that using PRIMUS, which combines self-supervision, multimodal
supervision, and nearest-neighbor supervision, can significantly enhance
downstream performance. With fewer than 500 labeled samples per class, PRIMUS
effectively enhances downstream performance by up to 15% in held-out test data,
compared to the state-of-the-art multimodal training method. To benefit the
broader community, our code and pre-trained IMU encoders will be made publicly
available at github.com/nokia-bell-labs upon publication.
]]></content:encoded>
<pubDate>2024-11-22T18:46:30Z</pubDate>
</item>
<item>
<title>Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large
  Language Models</title>
<link>http://arxiv.org/abs/2411.14432v1</link>
<guid>http://arxiv.org/abs/2411.14432v1</guid>
<content:encoded><![CDATA[
<div> , , , , 

Insight-VDPO Insight-V <br /><br />: <br />Insight-V <div>
Large Language Models (LLMs) demonstrate enhanced capabilities and
reliability by reasoning more, evolving from Chain-of-Thought prompting to
product-level solutions like OpenAI o1. Despite various efforts to improve LLM
reasoning, high-quality long-chain reasoning data and optimized training
pipelines still remain inadequately explored in vision-language tasks. In this
paper, we present Insight-V, an early effort to 1) scalably produce long and
robust reasoning data for complex multi-modal tasks, and 2) an effective
training pipeline to enhance the reasoning capabilities of multi-modal large
language models (MLLMs). Specifically, to create long and structured reasoning
data without human labor, we design a two-step pipeline with a progressive
strategy to generate sufficiently long and diverse reasoning paths and a
multi-granularity assessment method to ensure data quality. We observe that
directly supervising MLLMs with such long and complex reasoning data will not
yield ideal reasoning ability. To tackle this problem, we design a multi-agent
system consisting of a reasoning agent dedicated to performing long-chain
reasoning and a summary agent trained to judge and summarize reasoning results.
We further incorporate an iterative DPO algorithm to enhance the reasoning
agent's generation stability and quality. Based on the popular LLaVA-NeXT model
and our stronger base MLLM, we demonstrate significant performance gains across
challenging multi-modal benchmarks requiring visual reasoning. Benefiting from
our multi-agent system, Insight-V can also easily maintain or improve
performance on perception-focused multi-modal tasks.
]]></content:encoded>
<pubDate>2024-11-21T18:59:55Z</pubDate>
</item>
<item>
<title>REDUCIO! Generating 1024$\times$1024 Video within 16 Seconds using
  Extremely Compressed Motion Latents</title>
<link>http://arxiv.org/abs/2411.13552v1</link>
<guid>http://arxiv.org/abs/2411.13552v1</guid>
<content:encoded><![CDATA[
<div> : , , VAE, , <br />
<br />
VAE"Reducio"2D VAE641KReducio-DiTGPULDMs3.2KReducio-DiTA100 GPU15.5161024*1024https://github.com/microsoft/Reducio-VAE <br /><br />: VAELDMs <div>
Commercial video generation models have exhibited realistic, high-fidelity
results but are still restricted to limited access. One crucial obstacle for
large-scale applications is the expensive training and inference cost. In this
paper, we argue that videos contain much more redundant information than
images, thus can be encoded by very few motion latents based on a content
image. Towards this goal, we design an image-conditioned VAE to encode a video
to an extremely compressed motion latent space. This magic Reducio charm
enables 64x reduction of latents compared to a common 2D VAE, without
sacrificing the quality. Training diffusion models on such a compact
representation easily allows for generating 1K resolution videos. We then adopt
a two-stage video generation paradigm, which performs text-to-image and
text-image-to-video sequentially. Extensive experiments show that our
Reducio-DiT achieves strong performance in evaluation, though trained with
limited GPU resources. More importantly, our method significantly boost the
efficiency of video LDMs both in training and inference. We train Reducio-DiT
in around 3.2K training hours in total and generate a 16-frame 1024*1024 video
clip within 15.5 seconds on a single A100 GPU. Code released at
https://github.com/microsoft/Reducio-VAE .
]]></content:encoded>
<pubDate>2024-11-20T18:59:52Z</pubDate>
</item>
<item>
<title>BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games</title>
<link>http://arxiv.org/abs/2411.13543v1</link>
<guid>http://arxiv.org/abs/2411.13543v1</guid>
<content:encoded><![CDATA[
<div> BALROG
<br /><br />
1. 
2. BALROG
3. BALROG
4. 
5. 
<br /><br />:
BALROG <div>
Large Language Models (LLMs) and Vision Language Models (VLMs) possess
extensive knowledge and exhibit promising reasoning abilities; however, they
still struggle to perform well in complex, dynamic environments. Real-world
tasks require handling intricate interactions, advanced spatial reasoning,
long-term planning, and continuous exploration of new strategies-areas in which
we lack effective methodologies for comprehensively evaluating these
capabilities. To address this gap, we introduce BALROG, a novel benchmark
designed to assess the agentic capabilities of LLMs and VLMs through a diverse
set of challenging games. Our benchmark incorporates a range of existing
reinforcement learning environments with varying levels of difficulty,
including tasks that are solvable by non-expert humans in seconds to extremely
challenging ones that may take years to master (e.g., the NetHack Learning
Environment). We devise fine-grained metrics to measure performance and conduct
an extensive evaluation of several popular open-source and closed-source LLMs
and VLMs. Our findings indicate that while current models achieve partial
success in the easier games, they struggle significantly with more challenging
tasks. Notably, we observe severe deficiencies in vision-based decision-making,
as models perform worse when visual representations of the environments are
provided. We release BALROG as an open and user-friendly benchmark to
facilitate future research and development in the agentic community.
]]></content:encoded>
<pubDate>2024-11-20T18:54:32Z</pubDate>
</item>
<item>
<title>Reinforcement Learning, Collusion, and the Folk Theorem</title>
<link>http://arxiv.org/abs/2411.12725v1</link>
<guid>http://arxiv.org/abs/2411.12725v1</guid>
<content:encoded><![CDATA[
<div> ,,,,

 <br /><br />:  <div>
We explore the behaviour emerging from learning agents repeatedly interacting
strategically for a wide range of learning dynamics that includes projected
gradient, replicator and log-barrier dynamics. Going beyond the
better-understood classes of potential games and zero-sum games, we consider
the setting of a general repeated game with finite recall, for different forms
of monitoring. We obtain a Folk Theorem-like result and characterise the set of
payoff vectors that can be obtained by these dynamics, discovering a wide range
of possibilities for the emergence of algorithmic collusion.
]]></content:encoded>
<pubDate>2024-11-19T18:45:55Z</pubDate>
</item>
<item>
<title>Generative World Explorer</title>
<link>http://arxiv.org/abs/2411.11844v1</link>
<guid>http://arxiv.org/abs/2411.11844v1</guid>
<content:encoded><![CDATA[
<div> <br />
<br />
Generative World Explorer (Genex)GenexGenex-DBGenex<br /><br />: GenexGenex <div>
Planning with partial observation is a central challenge in embodied AI. A
majority of prior works have tackled this challenge by developing agents that
physically explore their environment to update their beliefs about the world
state.In contrast, humans can $\textit{imagine}$ unseen parts of the world
through a mental exploration and $\textit{revise}$ their beliefs with imagined
observations. Such updated beliefs can allow them to make more informed
decisions, without necessitating the physical exploration of the world at all
times. To achieve this human-like ability, we introduce the $\textit{Generative
World Explorer (Genex)}$, an egocentric world exploration framework that allows
an agent to mentally explore a large-scale 3D world (e.g., urban scenes) and
acquire imagined observations to update its belief. This updated belief will
then help the agent to make a more informed decision at the current step. To
train $\textit{Genex}$, we create a synthetic urban scene dataset, Genex-DB.
Our experimental results demonstrate that (1) $\textit{Genex}$ can generate
high-quality and consistent observations during long-horizon exploration of a
large virtual physical world and (2) the beliefs updated with the generated
observations can inform an existing decision-making model (e.g., an LLM agent)
to make better plans.
]]></content:encoded>
<pubDate>2024-11-18T18:59:31Z</pubDate>
</item>
<item>
<title>Enhancing the Reasoning Ability of Multimodal Large Language Models via
  Mixed Preference Optimization</title>
<link>http://arxiv.org/abs/2411.10442v1</link>
<guid>http://arxiv.org/abs/2411.10442v1</guid>
<content:encoded><![CDATA[
<div> 
<br /><br />:
POMLLMsMMPRMLLMsMPOInternVL2-8B-MPOMathVista67.0InternVL2-8B8.7InternVL2-76BMLLMs <div>
Existing open-source multimodal large language models (MLLMs) generally
follow a training process involving pre-training and supervised fine-tuning.
However, these models suffer from distribution shifts, which limit their
multimodal reasoning, particularly in the Chain-of-Thought (CoT) performance.
To address this, we introduce a preference optimization (PO) process to enhance
the multimodal reasoning capabilities of MLLMs. Specifically, (1) on the data
side, we design an automated preference data construction pipeline to create
MMPR, a high-quality, large-scale multimodal reasoning preference dataset. and
(2) on the model side, we explore integrating PO with MLLMs, developing a
simple yet effective method, termed Mixed Preference Optimization (MPO), which
boosts multimodal CoT performance. Our approach demonstrates improved
performance across multiple benchmarks, particularly in multimodal reasoning
tasks. Notably, our model, InternVL2-8B-MPO, achieves an accuracy of 67.0 on
MathVista, outperforming InternVL2-8B by 8.7 points and achieving performance
comparable to the 10x larger InternVL2-76B. We hope this study could inspire
further advancements in MLLMs. Code, data, and model shall be publicly
released.
]]></content:encoded>
<pubDate>2024-11-15T18:59:27Z</pubDate>
</item>
<item>
<title>LLaVA-o1: Let Vision Language Models Reason Step-by-Step</title>
<link>http://arxiv.org/abs/2411.10440v1</link>
<guid>http://arxiv.org/abs/2411.10440v1</guid>
<content:encoded><![CDATA[
<div> LLaVA-o1

LLaVA-o1LLaVA-o1LLaVA-o1LLaVA-o1-100k100kLLaVA-o18.9%Gemini-1.5-proGPT-4o-mini Llama-3.2-90B-Vision-Instruct <br /><br />: LLaVA-o1LLaVA-o1 <div>
Large language models have demonstrated substantial advancements in reasoning
capabilities, particularly through inference-time scaling, as illustrated by
models such as OpenAI's o1. However, current Vision-Language Models (VLMs)
often struggle to perform systematic and structured reasoning, especially when
handling complex visual question-answering tasks. In this work, we introduce
LLaVA-o1, a novel VLM designed to conduct autonomous multistage reasoning.
Unlike chain-of-thought prompting, LLaVA-o1 independently engages in sequential
stages of summarization, visual interpretation, logical reasoning, and
conclusion generation. This structured approach enables LLaVA-o1 to achieve
marked improvements in precision on reasoning-intensive tasks. To accomplish
this, we compile the LLaVA-o1-100k dataset, integrating samples from various
visual question answering sources and providing structured reasoning
annotations. Besides, we propose an inference-time stage-level beam search
method, which enables effective inference-time scaling. Remarkably, with only
100k training samples and a simple yet effective inference time scaling method,
LLaVA-o1 not only outperforms its base model by 8.9% on a wide range of
multimodal reasoning benchmarks, but also surpasses the performance of larger
and even closed-source models, such as Gemini-1.5-pro, GPT-4o-mini, and
Llama-3.2-90B-Vision-Instruct.
]]></content:encoded>
<pubDate>2024-11-15T18:58:31Z</pubDate>
</item>
<item>
<title>Mitigating Hallucination in Multimodal Large Language Model via
  Hallucination-targeted Direct Preference Optimization</title>
<link>http://arxiv.org/abs/2411.10436v1</link>
<guid>http://arxiv.org/abs/2411.10436v1</guid>
<content:encoded><![CDATA[
Multimodal Large Language Models (MLLMs) are known to hallucinate, which
limits their practical applications. Recent works have attempted to apply
Direct Preference Optimization (DPO) to enhance the performance of MLLMs, but
have shown inconsistent improvements in mitigating hallucinations. To address
this issue more effectively, we introduce Hallucination-targeted Direct
Preference Optimization (HDPO) to reduce hallucinations in MLLMs. Unlike
previous approaches, our method tackles hallucinations from their diverse forms
and causes. Specifically, we develop three types of preference pair data
targeting the following causes of MLLM hallucinations: (1) insufficient visual
capabilities, (2) long context generation, and (3) multimodal conflicts.
Experimental results demonstrate that our method achieves superior performance
across multiple hallucination evaluation datasets, surpassing most
state-of-the-art (SOTA) methods and highlighting the potential of our approach.
Ablation studies and in-depth analyses further confirm the effectiveness of our
method and suggest the potential for further improvements through scaling up.
]]></content:encoded>
<pubDate>2024-11-15T18:56:01Z</pubDate>
</item>
<item>
<title>Fair Division via the Cake-Cutting Share</title>
<link>http://arxiv.org/abs/2411.10434v1</link>
<guid>http://arxiv.org/abs/2411.10434v1</guid>
<content:encoded><![CDATA[
In this paper, we consider the classic fair division problem of allocating
$m$ divisible items to $n$ agents with linear valuations over the items. We
define novel notions of fair shares from the perspective of individual agents
via the cake-cutting process. These shares generalize the notion of
proportionality by taking into account the valuations of other agents via
constraints capturing envy. We study what fraction (approximation) of these
shares are achievable in the worst case, and present tight and non-trivial
approximation bounds as a function of $n$ and $m$. In particular, we show a
tight approximation bound of $\Theta(\sqrt{n})$ for various notions of such
shares. We show this bound via a novel application of dual fitting, which may
be of independent interest. We also present a bound of $O(m^{2/3})$ for a
strict notion of share, with an almost matching lower bound. We further develop
weaker notions of shares whose approximation bounds interpolate smoothly
between proportionality and the shares described above. We finally present
empirical results showing that our definitions lead to more reasonable shares
than the standard fair share notion of proportionality.
]]></content:encoded>
<pubDate>2024-11-15T18:55:01Z</pubDate>
</item>
<item>
<title>M-VAR: Decoupled Scale-wise Autoregressive Modeling for High-Quality
  Image Generation</title>
<link>http://arxiv.org/abs/2411.10433v1</link>
<guid>http://arxiv.org/abs/2411.10433v1</guid>
<content:encoded><![CDATA[
There exists recent work in computer vision, named VAR, that proposes a new
autoregressive paradigm for image generation. Diverging from the vanilla
next-token prediction, VAR structurally reformulates the image generation into
a coarse to fine next-scale prediction. In this paper, we show that this
scale-wise autoregressive framework can be effectively decoupled into
\textit{intra-scale modeling}, which captures local spatial dependencies within
each scale, and \textit{inter-scale modeling}, which models cross-scale
relationships progressively from coarse-to-fine scales. This decoupling
structure allows to rebuild VAR in a more computationally efficient manner.
Specifically, for intra-scale modeling -- crucial for generating high-fidelity
images -- we retain the original bidirectional self-attention design to ensure
comprehensive modeling; for inter-scale modeling, which semantically connects
different scales but is computationally intensive, we apply linear-complexity
mechanisms like Mamba to substantially reduce computational overhead. We term
this new framework M-VAR. Extensive experiments demonstrate that our method
outperforms existing models in both image quality and generation speed. For
example, our 1.5B model, with fewer parameters and faster inference speed,
outperforms the largest VAR-d30-2B. Moreover, our largest model M-VAR-d32
impressively registers 1.78 FID on ImageNet 256$\times$256 and outperforms the
prior-art autoregressive models LlamaGen/VAR by 0.4/0.19 and popular diffusion
models LDM/DiT by 1.82/0.49, respectively. Code is avaiable at
\url{https://github.com/OliverRensu/MVAR}.
]]></content:encoded>
<pubDate>2024-11-15T18:54:42Z</pubDate>
</item>
<item>
<title>MagicQuill: An Intelligent Interactive Image Editing System</title>
<link>http://arxiv.org/abs/2411.09703v1</link>
<guid>http://arxiv.org/abs/2411.09703v1</guid>
<content:encoded><![CDATA[
<div> : , MagicQuill, , , 
:
MagicQuill (MLLM) MagicQuillhttps://magic-quill.github.io  <br /><br />: <div>
Image editing involves a variety of complex tasks and requires efficient and
precise manipulation techniques. In this paper, we present MagicQuill, an
integrated image editing system that enables swift actualization of creative
ideas. Our system features a streamlined yet functionally robust interface,
allowing for the articulation of editing operations (e.g., inserting elements,
erasing objects, altering color) with minimal input. These interactions are
monitored by a multimodal large language model (MLLM) to anticipate editing
intentions in real time, bypassing the need for explicit prompt entry. Finally,
we apply a powerful diffusion prior, enhanced by a carefully learned two-branch
plug-in module, to process editing requests with precise control. Experimental
results demonstrate the effectiveness of MagicQuill in achieving high-quality
image edits. Please visit https://magic-quill.github.io to try out our system.
]]></content:encoded>
<pubDate>2024-11-14T18:59:57Z</pubDate>
</item>
<item>
<title>GaussianAnything: Interactive Point Cloud Latent Diffusion for 3D
  Generation</title>
<link>http://arxiv.org/abs/2411.08033v1</link>
<guid>http://arxiv.org/abs/2411.08033v1</guid>
<content:encoded><![CDATA[
<div> Point Cloud-structured Latent space, Variational Autoencoder (VAE), RGB-D, 3D generation, disentanglement<br />
<br />
3DRGB-D-N(ormal)3D-3D/-3D3D<br /><br />: 3D3D/-3D3D <div>
While 3D content generation has advanced significantly, existing methods
still face challenges with input formats, latent space design, and output
representations. This paper introduces a novel 3D generation framework that
addresses these challenges, offering scalable, high-quality 3D generation with
an interactive Point Cloud-structured Latent space. Our framework employs a
Variational Autoencoder (VAE) with multi-view posed RGB-D(epth)-N(ormal)
renderings as input, using a unique latent space design that preserves 3D shape
information, and incorporates a cascaded latent diffusion model for improved
shape-texture disentanglement. The proposed method, GaussianAnything, supports
multi-modal conditional 3D generation, allowing for point cloud, caption, and
single/multi-view image inputs. Notably, the newly proposed latent space
naturally enables geometry-texture disentanglement, thus allowing 3D-aware
editing. Experimental results demonstrate the effectiveness of our approach on
multiple datasets, outperforming existing methods in both text- and
image-conditioned 3D generation.
]]></content:encoded>
<pubDate>2024-11-12T18:59:32Z</pubDate>
</item>
<item>
<title>LLMPhy: Complex Physical Reasoning Using Large Language Models and World
  Models</title>
<link>http://arxiv.org/abs/2411.08027v1</link>
<guid>http://arxiv.org/abs/2411.08027v1</guid>
<content:encoded><![CDATA[
<div> TraySimLLMPhy
<br />
TraySimLLMPhy-shotLLMTraySimLLMPhy-shot
<br /><br />: <br />TraySimLLMPhy-shot <div>
Physical reasoning is an important skill needed for robotic agents when
operating in the real world. However, solving such reasoning problems often
involves hypothesizing and reflecting over complex multi-body interactions
under the effect of a multitude of physical forces and thus learning all such
interactions poses a significant hurdle for state-of-the-art machine learning
frameworks, including large language models (LLMs). To study this problem, we
propose a new physical reasoning task and a dataset, dubbed TraySim. Our task
involves predicting the dynamics of several objects on a tray that is given an
external impact -- the domino effect of the ensued object interactions and
their dynamics thus offering a challenging yet controlled setup, with the goal
of reasoning being to infer the stability of the objects after the impact. To
solve this complex physical reasoning task, we present LLMPhy, a zero-shot
black-box optimization framework that leverages the physics knowledge and
program synthesis abilities of LLMs, and synergizes these abilities with the
world models built into modern physics engines. Specifically, LLMPhy uses an
LLM to generate code to iteratively estimate the physical hyperparameters of
the system (friction, damping, layout, etc.) via an implicit
analysis-by-synthesis approach using a (non-differentiable) simulator in the
loop and uses the inferred parameters to imagine the dynamics of the scene
towards solving the reasoning task. To show the effectiveness of LLMPhy, we
present experiments on our TraySim dataset to predict the steady-state poses of
the objects. Our results show that the combination of the LLM and the physics
engine leads to state-of-the-art zero-shot physical reasoning performance,
while demonstrating superior convergence against standard black-box
optimization methods and better estimation of the physical parameters.
]]></content:encoded>
<pubDate>2024-11-12T18:56:58Z</pubDate>
</item>
<item>
<title>Incentive Design with Spillovers</title>
<link>http://arxiv.org/abs/2411.08026v1</link>
<guid>http://arxiv.org/abs/2411.08026v1</guid>
<content:encoded><![CDATA[
A principal uses payments conditioned on stochastic outcomes of a team
project to elicit costly effort from the team members. We develop a multi-agent
generalization of a classic first-order approach to contract optimization by
leveraging methods from network games. The main results characterize the
optimal allocation of incentive pay across agents and outcomes. Incentive
optimality requires equalizing, across agents, a product of (i) individual
productivity (ii) organizational centrality and (iii) responsiveness to
monetary incentives.
]]></content:encoded>
<pubDate>2024-11-12T18:56:31Z</pubDate>
</item>
<item>
<title>Tooling or Not Tooling? The Impact of Tools on Language Agents for
  Chemistry Problem Solving</title>
<link>http://arxiv.org/abs/2411.07228v1</link>
<guid>http://arxiv.org/abs/2411.07228v1</guid>
<content:encoded><![CDATA[
<div> ChemAgent, LLM-based agents, tools, specialized chemistry tasks, general chemistry questions
<br /><br />
LLMLLMChemCrowCoscientistChemAgentChemCrowChemAgentLLMs 
<br /><br />: LLMChemAgent <div>
To enhance large language models (LLMs) for chemistry problem solving,
several LLM-based agents augmented with tools have been proposed, such as
ChemCrow and Coscientist. However, their evaluations are narrow in scope,
leaving a large gap in understanding the benefits of tools across diverse
chemistry tasks. To bridge this gap, we develop ChemAgent, an enhanced
chemistry agent over ChemCrow, and conduct a comprehensive evaluation of its
performance on both specialized chemistry tasks and general chemistry
questions. Surprisingly, ChemAgent does not consistently outperform its base
LLMs without tools. Our error analysis with a chemistry expert suggests that:
For specialized chemistry tasks, such as synthesis prediction, we should
augment agents with specialized tools; however, for general chemistry questions
like those in exams, agents' ability to reason correctly with chemistry
knowledge matters more, and tool augmentation does not always help.
]]></content:encoded>
<pubDate>2024-11-11T18:46:37Z</pubDate>
</item>
<item>
<title>DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile
  Manipulation</title>
<link>http://arxiv.org/abs/2411.04999v1</link>
<guid>http://arxiv.org/abs/2411.04999v1</guid>
<content:encoded><![CDATA[
<div> , , , ,  <br />
:<br />
DynaMem3DLLM-DynaMemStretch SE370%2https://dynamem.github.io/ <div>
Significant progress has been made in open-vocabulary mobile manipulation,
where the goal is for a robot to perform tasks in any environment given a
natural language description. However, most current systems assume a static
environment, which limits the system's applicability in real-world scenarios
where environments frequently change due to human intervention or the robot's
own actions. In this work, we present DynaMem, a new approach to open-world
mobile manipulation that uses a dynamic spatio-semantic memory to represent a
robot's environment. DynaMem constructs a 3D data structure to maintain a
dynamic memory of point clouds, and answers open-vocabulary object localization
queries using multimodal LLMs or open-vocabulary features generated by
state-of-the-art vision-language models. Powered by DynaMem, our robots can
explore novel environments, search for objects not found in memory, and
continuously update the memory as objects move, appear, or disappear in the
scene. We run extensive experiments on the Stretch SE3 robots in three real and
nine offline scenes, and achieve an average pick-and-drop success rate of 70%
on non-stationary objects, which is more than a 2x improvement over
state-of-the-art static systems. Our code as well as our experiment and
deployment videos are open sourced and can be found on our project website:
https://dynamem.github.io/
]]></content:encoded>
<pubDate>2024-11-07T18:59:27Z</pubDate>
</item>
<item>
<title>LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation</title>
<link>http://arxiv.org/abs/2411.04997v1</link>
<guid>http://arxiv.org/abs/2411.04997v1</guid>
<content:encoded><![CDATA[
<div> CLIP, multimodal foundational model, LLM, GPT-4, LLaMA<br />
LLM2CLIP, LLMCLIP

<br /><br />:
CLIPLLM2CLIPLLMCLIPCLIPLLM <div>
CLIP is one of the most important multimodal foundational models today. What
powers CLIP's capabilities? The rich supervision signals provided by natural
language, the carrier of human knowledge, shape a powerful cross-modal
representation space. However, with the rapid advancements in large language
models LLMs like GPT-4 and LLaMA, the boundaries of language comprehension and
generation are continually being pushed. This raises an intriguing question:
can the capabilities of LLMs be harnessed to further improve multimodal
representation learning? The potential benefits of incorporating LLMs into CLIP
are clear. LLMs' strong textual understanding can fundamentally improve CLIP's
ability to handle image captions, drastically enhancing its ability to process
long and complex texts, a well-known limitation of vanilla CLIP. Moreover, LLMs
are trained on a vast corpus of text, possessing open-world knowledge. This
allows them to expand on caption information during training, increasing the
efficiency of the learning process. In this paper, we propose LLM2CLIP, a novel
approach that embraces the power of LLMs to unlock CLIP's potential. By
fine-tuning the LLM in the caption space with contrastive learning, we extract
its textual capabilities into the output embeddings, significantly improving
the output layer's textual discriminability. We then design an efficient
training process where the fine-tuned LLM acts as a powerful teacher for CLIP's
visual encoder. Thanks to the LLM's presence, we can now incorporate longer and
more complex captions without being restricted by vanilla CLIP's text encoder's
context window and ability limitations. Our experiments demonstrate that this
approach brings substantial improvements in cross-modal tasks.
]]></content:encoded>
<pubDate>2024-11-07T18:59:16Z</pubDate>
</item>
<item>
<title>HourVideo: 1-Hour Video-Language Understanding</title>
<link>http://arxiv.org/abs/2411.04998v1</link>
<guid>http://arxiv.org/abs/2411.04998v1</guid>
<content:encoded><![CDATA[
We present HourVideo, a benchmark dataset for hour-long video-language
understanding. Our dataset consists of a novel task suite comprising
summarization, perception (recall, tracking), visual reasoning (spatial,
temporal, predictive, causal, counterfactual), and navigation (room-to-room,
object retrieval) tasks. HourVideo includes 500 manually curated egocentric
videos from the Ego4D dataset, spanning durations of 20 to 120 minutes, and
features 12,976 high-quality, five-way multiple-choice questions. Benchmarking
results reveal that multimodal models, including GPT-4 and LLaVA-NeXT, achieve
marginal improvements over random chance. In stark contrast, human experts
significantly outperform the state-of-the-art long-context multimodal model,
Gemini Pro 1.5 (85.0% vs. 37.3%), highlighting a substantial gap in multimodal
capabilities. Our benchmark, evaluation toolkit, prompts, and documentation are
available at https://hourvideo.stanford.edu
]]></content:encoded>
<pubDate>2024-11-07T18:59:16Z</pubDate>
</item>
<item>
<title>Mixture-of-Transformers: A Sparse and Scalable Architecture for
  Multi-Modal Foundation Models</title>
<link>http://arxiv.org/abs/2411.04996v1</link>
<guid>http://arxiv.org/abs/2411.04996v1</guid>
<content:encoded><![CDATA[
The development of large language models (LLMs) has expanded to multi-modal
systems capable of processing text, images, and speech within a unified
framework. Training these models demands significantly larger datasets and
computational resources compared to text-only LLMs. To address the scaling
challenges, we introduce Mixture-of-Transformers (MoT), a sparse multi-modal
transformer architecture that significantly reduces pretraining computational
costs. MoT decouples non-embedding parameters of the model by modality --
including feed-forward networks, attention matrices, and layer normalization --
enabling modality-specific processing with global self-attention over the full
input sequence. We evaluate MoT across multiple settings and model scales. In
the Chameleon 7B setting (autoregressive text-and-image generation), MoT
matches the dense baseline's performance using only 55.8\% of the FLOPs. When
extended to include speech, MoT reaches speech performance comparable to the
dense baseline with only 37.2\% of the FLOPs. In the Transfusion setting, where
text and image are trained with different objectives, a 7B MoT model matches
the image modality performance of the dense baseline with one third of the
FLOPs, and a 760M MoT model outperforms a 1.4B dense baseline across key image
generation metrics. System profiling further highlights MoT's practical
benefits, achieving dense baseline image quality in 47.2\% of the wall-clock
time and text quality in 75.6\% of the wall-clock time (measured on AWS
p4de.24xlarge instances with NVIDIA A100 GPUs).
]]></content:encoded>
<pubDate>2024-11-07T18:59:06Z</pubDate>
</item>
<item>
<title>MME-Finance: A Multimodal Finance Benchmark for Expert-level
  Understanding and Reasoning</title>
<link>http://arxiv.org/abs/2411.03314v1</link>
<guid>http://arxiv.org/abs/2411.03314v1</guid>
<content:encoded><![CDATA[
<div> , , MME-Finance, , <br />
<br />
MME-Finance1019MME-FinanceMME-Finance <br /><br />: <br />MME-Finance19MME-Finance <div>
In recent years, multimodal benchmarks for general domains have guided the
rapid development of multimodal models on general tasks. However, the financial
field has its peculiarities. It features unique graphical images (e.g.,
candlestick charts, technical indicator charts) and possesses a wealth of
specialized financial knowledge (e.g., futures, turnover rate). Therefore,
benchmarks from general fields often fail to measure the performance of
multimodal models in the financial domain, and thus cannot effectively guide
the rapid development of large financial models. To promote the development of
large financial multimodal models, we propose MME-Finance, an bilingual
open-ended and practical usage-oriented Visual Question Answering (VQA)
benchmark. The characteristics of our benchmark are finance and expertise,
which include constructing charts that reflect the actual usage needs of users
(e.g., computer screenshots and mobile photography), creating questions
according to the preferences in financial domain inquiries, and annotating
questions by experts with 10+ years of experience in the financial industry.
Additionally, we have developed a custom-designed financial evaluation system
in which visual information is first introduced in the multi-modal evaluation
process. Extensive experimental evaluations of 19 mainstream MLLMs are
conducted to test their perception, reasoning, and cognition capabilities. The
results indicate that models performing well on general benchmarks cannot do
well on MME-Finance; for instance, the top-performing open-source and
closed-source models obtain 65.69 (Qwen2VL-72B) and 63.18 (GPT-4o),
respectively. Their performance is particularly poor in categories most
relevant to finance, such as candlestick charts and technical indicator charts.
In addition, we propose a Chinese version, which helps compare performance of
MLLMs under a Chinese context.
]]></content:encoded>
<pubDate>2024-11-05T18:59:51Z</pubDate>
</item>
<item>
<title>Adaptive Caching for Faster Video Generation with Diffusion Transformers</title>
<link>http://arxiv.org/abs/2411.02397v1</link>
<guid>http://arxiv.org/abs/2411.02397v1</guid>
<content:encoded><![CDATA[
<div> Diffusion Transformers(DiTs)Adaptive Caching(AdaCache)Motion Regularization(MoReg)
<br /><br />
DiTsAdaptive Caching(AdaCache)Motion Regularization (MoReg)DiTOpen-Sora 720p - 2s4.7<br /><br />: <div>
Generating temporally-consistent high-fidelity videos can be computationally
expensive, especially over longer temporal spans. More-recent Diffusion
Transformers (DiTs) -- despite making significant headway in this context --
have only heightened such challenges as they rely on larger models and heavier
attention mechanisms, resulting in slower inference speeds. In this paper, we
introduce a training-free method to accelerate video DiTs, termed Adaptive
Caching (AdaCache), which is motivated by the fact that "not all videos are
created equal": meaning, some videos require fewer denoising steps to attain a
reasonable quality than others. Building on this, we not only cache
computations through the diffusion process, but also devise a caching schedule
tailored to each video generation, maximizing the quality-latency trade-off. We
further introduce a Motion Regularization (MoReg) scheme to utilize video
information within AdaCache, essentially controlling the compute allocation
based on motion content. Altogether, our plug-and-play contributions grant
significant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video
generation) without sacrificing the generation quality, across multiple video
DiT baselines.
]]></content:encoded>
<pubDate>2024-11-04T18:59:44Z</pubDate>
</item>
<item>
<title>Training-free Regional Prompting for Diffusion Transformers</title>
<link>http://arxiv.org/abs/2411.02395v1</link>
<guid>http://arxiv.org/abs/2411.02395v1</guid>
<content:encoded><![CDATA[
<div> : Diffusion models, text-to-image generation, FLUX.1, regional prompting, DiT architecture

:
DiffusionT5LlamaUNet-basedDiffusion TransformerDiTSD3FLUX.1FLUX.1DiThttps://github.com/antonioo-c/Regional-Prompting-FLUX  <div>
Diffusion models have demonstrated excellent capabilities in text-to-image
generation. Their semantic understanding (i.e., prompt following) ability has
also been greatly improved with large language models (e.g., T5, Llama).
However, existing models cannot perfectly handle long and complex text prompts,
especially when the text prompts contain various objects with numerous
attributes and interrelated spatial relationships. While many regional
prompting methods have been proposed for UNet-based models (SD1.5, SDXL), but
there are still no implementations based on the recent Diffusion Transformer
(DiT) architecture, such as SD3 and FLUX.1.In this report, we propose and
implement regional prompting for FLUX.1 based on attention manipulation, which
enables DiT with fined-grained compositional text-to-image generation
capability in a training-free manner. Code is available at
https://github.com/antonioo-c/Regional-Prompting-FLUX.
]]></content:encoded>
<pubDate>2024-11-04T18:59:05Z</pubDate>
</item>
<item>
<title>Attacking Vision-Language Computer Agents via Pop-ups</title>
<link>http://arxiv.org/abs/2411.02391v1</link>
<guid>http://arxiv.org/abs/2411.02391v1</guid>
<content:encoded><![CDATA[
Autonomous agents powered by large vision and language models (VLM) have
demonstrated significant potential in completing daily computer tasks, such as
browsing the web to book travel and operating desktop software, which requires
agents to understand these interfaces. Despite such visual inputs becoming more
integrated into agentic applications, what types of risks and attacks exist
around them still remain unclear. In this work, we demonstrate that VLM agents
can be easily attacked by a set of carefully designed adversarial pop-ups,
which human users would typically recognize and ignore. This distraction leads
agents to click these pop-ups instead of performing the tasks as usual.
Integrating these pop-ups into existing agent testing environments like OSWorld
and VisualWebArena leads to an attack success rate (the frequency of the agent
clicking the pop-ups) of 86% on average and decreases the task success rate by
47%. Basic defense techniques such as asking the agent to ignore pop-ups or
including an advertisement notice, are ineffective against the attack.
]]></content:encoded>
<pubDate>2024-11-04T18:56:42Z</pubDate>
</item>
<item>
<title>How Far is Video Generation from World Model: A Physical Law Perspective</title>
<link>http://arxiv.org/abs/2411.02385v1</link>
<guid>http://arxiv.org/abs/2411.02385v1</guid>
<content:encoded><![CDATA[
OpenAI's Sora highlights the potential of video generation for developing
world models that adhere to fundamental physical laws. However, the ability of
video generation models to discover such laws purely from visual data without
human priors can be questioned. A world model learning the true law should give
predictions robust to nuances and correctly extrapolate on unseen scenarios. In
this work, we evaluate across three key scenarios: in-distribution,
out-of-distribution, and combinatorial generalization. We developed a 2D
simulation testbed for object movement and collisions to generate videos
deterministically governed by one or more classical mechanics laws. This
provides an unlimited supply of data for large-scale experimentation and
enables quantitative evaluation of whether the generated videos adhere to
physical laws. We trained diffusion-based video generation models to predict
object movements based on initial frames. Our scaling experiments show perfect
generalization within the distribution, measurable scaling behavior for
combinatorial generalization, but failure in out-of-distribution scenarios.
Further experiments reveal two key insights about the generalization mechanisms
of these models: (1) the models fail to abstract general physical rules and
instead exhibit "case-based" generalization behavior, i.e., mimicking the
closest training example; (2) when generalizing to new cases, models are
observed to prioritize different factors when referencing training data: color
> size > velocity > shape. Our study suggests that scaling alone is
insufficient for video generation models to uncover fundamental physical laws,
despite its role in Sora's broader success. See our project page at
https://phyworld.github.io
]]></content:encoded>
<pubDate>2024-11-04T18:53:05Z</pubDate>
</item>
<item>
<title>Teaching Embodied Reinforcement Learning Agents: Informativeness and
  Diversity of Language Use</title>
<link>http://arxiv.org/abs/2410.24218v1</link>
<guid>http://arxiv.org/abs/2410.24218v1</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, embodied agents, language input, task learning, diverse and informative language feedback
<br /><br />
RL
<br /><br />:  <div>
In real-world scenarios, it is desirable for embodied agents to have the
ability to leverage human language to gain explicit or implicit knowledge for
learning tasks. Despite recent progress, most previous approaches adopt simple
low-level instructions as language inputs, which may not reflect natural human
communication. It's not clear how to incorporate rich language use to
facilitate task learning. To address this question, this paper studies
different types of language inputs in facilitating reinforcement learning (RL)
embodied agents. More specifically, we examine how different levels of language
informativeness (i.e., feedback on past behaviors and future guidance) and
diversity (i.e., variation of language expressions) impact agent learning and
inference. Our empirical results based on four RL benchmarks demonstrate that
agents trained with diverse and informative language feedback can achieve
enhanced generalization and fast adaptation to new tasks. These findings
highlight the pivotal role of language use in teaching embodied agents new
tasks in an open world. Project website:
https://github.com/sled-group/Teachable_RL
]]></content:encoded>
<pubDate>2024-10-31T17:59:52Z</pubDate>
</item>
<item>
<title>RelationBooth: Towards Relation-Aware Customized Object Generation</title>
<link>http://arxiv.org/abs/2410.23280v1</link>
<guid>http://arxiv.org/abs/2410.23280v1</guid>
<content:encoded><![CDATA[
<div> : , , , RelationBooth, 

:<br /><br />RelationBoothRelationBooth <div>
Customized image generation is crucial for delivering personalized content
based on user-provided image prompts, aligning large-scale text-to-image
diffusion models with individual needs. However, existing models often overlook
the relationships between customized objects in generated images. Instead, this
work addresses that gap by focusing on relation-aware customized image
generation, which aims to preserve the identities from image prompts while
maintaining the predicate relations described in text prompts. Specifically, we
introduce RelationBooth, a framework that disentangles identity and relation
learning through a well-curated dataset. Our training data consists of
relation-specific images, independent object images containing identity
information, and text prompts to guide relation generation. Then, we propose
two key modules to tackle the two main challenges: generating accurate and
natural relations, especially when significant pose adjustments are required,
and avoiding object confusion in cases of overlap. First, we introduce a
keypoint matching loss that effectively guides the model in adjusting object
poses closely tied to their relationships. Second, we incorporate local
features from the image prompts to better distinguish between objects,
preventing confusion in overlapping cases. Extensive results on three
benchmarks demonstrate the superiority of RelationBooth in generating precise
relations while preserving object identities across a diverse set of objects
and relations. The source code and trained models will be made available to the
public.
]]></content:encoded>
<pubDate>2024-10-30T17:57:21Z</pubDate>
</item>
<item>
<title>SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video
  Generation</title>
<link>http://arxiv.org/abs/2410.23277v1</link>
<guid>http://arxiv.org/abs/2410.23277v1</guid>
<content:encoded><![CDATA[
<div> 

SlowFast-VGenLoRA200kSlowFast-VGen <br /><br />: SlowFast-VGenLoRA <div>
Human beings are endowed with a complementary learning system, which bridges
the slow learning of general world dynamics with fast storage of episodic
memory from a new experience. Previous video generation models, however,
primarily focus on slow learning by pre-training on vast amounts of data,
overlooking the fast learning phase crucial for episodic memory storage. This
oversight leads to inconsistencies across temporally distant frames when
generating longer videos, as these frames fall beyond the model's context
window. To this end, we introduce SlowFast-VGen, a novel dual-speed learning
system for action-driven long video generation. Our approach incorporates a
masked conditional video diffusion model for the slow learning of world
dynamics, alongside an inference-time fast learning strategy based on a
temporal LoRA module. Specifically, the fast learning process updates its
temporal LoRA parameters based on local inputs and outputs, thereby efficiently
storing episodic memory in its parameters. We further propose a slow-fast
learning loop algorithm that seamlessly integrates the inner fast learning loop
into the outer slow learning loop, enabling the recall of prior multi-episode
experiences for context-aware skill learning. To facilitate the slow learning
of an approximate world model, we collect a large-scale dataset of 200k videos
with language action annotations, covering a wide range of scenarios. Extensive
experiments show that SlowFast-VGen outperforms baselines across various
metrics for action-driven video generation, achieving an FVD score of 514
compared to 782, and maintaining consistency in longer videos, with an average
of 0.37 scene cuts versus 0.89. The slow-fast learning loop algorithm
significantly enhances performances on long-horizon planning tasks as well.
Project Website: https://slowfast-vgen.github.io
]]></content:encoded>
<pubDate>2024-10-30T17:55:52Z</pubDate>
</item>
<item>
<title>FISHNET: Financial Intelligence from Sub-querying, Harmonizing,
  Neural-Conditioning, Expert Swarms, and Task Planning</title>
<link>http://arxiv.org/abs/2410.19727v1</link>
<guid>http://arxiv.org/abs/2410.19727v1</guid>
<content:encoded><![CDATA[
<div> : LLMFISHNET

:
LLMsFISHNETFinancial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning, Expert swarming, and Task planning98000FISHNET5.0% Routing61.8%45.6% RAG R-PrecisionFISHNET<br /><br /> <div>
Financial intelligence generation from vast data sources has typically relied
on traditional methods of knowledge-graph construction or database engineering.
Recently, fine-tuned financial domain-specific Large Language Models (LLMs),
have emerged. While these advancements are promising, limitations such as high
inference costs, hallucinations, and the complexity of concurrently analyzing
high-dimensional financial data, emerge. This motivates our invention FISHNET
(Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning,
Expert swarming, and Task planning), an agentic architecture that accomplishes
highly complex analytical tasks for more than 98,000 regulatory filings that
vary immensely in terms of semantics, data hierarchy, or format. FISHNET shows
remarkable performance for financial insight generation (61.8% success rate
over 5.0% Routing, 45.6% RAG R-Precision). We conduct rigorous ablations to
empirically prove the success of FISHNET, each agent's importance, and the
optimized performance of assembling all agents. Our modular architecture can be
leveraged for a myriad of use-cases, enabling scalability, flexibility, and
data integrity that are critical for financial tasks.
]]></content:encoded>
<pubDate>2024-10-25T17:53:47Z</pubDate>
</item>
<item>
<title>CAMEL-Bench: A Comprehensive Arabic LMM Benchmark</title>
<link>http://arxiv.org/abs/2410.18976v1</link>
<guid>http://arxiv.org/abs/2410.18976v1</guid>
<content:encoded><![CDATA[
<div> LMMs, Arabic language, CAMEL-Bench, evaluation, model assessment <br />
LMM(CAMEL-Bench)838CAMEL-Bench29,036LMMGPT-4o62%<br />
1. LMMCAMEL-Bench
2. CAMEL-Bench838
3. 29,036
4. LMM
5.  <div>
Recent years have witnessed a significant interest in developing large
multimodal models (LMMs) capable of performing various visual reasoning and
understanding tasks. This has led to the introduction of multiple LMM
benchmarks to evaluate LMMs on different tasks. However, most existing LMM
evaluation benchmarks are predominantly English-centric. In this work, we
develop a comprehensive LMM evaluation benchmark for the Arabic language to
represent a large population of over 400 million speakers. The proposed
benchmark, named CAMEL-Bench, comprises eight diverse domains and 38
sub-domains including, multi-image understanding, complex visual perception,
handwritten document understanding, video understanding, medical imaging, plant
diseases, and remote sensing-based land use understanding to evaluate broad
scenario generalizability. Our CAMEL-Bench comprises around 29,036 questions
that are filtered from a larger pool of samples, where the quality is manually
verified by native speakers to ensure reliable model assessment. We conduct
evaluations of both closed-source, including GPT-4 series, and open-source
LMMs. Our analysis reveals the need for substantial improvement, especially
among the best open-source models, with even the closed-source GPT-4o achieving
an overall score of 62%. Our benchmark and evaluation scripts are open-sourced.
]]></content:encoded>
<pubDate>2024-10-24T17:59:38Z</pubDate>
</item>
<item>
<title>3D-Adapter: Geometry-Consistent Multi-View Diffusion for High-Quality 3D
  Generation</title>
<link>http://arxiv.org/abs/2410.18974v1</link>
<guid>http://arxiv.org/abs/2410.18974v1</guid>
<content:encoded><![CDATA[
<div> 3D-Adapter, 3D geometry awareness, image diffusion models, 3D generation, pretrained base model

:<br /><br />3D-Adapter3D3D3D-Adapter3DRGBD3D-Adapter3D-AdapterStable Diffusion3D3D-Adapter3D3D <div>
Multi-view image diffusion models have significantly advanced open-domain 3D
object generation. However, most existing models rely on 2D network
architectures that lack inherent 3D biases, resulting in compromised geometric
consistency. To address this challenge, we introduce 3D-Adapter, a plug-in
module designed to infuse 3D geometry awareness into pretrained image diffusion
models. Central to our approach is the idea of 3D feedback augmentation: for
each denoising step in the sampling loop, 3D-Adapter decodes intermediate
multi-view features into a coherent 3D representation, then re-encodes the
rendered RGBD views to augment the pretrained base model through feature
addition. We study two variants of 3D-Adapter: a fast feed-forward version
based on Gaussian splatting and a versatile training-free version utilizing
neural fields and meshes. Our extensive experiments demonstrate that 3D-Adapter
not only greatly enhances the geometry quality of text-to-multi-view models
such as Instant3D and Zero123++, but also enables high-quality 3D generation
using the plain text-to-image Stable Diffusion. Furthermore, we showcase the
broad application potential of 3D-Adapter by presenting high quality results in
text-to-3D, image-to-3D, text-to-texture, and text-to-avatar tasks.
]]></content:encoded>
<pubDate>2024-10-24T17:59:30Z</pubDate>
</item>
<item>
<title>Deep Insights into Cognitive Decline: A Survey of Leveraging
  Non-Intrusive Modalities with Deep Learning Techniques</title>
<link>http://arxiv.org/abs/2410.18972v1</link>
<guid>http://arxiv.org/abs/2410.18972v1</guid>
<content:encoded><![CDATA[
Cognitive decline is a natural part of aging, often resulting in reduced
cognitive abilities. In some cases, however, this decline is more pronounced,
typically due to disorders such as Alzheimer's disease. Early detection of
anomalous cognitive decline is crucial, as it can facilitate timely
professional intervention. While medical data can help in this detection, it
often involves invasive procedures. An alternative approach is to employ
non-intrusive techniques such as speech or handwriting analysis, which do not
necessarily affect daily activities. This survey reviews the most relevant
methodologies that use deep learning techniques to automate the cognitive
decline estimation task, including audio, text, and visual processing. We
discuss the key features and advantages of each modality and methodology,
including state-of-the-art approaches like Transformer architecture and
foundation models. In addition, we present works that integrate different
modalities to develop multimodal models. We also highlight the most significant
datasets and the quantitative results from studies using these resources. From
this review, several conclusions emerge. In most cases, the textual modality
achieves the best results and is the most relevant for detecting cognitive
decline. Moreover, combining various approaches from individual modalities into
a multimodal model consistently enhances performance across nearly all
scenarios.
]]></content:encoded>
<pubDate>2024-10-24T17:59:21Z</pubDate>
</item>
<item>
<title>Prioritized Generative Replay</title>
<link>http://arxiv.org/abs/2410.18082v1</link>
<guid>http://arxiv.org/abs/2410.18082v1</guid>
<content:encoded><![CDATA[
<div> 
<br />
-to-
<br /><br />
:  <div>
Sample-efficient online reinforcement learning often uses replay buffers to
store experience for reuse when updating the value function. However, uniform
replay is inefficient, since certain classes of transitions can be more
relevant to learning. While prioritization of more useful samples is helpful,
this strategy can also lead to overfitting, as useful samples are likely to be
more rare. In this work, we instead propose a prioritized, parametric version
of an agent's memory, using generative models to capture online experience.
This paradigm enables (1) densification of past experience, with new
generations that benefit from the generative model's generalization capacity
and (2) guidance via a family of "relevance functions" that push these
generations towards more useful parts of an agent's acquired history. We show
this recipe can be instantiated using conditional diffusion models and simple
relevance functions such as curiosity- or value-based metrics. Our approach
consistently improves performance and sample efficiency in both state- and
pixel-based domains. We expose the mechanisms underlying these gains, showing
how guidance promotes diversity in our generated transitions and reduces
overfitting. We also showcase how our approach can train policies with even
higher update-to-data ratios than before, opening up avenues to better scale
online RL agents.
]]></content:encoded>
<pubDate>2024-10-23T17:59:52Z</pubDate>
</item>
<item>
<title>UnCLe: Unsupervised Continual Learning of Depth Completion</title>
<link>http://arxiv.org/abs/2410.18074v1</link>
<guid>http://arxiv.org/abs/2410.18074v1</guid>
<content:encoded><![CDATA[
<div> UnCLe, benchmark, unsupervised continual learning, depth completion, catastrophic forgetting
<br />
UnCLeUnCLeUnCLe 
<br /><br />: UnCLe <div>
We propose UnCLe, a standardized benchmark for Unsupervised Continual
Learning of a multimodal depth estimation task: Depth completion aims to infer
a dense depth map from a pair of synchronized RGB image and sparse depth map.
We benchmark depth completion models under the practical scenario of
unsupervised learning over continuous streams of data. Existing methods are
typically trained on a static, or stationary, dataset. However, when adapting
to novel non-stationary distributions, they "catastrophically forget"
previously learned information. UnCLe simulates these non-stationary
distributions by adapting depth completion models to sequences of datasets
containing diverse scenes captured from distinct domains using different visual
and range sensors. We adopt representative methods from continual learning
paradigms and translate them to enable unsupervised continual learning of depth
completion. We benchmark these models for indoor and outdoor and investigate
the degree of catastrophic forgetting through standard quantitative metrics.
Furthermore, we introduce model inversion quality as an additional measure of
forgetting. We find that unsupervised continual learning of depth completion is
an open problem, and we invite researchers to leverage UnCLe as a development
platform.
]]></content:encoded>
<pubDate>2024-10-23T17:56:33Z</pubDate>
</item>
<item>
<title>WorldSimBench: Towards Video Generation Models as World Simulators</title>
<link>http://arxiv.org/abs/2410.18072v1</link>
<guid>http://arxiv.org/abs/2410.18072v1</guid>
<content:encoded><![CDATA[
Recent advancements in predictive models have demonstrated exceptional
capabilities in predicting the future state of objects and scenes. However, the
lack of categorization based on inherent characteristics continues to hinder
the progress of predictive model development. Additionally, existing benchmarks
are unable to effectively evaluate higher-capability, highly embodied
predictive models from an embodied perspective. In this work, we classify the
functionalities of predictive models into a hierarchy and take the first step
in evaluating World Simulators by proposing a dual evaluation framework called
WorldSimBench. WorldSimBench includes Explicit Perceptual Evaluation and
Implicit Manipulative Evaluation, encompassing human preference assessments
from the visual perspective and action-level evaluations in embodied tasks,
covering three representative embodied scenarios: Open-Ended Embodied
Environment, Autonomous, Driving, and Robot Manipulation. In the Explicit
Perceptual Evaluation, we introduce the HF-Embodied Dataset, a video assessment
dataset based on fine-grained human feedback, which we use to train a Human
Preference Evaluator that aligns with human perception and explicitly assesses
the visual fidelity of World Simulators. In the Implicit Manipulative
Evaluation, we assess the video-action consistency of World Simulators by
evaluating whether the generated situation-aware video can be accurately
translated into the correct control signals in dynamic environments. Our
comprehensive evaluation offers key insights that can drive further innovation
in video generation models, positioning World Simulators as a pivotal
advancement toward embodied artificial intelligence.
]]></content:encoded>
<pubDate>2024-10-23T17:56:11Z</pubDate>
</item>
<item>
<title>TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing
  Prompts</title>
<link>http://arxiv.org/abs/2410.18071v1</link>
<guid>http://arxiv.org/abs/2410.18071v1</guid>
<content:encoded><![CDATA[
Recently, multimodal large language models (MLLMs) have received much
attention for their impressive capabilities. The evaluation of MLLMs is
becoming critical to analyzing attributes of MLLMs and providing valuable
insights. However, current benchmarks overlook the problem of prompt
sensitivity - minor prompt variations may lead to significant performance
fluctuations. Thus, inappropriate prompts may obscure the models' capabilities,
underestimating the models' performance. Moreover, different models have
different preferences for different prompts, and thus, using the same prompt
for all models will cause evaluation bias. This paper analyzes this deficiency
in existing benchmarks and further introduces a new evaluation framework named
TP-Eval, which introduces a prompt customization method to reduce evaluation
biases and tap models' potential. TP-Eval will rewrite the original prompts to
different customized prompts for different models. In particular, we propose
some well-designed modules for prompt customization tailored to the scenario of
MLLM evaluation. Extensive experiments demonstrate the effectiveness of our
approach to uncovering models' capabilities, and TP-Eval should benefit the
community in developing more comprehensive and convincing MLLM evaluation
benchmarks.
]]></content:encoded>
<pubDate>2024-10-23T17:54:43Z</pubDate>
</item>
<item>
<title>Altogether: Image Captioning via Re-aligning Alt-text</title>
<link>http://arxiv.org/abs/2410.17251v1</link>
<guid>http://arxiv.org/abs/2410.17251v1</guid>
<content:encoded><![CDATA[
<div> : , , Alt-text, , 
: 
Alt-text<br /><br /> <div>
This paper focuses on creating synthetic data to improve the quality of image
captions. Existing works typically have two shortcomings. First, they caption
images from scratch, ignoring existing alt-text metadata, and second, lack
transparency if the captioners' training data (e.g. GPT) is unknown. In this
paper, we study a principled approach Altogether based on the key idea to edit
and re-align existing alt-texts associated with the images. To generate
training data, we perform human annotation where annotators start with the
existing alt-text and re-align it to the image content in multiple rounds,
consequently constructing captions with rich visual concepts. This differs from
prior work that carries out human annotation as a one-time description task
solely based on images and annotator knowledge. We train a captioner on this
data that generalizes the process of re-aligning alt-texts at scale. Our
results show our Altogether approach leads to richer image captions that also
improve text-to-image generation and zero-shot image classification tasks.
]]></content:encoded>
<pubDate>2024-10-22T17:59:57Z</pubDate>
</item>
<item>
<title>Frontiers in Intelligent Colonoscopy</title>
<link>http://arxiv.org/abs/2410.17241v1</link>
<guid>http://arxiv.org/abs/2410.17241v1</guid>
<content:encoded><![CDATA[
<div> : Colonoscopy, Multimodal, Scene perception, Vision-language understanding, Intelligent techniques

:<br /><br /> ColonINSTColonGPThttps://github.com/ai4colonoscopy/IntelliScope <div>
Colonoscopy is currently one of the most sensitive screening methods for
colorectal cancer. This study investigates the frontiers of intelligent
colonoscopy techniques and their prospective implications for multimodal
medical applications. With this goal, we begin by assessing the current
data-centric and model-centric landscapes through four tasks for colonoscopic
scene perception, including classification, detection, segmentation, and
vision-language understanding. This assessment enables us to identify
domain-specific challenges and reveals that multimodal research in colonoscopy
remains open for further exploration. To embrace the coming multimodal era, we
establish three foundational initiatives: a large-scale multimodal instruction
tuning dataset ColonINST, a colonoscopy-designed multimodal language model
ColonGPT, and a multimodal benchmark. To facilitate ongoing monitoring of this
rapidly evolving field, we provide a public website for the latest updates:
https://github.com/ai4colonoscopy/IntelliScope.
]]></content:encoded>
<pubDate>2024-10-22T17:57:12Z</pubDate>
</item>
<item>
<title>xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video
  Even in VLMs</title>
<link>http://arxiv.org/abs/2410.16267v1</link>
<guid>http://arxiv.org/abs/2410.16267v1</guid>
<content:encoded><![CDATA[
<div> : xGen-MM-Vid, BLIP-3-Video, multimodal language model, temporal encoder, visual tokens

xGen-MM-VidBLIP-3-Video BLIP-3-VideoBLIP3-Video32 vs. 4608 tokensToken Turing MachinesBLIP-3-Video34B4Bhttps://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html

<br /><br />: 
xGen-MM-VidBLIP-3-VideoBLIP-3-VideoBLIP-3-Video <div>
We present xGen-MM-Vid (BLIP-3-Video): a multimodal language model for
videos, particularly designed to efficiently capture temporal information over
multiple frames. BLIP-3-Video takes advantage of the 'temporal encoder' in
addition to the conventional visual tokenizer, which maps a sequence of tokens
over multiple frames into a compact set of visual tokens. This enables
BLIP3-Video to use much fewer visual tokens than its competing models (e.g., 32
vs. 4608 tokens). We explore different types of temporal encoders, including
learnable spatio-temporal pooling as well as sequential models like Token
Turing Machines. We experimentally confirm that BLIP-3-Video obtains video
question-answering accuracies comparable to much larger state-of-the-art models
(e.g., 34B), while being much smaller (i.e., 4B) and more efficient by using
fewer visual tokens. The project website is at
https://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html
]]></content:encoded>
<pubDate>2024-10-21T17:59:11Z</pubDate>
</item>
<item>
<title>3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with
  View-consistent 2D Diffusion Priors</title>
<link>http://arxiv.org/abs/2410.16266v1</link>
<guid>http://arxiv.org/abs/2410.16266v1</guid>
<content:encoded><![CDATA[
<div> novel-view synthesis, 3D Gaussian splatting, 3DGS-Enhancer, view-consistent latent features, spatial-temporal decoder 

3D3DGS3DGS-Enhancer3DGS2D3D3DGS-Enhancer-3DGS3DGS-Enhancer

<br /><br />: 
1. novel-view synthesis3D Gaussian splatting
2. 3DGS-Enhancer3DGS
3. 2D3D
4. -
5. 3DGS-Enhancer <div>
Novel-view synthesis aims to generate novel views of a scene from multiple
input images or videos, and recent advancements like 3D Gaussian splatting
(3DGS) have achieved notable success in producing photorealistic renderings
with efficient pipelines. However, generating high-quality novel views under
challenging settings, such as sparse input views, remains difficult due to
insufficient information in under-sampled areas, often resulting in noticeable
artifacts. This paper presents 3DGS-Enhancer, a novel pipeline for enhancing
the representation quality of 3DGS representations. We leverage 2D video
diffusion priors to address the challenging 3D view consistency problem,
reformulating it as achieving temporal consistency within a video generation
process. 3DGS-Enhancer restores view-consistent latent features of rendered
novel views and integrates them with the input views through a spatial-temporal
decoder. The enhanced views are then used to fine-tune the initial 3DGS model,
significantly improving its rendering performance. Extensive experiments on
large-scale datasets of unbounded scenes demonstrate that 3DGS-Enhancer yields
superior reconstruction performance and high-fidelity rendering results
compared to state-of-the-art methods. The project webpage is
https://xiliu8006.github.io/3DGS-Enhancer-project .
]]></content:encoded>
<pubDate>2024-10-21T17:59:09Z</pubDate>
</item>
<item>
<title>Agent-to-Sim: Learning Interactive Behavior Models from Casual
  Longitudinal Videos</title>
<link>http://arxiv.org/abs/2410.16259v1</link>
<guid>http://arxiv.org/abs/2410.16259v1</guid>
<content:encoded><![CDATA[
We present Agent-to-Sim (ATS), a framework for learning interactive behavior
models of 3D agents from casual longitudinal video collections. Different from
prior works that rely on marker-based tracking and multiview cameras, ATS
learns natural behaviors of animal and human agents non-invasively through
video observations recorded over a long time-span (e.g., a month) in a single
environment. Modeling 3D behavior of an agent requires persistent 3D tracking
(e.g., knowing which point corresponds to which) over a long time period. To
obtain such data, we develop a coarse-to-fine registration method that tracks
the agent and the camera over time through a canonical 3D space, resulting in a
complete and persistent spacetime 4D representation. We then train a generative
model of agent behaviors using paired data of perception and motion of an agent
queried from the 4D reconstruction. ATS enables real-to-sim transfer from video
recordings of an agent to an interactive behavior simulator. We demonstrate
results on pets (e.g., cat, dog, bunny) and human given monocular RGBD videos
captured by a smartphone.
]]></content:encoded>
<pubDate>2024-10-21T17:57:50Z</pubDate>
</item>
<item>
<title>Elucidating the design space of language models for image generation</title>
<link>http://arxiv.org/abs/2410.16257v1</link>
<guid>http://arxiv.org/abs/2410.16257v1</guid>
<content:encoded><![CDATA[
The success of autoregressive (AR) language models in text generation has
inspired the computer vision community to adopt Large Language Models (LLMs)
for image generation. However, considering the essential differences between
text and image modalities, the design space of language models for image
generation remains underexplored. We observe that image tokens exhibit greater
randomness compared to text tokens, which presents challenges when training
with token prediction. Nevertheless, AR models demonstrate their potential by
effectively learning patterns even from a seemingly suboptimal optimization
problem. Our analysis also reveals that while all models successfully grasp the
importance of local information in image generation, smaller models struggle to
capture the global context. In contrast, larger models showcase improved
capabilities in this area, helping to explain the performance gains achieved
when scaling up model size. We further elucidate the design space of language
models for vision generation, including tokenizer choice, model choice, model
scalability, vocabulary design, and sampling strategy through extensive
comparative experiments. Our work is the first to analyze the optimization
behavior of language models in vision generation, and we believe it can inspire
more effective designs when applying LMs to other domains. Finally, our
elucidated language model for image generation, termed as ELM, achieves
state-of-the-art performance on the ImageNet 256*256 benchmark. The code is
available at https://github.com/Pepperlll/LMforImageGeneration.git.
]]></content:encoded>
<pubDate>2024-10-21T17:57:04Z</pubDate>
</item>
<item>
<title>BiGR: Harnessing Binary Latent Codes for Image Generation and Improved
  Visual Representation Capabilities</title>
<link>http://arxiv.org/abs/2410.14672v1</link>
<guid>http://arxiv.org/abs/2410.14672v1</guid>
<content:encoded><![CDATA[
<div> conditional image generation, binary latent codes, generative training, entropy-ordered sampling, vision tasks
<br /><br />:<br />
BiGRBiGRBiGRFID-50kBiGR-shotBiGR <div>
We introduce BiGR, a novel conditional image generation model using compact
binary latent codes for generative training, focusing on enhancing both
generation and representation capabilities. BiGR is the first conditional
generative model that unifies generation and discrimination within the same
framework. BiGR features a binary tokenizer, a masked modeling mechanism, and a
binary transcoder for binary code prediction. Additionally, we introduce a
novel entropy-ordered sampling method to enable efficient image generation.
Extensive experiments validate BiGR's superior performance in generation
quality, as measured by FID-50k, and representation capabilities, as evidenced
by linear-probe accuracy. Moreover, BiGR showcases zero-shot generalization
across various vision tasks, enabling applications such as image inpainting,
outpainting, editing, interpolation, and enrichment, without the need for
structural modifications. Our findings suggest that BiGR unifies generative and
discriminative tasks effectively, paving the way for further advancements in
the field.
]]></content:encoded>
<pubDate>2024-10-18T17:59:04Z</pubDate>
</item>
<item>
<title>MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image
  Description and Reasoning Steps</title>
<link>http://arxiv.org/abs/2410.14668v1</link>
<guid>http://arxiv.org/abs/2410.14668v1</guid>
<content:encoded><![CDATA[
<div> : Multimodal Chain of Thought, MCoT, MiCEval, , 
:
Multimodal Chain of Thought (MCoT)(MLLMs)MiCEvalMLLMMiCEvalMiCEvalhttps://github.com/alenai97/MiCEval <br /><br /> <div>
Multimodal Chain of Thought (MCoT) is a popular prompting strategy for
improving the performance of multimodal large language models (MLLMs) across a
range of complex reasoning tasks. Despite its popularity, there is a notable
absence of automated methods for evaluating the quality of reasoning steps in
MCoT. To address this gap, we propose Multimodal Chain-of-Thought Evaluation
(MiCEval), a framework designed to assess the correctness of reasoning chains
by evaluating the quality of both the description and each reasoning step. The
evaluation of the description component focuses on the accuracy of the image
descriptions, while the reasoning step evaluates the quality of each step as it
is conditionally generated based on the preceding steps. MiCEval is built upon
a fine-grained dataset with annotations that rate each step according to
correctness, relevance, and informativeness. Extensive experiments on four
state-of-the-art MLLMs show that step-wise evaluations using MiCEval align more
closely with human judgments compared to existing methods based on cosine
similarity or fine-tuning approaches. MiCEval datasets and code can be found in
https://github.com/alenai97/MiCEval.
]]></content:encoded>
<pubDate>2024-10-18T17:57:40Z</pubDate>
</item>
<item>
<title>Fluid: Scaling Autoregressive Text-to-image Generative Models with
  Continuous Tokens</title>
<link>http://arxiv.org/abs/2410.13863v1</link>
<guid>http://arxiv.org/abs/2410.13863v1</guid>
<content:encoded><![CDATA[
<div> : autoregressive models, text-to-image generation, discrete tokens, continuous tokens, transformer architectures

:<br /><br />BERTGPTFIDGenEvalGenEvalGenEvalFluidFluid 10.5BMS-COCO 30KFID6.16GenEval0.69 <div>
Scaling up autoregressive models in vision has not proven as beneficial as in
large language models. In this work, we investigate this scaling problem in the
context of text-to-image generation, focusing on two critical factors: whether
models use discrete or continuous tokens, and whether tokens are generated in a
random or fixed raster order using BERT- or GPT-like transformer architectures.
Our empirical results show that, while all models scale effectively in terms of
validation loss, their evaluation performance -- measured by FID, GenEval
score, and visual quality -- follows different trends. Models based on
continuous tokens achieve significantly better visual quality than those using
discrete tokens. Furthermore, the generation order and attention mechanisms
significantly affect the GenEval score: random-order models achieve notably
better GenEval scores compared to raster-order models. Inspired by these
findings, we train Fluid, a random-order autoregressive model on continuous
tokens. Fluid 10.5B model achieves a new state-of-the-art zero-shot FID of 6.16
on MS-COCO 30K, and 0.69 overall score on the GenEval benchmark. We hope our
findings and results will encourage future efforts to further bridge the
scaling gap between vision and language models.
]]></content:encoded>
<pubDate>2024-10-17T17:59:59Z</pubDate>
</item>
<item>
<title>PUMA: Empowering Unified MLLM with Multi-granular Visual Generation</title>
<link>http://arxiv.org/abs/2410.13861v1</link>
<guid>http://arxiv.org/abs/2410.13861v1</guid>
<content:encoded><![CDATA[
<div> -MLLMPUMA

PUMAMLLMMLLMPUMAMLLMPUMAhttps://github.com/rongyaofang/PUMA<br /><br />: -MLLMPUMAMLLM <div>
Recent advancements in multimodal foundation models have yielded significant
progress in vision-language understanding. Initial attempts have also explored
the potential of multimodal large language models (MLLMs) for visual content
generation. However, existing works have insufficiently addressed the varying
granularity demands of different image generation tasks within a unified MLLM
paradigm - from the diversity required in text-to-image generation to the
precise controllability needed in image manipulation. In this work, we propose
PUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA
unifies multi-granular visual features as both inputs and outputs of MLLMs,
elegantly addressing the different granularity requirements of various image
generation tasks within a unified MLLM framework. Following multimodal
pretraining and task-specific instruction tuning, PUMA demonstrates proficiency
in a wide range of multimodal tasks. This work represents a significant step
towards a truly unified MLLM capable of adapting to the granularity demands of
various visual tasks. The code and model will be released in
https://github.com/rongyaofang/PUMA.
]]></content:encoded>
<pubDate>2024-10-17T17:59:57Z</pubDate>
</item>
<item>
<title>$-$MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large
  Language Models</title>
<link>http://arxiv.org/abs/2410.13859v1</link>
<guid>http://arxiv.org/abs/2410.13859v1</guid>
<content:encoded><![CDATA[
Despite the significant progress in multimodal large language models (MLLMs),
their high computational cost remains a barrier to real-world deployment.
Inspired by the mixture of depths (MoDs) in natural language processing, we aim
to address this limitation from the perspective of ``activated tokens''. Our
key insight is that if most tokens are redundant for the layer computation,
then can be skipped directly via the MoD layer. However, directly converting
the dense layers of MLLMs to MoD layers leads to substantial performance
degradation. To address this issue, we propose an innovative MoD adaptation
strategy for existing MLLMs called $\gamma$-MoD. In $\gamma$-MoD, a novel
metric is proposed to guide the deployment of MoDs in the MLLM, namely rank of
attention maps (ARank). Through ARank, we can effectively identify which layer
is redundant and should be replaced with the MoD layer. Based on ARank, we
further propose two novel designs to maximize the computational sparsity of
MLLM while maintaining its performance, namely shared vision-language router
and masked routing learning. With these designs, more than 90% dense layers of
the MLLM can be effectively converted to the MoD ones. To validate our method,
we apply it to three popular MLLMs, and conduct extensive experiments on 9
benchmark datasets. Experimental results not only validate the significant
efficiency benefit of $\gamma$-MoD to existing MLLMs but also confirm its
generalization ability on various MLLMs. For example, with a minor performance
drop, i.e., -1.5%, $\gamma$-MoD can reduce the training and inference time of
LLaVA-HR by 31.0% and 53.2%, respectively.
]]></content:encoded>
<pubDate>2024-10-17T17:59:53Z</pubDate>
</item>
<item>
<title>The Curse of Multi-Modalities: Evaluating Hallucinations of Large
  Multimodal Models across Language, Visual, and Audio</title>
<link>http://arxiv.org/abs/2410.12787v1</link>
<guid>http://arxiv.org/abs/2410.12787v1</guid>
<content:encoded><![CDATA[
<div> CMM
<br /><br />:
CMM <div>
Recent advancements in large multimodal models (LMMs) have significantly
enhanced performance across diverse tasks, with ongoing efforts to further
integrate additional modalities such as video and audio. However, most existing
LMMs remain vulnerable to hallucinations, the discrepancy between the factual
multimodal input and the generated textual output, which has limited their
applicability in various real-world scenarios. This paper presents the first
systematic investigation of hallucinations in LMMs involving the three most
common modalities: language, visual, and audio. Our study reveals two key
contributors to hallucinations: overreliance on unimodal priors and spurious
inter-modality correlations. To address these challenges, we introduce the
benchmark The Curse of Multi-Modalities (CMM), which comprehensively evaluates
hallucinations in LMMs, providing a detailed analysis of their underlying
issues. Our findings highlight key vulnerabilities, including imbalances in
modality integration and biases from training data, underscoring the need for
balanced cross-modal learning and enhanced hallucination mitigation strategies.
Based on our observations and findings, we suggest potential research
directions that could enhance the reliability of LMMs.
]]></content:encoded>
<pubDate>2024-10-16T17:59:02Z</pubDate>
</item>
<item>
<title>JudgeBench: A Benchmark for Evaluating LLM-based Judges</title>
<link>http://arxiv.org/abs/2410.12784v1</link>
<guid>http://arxiv.org/abs/2410.12784v1</guid>
<content:encoded><![CDATA[
<div> , LLM-based judges, JudgeBench, , GitHub
:<br />
LLM-based judgesJudgeBenchJudgeBenchGPT-4oJudgeBenchLLM-based judgesGitHub <div>
LLM-based judges have emerged as a scalable alternative to human evaluation
and are increasingly used to assess, compare, and improve models. However, the
reliability of LLM-based judges themselves is rarely scrutinized. As LLMs
become more advanced, their responses grow more sophisticated, requiring
stronger judges to evaluate them. Existing benchmarks primarily focus on a
judge's alignment with human preferences, but often fail to account for more
challenging tasks where crowdsourced human preference is a poor indicator of
factual and logical correctness. To address this, we propose a novel evaluation
framework to objectively evaluate LLM-based judges. Based on this framework, we
propose JudgeBench, a benchmark for evaluating LLM-based judges on challenging
response pairs spanning knowledge, reasoning, math, and coding. JudgeBench
leverages a novel pipeline for converting existing difficult datasets into
challenging response pairs with preference labels reflecting objective
correctness. Our comprehensive evaluation on a collection of prompted judges,
fine-tuned judges, multi-agent judges, and reward models shows that JudgeBench
poses a significantly greater challenge than previous benchmarks, with many
strong models (e.g., GPT-4o) performing just slightly better than random
guessing. Overall, JudgeBench offers a reliable platform for assessing
increasingly advanced LLM-based judges. Data and code are available at
https://github.com/ScalerLab/JudgeBench .
]]></content:encoded>
<pubDate>2024-10-16T17:58:19Z</pubDate>
</item>
<item>
<title>On the Effectiveness of Dataset Alignment for Fake Image Detection</title>
<link>http://arxiv.org/abs/2410.11835v1</link>
<guid>http://arxiv.org/abs/2410.11835v1</guid>
<content:encoded><![CDATA[
<div> fake image detector, latent diffusion models (LDMs), dataset alignment, generative models, image generation<br />
<br />
LDMs/LDMsLDMs<br /><br />: <br />fake image detectorLDMs <div>
As latent diffusion models (LDMs) democratize image generation capabilities,
there is a growing need to detect fake images. A good detector should focus on
the generative models fingerprints while ignoring image properties such as
semantic content, resolution, file format, etc. Fake image detectors are
usually built in a data driven way, where a model is trained to separate real
from fake images. Existing works primarily investigate network architecture
choices and training recipes. In this work, we argue that in addition to these
algorithmic choices, we also require a well aligned dataset of real/fake images
to train a robust detector. For the family of LDMs, we propose a very simple
way to achieve this: we reconstruct all the real images using the LDMs
autoencoder, without any denoising operation. We then train a model to separate
these real images from their reconstructions. The fakes created this way are
extremely similar to the real ones in almost every aspect (e.g., size, aspect
ratio, semantic content), which forces the model to look for the LDM decoders
artifacts. We empirically show that this way of creating aligned real/fake
datasets, which also sidesteps the computationally expensive denoising process,
helps in building a detector that focuses less on spurious correlations,
something that a very popular existing method is susceptible to. Finally, to
demonstrate just how effective the alignment in a dataset can be, we build a
detector using images that are not natural objects, and present promising
results. Overall, our work identifies the subtle but significant issues that
arise when training a fake image detector and proposes a simple and inexpensive
solution to address these problems.
]]></content:encoded>
<pubDate>2024-10-15T17:58:07Z</pubDate>
</item>
<item>
<title>Tex4D: Zero-shot 4D Scene Texturing with Video Diffusion Models</title>
<link>http://arxiv.org/abs/2410.10821v1</link>
<guid>http://arxiv.org/abs/2410.10821v1</guid>
<content:encoded><![CDATA[
<div> 4D, , , , 

Tex4D-shot3D4DUVDDIMTex4D4D <br /><br />: 4D, , , ,  <div>
3D meshes are widely used in computer vision and graphics for their
efficiency in animation and minimal memory use, playing a crucial role in
movies, games, AR, and VR. However, creating temporally consistent and
realistic textures for mesh sequences remains labor-intensive for professional
artists. On the other hand, while video diffusion models excel at text-driven
video generation, they often lack 3D geometry awareness and struggle with
achieving multi-view consistent texturing for 3D meshes. In this work, we
present Tex4D, a zero-shot approach that integrates inherent 3D geometry
knowledge from mesh sequences with the expressiveness of video diffusion models
to produce multi-view and temporally consistent 4D textures. Given an
untextured mesh sequence and a text prompt as inputs, our method enhances
multi-view consistency by synchronizing the diffusion process across different
views through latent aggregation in the UV space. To ensure temporal
consistency, we leverage prior knowledge from a conditional video generation
model for texture synthesis. However, straightforwardly combining the video
diffusion model and the UV texture aggregation leads to blurry results. We
analyze the underlying causes and propose a simple yet effective modification
to the DDIM sampling process to address this issue. Additionally, we introduce
a reference latent texture to strengthen the correlation between frames during
the denoising process. To the best of our knowledge, Tex4D is the first method
specifically designed for 4D scene texturing. Extensive experiments demonstrate
its superiority in producing multi-view and multi-frame consistent videos based
on untextured mesh sequences.
]]></content:encoded>
<pubDate>2024-10-14T17:59:59Z</pubDate>
</item>
<item>
<title>When Does Perceptual Alignment Benefit Vision Representations?</title>
<link>http://arxiv.org/abs/2410.10817v1</link>
<guid>http://arxiv.org/abs/2410.10817v1</guid>
<content:encoded><![CDATA[
<div> <br />
:<br />
3D <div>
Humans judge perceptual similarity according to diverse visual attributes,
including scene layout, subject location, and camera pose. Existing vision
models understand a wide range of semantic abstractions but improperly weigh
these attributes and thus make inferences misaligned with human perception.
While vision representations have previously benefited from alignment in
contexts like image generation, the utility of perceptually aligned
representations in more general-purpose settings remains unclear. Here, we
investigate how aligning vision model representations to human perceptual
judgments impacts their usability across diverse computer vision tasks. We
finetune state-of-the-art models on human similarity judgments for image
triplets and evaluate them across standard vision benchmarks. We find that
aligning models to perceptual judgments yields representations that improve
upon the original backbones across many downstream tasks, including counting,
segmentation, depth estimation, instance retrieval, and retrieval-augmented
generation. In addition, we find that performance is widely preserved on other
tasks, including specialized out-of-distribution domains such as in medical
imaging and 3D environment frames. Our results suggest that injecting an
inductive bias about human perceptual knowledge into vision models can
contribute to better representations.
]]></content:encoded>
<pubDate>2024-10-14T17:59:58Z</pubDate>
</item>
<item>
<title>TemporalBench: Benchmarking Fine-grained Temporal Understanding for
  Multimodal Video Models</title>
<link>http://arxiv.org/abs/2410.10818v1</link>
<guid>http://arxiv.org/abs/2410.10818v1</guid>
<content:encoded><![CDATA[
Understanding fine-grained temporal dynamics is crucial for multimodal video
comprehension and generation. Due to the lack of fine-grained temporal
annotations, existing video benchmarks mostly resemble static image benchmarks
and are incompetent at evaluating models for temporal understanding. In this
paper, we introduce TemporalBench, a new benchmark dedicated to evaluating
fine-grained temporal understanding in videos. TemporalBench consists of ~10K
video question-answer pairs, derived from ~2K high-quality human annotations
detailing the temporal dynamics in video clips. As a result, our benchmark
provides a unique testbed for evaluating various temporal understanding and
reasoning abilities such as action frequency, motion magnitude, event order,
etc. Moreover, it enables evaluations on various tasks like both video question
answering and captioning, both short and long video understanding, as well as
different models such as multimodal video embedding models and text generation
models. Results show that state-of-the-art models like GPT-4o achieve only
38.5% question answering accuracy on TemporalBench, demonstrating a significant
gap (~30%) between humans and AI in temporal understanding. Furthermore, we
notice a critical pitfall for multi-choice QA where LLMs can detect the subtle
changes in negative captions and find a centralized description as a cue for
its prediction, where we propose Multiple Binary Accuracy (MBA) to correct such
bias. We hope that TemporalBench can foster research on improving models'
temporal reasoning capabilities. Both dataset and evaluation code will be made
available.
]]></content:encoded>
<pubDate>2024-10-14T17:59:58Z</pubDate>
</item>
<item>
<title>LVD-2M: A Long-take Video Dataset with Temporally Dense Captions</title>
<link>http://arxiv.org/abs/2410.10816v1</link>
<guid>http://arxiv.org/abs/2410.10816v1</guid>
<content:encoded><![CDATA[
The efficacy of video generation models heavily depends on the quality of
their training datasets. Most previous video generation models are trained on
short video clips, while recently there has been increasing interest in
training long video generation models directly on longer videos. However, the
lack of such high-quality long videos impedes the advancement of long video
generation. To promote research in long video generation, we desire a new
dataset with four key features essential for training long video generation
models: (1) long videos covering at least 10 seconds, (2) long-take videos
without cuts, (3) large motion and diverse contents, and (4) temporally dense
captions. To achieve this, we introduce a new pipeline for selecting
high-quality long-take videos and generating temporally dense captions.
Specifically, we define a set of metrics to quantitatively assess video quality
including scene cuts, dynamic degrees, and semantic-level quality, enabling us
to filter high-quality long-take videos from a large amount of source videos.
Subsequently, we develop a hierarchical video captioning pipeline to annotate
long videos with temporally-dense captions. With this pipeline, we curate the
first long-take video dataset, LVD-2M, comprising 2 million long-take videos,
each covering more than 10 seconds and annotated with temporally dense
captions. We further validate the effectiveness of LVD-2M by fine-tuning video
generation models to generate long videos with dynamic motions. We believe our
work will significantly contribute to future research in long video generation.
]]></content:encoded>
<pubDate>2024-10-14T17:59:56Z</pubDate>
</item>
<item>
<title>HART: Efficient Visual Generation with Hybrid Autoregressive Transformer</title>
<link>http://arxiv.org/abs/2410.10812v1</link>
<guid>http://arxiv.org/abs/2410.10812v1</guid>
<content:encoded><![CDATA[
We introduce Hybrid Autoregressive Transformer (HART), an autoregressive (AR)
visual generation model capable of directly generating 1024x1024 images,
rivaling diffusion models in image generation quality. Existing AR models face
limitations due to the poor image reconstruction quality of their discrete
tokenizers and the prohibitive training costs associated with generating 1024px
images. To address these challenges, we present the hybrid tokenizer, which
decomposes the continuous latents from the autoencoder into two components:
discrete tokens representing the big picture and continuous tokens representing
the residual components that cannot be represented by the discrete tokens. The
discrete component is modeled by a scalable-resolution discrete AR model, while
the continuous component is learned with a lightweight residual diffusion
module with only 37M parameters. Compared with the discrete-only VAR tokenizer,
our hybrid approach improves reconstruction FID from 2.11 to 0.30 on MJHQ-30K,
leading to a 31% generation FID improvement from 7.85 to 5.38. HART also
outperforms state-of-the-art diffusion models in both FID and CLIP score, with
4.5-7.7x higher throughput and 6.9-13.4x lower MACs. Our code is open sourced
at https://github.com/mit-han-lab/hart.
]]></content:encoded>
<pubDate>2024-10-14T17:59:42Z</pubDate>
</item>
<item>
<title>SceneCraft: Layout-Guided 3D Scene Generation</title>
<link>http://arxiv.org/abs/2410.09049v1</link>
<guid>http://arxiv.org/abs/2410.09049v1</guid>
<content:encoded><![CDATA[
<div> , , , 3D, 

SceneCraft3D2DNeRF <div>
The creation of complex 3D scenes tailored to user specifications has been a
tedious and challenging task with traditional 3D modeling tools. Although some
pioneering methods have achieved automatic text-to-3D generation, they are
generally limited to small-scale scenes with restricted control over the shape
and texture. We introduce SceneCraft, a novel method for generating detailed
indoor scenes that adhere to textual descriptions and spatial layout
preferences provided by users. Central to our method is a rendering-based
technique, which converts 3D semantic layouts into multi-view 2D proxy maps.
Furthermore, we design a semantic and depth conditioned diffusion model to
generate multi-view images, which are used to learn a neural radiance field
(NeRF) as the final scene representation. Without the constraints of panorama
image generation, we surpass previous methods in supporting complicated indoor
space generation beyond a single room, even as complicated as a whole
multi-bedroom apartment with irregular shapes and layouts. Through experimental
analysis, we demonstrate that our method significantly outperforms existing
approaches in complex indoor scene generation with diverse textures, consistent
geometry, and realistic visual quality. Code and more results are available at:
https://orangesodahub.github.io/SceneCraft
]]></content:encoded>
<pubDate>2024-10-11T17:59:58Z</pubDate>
</item>
<item>
<title>Emerging Pixel Grounding in Large Multimodal Models Without Grounding
  Supervision</title>
<link>http://arxiv.org/abs/2410.08209v1</link>
<guid>http://arxiv.org/abs/2410.08209v1</guid>
<content:encoded><![CDATA[
<div> LMMs, grounding, supervision, DIFFLMM, attention maps
<br />
LMMsLMMsLMMsDIFFLMMLMMCLIP44.2GLaMM <div>
Current large multimodal models (LMMs) face challenges in grounding, which
requires the model to relate language components to visual entities. Contrary
to the common practice that fine-tunes LMMs with additional grounding
supervision, we find that the grounding ability can in fact emerge in LMMs
trained without explicit grounding supervision. To reveal this emerging
grounding, we introduce an "attend-and-segment" method which leverages
attention maps from standard LMMs to perform pixel-level segmentation.
Furthermore, to enhance the grounding ability, we propose DIFFLMM, an LMM
utilizing a diffusion-based visual encoder, as opposed to the standard CLIP
visual encoder, and trained with the same weak supervision. Without being
constrained by the biases and limited scale of grounding-specific supervision
data, our approach is more generalizable and scalable. We achieve competitive
performance on both grounding-specific and general visual question answering
benchmarks, compared with grounding LMMs and generalist LMMs, respectively.
Notably, we achieve a 44.2 grounding mask recall on grounded conversation
generation without any grounding supervision, outperforming the extensively
supervised model GLaMM. Project page: https://groundLMM.github.io.
]]></content:encoded>
<pubDate>2024-10-10T17:59:55Z</pubDate>
</item>
<item>
<title>Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large
  Language Models with Endogenous Visual Pre-training</title>
<link>http://arxiv.org/abs/2410.08202v1</link>
<guid>http://arxiv.org/abs/2410.08202v1</guid>
<content:encoded><![CDATA[
<div> Monolithic Multimodal Large Language Models, Visual encoding, Language decoding, Delta tuning, Endogenous Visual Pre-training
Summary:<br />
Mono-InternVLMLLMEViPMono-InternVLMono-InternVLMLLM6Mono-InternVL67%<br />
<br />
: <br />
- Mono-InternVLMLLM
- EViPMono-InternVL
- Mono-InternVLMLLM6
- Mono-InternVL67% <div>
The rapid advancement of Large Language Models (LLMs) has led to an influx of
efforts to extend their capabilities to multimodal tasks. Among them, growing
attention has been focused on monolithic Multimodal Large Language Models
(MLLMs) that integrate visual encoding and language decoding into a single LLM.
Despite the structural simplicity and deployment-friendliness, training a
monolithic MLLM with promising performance still remains challenging. In
particular, the popular approaches adopt continuous pre-training to extend a
pre-trained LLM to a monolithic MLLM, which suffers from catastrophic
forgetting and leads to performance degeneration. In this paper, we aim to
overcome this limitation from the perspective of delta tuning. Specifically,
our core idea is to embed visual parameters into a pre-trained LLM, thereby
incrementally learning visual knowledge from massive data via delta tuning,
i.e., freezing the LLM when optimizing the visual parameters. Based on this
principle, we present Mono-InternVL, a novel monolithic MLLM that seamlessly
integrates a set of visual experts via a multimodal mixture-of-experts
structure. Moreover, we propose an innovative pre-training strategy to maximize
the visual capability of Mono-InternVL, namely Endogenous Visual Pre-training
(EViP). In particular, EViP is designed as a progressive learning process for
visual experts, which aims to fully exploit the visual knowledge from noisy
data to high-quality data. To validate our approach, we conduct extensive
experiments on 16 benchmarks. Experimental results not only validate the
superior performance of Mono-InternVL compared to the state-of-the-art MLLM on
6 multimodal benchmarks, e.g., +113 points over InternVL-1.5 on OCRBench, but
also confirm its better deployment efficiency, with first token latency reduced
by up to 67%.
]]></content:encoded>
<pubDate>2024-10-10T17:59:22Z</pubDate>
</item>
<item>
<title>MM-Ego: Towards Building Egocentric Multimodal LLMs</title>
<link>http://arxiv.org/abs/2410.07177v1</link>
<guid>http://arxiv.org/abs/2410.07177v1</guid>
<content:encoded><![CDATA[
<div> : egocentric video, multimodal foundation model, QA data, data engine, Memory Pointer Prompting<br />
<br />
700306297026MM-EgoLLM<br /><br />:  <div>
This research aims to comprehensively explore building a multimodal
foundation model for egocentric video understanding. To achieve this goal, we
work on three fronts. First, as there is a lack of QA data for egocentric video
understanding, we develop a data engine that efficiently generates 7M
high-quality QA samples for egocentric videos ranging from 30 seconds to one
hour long, based on human-annotated data. This is currently the largest
egocentric QA dataset. Second, we contribute a challenging egocentric QA
benchmark with 629 videos and 7,026 questions to evaluate the models' ability
in recognizing and memorizing visual details across videos of varying lengths.
We introduce a new de-biasing evaluation method to help mitigate the
unavoidable language bias present in the models being evaluated. Third, we
propose a specialized multimodal architecture featuring a novel "Memory Pointer
Prompting" mechanism. This design includes a global glimpse step to gain an
overarching understanding of the entire video and identify key visual
information, followed by a fallback step that utilizes the key visual
information to generate responses. This enables the model to more effectively
comprehend extended video content. With the data, benchmark, and model, we
successfully build MM-Ego, an egocentric multimodal LLM that shows powerful
performance on egocentric video understanding.
]]></content:encoded>
<pubDate>2024-10-09T17:59:59Z</pubDate>
</item>
<item>
<title>Do better language models have crisper vision?</title>
<link>http://arxiv.org/abs/2410.07173v1</link>
<guid>http://arxiv.org/abs/2410.07173v1</guid>
<content:encoded><![CDATA[
<div> Visual Text Representation Benchmark, ViTeRB, large-scale decoder-based LLMs, ShareLock, CLIP-like model
<br /><br />
<br />
-Visual Text Representation Benchmark (ViTeRB)ShareLockCLIPShareLock563k-ImageNet51%1GPU <div>
How well do text-only Large Language Models (LLMs) grasp the visual world? As
LLMs are increasingly used in computer vision, addressing this question becomes
both fundamental and pertinent. However, existing studies have primarily
focused on limited scenarios, such as their ability to generate visual content
or cluster multimodal data. To this end, we propose the Visual Text
Representation Benchmark (ViTeRB) to isolate key properties that make language
models well-aligned with the visual world. With this, we identify large-scale
decoder-based LLMs as ideal candidates for representing text in vision-centric
contexts, counter to the current practice of utilizing text encoders. Building
on these findings, we propose ShareLock, an ultra-lightweight CLIP-like model.
By leveraging precomputable frozen features from strong vision and language
models, ShareLock achieves an impressive 51% accuracy on ImageNet despite
utilizing just 563k image-caption pairs. Moreover, training requires only 1 GPU
hour (or 10 hours including the precomputation of features) - orders of
magnitude less than prior methods. Code will be released.
]]></content:encoded>
<pubDate>2024-10-09T17:59:33Z</pubDate>
</item>
<item>
<title>IterComp: Iterative Composition-Aware Feedback Learning from Model
  Gallery for Text-to-Image Generation</title>
<link>http://arxiv.org/abs/2410.07171v1</link>
<guid>http://arxiv.org/abs/2410.07171v1</guid>
<content:encoded><![CDATA[
Advanced diffusion models like RPG, Stable Diffusion 3 and FLUX have made
notable strides in compositional text-to-image generation. However, these
methods typically exhibit distinct strengths for compositional generation, with
some excelling in handling attribute binding and others in spatial
relationships. This disparity highlights the need for an approach that can
leverage the complementary strengths of various models to comprehensively
improve the composition capability. To this end, we introduce IterComp, a novel
framework that aggregates composition-aware model preferences from multiple
models and employs an iterative feedback learning approach to enhance
compositional generation. Specifically, we curate a gallery of six powerful
open-source diffusion models and evaluate their three key compositional
metrics: attribute binding, spatial relationships, and non-spatial
relationships. Based on these metrics, we develop a composition-aware model
preference dataset comprising numerous image-rank pairs to train
composition-aware reward models. Then, we propose an iterative feedback
learning method to enhance compositionality in a closed-loop manner, enabling
the progressive self-refinement of both the base diffusion model and reward
models over multiple iterations. Theoretical proof demonstrates the
effectiveness and extensive experiments show our significant superiority over
previous SOTA methods (e.g., Omost and FLUX), particularly in multi-category
object composition and complex semantic alignment. IterComp opens new research
avenues in reward feedback learning for diffusion models and compositional
generation. Code: https://github.com/YangLing0818/IterComp
]]></content:encoded>
<pubDate>2024-10-09T17:59:13Z</pubDate>
</item>
<item>
<title>Grounding Partially-Defined Events in Multimodal Data</title>
<link>http://arxiv.org/abs/2410.05267v1</link>
<guid>http://arxiv.org/abs/2410.05267v1</guid>
<content:encoded><![CDATA[
<div> LLM<br />
:<br />
MultiVENT-G14.51,16822.8KLLMMultiVENT-G <div>
How are we able to learn about complex current events just from short
snippets of video? While natural language enables straightforward ways to
represent under-specified, partially observable events, visual data does not
facilitate analogous methods and, consequently, introduces unique challenges in
event understanding. With the growing prevalence of vision-capable AI agents,
these systems must be able to model events from collections of unstructured
video data. To tackle robust event modeling in multimodal settings, we
introduce a multimodal formulation for partially-defined events and cast the
extraction of these events as a three-stage span retrieval task. We propose a
corresponding benchmark for this task, MultiVENT-G, that consists of 14.5 hours
of densely annotated current event videos and 1,168 text documents, containing
22.8K labeled event-centric entities. We propose a collection of LLM-driven
approaches to the task of multimodal event analysis, and evaluate them on
MultiVENT-G. Results illustrate the challenges that abstract event
understanding poses and demonstrates promise in event-centric video-language
systems.
]]></content:encoded>
<pubDate>2024-10-07T17:59:48Z</pubDate>
</item>
<item>
<title>Unraveling Cross-Modality Knowledge Conflict in Large Vision-Language
  Models</title>
<link>http://arxiv.org/abs/2410.03659v1</link>
<guid>http://arxiv.org/abs/2410.03659v1</guid>
<content:encoded><![CDATA[
<div> : Large Vision-Language Models, Parametric Knowledge Conflict, Cross-Modality, Inference Process, Dynamic Contrastive Decoding

LVLMsLVLMslogitslogitsViQuAEInfoSeekLLaVA-34B2.24%

<br /><br />: 
-LVLMs <div>
Large Vision-Language Models (LVLMs) have demonstrated impressive
capabilities for capturing and reasoning over multimodal inputs. However, these
models are prone to parametric knowledge conflicts, which arise from
inconsistencies of represented knowledge between their vision and language
components. In this paper, we formally define the problem of
$\textbf{cross-modality parametric knowledge conflict}$ and present a
systematic approach to detect, interpret, and mitigate them. We introduce a
pipeline that identifies conflicts between visual and textual answers, showing
a persistently high conflict rate across modalities in recent LVLMs regardless
of the model size. We further investigate how these conflicts interfere with
the inference process and propose a contrastive metric to discern the
conflicting samples from the others. Building on these insights, we develop a
novel dynamic contrastive decoding method that removes undesirable logits
inferred from the less confident modality components based on answer
confidence. For models that do not provide logits, we also introduce two
prompt-based strategies to mitigate the conflicts. Our methods achieve
promising improvements in accuracy on both the ViQuAE and InfoSeek datasets.
Specifically, using LLaVA-34B, our proposed dynamic contrastive decoding
improves an average accuracy of 2.24%.
]]></content:encoded>
<pubDate>2024-10-04T17:59:28Z</pubDate>
</item>
<item>
<title>Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short
  Videos</title>
<link>http://arxiv.org/abs/2410.02763v1</link>
<guid>http://arxiv.org/abs/2410.02763v1</guid>
<content:encoded><![CDATA[
<div> : LMMs, Vinoground, , , 

:
1. LMMs
2. LMMs
3. VinogroundLMMLMM
4. GPT-4o~50%~90%
5. 

:<br /><br />LMMsVinogroundGPT-4o50% <div>
There has been growing sentiment recently that modern large multimodal models
(LMMs) have addressed most of the key challenges related to short video
comprehension. As a result, both academia and industry are gradually shifting
their attention towards the more complex challenges posed by understanding
long-form videos. However, is this really the case? Our studies indicate that
LMMs still lack many fundamental reasoning capabilities even when dealing with
short videos. We introduce Vinoground, a temporal counterfactual LMM evaluation
benchmark encompassing 1000 short and natural video-caption pairs. We
demonstrate that existing LMMs severely struggle to distinguish temporal
differences between different actions and object transformations. For example,
the best model GPT-4o only obtains ~50% on our text and video scores, showing a
large gap compared to the human baseline of ~90%. All open-source multimodal
models and CLIP-based models perform much worse, producing mostly random chance
performance. Through this work, we shed light onto the fact that temporal
reasoning in short videos is a problem yet to be fully solved. The dataset and
evaluation code are available at https://vinoground.github.io.
]]></content:encoded>
<pubDate>2024-10-03T17:59:58Z</pubDate>
</item>
<item>
<title>Loong: Generating Minute-level Long Videos with Autoregressive Language
  Models</title>
<link>http://arxiv.org/abs/2410.02757v1</link>
<guid>http://arxiv.org/abs/2410.02757v1</guid>
<content:encoded><![CDATA[
<div>     Loong

LLMsLLMsLoongLLMsLoongLoong10https://epiphqny.github.io/Loong-video<br /><br />: LoongLLMsLLMs <div>
It is desirable but challenging to generate content-rich long videos in the
scale of minutes. Autoregressive large language models (LLMs) have achieved
great success in generating coherent and long sequences of tokens in the domain
of natural language processing, while the exploration of autoregressive LLMs
for video generation is limited to generating short videos of several seconds.
In this work, we conduct a deep analysis of the challenges that prevent
autoregressive LLM-based video generators from generating long videos. Based on
the observations and analysis, we propose Loong, a new autoregressive LLM-based
video generator that can generate minute-long videos. Specifically, we model
the text tokens and video tokens as a unified sequence for autoregressive LLMs
and train the model from scratch. We propose progressive short-to-long training
with a loss re-weighting scheme to mitigate the loss imbalance problem for long
video training. We further investigate inference strategies, including video
token re-encoding and sampling strategies, to diminish error accumulation
during inference. Our proposed Loong can be trained on 10-second videos and be
extended to generate minute-level long videos conditioned on text prompts, as
demonstrated by the results. More samples are available at:
https://epiphqny.github.io/Loong-video.
]]></content:encoded>
<pubDate>2024-10-03T17:59:02Z</pubDate>
</item>
<item>
<title>ReLIC: A Recipe for 64k Steps of In-Context Reinforcement Learning for
  Embodied AI</title>
<link>http://arxiv.org/abs/2410.02751v1</link>
<guid>http://arxiv.org/abs/2410.02751v1</guid>
<content:encoded><![CDATA[
Intelligent embodied agents need to quickly adapt to new scenarios by
integrating long histories of experience into decision-making. For instance, a
robot in an unfamiliar house initially wouldn't know the locations of objects
needed for tasks and might perform inefficiently. However, as it gathers more
experience, it should learn the layout of its environment and remember where
objects are, allowing it to complete new tasks more efficiently. To enable such
rapid adaptation to new tasks, we present ReLIC, a new approach for in-context
reinforcement learning (RL) for embodied agents. With ReLIC, agents are capable
of adapting to new environments using 64,000 steps of in-context experience
with full attention while being trained through self-generated experience via
RL. We achieve this by proposing a novel policy update scheme for on-policy RL
called "partial updates'' as well as a Sink-KV mechanism that enables effective
utilization of a long observation history for embodied agents. Our method
outperforms a variety of meta-RL baselines in adapting to unseen houses in an
embodied multi-object navigation task. In addition, we find that ReLIC is
capable of few-shot imitation learning despite never being trained with expert
demonstrations. We also provide a comprehensive analysis of ReLIC, highlighting
that the combination of large-scale RL training, the proposed partial updates
scheme, and the Sink-KV are essential for effective in-context learning. The
code for ReLIC and all our experiments is at https://github.com/aielawady/relic
]]></content:encoded>
<pubDate>2024-10-03T17:58:11Z</pubDate>
</item>
<item>
<title>Windowed MAPF with Completeness Guarantees</title>
<link>http://arxiv.org/abs/2410.01798v1</link>
<guid>http://arxiv.org/abs/2410.01798v1</guid>
<content:encoded><![CDATA[
<div> Windowed MAPF, completeness, heuristic update, SS-CBS, agent independence<br />
<br />
WinC-MAPFSingle-Step CBS (SS-CBS)CBSSS-CBS<br /><br />: <br />WinC-MAPFSS-CBS <div>
Traditional multi-agent path finding (MAPF) methods try to compute entire
start-goal paths which are collision free. However, computing an entire path
can take too long for MAPF systems where agents need to replan fast. Methods
that address this typically employ a "windowed" approach and only try to find
collision free paths for a small windowed timestep horizon. This adaptation
comes at the cost of incompleteness; all current windowed approaches can become
stuck in deadlock or livelock. Our main contribution is to introduce our
framework, WinC-MAPF, for Windowed MAPF that enables completeness. Our
framework uses heuristic update insights from single-agent real-time heuristic
search algorithms as well as agent independence ideas from MAPF algorithms. We
also develop Single-Step CBS (SS-CBS), an instantiation of this framework using
a novel modification to CBS. We show how SS-CBS, which only plans a single step
and updates heuristics, can effectively solve tough scenarios where existing
windowed approaches fail.
]]></content:encoded>
<pubDate>2024-10-02T17:55:46Z</pubDate>
</item>
<item>
<title>Bellman Diffusion: Generative Modeling as Learning a Linear Operator in
  the Distribution Space</title>
<link>http://arxiv.org/abs/2410.01796v1</link>
<guid>http://arxiv.org/abs/2410.01796v1</guid>
<content:encoded><![CDATA[
<div> DGMs, EBMs, SGMs, MDPs, RL
<br />
DGMMDPDGMMDPRL1.5DGMMDP
<br /><br />: 
(MDP) <div>
Deep Generative Models (DGMs), including Energy-Based Models (EBMs) and
Score-based Generative Models (SGMs), have advanced high-fidelity data
generation and complex continuous distribution approximation. However, their
application in Markov Decision Processes (MDPs), particularly in distributional
Reinforcement Learning (RL), remains underexplored, with conventional
histogram-based methods dominating the field. This paper rigorously highlights
that this application gap is caused by the nonlinearity of modern DGMs, which
conflicts with the linearity required by the Bellman equation in MDPs. For
instance, EBMs involve nonlinear operations such as exponentiating energy
functions and normalizing constants. To address this, we introduce Bellman
Diffusion, a novel DGM framework that maintains linearity in MDPs through
gradient and scalar field modeling. With divergence-based training techniques
to optimize neural network proxies and a new type of stochastic differential
equation (SDE) for sampling, Bellman Diffusion is guaranteed to converge to the
target distribution. Our empirical results show that Bellman Diffusion achieves
accurate field estimations and is a capable image generator, converging 1.5x
faster than the traditional histogram-based baseline in distributional RL
tasks. This work enables the effective integration of DGMs into MDP
applications, unlocking new avenues for advanced decision-making frameworks.
]]></content:encoded>
<pubDate>2024-10-02T17:53:23Z</pubDate>
</item>
<item>
<title>MM1.5: Methods, Analysis &amp; Insights from Multimodal LLM Fine-tuning</title>
<link>http://arxiv.org/abs/2409.20566v1</link>
<guid>http://arxiv.org/abs/2409.20566v1</guid>
<content:encoded><![CDATA[
<div> : MM1.5, , ,  centric , 

:<br /><br />
MM1.5(MLLMs)MM1.5MM1OCR10300(MoE)(1030)MM1.5-VideoMM1.5-UIUIMLLM <div>
We present MM1.5, a new family of multimodal large language models (MLLMs)
designed to enhance capabilities in text-rich image understanding, visual
referring and grounding, and multi-image reasoning. Building upon the MM1
architecture, MM1.5 adopts a data-centric approach to model training,
systematically exploring the impact of diverse data mixtures across the entire
model training lifecycle. This includes high-quality OCR data and synthetic
captions for continual pre-training, as well as an optimized visual
instruction-tuning data mixture for supervised fine-tuning. Our models range
from 1B to 30B parameters, encompassing both dense and mixture-of-experts (MoE)
variants, and demonstrate that careful data curation and training strategies
can yield strong performance even at small scales (1B and 3B). Additionally, we
introduce two specialized variants: MM1.5-Video, designed for video
understanding, and MM1.5-UI, tailored for mobile UI understanding. Through
extensive empirical studies and ablations, we provide detailed insights into
the training processes and decisions that inform our final designs, offering
valuable guidance for future research in MLLM development.
]]></content:encoded>
<pubDate>2024-09-30T17:59:34Z</pubDate>
</item>
<item>
<title>LaMMA-P: Generalizable Multi-Agent Long-Horizon Task Allocation and
  Planning with LM-Driven PDDL Planner</title>
<link>http://arxiv.org/abs/2409.20560v1</link>
<guid>http://arxiv.org/abs/2409.20560v1</guid>
<content:encoded><![CDATA[
<div> Language models, natural language understanding, multi-agent planning, LaMMA-P, long-horizon tasks
<br /><br />:
PDDLLaMMA-PLaMMA-PMAT-THORAI2-THORLaMMA-P105%36%https://lamma-p.github.io <div>
Language models (LMs) possess a strong capability to comprehend natural
language, making them effective in translating human instructions into detailed
plans for simple robot tasks. Nevertheless, it remains a significant challenge
to handle long-horizon tasks, especially in subtask identification and
allocation for cooperative heterogeneous robot teams. To address this issue, we
propose a Language Model-Driven Multi-Agent PDDL Planner (LaMMA-P), a novel
multi-agent task planning framework that achieves state-of-the-art performance
on long-horizon tasks. LaMMA-P integrates the strengths of the LMs' reasoning
capability and the traditional heuristic search planner to achieve a high
success rate and efficiency while demonstrating strong generalization across
tasks. Additionally, we create MAT-THOR, a comprehensive benchmark that
features household tasks with two different levels of complexity based on the
AI2-THOR environment. The experimental results demonstrate that LaMMA-P
achieves a 105% higher success rate and 36% higher efficiency than existing
LM-based multi-agent planners. The experimental videos, code, and datasets of
this work as well as the detailed prompts used in each module are available at
https://lamma-p.github.io.
]]></content:encoded>
<pubDate>2024-09-30T17:58:18Z</pubDate>
</item>
<item>
<title>Supervised Multi-Modal Fission Learning</title>
<link>http://arxiv.org/abs/2409.20559v1</link>
<guid>http://arxiv.org/abs/2409.20559v1</guid>
<content:encoded><![CDATA[
Learning from multimodal datasets can leverage complementary information and
improve performance in prediction tasks. A commonly used strategy to account
for feature correlations in high-dimensional datasets is the latent variable
approach. Several latent variable methods have been proposed for multimodal
datasets. However, these methods either focus on extracting the shared
component across all modalities or on extracting both a shared component and
individual components specific to each modality. To address this gap, we
propose a Multi-Modal Fission Learning (MMFL) model that simultaneously
identifies globally joint, partially joint, and individual components
underlying the features of multimodal datasets. Unlike existing latent variable
methods, MMFL uses supervision from the response variable to identify
predictive latent components and has a natural extension for incorporating
incomplete multimodal data. Through simulation studies, we demonstrate that
MMFL outperforms various existing multimodal algorithms in both complete and
incomplete modality settings. We applied MMFL to a real-world case study for
early prediction of Alzheimers Disease using multimodal neuroimaging and
genomics data from the Alzheimers Disease Neuroimaging Initiative (ADNI)
dataset. MMFL provided more accurate predictions and better insights into
within- and across-modality correlations compared to existing methods.
]]></content:encoded>
<pubDate>2024-09-30T17:58:03Z</pubDate>
</item>
<item>
<title>Propose, Assess, Search: Harnessing LLMs for Goal-Oriented Planning in
  Instructional Videos</title>
<link>http://arxiv.org/abs/2409.20557v1</link>
<guid>http://arxiv.org/abs/2409.20557v1</guid>
<content:encoded><![CDATA[
Goal-oriented planning, or anticipating a series of actions that transition
an agent from its current state to a predefined objective, is crucial for
developing intelligent assistants aiding users in daily procedural tasks. The
problem presents significant challenges due to the need for comprehensive
knowledge of temporal and hierarchical task structures, as well as strong
capabilities in reasoning and planning. To achieve this, prior work typically
relies on extensive training on the target dataset, which often results in
significant dataset bias and a lack of generalization to unseen tasks. In this
work, we introduce VidAssist, an integrated framework designed for
zero/few-shot goal-oriented planning in instructional videos. VidAssist
leverages large language models (LLMs) as both the knowledge base and the
assessment tool for generating and evaluating action plans, thus overcoming the
challenges of acquiring procedural knowledge from small-scale, low-diversity
datasets. Moreover, VidAssist employs a breadth-first search algorithm for
optimal plan generation, in which a composite of value functions designed for
goal-oriented planning is utilized to assess the predicted actions at each
step. Extensive experiments demonstrate that VidAssist offers a unified
framework for different goal-oriented planning setups, e.g., visual planning
for assistance (VPA) and procedural planning (PP), and achieves remarkable
performance in zero-shot and few-shot setups. Specifically, our few-shot model
outperforms the prior fully supervised state-of-the-art method by +7.7% in VPA
and +4.81% PP task on the COIN dataset while predicting 4 future actions. Code,
and models are publicly available at https://sites.google.com/view/vidassist.
]]></content:encoded>
<pubDate>2024-09-30T17:57:28Z</pubDate>
</item>
<item>
<title>Inverse Painting: Reconstructing The Painting Process</title>
<link>http://arxiv.org/abs/2409.20556v1</link>
<guid>http://arxiv.org/abs/2409.20556v1</guid>
<content:encoded><![CDATA[
Given an input painting, we reconstruct a time-lapse video of how it may have
been painted. We formulate this as an autoregressive image generation problem,
in which an initially blank "canvas" is iteratively updated. The model learns
from real artists by training on many painting videos. Our approach
incorporates text and region understanding to define a set of painting
"instructions" and updates the canvas with a novel diffusion-based renderer.
The method extrapolates beyond the limited, acrylic style paintings on which it
has been trained, showing plausible results for a wide range of artistic styles
and genres.
]]></content:encoded>
<pubDate>2024-09-30T17:56:52Z</pubDate>
</item>
<item>
<title>PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation</title>
<link>http://arxiv.org/abs/2409.18964v1</link>
<guid>http://arxiv.org/abs/2409.18964v1</guid>
<content:encoded><![CDATA[
<div> : PhysGen, , , , 

PhysGeniiiiiiPhysGenPhysGen <div>
We present PhysGen, a novel image-to-video generation method that converts a
single image and an input condition (e.g., force and torque applied to an
object in the image) to produce a realistic, physically plausible, and
temporally consistent video. Our key insight is to integrate model-based
physical simulation with a data-driven video generation process, enabling
plausible image-space dynamics. At the heart of our system are three core
components: (i) an image understanding module that effectively captures the
geometry, materials, and physical parameters of the image; (ii) an image-space
dynamics simulation model that utilizes rigid-body physics and inferred
parameters to simulate realistic behaviors; and (iii) an image-based rendering
and refinement module that leverages generative video diffusion to produce
realistic video footage featuring the simulated motion. The resulting videos
are realistic in both physics and appearance and are even precisely
controllable, showcasing superior results over existing data-driven
image-to-video generation works through quantitative comparison and
comprehensive user study. PhysGen's resulting videos can be used for various
downstream applications, such as turning an image into a realistic animation or
allowing users to interact with the image and create various dynamics. Project
page: https://stevenlsw.github.io/physgen/
]]></content:encoded>
<pubDate>2024-09-27T17:59:57Z</pubDate>
</item>
<item>
<title>Ruler: A Model-Agnostic Method to Control Generated Length for Large
  Language Models</title>
<link>http://arxiv.org/abs/2409.18943v1</link>
<guid>http://arxiv.org/abs/2409.18943v1</guid>
<content:encoded><![CDATA[
<div> Ruler<br />
TLGPrecise MatchPMFlexible MatchFMRulerMeta Length TokensMLTsRulerMLTRulerPM27.97FM29.57Ruler <div>
The instruction-following ability of large language models enables humans to
interact with AI agents in a natural way. However, when required to generate
responses of a specific length, large language models often struggle to meet
users' needs due to their inherent difficulty in accurately perceiving
numerical constraints. To explore the ability of large language models to
control the length of generated responses, we propose the Target Length
Generation Task (TLG) and design two metrics, Precise Match (PM) and Flexible
Match (FM) to evaluate the model's performance in adhering to specified
response lengths. Furthermore, we introduce a novel, model-agnostic approach
called Ruler, which employs Meta Length Tokens (MLTs) to enhance the
instruction-following ability of large language models under length-constrained
instructions. Specifically, Ruler equips LLMs with the ability to generate
responses of a specified length based on length constraints within the
instructions. Moreover, Ruler can automatically generate appropriate MLT when
length constraints are not explicitly provided, demonstrating excellent
versatility and generalization. Comprehensive experiments show the
effectiveness of Ruler across different LLMs on Target Length Generation Task,
e.g., at All Level 27.97 average gain on PM, 29.57 average gain on FM. In
addition, we conduct extensive ablation experiments to further substantiate the
efficacy and generalization of Ruler. Our code and data is available at
https://github.com/Geaming2002/Ruler.
]]></content:encoded>
<pubDate>2024-09-27T17:44:58Z</pubDate>
</item>
<item>
<title>FlowTurbo: Towards Real-time Flow-Based Image Generation with Velocity
  Refiner</title>
<link>http://arxiv.org/abs/2409.18128v1</link>
<guid>http://arxiv.org/abs/2409.18128v1</guid>
<content:encoded><![CDATA[
<div> : , , , , 

FlowTurboFlowTurboFlowTurbo53.1%58.3%29.8%38.5%FlowTurboImageNet100(ms/img)FID2.1238(ms/img)FID3.93https://github.com/shiml20/FlowTurbo<br /><br />: FlowTurbo <div>
Building on the success of diffusion models in visual generation, flow-based
models reemerge as another prominent family of generative models that have
achieved competitive or better performance in terms of both visual quality and
inference speed. By learning the velocity field through flow-matching,
flow-based models tend to produce a straighter sampling trajectory, which is
advantageous during the sampling process. However, unlike diffusion models for
which fast samplers are well-developed, efficient sampling of flow-based
generative models has been rarely explored. In this paper, we propose a
framework called FlowTurbo to accelerate the sampling of flow-based models
while still enhancing the sampling quality. Our primary observation is that the
velocity predictor's outputs in the flow-based models will become stable during
the sampling, enabling the estimation of velocity via a lightweight velocity
refiner. Additionally, we introduce several techniques including a pseudo
corrector and sample-aware compilation to further reduce inference time. Since
FlowTurbo does not change the multi-step sampling paradigm, it can be
effectively applied for various tasks such as image editing, inpainting, etc.
By integrating FlowTurbo into different flow-based models, we obtain an
acceleration ratio of 53.1%$\sim$58.3% on class-conditional generation and
29.8%$\sim$38.5% on text-to-image generation. Notably, FlowTurbo reaches an FID
of 2.12 on ImageNet with 100 (ms / img) and FID of 3.93 with 38 (ms / img),
achieving the real-time image generation and establishing the new
state-of-the-art. Code is available at https://github.com/shiml20/FlowTurbo.
]]></content:encoded>
<pubDate>2024-09-26T17:59:51Z</pubDate>
</item>
<item>
<title>Lotus: Diffusion-based Visual Foundation Model for High-quality Dense
  Prediction</title>
<link>http://arxiv.org/abs/2409.18124v1</link>
<guid>http://arxiv.org/abs/2409.18124v1</guid>
<content:encoded><![CDATA[
<div> -shotLotus
<br /><br />
 Lotus/LotusLotus-shot
<br />: LotusLotus <div>
Leveraging the visual priors of pre-trained text-to-image diffusion models
offers a promising solution to enhance zero-shot generalization in dense
prediction tasks. However, existing methods often uncritically use the original
diffusion formulation, which may not be optimal due to the fundamental
differences between dense prediction and image generation. In this paper, we
provide a systemic analysis of the diffusion formulation for the dense
prediction, focusing on both quality and efficiency. And we find that the
original parameterization type for image generation, which learns to predict
noise, is harmful for dense prediction; the multi-step noising/denoising
diffusion process is also unnecessary and challenging to optimize. Based on
these insights, we introduce Lotus, a diffusion-based visual foundation model
with a simple yet effective adaptation protocol for dense prediction.
Specifically, Lotus is trained to directly predict annotations instead of
noise, thereby avoiding harmful variance. We also reformulate the diffusion
process into a single-step procedure, simplifying optimization and
significantly boosting inference speed. Additionally, we introduce a novel
tuning strategy called detail preserver, which achieves more accurate and
fine-grained predictions. Without scaling up the training data or model
capacity, Lotus achieves SoTA performance in zero-shot depth and normal
estimation across various datasets. It also significantly enhances efficiency,
being hundreds of times faster than most existing diffusion-based methods.
]]></content:encoded>
<pubDate>2024-09-26T17:58:55Z</pubDate>
</item>
<item>
<title>Molmo and PixMo: Open Weights and Open Data for State-of-the-Art
  Multimodal Models</title>
<link>http://arxiv.org/abs/2409.17146v1</link>
<guid>http://arxiv.org/abs/2409.17146v1</guid>
<content:encoded><![CDATA[
<div> Molmo<br />
:<br />
MolmoVLM2D Molmo72B <div>
Today's most advanced multimodal models remain proprietary. The strongest
open-weight models rely heavily on synthetic data from proprietary VLMs to
achieve good performance, effectively distilling these closed models into open
ones. As a result, the community is still missing foundational knowledge about
how to build performant VLMs from scratch. We present Molmo, a new family of
VLMs that are state-of-the-art in their class of openness. Our key innovation
is a novel, highly detailed image caption dataset collected entirely from human
annotators using speech-based descriptions. To enable a wide array of user
interactions, we also introduce a diverse dataset mixture for fine-tuning that
includes in-the-wild Q&amp;A and innovative 2D pointing data. The success of our
approach relies on careful choices for the model architecture details, a
well-tuned training pipeline, and, most critically, the quality of our newly
collected datasets, all of which will be released. The best-in-class 72B model
within the Molmo family not only outperforms others in the class of open weight
and data models but also compares favorably against proprietary systems like
GPT-4o, Claude 3.5, and Gemini 1.5 on both academic benchmarks and human
evaluation.
  We will be releasing all of our model weights, captioning and fine-tuning
data, and source code in the near future. Select model weights, inference code,
and demo are available at https://molmo.allenai.org.
]]></content:encoded>
<pubDate>2024-09-25T17:59:51Z</pubDate>
</item>
<item>
<title>Turn Every Application into an Agent: Towards Efficient
  Human-Agent-Computer Interaction with API-First LLM-Based Agents</title>
<link>http://arxiv.org/abs/2409.17140v1</link>
<guid>http://arxiv.org/abs/2409.17140v1</guid>
<content:encoded><![CDATA[
<div> LLMUIAPIAgent OS
<br /><br />
AXISLLMAPIUIAPIOffice WordAXIS65%-70%38%-53%97%-98%--LLMUIAgent OS 
<br /><br /> 
: <div>
Multimodal large language models (MLLMs) have enabled LLM-based agents to
directly interact with application user interfaces (UIs), enhancing agents'
performance in complex tasks. However, these agents often suffer from high
latency and low reliability due to the extensive sequential UI interactions. To
address this issue, we propose AXIS, a novel LLM-based agents framework
prioritize actions through application programming interfaces (APIs) over UI
actions. This framework also facilitates the creation and expansion of APIs
through automated exploration of applications. Our experiments on Office Word
demonstrate that AXIS reduces task completion time by 65%-70% and cognitive
workload by 38%-53%, while maintaining accuracy of 97%-98% compare to humans.
Our work contributes to a new human-agent-computer interaction (HACI) framework
and a fresh UI design principle for application providers in the era of LLMs.
It also explores the possibility of turning every applications into agents,
paving the way towards an agent-centric operating system (Agent OS).
]]></content:encoded>
<pubDate>2024-09-25T17:58:08Z</pubDate>
</item>
<item>
<title>Gen2Act: Human Video Generation in Novel Scenarios enables Generalizable
  Robot Manipulation</title>
<link>http://arxiv.org/abs/2409.16283v1</link>
<guid>http://arxiv.org/abs/2409.16283v1</guid>
<content:encoded><![CDATA[
<div> , , , , 

Gen2Act-shotGen2ActGen2Act Videos are at https://homangab.github.io/gen2act/ <br /><br />:  <div>
How can robot manipulation policies generalize to novel tasks involving
unseen object types and new motions? In this paper, we provide a solution in
terms of predicting motion information from web data through human video
generation and conditioning a robot policy on the generated video. Instead of
attempting to scale robot data collection which is expensive, we show how we
can leverage video generation models trained on easily available web data, for
enabling generalization. Our approach Gen2Act casts language-conditioned
manipulation as zero-shot human video generation followed by execution with a
single policy conditioned on the generated video. To train the policy, we use
an order of magnitude less robot interaction data compared to what the video
prediction model was trained on. Gen2Act doesn't require fine-tuning the video
model at all and we directly use a pre-trained model for generating human
videos. Our results on diverse real-world scenarios show how Gen2Act enables
manipulating unseen object types and performing novel motions for tasks not
present in the robot data. Videos are at https://homangab.github.io/gen2act/
]]></content:encoded>
<pubDate>2024-09-24T17:57:33Z</pubDate>
</item>
<item>
<title>MonoFormer: One Transformer for Both Diffusion and Autoregression</title>
<link>http://arxiv.org/abs/2409.16280v1</link>
<guid>http://arxiv.org/abs/2409.16280v1</guid>
<content:encoded><![CDATA[
<div> Transformer, multimodality, autoregression, diffusion, image generation

Transformer
Transformerautoregressiondiffusion



<br /><br />:
Transformer <div>
Most existing multimodality methods use separate backbones for
autoregression-based discrete text generation and diffusion-based continuous
visual generation, or the same backbone by discretizing the visual data to use
autoregression for both text and visual generation. In this paper, we propose
to study a simple idea: share one transformer for both autoregression and
diffusion. The feasibility comes from two main aspects: (i) Transformer is
successfully applied to diffusion for visual generation, and (ii) transformer
training for autoregression and diffusion is very similar, and the difference
merely lies in that diffusion uses bidirectional attention mask and
autoregression uses causal attention mask. Experimental results show that our
approach achieves comparable image generation performance to current
state-of-the-art methods as well as maintains the text generation capability.
The project is publicly available at https://monoformer.github.io/.
]]></content:encoded>
<pubDate>2024-09-24T17:51:04Z</pubDate>
</item>
<item>
<title>Semantic Refocused Tuning for Open-Vocabulary Panoptic Segmentation</title>
<link>http://arxiv.org/abs/2409.16278v1</link>
<guid>http://arxiv.org/abs/2409.16278v1</guid>
<content:encoded><![CDATA[
Open-vocabulary panoptic segmentation is an emerging task aiming to
accurately segment the image into semantically meaningful masks based on a set
of texts. Despite existing efforts, it remains challenging to develop a
high-performing method that generalizes effectively across new domains and
requires minimal training resources. Our in-depth analysis of current methods
reveals a crucial insight: mask classification is the main performance
bottleneck for open-vocab. panoptic segmentation. Based on this, we propose
Semantic Refocused Tuning (SMART), a novel framework that greatly enhances
open-vocab. panoptic segmentation by improving mask classification through two
key innovations. First, SMART adopts a multimodal Semantic-guided Mask
Attention mechanism that injects task-awareness into the regional information
extraction process. This enables the model to capture task-specific and
contextually relevant information for more effective mask classification.
Second, it incorporates Query Projection Tuning, which strategically fine-tunes
the query projection layers within the Vision Language Model (VLM) used for
mask classification. This adjustment allows the model to adapt the image focus
of mask tokens to new distributions with minimal training resources, while
preserving the VLM's pre-trained knowledge. Extensive ablation studies confirm
the superiority of our approach. Notably, SMART sets new state-of-the-art
results, demonstrating improvements of up to +1.3 PQ and +5.4 mIoU across
representative benchmarks, while reducing training costs by nearly 10x compared
to the previous best method. Our code and data will be released.
]]></content:encoded>
<pubDate>2024-09-24T17:50:28Z</pubDate>
</item>
<item>
<title>Vista3D: Unravel the 3D Darkside of a Single Image</title>
<link>http://arxiv.org/abs/2409.12193v1</link>
<guid>http://arxiv.org/abs/2409.12193v1</guid>
<content:encoded><![CDATA[
<div> : Vista3D, 3D, , , 
: 
Vista3D53DSplattingSplattingSDFVista3D3DGitHub <div>
We embark on the age-old quest: unveiling the hidden dimensions of objects
from mere glimpses of their visible parts. To address this, we present Vista3D,
a framework that realizes swift and consistent 3D generation within a mere 5
minutes. At the heart of Vista3D lies a two-phase approach: the coarse phase
and the fine phase. In the coarse phase, we rapidly generate initial geometry
with Gaussian Splatting from a single image. In the fine phase, we extract a
Signed Distance Function (SDF) directly from learned Gaussian Splatting,
optimizing it with a differentiable isosurface representation. Furthermore, it
elevates the quality of generation by using a disentangled representation with
two independent implicit functions to capture both visible and obscured aspects
of objects. Additionally, it harmonizes gradients from 2D diffusion prior with
3D-aware diffusion priors by angular diffusion prior composition. Through
extensive evaluation, we demonstrate that Vista3D effectively sustains a
balance between the consistency and diversity of the generated 3D objects.
Demos and code will be available at https://github.com/florinshen/Vista3D.
]]></content:encoded>
<pubDate>2024-09-18T17:59:44Z</pubDate>
</item>
<item>
<title>Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at
  Any Resolution</title>
<link>http://arxiv.org/abs/2409.12191v1</link>
<guid>http://arxiv.org/abs/2409.12191v1</guid>
<content:encoded><![CDATA[
<div> Qwen2-VL Series, upgrade, Naive Dynamic Resolution, Multimodal Rotary Position Embedding, scaling laws<br />
<br />
Qwen2-VL SeriesQwen-VLNaive Dynamic ResolutionMultimodal Rotary Position EmbeddingM-RoPELVLMsQwen2-VLQwen2-VL-72B\url{https://github.com/QwenLM/Qwen2-VL} <br /><br />: Qwen2-VL <div>
We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL
models that redefines the conventional predetermined-resolution approach in
visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism,
which enables the model to dynamically process images of varying resolutions
into different numbers of visual tokens. This approach allows the model to
generate more efficient and accurate visual representations, closely aligning
with human perceptual processes. The model also integrates Multimodal Rotary
Position Embedding (M-RoPE), facilitating the effective fusion of positional
information across text, images, and videos. We employ a unified paradigm for
processing both images and videos, enhancing the model's visual perception
capabilities. To explore the potential of large multimodal models, Qwen2-VL
investigates the scaling laws for large vision-language models (LVLMs). By
scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the
amount of training data, the Qwen2-VL Series achieves highly competitive
performance. Notably, the Qwen2-VL-72B model achieves results comparable to
leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal
benchmarks, outperforming other generalist models. Code is available at
\url{https://github.com/QwenLM/Qwen2-VL}.
]]></content:encoded>
<pubDate>2024-09-18T17:59:32Z</pubDate>
</item>
<item>
<title>Phidias: A Generative Model for Creating 3D Content from Text, Image,
  and 3D Conditions with Reference-Augmented Diffusion</title>
<link>http://arxiv.org/abs/2409.11406v1</link>
<guid>http://arxiv.org/abs/2409.11406v1</guid>
<content:encoded><![CDATA[
<div> 3DPhidias<br />
<br />
Phidias3D3D123D3PhidiasPhidias3D3D

<br /><br />: 3DPhidias3DPhidias3D <div>
In 3D modeling, designers often use an existing 3D model as a reference to
create new ones. This practice has inspired the development of Phidias, a novel
generative model that uses diffusion for reference-augmented 3D generation.
Given an image, our method leverages a retrieved or user-provided 3D reference
model to guide the generation process, thereby enhancing the generation
quality, generalization ability, and controllability. Our model integrates
three key components: 1) meta-ControlNet that dynamically modulates the
conditioning strength, 2) dynamic reference routing that mitigates misalignment
between the input image and 3D reference, and 3) self-reference augmentations
that enable self-supervised training with a progressive curriculum.
Collectively, these designs result in a clear improvement over existing
methods. Phidias establishes a unified framework for 3D generation using text,
image, and 3D conditions with versatile applications.
]]></content:encoded>
<pubDate>2024-09-17T17:59:33Z</pubDate>
</item>
<item>
<title>NVLM: Open Frontier-Class Multimodal LLMs</title>
<link>http://arxiv.org/abs/2409.11402v1</link>
<guid>http://arxiv.org/abs/2409.11402v1</guid>
<content:encoded><![CDATA[
<div> NVLM 1.0, multimodal, language models, vision-language tasks, model design<br />
 NVLM 1.0 -<br /><br />: <br /> NVLM 1.0 - <div>
We introduce NVLM 1.0, a family of frontier-class multimodal large language
models (LLMs) that achieve state-of-the-art results on vision-language tasks,
rivaling the leading proprietary models (e.g., GPT-4o) and open-access models
(e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved
text-only performance over its LLM backbone after multimodal training. In terms
of model design, we perform a comprehensive comparison between decoder-only
multimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g.,
Flamingo). Based on the strengths and weaknesses of both approaches, we propose
a novel architecture that enhances both training efficiency and multimodal
reasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for
tile-based dynamic high-resolution images, which significantly boosts
performance on multimodal reasoning and OCR-related tasks. Regarding training
data, we meticulously curate and provide detailed information on our multimodal
pretraining and supervised fine-tuning datasets. Our findings indicate that
dataset quality and task diversity are more important than scale, even during
the pretraining phase, across all architectures. Notably, we develop
production-grade multimodality for the NVLM-1.0 models, enabling them to excel
in vision-language tasks while maintaining and even improving text-only
performance compared to their LLM backbones. To achieve this, we craft and
integrate a high-quality text-only dataset into multimodal training, alongside
a substantial amount of multimodal math and reasoning data, leading to enhanced
math and coding capabilities across modalities. To advance research in the
field, we are releasing the model weights and will open-source the code for the
community: https://nvlm-project.github.io/.
]]></content:encoded>
<pubDate>2024-09-17T17:59:06Z</pubDate>
</item>
<item>
<title>Distributed Perception Aware Safe Leader Follower System via Control
  Barrier Methods</title>
<link>http://arxiv.org/abs/2409.11394v1</link>
<guid>http://arxiv.org/abs/2409.11394v1</guid>
<content:encoded><![CDATA[
This paper addresses a distributed leader-follower formation control problem
for a group of agents, each using a body-fixed camera with a limited field of
view (FOV) for state estimation. The main challenge arises from the need to
coordinate the agents' movements with their cameras' FOV to maintain visibility
of the leader for accurate and reliable state estimation. To address this
challenge, we propose a novel perception-aware distributed leader-follower safe
control scheme that incorporates FOV limits as state constraints. A Control
Barrier Function (CBF) based quadratic program is employed to ensure the
forward invariance of a safety set defined by these constraints. Furthermore,
new neural network based and double bounding boxes based estimators, combined
with temporal filters, are developed to estimate system states directly from
real-time image data, providing consistent performance across various
environments. Comparison results in the Gazebo simulator demonstrate the
effectiveness and robustness of the proposed framework in two distinct
environments.
]]></content:encoded>
<pubDate>2024-09-17T17:54:56Z</pubDate>
</item>
<item>
<title>LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless
  Integration of Multi Active/Passive Core-Agents</title>
<link>http://arxiv.org/abs/2409.11393v1</link>
<guid>http://arxiv.org/abs/2409.11393v1</guid>
<content:encoded><![CDATA[
The integration of tools in LLM-based agents overcame the difficulties of
standalone LLMs and traditional agents' limited capabilities. However, the
conjunction of these technologies and the proposed enhancements in several
state-of-the-art works followed a non-unified software architecture resulting
in a lack of modularity. Indeed, they focused mainly on functionalities and
overlooked the definition of the component's boundaries within the agent. This
caused terminological and architectural ambiguities between researchers which
we addressed in this paper by proposing a unified framework that establishes a
clear foundation for LLM-based agents' development from both functional and
software architectural perspectives.
  Our framework, LLM-Agent-UMF (LLM-based Agent Unified Modeling Framework),
clearly distinguishes between the different components of an agent, setting
LLMs, and tools apart from a newly introduced element: the core-agent, playing
the role of the central coordinator of the agent which comprises five modules:
planning, memory, profile, action, and security, the latter often neglected in
previous works. Differences in the internal structure of core-agents led us to
classify them into a taxonomy of passive and active types. Based on this, we
proposed different multi-core agent architectures combining unique
characteristics of various individual agents.
  For evaluation purposes, we applied this framework to a selection of
state-of-the-art agents, thereby demonstrating its alignment with their
functionalities and clarifying the overlooked architectural aspects. Moreover,
we thoroughly assessed four of our proposed architectures by integrating
distinctive agents into hybrid active/passive core-agents' systems. This
analysis provided clear insights into potential improvements and highlighted
the challenges involved in the combination of specific agents.
]]></content:encoded>
<pubDate>2024-09-17T17:54:17Z</pubDate>
</item>
<item>
<title>Pennsieve - A Collaborative Platform for Translational Neuroscience and
  Beyond</title>
<link>http://arxiv.org/abs/2409.10509v1</link>
<guid>http://arxiv.org/abs/2409.10509v1</guid>
<content:encoded><![CDATA[
<div>     Pennsieve
:<br /><br />PennsievePennsievePennsievePennsieveSPARCHEALPRECSIONRE-JOIN80Pennsieve125TB35TB350FAIRPennsieve <div>
The exponential growth of neuroscientific data necessitates platforms that
facilitate data management and multidisciplinary collaboration. In this paper,
we introduce Pennsieve - an open-source, cloud-based scientific data management
platform built to meet these needs. Pennsieve supports complex multimodal
datasets and provides tools for data visualization and analyses. It takes a
comprehensive approach to data integration, enabling researchers to define
custom metadata schemas and utilize advanced tools to filter and query their
data. Pennsieve's modular architecture allows external applications to extend
its capabilities, and collaborative workspaces with peer-reviewed data
publishing mechanisms promote high-quality datasets optimized for downstream
analysis, both in the cloud and on-premises.
  Pennsieve forms the core for major neuroscience research programs including
the NIH SPARC Initiative, NIH HEAL Initiative's PRECISION Human Pain Network,
and NIH HEAL RE-JOIN Initiative. It serves more than 80 research groups
worldwide, along with several large-scale, inter-institutional projects at
clinical sites through the University of Pennsylvania. Underpinning the
SPARC.Science, Epilepsy.Science, and Pennsieve Discover portals, Pennsieve
stores over 125 TB of scientific data, with 35 TB of data publicly available
across more than 350 high-impact datasets. It adheres to the findable,
accessible, interoperable, and reusable (FAIR) principles of data sharing and
is recognized as one of the NIH-approved Data Repositories. By facilitating
scientific data management, discovery, and analysis, Pennsieve fosters a robust
and collaborative research ecosystem for neuroscience and beyond.
]]></content:encoded>
<pubDate>2024-09-16T17:55:58Z</pubDate>
</item>
<item>
<title>MusicLIME: Explainable Multimodal Music Understanding</title>
<link>http://arxiv.org/abs/2409.10496v1</link>
<guid>http://arxiv.org/abs/2409.10496v1</guid>
<content:encoded><![CDATA[
<div> <br />
:<br />
MusicLIMEMusicLIME <div>
Multimodal models are critical for music understanding tasks, as they capture
the complex interplay between audio and lyrics. However, as these models become
more prevalent, the need for explainability grows-understanding how these
systems make decisions is vital for ensuring fairness, reducing bias, and
fostering trust. In this paper, we introduce MusicLIME, a model-agnostic
feature importance explanation method designed for multimodal music models.
Unlike traditional unimodal methods, which analyze each modality separately
without considering the interaction between them, often leading to incomplete
or misleading explanations, MusicLIME reveals how audio and lyrical features
interact and contribute to predictions, providing a holistic view of the
model's decision-making. Additionally, we enhance local explanations by
aggregating them into global explanations, giving users a broader perspective
of model behavior. Through this work, we contribute to improving the
interpretability of multimodal music models, empowering users to make informed
choices, and fostering more equitable, fair, and transparent music
understanding systems.
]]></content:encoded>
<pubDate>2024-09-16T17:28:21Z</pubDate>
</item>
<item>
<title>The unknotting number, hard unknot diagrams, and reinforcement learning</title>
<link>http://arxiv.org/abs/2409.09032v1</link>
<guid>http://arxiv.org/abs/2409.09032v1</guid>
<content:encoded><![CDATA[
<div> 

 unknotting number  57k  unknotting number unknotting crossings  unknotting number  2.6  hard unknot diagrams 43  12  unknotting number<br /><br /> unknotting number  unknot diagrams  <div>
We have developed a reinforcement learning agent that often finds a minimal
sequence of unknotting crossing changes for a knot diagram with up to 200
crossings, hence giving an upper bound on the unknotting number. We have used
this to determine the unknotting number of 57k knots. We took diagrams of
connected sums of such knots with oppositely signed signatures, where the
summands were overlaid. The agent has found examples where several of the
crossing changes in an unknotting collection of crossings result in hyperbolic
knots. Based on this, we have shown that, given knots $K$ and $K'$ that satisfy
some mild assumptions, there is a diagram of their connected sum and $u(K) +
u(K')$ unknotting crossings such that changing any one of them results in a
prime knot. As a by-product, we have obtained a dataset of 2.6 million distinct
hard unknot diagrams; most of them under 35 crossings. Assuming the additivity
of the unknotting number, we have determined the unknotting number of 43 at
most 12-crossing knots for which the unknotting number is unknown.
]]></content:encoded>
<pubDate>2024-09-13T17:59:52Z</pubDate>
</item>
<item>
<title>Agents in Software Engineering: Survey, Landscape, and Vision</title>
<link>http://arxiv.org/abs/2409.09030v1</link>
<guid>http://arxiv.org/abs/2409.09030v1</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Software Engineering, Agents, Perception, Memory

:<br /><br />LLMsSELLMSEGitHub <div>
In recent years, Large Language Models (LLMs) have achieved remarkable
success and have been widely used in various downstream tasks, especially in
the tasks of the software engineering (SE) field. We find that many studies
combining LLMs with SE have employed the concept of agents either explicitly or
implicitly. However, there is a lack of an in-depth survey to sort out the
development context of existing works, analyze how existing works combine the
LLM-based agent technologies to optimize various tasks, and clarify the
framework of LLM-based agents in SE. In this paper, we conduct the first survey
of the studies on combining LLM-based agents with SE and present a framework of
LLM-based agents in SE which includes three key modules: perception, memory,
and action. We also summarize the current challenges in combining the two
fields and propose future opportunities in response to existing challenges. We
maintain a GitHub repository of the related papers at:
https://github.com/DeepSoftwareAnalytics/Awesome-Agent4SE.
]]></content:encoded>
<pubDate>2024-09-13T17:55:58Z</pubDate>
</item>
<item>
<title>AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM
  Agents</title>
<link>http://arxiv.org/abs/2409.09013v1</link>
<guid>http://arxiv.org/abs/2409.09013v1</guid>
<content:encoded><![CDATA[
To be safely and successfully deployed, LLMs must simultaneously satisfy
truthfulness and utility goals. Yet, often these two goals compete (e.g., an AI
agent assisting a used car salesman selling a car with flaws), partly due to
ambiguous or misleading user instructions. We propose AI-LieDar, a framework to
study how LLM-based agents navigate scenarios with utility-truthfulness
conflicts in a multi-turn interactive setting. We design a set of realistic
scenarios where language agents are instructed to achieve goals that are in
conflict with being truthful during a multi-turn conversation with simulated
human agents. To evaluate the truthfulness at large scale, we develop a
truthfulness detector inspired by psychological literature to assess the
agents' responses. Our experiment demonstrates that all models are truthful
less than 50% of the time, although truthfulness and goal achievement (utility)
rates vary across models. We further test the steerability of LLMs towards
truthfulness, finding that models follow malicious instructions to deceive, and
even truth-steered models can still lie. These findings reveal the complex
nature of truthfulness in LLMs and underscore the importance of further
research to ensure the safe and reliable deployment of LLMs and AI agents.
]]></content:encoded>
<pubDate>2024-09-13T17:41:12Z</pubDate>
</item>
<item>
<title>Click2Mask: Local Editing with Dynamic Mask Generation</title>
<link>http://arxiv.org/abs/2409.08272v1</link>
<guid>http://arxiv.org/abs/2409.08272v1</guid>
<content:encoded><![CDATA[
<div> Click2Mask, local image editing, generative models, Blended Latent Diffusion, user-friendly<br />
<br />
Click2MaskBlended Latent Diffusion (BLD) CLIPClick2Mask<br /><br />: <div>
Recent advancements in generative models have revolutionized image generation
and editing, making these tasks accessible to non-experts. This paper focuses
on local image editing, particularly the task of adding new content to a
loosely specified area. Existing methods often require a precise mask or a
detailed description of the location, which can be cumbersome and prone to
errors. We propose Click2Mask, a novel approach that simplifies the local
editing process by requiring only a single point of reference (in addition to
the content description). A mask is dynamically grown around this point during
a Blended Latent Diffusion (BLD) process, guided by a masked CLIP-based
semantic loss. Click2Mask surpasses the limitations of segmentation-based and
fine-tuning dependent methods, offering a more user-friendly and contextually
accurate solution. Our experiments demonstrate that Click2Mask not only
minimizes user effort but also delivers competitive or superior local image
manipulation results compared to SoTA methods, according to both human
judgement and automatic metrics. Key contributions include the simplification
of user input, the ability to freely add objects unconstrained by existing
segments, and the integration potential of our dynamic mask approach within
other editing methods.
]]></content:encoded>
<pubDate>2024-09-12T17:59:04Z</pubDate>
</item>
<item>
<title>Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale</title>
<link>http://arxiv.org/abs/2409.08264v1</link>
<guid>http://arxiv.org/abs/2409.08264v1</guid>
<content:encoded><![CDATA[
<div> Agent, Windows Agent Arena, multi-modal tasks, benchmark, Navi
<br /><br />:
Windows Agent ArenaWindows150Navi <div>
Large language models (LLMs) show remarkable potential to act as computer
agents, enhancing human productivity and software accessibility in multi-modal
tasks that require planning and reasoning. However, measuring agent performance
in realistic environments remains a challenge since: (i) most benchmarks are
limited to specific modalities or domains (e.g. text-only, web navigation, Q&amp;A,
coding) and (ii) full benchmark evaluations are slow (on order of magnitude of
days) given the multi-step sequential nature of tasks. To address these
challenges, we introduce the Windows Agent Arena: a reproducible, general
environment focusing exclusively on the Windows operating system (OS) where
agents can operate freely within a real Windows OS and use the same wide range
of applications, tools, and web browsers available to human users when solving
tasks. We adapt the OSWorld framework (Xie et al., 2024) to create 150+ diverse
Windows tasks across representative domains that require agent abilities in
planning, screen understanding, and tool usage. Our benchmark is scalable and
can be seamlessly parallelized in Azure for a full benchmark evaluation in as
little as 20 minutes. To demonstrate Windows Agent Arena's capabilities, we
also introduce a new multi-modal agent, Navi. Our agent achieves a success rate
of 19.5% in the Windows domain, compared to 74.5% performance of an unassisted
human. Navi also demonstrates strong performance on another popular web-based
benchmark, Mind2Web. We offer extensive quantitative and qualitative analysis
of Navi's performance, and provide insights into the opportunities for future
research in agent development and data generation using Windows Agent Arena.
  Webpage: https://microsoft.github.io/WindowsAgentArena
  Code: https://github.com/microsoft/WindowsAgentArena
]]></content:encoded>
<pubDate>2024-09-12T17:56:43Z</pubDate>
</item>
<item>
<title>DreamMesh: Jointly Manipulating and Texturing Triangle Meshes for
  Text-to-3D Generation</title>
<link>http://arxiv.org/abs/2409.07454v1</link>
<guid>http://arxiv.org/abs/2409.07454v1</guid>
<content:encoded><![CDATA[
<div> NeRF, 3D generation, DreamMesh, 3D architecture, text-guided Jacobians
<br /><br />
1. DreamMesh3D3D
2. DreamMesh2D
3. DreamMesh
4. DreamMesh3D3D
5. https://dreammesh.github.io
<br /><br />: DreamMesh3D3D <div>
Learning radiance fields (NeRF) with powerful 2D diffusion models has
garnered popularity for text-to-3D generation. Nevertheless, the implicit 3D
representations of NeRF lack explicit modeling of meshes and textures over
surfaces, and such surface-undefined way may suffer from the issues, e.g.,
noisy surfaces with ambiguous texture details or cross-view inconsistency. To
alleviate this, we present DreamMesh, a novel text-to-3D architecture that
pivots on well-defined surfaces (triangle meshes) to generate high-fidelity
explicit 3D model. Technically, DreamMesh capitalizes on a distinctive
coarse-to-fine scheme. In the coarse stage, the mesh is first deformed by
text-guided Jacobians and then DreamMesh textures the mesh with an interlaced
use of 2D diffusion models in a tuning free manner from multiple viewpoints. In
the fine stage, DreamMesh jointly manipulates the mesh and refines the texture
map, leading to high-quality triangle meshes with high-fidelity textured
materials. Extensive experiments demonstrate that DreamMesh significantly
outperforms state-of-the-art text-to-3D methods in faithfully generating 3D
content with richer textual details and enhanced geometry. Our project page is
available at https://dreammesh.github.io.
]]></content:encoded>
<pubDate>2024-09-11T17:59:02Z</pubDate>
</item>
<item>
<title>"My Grade is Wrong!": A Contestable AI Framework for Interactive
  Feedback in Evaluating Student Essays</title>
<link>http://arxiv.org/abs/2409.07453v1</link>
<guid>http://arxiv.org/abs/2409.07453v1</guid>
<content:encoded><![CDATA[
<div> : , , CAELF, , 
:<br /><br />CAELFCAELFTA500CAELFLLM <div>
Interactive feedback, where feedback flows in both directions between teacher
and student, is more effective than traditional one-way feedback. However, it
is often too time-consuming for widespread use in educational practice. While
Large Language Models (LLMs) have potential for automating feedback, they
struggle with reasoning and interaction in an interactive setting. This paper
introduces CAELF, a Contestable AI Empowered LLM Framework for automating
interactive feedback. CAELF allows students to query, challenge, and clarify
their feedback by integrating a multi-agent system with computational
argumentation. Essays are first assessed by multiple Teaching-Assistant Agents
(TA Agents), and then a Teacher Agent aggregates the evaluations through formal
reasoning to generate feedback and grades. Students can further engage with the
feedback to refine their understanding. A case study on 500 critical thinking
essays with user studies demonstrates that CAELF significantly improves
interactive feedback, enhancing the reasoning and interaction capabilities of
LLMs. This approach offers a promising solution to overcoming the time and
resource barriers that have limited the adoption of interactive feedback in
educational settings.
]]></content:encoded>
<pubDate>2024-09-11T17:59:01Z</pubDate>
</item>
<item>
<title>Hi3D: Pursuing High-Resolution Image-to-3D Generation with Video
  Diffusion Models</title>
<link>http://arxiv.org/abs/2409.07452v1</link>
<guid>http://arxiv.org/abs/2409.07452v1</guid>
<content:encoded><![CDATA[
Despite having tremendous progress in image-to-3D generation, existing
methods still struggle to produce multi-view consistent images with
high-resolution textures in detail, especially in the paradigm of 2D diffusion
that lacks 3D awareness. In this work, we present High-resolution Image-to-3D
model (Hi3D), a new video diffusion based paradigm that redefines a single
image to multi-view images as 3D-aware sequential image generation (i.e.,
orbital video generation). This methodology delves into the underlying temporal
consistency knowledge in video diffusion model that generalizes well to
geometry consistency across multiple views in 3D generation. Technically, Hi3D
first empowers the pre-trained video diffusion model with 3D-aware prior
(camera pose condition), yielding multi-view images with low-resolution texture
details. A 3D-aware video-to-video refiner is learnt to further scale up the
multi-view images with high-resolution texture details. Such high-resolution
multi-view images are further augmented with novel views through 3D Gaussian
Splatting, which are finally leveraged to obtain high-fidelity meshes via 3D
reconstruction. Extensive experiments on both novel view synthesis and single
view reconstruction demonstrate that our Hi3D manages to produce superior
multi-view consistency images with highly-detailed textures. Source code and
data are available at \url{https://github.com/yanghb22-fdu/Hi3D-Official}.
]]></content:encoded>
<pubDate>2024-09-11T17:58:57Z</pubDate>
</item>
<item>
<title>FreeEnhance: Tuning-Free Image Enhancement via Content-Consistent
  Noising-and-Denoising Process</title>
<link>http://arxiv.org/abs/2409.07451v1</link>
<guid>http://arxiv.org/abs/2409.07451v1</guid>
<content:encoded><![CDATA[
The emergence of text-to-image generation models has led to the recognition
that image enhancement, performed as post-processing, would significantly
improve the visual quality of the generated images. Exploring diffusion models
to enhance the generated images nevertheless is not trivial and necessitates to
delicately enrich plentiful details while preserving the visual appearance of
key content in the original image. In this paper, we propose a novel framework,
namely FreeEnhance, for content-consistent image enhancement using the
off-the-shelf image diffusion models. Technically, FreeEnhance is a two-stage
process that firstly adds random noise to the input image and then capitalizes
on a pre-trained image diffusion model (i.e., Latent Diffusion Models) to
denoise and enhance the image details. In the noising stage, FreeEnhance is
devised to add lighter noise to the region with higher frequency to preserve
the high-frequent patterns (e.g., edge, corner) in the original image. In the
denoising stage, we present three target properties as constraints to
regularize the predicted noise, enhancing images with high acutance and high
visual quality. Extensive experiments conducted on the HPDv2 dataset
demonstrate that our FreeEnhance outperforms the state-of-the-art image
enhancement models in terms of quantitative metrics and human preference. More
remarkably, FreeEnhance also shows higher human preference compared to the
commercial image enhancement solution of Magnific AI.
]]></content:encoded>
<pubDate>2024-09-11T17:58:50Z</pubDate>
</item>
<item>
<title>Cooptimizing Safety and Performance with a Control-Constrained
  Formulation</title>
<link>http://arxiv.org/abs/2409.06696v1</link>
<guid>http://arxiv.org/abs/2409.06696v1</guid>
<content:encoded><![CDATA[
<div> : , , , , --<br />
: <br />
-- <div>
Autonomous systems have witnessed a rapid increase in their capabilities, but
it remains a challenge for them to perform tasks both effectively and safely.
The fact that performance and safety can sometimes be competing objectives
renders the cooptimization between them difficult. One school of thought is to
treat this cooptimization as a constrained optimal control problem with a
performance-oriented objective function and safety as a constraint. However,
solving this constrained optimal control problem for general nonlinear systems
remains challenging. In this work, we use the general framework of constrained
optimal control, but given the safety state constraint, we convert it into an
equivalent control constraint, resulting in a state and time-dependent
control-constrained optimal control problem. This equivalent optimal control
problem can readily be solved using the dynamic programming principle. We show
the corresponding value function is a viscosity solution of a certain
Hamilton-Jacobi-Bellman Partial Differential Equation (HJB-PDE). Furthermore,
we demonstrate the effectiveness of our method with a two-dimensional case
study, and the experiment shows that the controller synthesized using our
method consistently outperforms the baselines, both in safety and performance.
]]></content:encoded>
<pubDate>2024-09-10T17:56:59Z</pubDate>
</item>
<item>
<title>DANCE: Deep Learning-Assisted Analysis of Protein Sequences Using Chaos
  Enhanced Kaleidoscopic Images</title>
<link>http://arxiv.org/abs/2409.06694v1</link>
<guid>http://arxiv.org/abs/2409.06694v1</guid>
<content:encoded><![CDATA[
<div> TCR, cancer, protein sequences, Chaos Game Representation, deep learning<br />
:<br />
DANCE(TCRs)TCRsTCR <div>
Cancer is a complex disease characterized by uncontrolled cell growth. T cell
receptors (TCRs), crucial proteins in the immune system, play a key role in
recognizing antigens, including those associated with cancer. Recent
advancements in sequencing technologies have facilitated comprehensive
profiling of TCR repertoires, uncovering TCRs with potent anti-cancer activity
and enabling TCR-based immunotherapies. However, analyzing these intricate
biomolecules necessitates efficient representations that capture their
structural and functional information. T-cell protein sequences pose unique
challenges due to their relatively smaller lengths compared to other
biomolecules. An image-based representation approach becomes a preferred choice
for efficient embeddings, allowing for the preservation of essential details
and enabling comprehensive analysis of T-cell protein sequences. In this paper,
we propose to generate images from the protein sequences using the idea of
Chaos Game Representation (CGR) using the Kaleidoscopic images approach. This
Deep Learning Assisted Analysis of Protein Sequences Using Chaos Enhanced
Kaleidoscopic Images (called DANCE) provides a unique way to visualize protein
sequences by recursively applying chaos game rules around a central seed point.
we perform the classification of the T cell receptors (TCRs) protein sequences
in terms of their respective target cancer cells, as TCRs are known for their
immune response against cancer disease. The TCR sequences are converted into
images using the DANCE method. We employ deep-learning vision models to perform
the classification to obtain insights into the relationship between the visual
patterns observed in the generated kaleidoscopic images and the underlying
protein properties. By combining CGR-based image generation with deep learning
classification, this study opens novel possibilities in the protein analysis
domain.
]]></content:encoded>
<pubDate>2024-09-10T17:55:59Z</pubDate>
</item>
<item>
<title>Promptable Closed-loop Traffic Simulation</title>
<link>http://arxiv.org/abs/2409.05863v1</link>
<guid>http://arxiv.org/abs/2409.05863v1</guid>
<content:encoded><![CDATA[
<div> Traffic simulation, ProSim, promptable, closed-loop, autonomous driving
:<br /><br />ProSimProSimProSim-Instruct-520k10M520khttps://ariostgx.github.io/ProSimProSimProSim-Instruct-520k <div>
Simulation stands as a cornerstone for safe and efficient autonomous driving
development. At its core a simulation system ought to produce realistic,
reactive, and controllable traffic patterns. In this paper, we propose ProSim,
a multimodal promptable closed-loop traffic simulation framework. ProSim allows
the user to give a complex set of numerical, categorical or textual prompts to
instruct each agent's behavior and intention. ProSim then rolls out a traffic
scenario in a closed-loop manner, modeling each agent's interaction with other
traffic participants. Our experiments show that ProSim achieves high prompt
controllability given different user prompts, while reaching competitive
performance on the Waymo Sim Agents Challenge when no prompt is given. To
support research on promptable traffic simulation, we create
ProSim-Instruct-520k, a multimodal prompt-scenario paired driving dataset with
over 10M text prompts for over 520k real-world driving scenarios. We will
release code of ProSim as well as data and labeling tools of
ProSim-Instruct-520k at https://ariostgx.github.io/ProSim.
]]></content:encoded>
<pubDate>2024-09-09T17:59:15Z</pubDate>
</item>
<item>
<title>A Survey on Knowledge Organization Systems of Research Fields: Resources
  and Challenges</title>
<link>http://arxiv.org/abs/2409.04432v1</link>
<guid>http://arxiv.org/abs/2409.04432v1</guid>
<content:encoded><![CDATA[
<div> KOSs, term lists, thesauri, taxonomies, ontologies, academic disciplines<br />
45<br /><br />:  <div>
Knowledge Organization Systems (KOSs), such as term lists, thesauri,
taxonomies, and ontologies, play a fundamental role in categorising, managing,
and retrieving information. In the academic domain, KOSs are often adopted for
representing research areas and their relationships, primarily aiming to
classify research articles, academic courses, patents, books, scientific
venues, domain experts, grants, software, experiment materials, and several
other relevant products and agents. These structured representations of
research areas, widely embraced by many academic fields, have proven effective
in empowering AI-based systems to i) enhance retrievability of relevant
documents, ii) enable advanced analytic solutions to quantify the impact of
academic research, and iii) analyse and forecast research dynamics. This paper
aims to present a comprehensive survey of the current KOS for academic
disciplines. We analysed and compared 45 KOSs according to five main
dimensions: scope, structure, curation, usage, and links to other KOSs. Our
results reveal a very heterogeneous scenario in terms of scope, scale, quality,
and usage, highlighting the need for more integrated solutions for representing
research knowledge across academic fields. We conclude by discussing the main
challenges and the most promising future directions.
]]></content:encoded>
<pubDate>2024-09-06T17:54:43Z</pubDate>
</item>
<item>
<title>VILA-U: a Unified Foundation Model Integrating Visual Understanding and
  Generation</title>
<link>http://arxiv.org/abs/2409.04429v1</link>
<guid>http://arxiv.org/abs/2409.04429v1</guid>
<content:encoded><![CDATA[
<div> Video, Image, Language, Understanding, Generation
<br /><br />
VILA-UVILA-UVILA-UVILA-U <div>
VILA-U is a Unified foundation model that integrates Video, Image, Language
understanding and generation. Traditional visual language models (VLMs) use
separate modules for understanding and generating visual content, which can
lead to misalignment and increased complexity. In contrast, VILA-U employs a
single autoregressive next-token prediction framework for both tasks,
eliminating the need for additional components like diffusion models. This
approach not only simplifies the model but also achieves near state-of-the-art
performance in visual language understanding and generation. The success of
VILA-U is attributed to two main factors: the unified vision tower that aligns
discrete visual tokens with textual inputs during pretraining, which enhances
visual perception, and autoregressive image generation can achieve similar
quality as diffusion models with high-quality dataset. This allows VILA-U to
perform comparably to more complex models using a fully token-based
autoregressive framework.
]]></content:encoded>
<pubDate>2024-09-06T17:49:56Z</pubDate>
</item>
<item>
<title>RLPF: Reinforcement Learning from Prediction Feedback for User
  Summarization with LLMs</title>
<link>http://arxiv.org/abs/2409.04421v1</link>
<guid>http://arxiv.org/abs/2409.04421v1</guid>
<content:encoded><![CDATA[
LLM-powered personalization agent systems employ Large Language Models (LLMs)
to predict users' behavior from their past activities. However, their
effectiveness often hinges on the ability to effectively leverage extensive,
long user historical data due to its inherent noise and length of such data.
Existing pretrained LLMs may generate summaries that are concise but lack the
necessary context for downstream tasks, hindering their utility in
personalization systems. To address these challenges, we introduce
Reinforcement Learning from Prediction Feedback (RLPF). RLPF fine-tunes LLMs to
generate concise, human-readable user summaries that are optimized for
downstream task performance. By maximizing the usefulness of the generated
summaries, RLPF effectively distills extensive user history data while
preserving essential information for downstream tasks. Our empirical evaluation
demonstrates significant improvements in both extrinsic downstream task utility
and intrinsic summary quality, surpassing baseline methods by up to 22% on
downstream task performance and achieving an up to 84.59% win rate on
Factuality, Abstractiveness, and Readability. RLPF also achieves a remarkable
74% reduction in context length while improving performance on 16 out of 19
unseen tasks and/or datasets, showcasing its generalizability. This approach
offers a promising solution for enhancing LLM personalization by effectively
transforming long, noisy user histories into informative and human-readable
representations.
]]></content:encoded>
<pubDate>2024-09-06T17:30:45Z</pubDate>
</item>
<item>
<title>ArtiFade: Learning to Generate High-quality Subject from Blemished
  Images</title>
<link>http://arxiv.org/abs/2409.03745v1</link>
<guid>http://arxiv.org/abs/2409.03745v1</guid>
<content:encoded><![CDATA[
<div> Subject-driven, text-to-image generation, ArtiFade, artifact removal, high-quality images<br /> 

ArtiFade-ArtiFade<br /><br />: ArtiFade- <div>
Subject-driven text-to-image generation has witnessed remarkable advancements
in its ability to learn and capture characteristics of a subject using only a
limited number of images. However, existing methods commonly rely on
high-quality images for training and may struggle to generate reasonable images
when the input images are blemished by artifacts. This is primarily attributed
to the inadequate capability of current techniques in distinguishing
subject-related features from disruptive artifacts. In this paper, we introduce
ArtiFade to tackle this issue and successfully generate high-quality
artifact-free images from blemished datasets. Specifically, ArtiFade exploits
fine-tuning of a pre-trained text-to-image model, aiming to remove artifacts.
The elimination of artifacts is achieved by utilizing a specialized dataset
that encompasses both unblemished images and their corresponding blemished
counterparts during fine-tuning. ArtiFade also ensures the preservation of the
original generative capabilities inherent within the diffusion model, thereby
enhancing the overall performance of subject-driven methods in generating
high-quality and artifact-free images. We further devise evaluation benchmarks
tailored for this task. Through extensive qualitative and quantitative
experiments, we demonstrate the generalizability of ArtiFade in effective
artifact removal under both in-distribution and out-of-distribution scenarios.
]]></content:encoded>
<pubDate>2024-09-05T17:57:59Z</pubDate>
</item>
<item>
<title>HiPrompt: Tuning-free Higher-Resolution Generation with Hierarchical
  MLLM Prompts</title>
<link>http://arxiv.org/abs/2409.02919v1</link>
<guid>http://arxiv.org/abs/2409.02919v1</guid>
<content:encoded><![CDATA[
<div> , , HiPrompt, , 

HiPromptHiPrompt <br /><br />: HiPrompt <div>
The potential for higher-resolution image generation using pretrained
diffusion models is immense, yet these models often struggle with issues of
object repetition and structural artifacts especially when scaling to 4K
resolution and higher. We figure out that the problem is caused by that, a
single prompt for the generation of multiple scales provides insufficient
efficacy. In response, we propose HiPrompt, a new tuning-free solution that
tackles the above problems by introducing hierarchical prompts. The
hierarchical prompts offer both global and local guidance. Specifically, the
global guidance comes from the user input that describes the overall content,
while the local guidance utilizes patch-wise descriptions from MLLMs to
elaborately guide the regional structure and texture generation. Furthermore,
during the inverse denoising process, the generated noise is decomposed into
low- and high-frequency spatial components. These components are conditioned on
multiple prompt levels, including detailed patch-wise descriptions and broader
image-level prompts, facilitating prompt-guided denoising under hierarchical
semantic guidance. It further allows the generation to focus more on local
spatial regions and ensures the generated images maintain coherent local and
global semantics, structures, and textures with high definition. Extensive
experiments demonstrate that HiPrompt outperforms state-of-the-art works in
higher-resolution image generation, significantly reducing object repetition
and enhancing structural quality.
]]></content:encoded>
<pubDate>2024-09-04T17:58:08Z</pubDate>
</item>
<item>
<title>Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of
  Encoders</title>
<link>http://arxiv.org/abs/2408.15998v1</link>
<guid>http://arxiv.org/abs/2408.15998v1</guid>
<content:encoded><![CDATA[
<div> :  (MLLMs), , , , 

 (MLLMs)  MLLMs MLLMs Pre-Alignment MLLMs EagleMLLM Model code: https://github.com/NVlabs/Eagle<br /><br />:  (MLLMs)  Pre-Alignment MLLMs EagleMLLM <div>
The ability to accurately interpret complex visual information is a crucial
topic of multimodal large language models (MLLMs). Recent work indicates that
enhanced visual perception significantly reduces hallucinations and improves
performance on resolution-sensitive tasks, such as optical character
recognition and document analysis. A number of recent MLLMs achieve this goal
using a mixture of vision encoders. Despite their success, there is a lack of
systematic comparisons and detailed ablation studies addressing critical
aspects, such as expert selection and the integration of multiple vision
experts. This study provides an extensive exploration of the design space for
MLLMs using a mixture of vision encoders and resolutions. Our findings reveal
several underlying principles common to various existing strategies, leading to
a streamlined yet effective design approach. We discover that simply
concatenating visual tokens from a set of complementary vision encoders is as
effective as more complex mixing architectures or strategies. We additionally
introduce Pre-Alignment to bridge the gap between vision-focused encoders and
language tokens, enhancing model coherence. The resulting family of MLLMs,
Eagle, surpasses other leading open-source models on major MLLM benchmarks.
Models and code: https://github.com/NVlabs/Eagle
]]></content:encoded>
<pubDate>2024-08-28T17:59:31Z</pubDate>
</item>
<item>
<title>GenRec: Unifying Video Generation and Recognition with Diffusion Models</title>
<link>http://arxiv.org/abs/2408.15241v1</link>
<guid>http://arxiv.org/abs/2408.15241v1</guid>
<content:encoded><![CDATA[
<div> GenRec<br />
GenRecGenRecGenRecSSV2K40075.8%87.2%GenRecSSV2EK-10046.549.3FVDGenRec <br /><br />: <br />GenRec <div>
Video diffusion models are able to generate high-quality videos by learning
strong spatial-temporal priors on large-scale datasets. In this paper, we aim
to investigate whether such priors derived from a generative process are
suitable for video recognition, and eventually joint optimization of generation
and recognition. Building upon Stable Video Diffusion, we introduce GenRec, the
first unified framework trained with a random-frame conditioning process so as
to learn generalized spatial-temporal representations. The resulting framework
can naturally supports generation and recognition, and more importantly is
robust even when visual inputs contain limited information. Extensive
experiments demonstrate the efficacy of GenRec for both recognition and
generation. In particular, GenRec achieves competitive recognition performance,
offering 75.8% and 87.2% accuracy on SSV2 and K400, respectively. GenRec also
performs the best class-conditioned image-to-video generation results,
achieving 46.5 and 49.3 FVD scores on SSV2 and EK-100 datasets. Furthermore,
GenRec demonstrates extraordinary robustness in scenarios that only limited
frames can be observed.
]]></content:encoded>
<pubDate>2024-08-27T17:59:41Z</pubDate>
</item>
<item>
<title>Into the Unknown Unknowns: Engaged Human Learning through Participation
  in Language Model Agent Conversations</title>
<link>http://arxiv.org/abs/2408.15232v1</link>
<guid>http://arxiv.org/abs/2408.15232v1</guid>
<content:encoded><![CDATA[
<div> Collaborative STORM, Co-STORM, LM agents, unknown unknowns, WildSeek dataset

:
Collaborative STORM (Co-STORM)  LM agents Co-STORM  LM agents  agents Co-STORM  WildSeek  Co-STORM  Co-STORM 70%  Co-STORM 78%  Co-STORM  RAG Co-STORM  <div>
While language model (LM)-powered chatbots and generative search engines
excel at answering concrete queries, discovering information in the terrain of
unknown unknowns remains challenging for users. To emulate the common
educational scenario where children/students learn by listening to and
participating in conversations of their parents/teachers, we create
Collaborative STORM (Co-STORM). Unlike QA systems that require users to ask all
the questions, Co-STORM lets users observe and occasionally steer the discourse
among several LM agents. The agents ask questions on the user's behalf,
allowing the user to discover unknown unknowns serendipitously. To facilitate
user interaction, Co-STORM assists users in tracking the discourse by
organizing the uncovered information into a dynamic mind map, ultimately
generating a comprehensive report as takeaways. For automatic evaluation, we
construct the WildSeek dataset by collecting real information-seeking records
with user goals. Co-STORM outperforms baseline methods on both discourse trace
and report quality. In a further human evaluation, 70% of participants prefer
Co-STORM over a search engine, and 78% favor it over a RAG chatbot.
]]></content:encoded>
<pubDate>2024-08-27T17:50:03Z</pubDate>
</item>
<item>
<title>A Practitioner's Guide to Continual Multimodal Pretraining</title>
<link>http://arxiv.org/abs/2408.14471v1</link>
<guid>http://arxiv.org/abs/2408.14471v1</guid>
<content:encoded><![CDATA[
<div> FoMo-in-Flux

FoMo-in-Flux63FoMo-in-FluxGitHubhttps://github.com/ExplainableML/fomo_in_flux<br /><br />: FoMo-in-Flux <div>
Multimodal foundation models serve numerous applications at the intersection
of vision and language. Still, despite being pretrained on extensive data, they
become outdated over time. To keep models updated, research into continual
pretraining mainly explores scenarios with either (1) infrequent,
indiscriminate updates on large-scale new data, or (2) frequent, sample-level
updates. However, practical model deployment often operates in the gap between
these two limit cases, as real-world applications often demand adaptation to
specific subdomains, tasks or concepts -- spread over the entire, varying life
cycle of a model. In this work, we complement current perspectives on continual
pretraining through a research test bed as well as provide comprehensive
guidance for effective continual model updates in such scenarios. We first
introduce FoMo-in-Flux, a continual multimodal pretraining benchmark with
realistic compute constraints and practical deployment requirements,
constructed over 63 datasets with diverse visual and semantic coverage. Using
FoMo-in-Flux, we explore the complex landscape of practical continual
pretraining through multiple perspectives: (1) A data-centric investigation of
data mixtures and stream orderings that emulate real-world deployment
situations, (2) a method-centric investigation ranging from simple fine-tuning
and traditional continual learning strategies to parameter-efficient updates
and model merging, (3) meta learning rate schedules and mechanistic design
choices, and (4) the influence of model and compute scaling. Together, our
insights provide a practitioner's guide to continual multimodal pretraining for
real-world deployment. Our benchmark and code is here:
https://github.com/ExplainableML/fomo_in_flux.
]]></content:encoded>
<pubDate>2024-08-26T17:59:01Z</pubDate>
</item>
<item>
<title>Foundational Model for Electron Micrograph Analysis: Instruction-Tuning
  Small-Scale Language-and-Vision Assistant for Enterprise Adoption</title>
<link>http://arxiv.org/abs/2408.13248v1</link>
<guid>http://arxiv.org/abs/2408.13248v1</guid>
<content:encoded><![CDATA[
<div> : <br />
<br />
MAEMIVQAMAEMIMAEMI <br /><br />: <div>
Semiconductor imaging and analysis are critical yet understudied in deep
learning, limiting our ability for precise control and optimization in
semiconductor manufacturing. We introduce a small-scale multimodal framework
for analyzing semiconductor electron microscopy images (MAEMI) through
vision-language instruction tuning. We generate a customized
instruction-following dataset using large multimodal models on microscopic
image analysis. We perform knowledge transfer from larger to smaller models
through knowledge distillation, resulting in improved accuracy of smaller
models on visual question answering (VQA) tasks. This approach eliminates the
need for expensive, human expert-annotated datasets for microscopic image
analysis tasks. Enterprises can further finetune MAEMI on their intellectual
data, enhancing privacy and performance on low-cost consumer hardware. Our
experiments show that MAEMI outperforms traditional methods, adapts to data
distribution shifts, and supports high-throughput screening.
]]></content:encoded>
<pubDate>2024-08-23T17:42:11Z</pubDate>
</item>
<item>
<title>xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed
  Representations</title>
<link>http://arxiv.org/abs/2408.12590v1</link>
<guid>http://arxiv.org/abs/2408.12590v1</guid>
<content:encoded><![CDATA[
<div> , , , , 

:<br /><br />xGen-VideoSyn-1LDMVidVAE1300VidVAEDiTT2V <div>
We present xGen-VideoSyn-1, a text-to-video (T2V) generation model capable of
producing realistic scenes from textual descriptions. Building on recent
advancements, such as OpenAI's Sora, we explore the latent diffusion model
(LDM) architecture and introduce a video variational autoencoder (VidVAE).
VidVAE compresses video data both spatially and temporally, significantly
reducing the length of visual tokens and the computational demands associated
with generating long-sequence videos. To further address the computational
costs, we propose a divide-and-merge strategy that maintains temporal
consistency across video segments. Our Diffusion Transformer (DiT) model
incorporates spatial and temporal self-attention layers, enabling robust
generalization across different timeframes and aspect ratios. We have devised a
data processing pipeline from the very beginning and collected over 13M
high-quality video-text pairs. The pipeline includes multiple steps such as
clipping, text detection, motion estimation, aesthetics scoring, and dense
captioning based on our in-house video-LLM model. Training the VidVAE and DiT
models required approximately 40 and 642 H100 days, respectively. Our model
supports over 14-second 720p video generation in an end-to-end way and
demonstrates competitive performance against state-of-the-art T2V models.
]]></content:encoded>
<pubDate>2024-08-22T17:55:22Z</pubDate>
</item>
<item>
<title>GRAB: A Challenging GRaph Analysis Benchmark for Large Multimodal Models</title>
<link>http://arxiv.org/abs/2408.11817v1</link>
<guid>http://arxiv.org/abs/2408.11817v1</guid>
<content:encoded><![CDATA[
<div> <br />
 GRAB21702320GRAB21.7%GRAB<br /><br />: GRAB217042320GRAB <div>
Large multimodal models (LMMs) have exhibited proficiencies across many
visual tasks. Although numerous well-known benchmarks exist to evaluate model
performance, they increasingly have insufficient headroom. As such, there is a
pressing need for a new generation of benchmarks challenging enough for the
next generation of LMMs. One area that LMMs show potential is graph analysis,
specifically, the tasks an analyst might typically perform when interpreting
figures such as estimating the mean, intercepts or correlations of functions
and data series. In this work, we introduce GRAB, a graph analysis benchmark,
fit for current and future frontier LMMs. Our benchmark is entirely synthetic,
ensuring high-quality, noise-free questions. GRAB is comprised of 2170
questions, covering four tasks and 23 graph properties. We evaluate 20 LMMs on
GRAB, finding it to be a challenging benchmark, with the highest performing
model attaining a score of just 21.7%. Finally, we conduct various ablations to
investigate where the models succeed and struggle. We release GRAB to encourage
progress in this important, growing domain.
]]></content:encoded>
<pubDate>2024-08-21T17:59:32Z</pubDate>
</item>
<item>
<title>Efficient Exploration and Discriminative World Model Learning with an
  Object-Centric Abstraction</title>
<link>http://arxiv.org/abs/2408.11816v1</link>
<guid>http://arxiv.org/abs/2408.11816v1</guid>
<content:encoded><![CDATA[
<div> 
<br />
2DMiniHack 
<br /><br />:  <div>
In the face of difficult exploration problems in reinforcement learning, we
study whether giving an agent an object-centric mapping (describing a set of
items and their attributes) allow for more efficient learning. We found this
problem is best solved hierarchically by modelling items at a higher level of
state abstraction to pixels, and attribute change at a higher level of temporal
abstraction to primitive actions. This abstraction simplifies the transition
dynamic by making specific future states easier to predict. We make use of this
to propose a fully model-based algorithm that learns a discriminative world
model, plans to explore efficiently with only a count-based intrinsic reward,
and can subsequently plan to reach any discovered (abstract) states.
  We demonstrate the model's ability to (i) efficiently solve single tasks,
(ii) transfer zero-shot and few-shot across item types and environments, and
(iii) plan across long horizons. Across a suite of 2D crafting and MiniHack
environments, we empirically show our model significantly out-performs
state-of-the-art low-level methods (without abstraction), as well as performant
model-free and model-based methods using the same abstraction. Finally, we show
how to reinforce learn low level object-perturbing policies, as well as
supervise learn the object mapping itself.
]]></content:encoded>
<pubDate>2024-08-21T17:59:31Z</pubDate>
</item>
<item>
<title>SEA: Supervised Embedding Alignment for Token-Level Visual-Textual
  Integration in MLLMs</title>
<link>http://arxiv.org/abs/2408.11813v1</link>
<guid>http://arxiv.org/abs/2408.11813v1</guid>
<content:encoded><![CDATA[
Multimodal Large Language Models (MLLMs) have recently demonstrated
remarkable perceptual and reasoning abilities, typically comprising a Vision
Encoder, an Adapter, and a Large Language Model (LLM). The adapter serves as
the critical bridge between the visual and language components. However,
training adapters with image-level supervision often results in significant
misalignment, undermining the LLMs' capabilities and limiting the potential of
Multimodal LLMs. To address this, we introduce Supervised Embedding Alignment
(SEA), a token-level alignment method that leverages vision-language
pre-trained models, such as CLIP, to align visual tokens with the LLM's
embedding space through contrastive learning. This approach ensures a more
coherent integration of visual and language representations, enhancing the
performance and interpretability of multimodal LLMs while preserving their
inherent capabilities. Extensive experiments show that SEA effectively improves
MLLMs, particularly for smaller models, without adding extra data or inference
computation. SEA also lays the groundwork for developing more general and
adaptable solutions to enhance multimodal systems.
]]></content:encoded>
<pubDate>2024-08-21T17:58:02Z</pubDate>
</item>
<item>
<title>EmbodiedSAM: Online Segment Any 3D Thing in Real Time</title>
<link>http://arxiv.org/abs/2408.11811v1</link>
<guid>http://arxiv.org/abs/2408.11811v1</guid>
<content:encoded><![CDATA[
Embodied tasks require the agent to fully understand 3D scenes simultaneously
with its exploration, so an online, real-time, fine-grained and
highly-generalized 3D perception model is desperately needed. Since
high-quality 3D data is limited, directly training such a model in 3D is almost
infeasible. Meanwhile, vision foundation models (VFM) has revolutionized the
field of 2D computer vision with superior performance, which makes the use of
VFM to assist embodied 3D perception a promising direction. However, most
existing VFM-assisted 3D perception methods are either offline or too slow that
cannot be applied in practical embodied tasks. In this paper, we aim to
leverage Segment Anything Model (SAM) for real-time 3D instance segmentation in
an online setting. This is a challenging problem since future frames are not
available in the input streaming RGB-D video, and an instance may be observed
in several frames so object matching between frames is required. To address
these challenges, we first propose a geometric-aware query lifting module to
represent the 2D masks generated by SAM by 3D-aware queries, which is then
iteratively refined by a dual-level query decoder. In this way, the 2D masks
are transferred to fine-grained shapes on 3D point clouds. Benefit from the
query representation for 3D masks, we can compute the similarity matrix between
the 3D masks from different views by efficient matrix operation, which enables
real-time inference. Experiments on ScanNet, ScanNet200, SceneNN and 3RScan
show our method achieves leading performance even compared with offline
methods. Our method also demonstrates great generalization ability in several
zero-shot dataset transferring experiments and show great potential in
open-vocabulary and data-efficient setting. Code and demo are available at
https://xuxw98.github.io/ESAM/, with only one RTX 3090 GPU required for
training and evaluation.
]]></content:encoded>
<pubDate>2024-08-21T17:57:06Z</pubDate>
</item>
<item>
<title>Accelerating Goal-Conditioned RL Algorithms and Research</title>
<link>http://arxiv.org/abs/2408.11052v1</link>
<guid>http://arxiv.org/abs/2408.11052v1</guid>
<content:encoded><![CDATA[
<div> JaxGCRL

JaxGCRLGPUGPUinfoNCE <br /><br />: JaxGCRL <div>
Self-supervision has the potential to transform reinforcement learning (RL),
paralleling the breakthroughs it has enabled in other areas of machine
learning. While self-supervised learning in other domains aims to find patterns
in a fixed dataset, self-supervised goal-conditioned reinforcement learning
(GCRL) agents discover new behaviors by learning from the goals achieved during
unstructured interaction with the environment. However, these methods have
failed to see similar success, both due to a lack of data from slow
environments as well as a lack of stable algorithms. We take a step toward
addressing both of these issues by releasing a high-performance codebase and
benchmark JaxGCRL for self-supervised GCRL, enabling researchers to train
agents for millions of environment steps in minutes on a single GPU. The key to
this performance is a combination of GPU-accelerated environments and a stable,
batched version of the contrastive reinforcement learning algorithm, based on
an infoNCE objective, that effectively makes use of this increased data
throughput. With this approach, we provide a foundation for future research in
self-supervised GCRL, enabling researchers to quickly iterate on new ideas and
evaluate them in a diverse set of challenging environments. Website + Code:
https://github.com/MichalBortkiewicz/JaxGCRL
]]></content:encoded>
<pubDate>2024-08-20T17:58:40Z</pubDate>
</item>
<item>
<title>FLAME: Learning to Navigate with Multimodal LLM in Urban Environments</title>
<link>http://arxiv.org/abs/2408.11051v1</link>
<guid>http://arxiv.org/abs/2408.11051v1</guid>
<content:encoded><![CDATA[
<div> Multimodal LLM-based agent, FLAME, urban VLN tasks, three-phase tuning technique, state-of-the-art performance<br />
<br />
FLAMELLMVLN<br />
1. FLAMELLMVLN
2. 
3. FLAMETouchdown7.3%
4. LLMLLMAI
<br /><br />: <br />FLAMEVLNLLMFLAMETouchdown7.3%LLMLLMAI <div>
Large Language Models (LLMs) have demonstrated potential in
Vision-and-Language Navigation (VLN) tasks, yet current applications face
challenges. While LLMs excel in general conversation scenarios, they struggle
with specialized navigation tasks, yielding suboptimal performance compared to
specialized VLN models. We introduce FLAME (FLAMingo-Architected Embodied
Agent), a novel Multimodal LLM-based agent and architecture designed for urban
VLN tasks that efficiently handles multiple observations. Our approach
implements a three-phase tuning technique for effective adaptation to
navigation tasks, including single perception tuning for street view
description, multiple perception tuning for trajectory summarization, and
end-to-end training on VLN datasets. The augmented datasets are synthesized
automatically. Experimental results demonstrate FLAME's superiority over
existing methods, surpassing state-of-the-art methods by a 7.3% increase in
task completion rate on Touchdown dataset. This work showcases the potential of
Multimodal LLMs (MLLMs) in complex navigation tasks, representing an
advancement towards practical applications of MLLMs in embodied AI. Project
page: https://flame-sjtu.github.io
]]></content:encoded>
<pubDate>2024-08-20T17:57:46Z</pubDate>
</item>
<item>
<title>MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context
  Generation with Speculative Decoding</title>
<link>http://arxiv.org/abs/2408.11049v1</link>
<guid>http://arxiv.org/abs/2408.11049v1</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) have become more prevalent in long-context
applications such as interactive chatbots, document analysis, and agent
workflows, but it is challenging to serve long-context requests with low
latency and high throughput. Speculative decoding (SD) is a widely used
technique to reduce latency without sacrificing performance but the
conventional wisdom suggests that its efficacy is limited to small batch sizes.
In MagicDec, we show that surprisingly SD can achieve speedup even for a high
throughput inference regime for moderate to long sequences. More interestingly,
an intelligent drafting strategy can achieve better speedup with increasing
batch size based on our rigorous analysis. MagicDec first identifies the
bottleneck shifts with increasing batch size and sequence length, and uses
these insights to deploy speculative decoding more effectively for high
throughput inference. Then, it leverages draft models with sparse KV cache to
address the KV bottleneck that scales with both sequence length and batch size.
]]></content:encoded>
<pubDate>2024-08-20T17:57:31Z</pubDate>
</item>
<item>
<title>SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse
  Views</title>
<link>http://arxiv.org/abs/2408.10195v1</link>
<guid>http://arxiv.org/abs/2408.10195v1</guid>
<content:encoded><![CDATA[
<div> Sparse-view images, 3D reconstruction, camera poses, diffusion model, textured mesh
<br /><br />
SpaRP2D3D3D3D20 <div>
Open-world 3D generation has recently attracted considerable attention. While
many single-image-to-3D methods have yielded visually appealing outcomes, they
often lack sufficient controllability and tend to produce hallucinated regions
that may not align with users' expectations. In this paper, we explore an
important scenario in which the input consists of one or a few unposed 2D
images of a single object, with little or no overlap. We propose a novel
method, SpaRP, to reconstruct a 3D textured mesh and estimate the relative
camera poses for these sparse-view images. SpaRP distills knowledge from 2D
diffusion models and finetunes them to implicitly deduce the 3D spatial
relationships between the sparse views. The diffusion model is trained to
jointly predict surrogate representations for camera poses and multi-view
images of the object under known poses, integrating all information from the
input sparse views. These predictions are then leveraged to accomplish 3D
reconstruction and pose estimation, and the reconstructed 3D model can be used
to further refine the camera poses of input views. Through extensive
experiments on three datasets, we demonstrate that our method not only
significantly outperforms baseline methods in terms of 3D reconstruction
quality and pose prediction accuracy but also exhibits strong efficiency. It
requires only about 20 seconds to produce a textured mesh and camera poses for
the input views. Project page: https://chaoxu.xyz/sparp.
]]></content:encoded>
<pubDate>2024-08-19T17:53:10Z</pubDate>
</item>
<item>
<title>Visual Agents as Fast and Slow Thinkers</title>
<link>http://arxiv.org/abs/2408.08862v1</link>
<guid>http://arxiv.org/abs/2408.08862v1</guid>
<content:encoded><![CDATA[
<div> : FaST, System 1/2 thinking, visual agents, cognitive intelligence, AI systems

:<br /><br />12FaSTFaST1/2FaSTFaST <div>
Achieving human-level intelligence requires refining cognitive distinctions
between System 1 and System 2 thinking. While contemporary AI, driven by large
language models, demonstrates human-like traits, it falls short of genuine
cognition. Transitioning from structured benchmarks to real-world scenarios
presents challenges for visual agents, often leading to inaccurate and overly
confident responses. To address the challenge, we introduce FaST, which
incorporates the Fast and Slow Thinking mechanism into visual agents. FaST
employs a switch adapter to dynamically select between System 1/2 modes,
tailoring the problem-solving approach to different task complexity. It tackles
uncertain and unseen objects by adjusting model confidence and integrating new
contextual data. With this novel design, we advocate a flexible system,
hierarchical reasoning capabilities, and a transparent decision-making
pipeline, all of which contribute to its ability to emulate human-like
cognitive processes in visual intelligence. Empirical results demonstrate that
FaST outperforms various well-known baselines, achieving 80.8% accuracy over
VQA^{v2} for visual question answering and 48.7% GIoU score over ReasonSeg for
reasoning segmentation, demonstrate FaST's superior performance. Extensive
testing validates the efficacy and robustness of FaST's core components,
showcasing its potential to advance the development of cognitive visual agents
in AI systems.
]]></content:encoded>
<pubDate>2024-08-16T17:44:02Z</pubDate>
</item>
<item>
<title>End-to-end Semantic-centric Video-based Multimodal Affective Computing</title>
<link>http://arxiv.org/abs/2408.07694v1</link>
<guid>http://arxiv.org/abs/2408.07694v1</guid>
<content:encoded><![CDATA[
<div> : ; ; ; ; <br />
<br />
SemanticMACTransformerSemanticMAC7 <br /><br />:SemanticMACTransformer <div>
In the pathway toward Artificial General Intelligence (AGI), understanding
human's affection is essential to enhance machine's cognition abilities. For
achieving more sensual human-AI interaction, Multimodal Affective Computing
(MAC) in human-spoken videos has attracted increasing attention. However,
previous methods are mainly devoted to designing multimodal fusion algorithms,
suffering from two issues: semantic imbalance caused by diverse pre-processing
operations and semantic mismatch raised by inconsistent affection content
contained in different modalities comparing with the multimodal ground truth.
Besides, the usage of manual features extractors make they fail in building
end-to-end pipeline for multiple MAC downstream tasks. To address above
challenges, we propose a novel end-to-end framework named SemanticMAC to
compute multimodal semantic-centric affection for human-spoken videos. We
firstly employ pre-trained Transformer model in multimodal data pre-processing
and design Affective Perceiver module to capture unimodal affective
information. Moreover, we present a semantic-centric approach to unify
multimodal representation learning in three ways, including gated feature
interaction, multi-task pseudo label generation, and intra-/inter-sample
contrastive learning. Finally, SemanticMAC effectively learn specific- and
shared-semantic representations in the guidance of semantic-centric labels.
Extensive experimental results demonstrate that our approach surpass the
state-of-the-art methods on 7 public datasets in four MAC downstream tasks.
]]></content:encoded>
<pubDate>2024-08-14T17:50:27Z</pubDate>
</item>
<item>
<title>Diversity Empowers Intelligence: Integrating Expertise of Software
  Engineering Agents</title>
<link>http://arxiv.org/abs/2408.07060v1</link>
<guid>http://arxiv.org/abs/2408.07060v1</guid>
<content:encoded><![CDATA[
<div> agent frameworks, SWE-Bench Lite, DEI, collaborative AI systems, software engineering challenges
<br /><br />:
(LLM)SWE-Bench Lite27%GitHubDEIDiversity Empowered IntelligenceDEISWESWE-Bench Lite27.3%DEI34.3%25%SWE-Bench Lite55% <div>
Large language model (LLM) agents have shown great potential in solving
real-world software engineering (SWE) problems. The most advanced open-source
SWE agent can resolve over 27% of real GitHub issues in SWE-Bench Lite.
However, these sophisticated agent frameworks exhibit varying strengths,
excelling in certain tasks while underperforming in others. To fully harness
the diversity of these agents, we propose DEI (Diversity Empowered
Intelligence), a framework that leverages their unique expertise. DEI functions
as a meta-module atop existing SWE agent frameworks, managing agent collectives
for enhanced problem-solving. Experimental results show that a DEI-guided
committee of agents is able to surpass the best individual agent's performance
by a large margin. For instance, a group of open-source SWE agents, with a
maximum individual resolve rate of 27.3% on SWE-Bench Lite, can achieve a 34.3%
resolve rate with DEI, making a 25% improvement and beating most closed-source
solutions. Our best-performing group excels with a 55% resolve rate, securing
the highest ranking on SWE-Bench Lite. Our findings contribute to the growing
body of research on collaborative AI systems and their potential to solve
complex software engineering challenges.
]]></content:encoded>
<pubDate>2024-08-13T17:50:28Z</pubDate>
</item>
<item>
<title>LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs</title>
<link>http://arxiv.org/abs/2408.07055v1</link>
<guid>http://arxiv.org/abs/2408.07055v1</guid>
<content:encoded><![CDATA[
<div> AgentWrite, LongWriter-6k, LongBench-Write, LLMs, DPO
<br /><br />
:
AgentWriteLLMs20,0006,000SFTLongWriter-6k2k32k10,000LongBench-WriteDPO9BLLM <div>
Current long context large language models (LLMs) can process inputs up to
100,000 tokens, yet struggle to generate outputs exceeding even a modest length
of 2,000 words. Through controlled experiments, we find that the model's
effective generation length is inherently bounded by the sample it has seen
during supervised fine-tuning (SFT). In other words, their output limitation is
due to the scarcity of long-output examples in existing SFT datasets. To
address this, we introduce AgentWrite, an agent-based pipeline that decomposes
ultra-long generation tasks into subtasks, enabling off-the-shelf LLMs to
generate coherent outputs exceeding 20,000 words. Leveraging AgentWrite, we
construct LongWriter-6k, a dataset containing 6,000 SFT data with output
lengths ranging from 2k to 32k words. By incorporating this dataset into model
training, we successfully scale the output length of existing models to over
10,000 words while maintaining output quality. We also develop LongBench-Write,
a comprehensive benchmark for evaluating ultra-long generation capabilities.
Our 9B parameter model, further improved through DPO, achieves state-of-the-art
performance on this benchmark, surpassing even much larger proprietary models.
In general, our work demonstrates that existing long context LLM already
possesses the potential for a larger output window--all you need is data with
extended output during model alignment to unlock this capability. Our code &
models are at: https://github.com/THUDM/LongWriter.
]]></content:encoded>
<pubDate>2024-08-13T17:46:12Z</pubDate>
</item>
<item>
<title>VisualAgentBench: Towards Large Multimodal Models as Visual Foundation
  Agents</title>
<link>http://arxiv.org/abs/2408.06327v1</link>
<guid>http://arxiv.org/abs/2408.06327v1</guid>
<content:encoded><![CDATA[
<div> VisualAgentBench<br />
LMMsLMMsVisualAgentBenchVABLMMsLMMsLMM APIVABLMMLMMs <div>
Large Multimodal Models (LMMs) have ushered in a new era in artificial
intelligence, merging capabilities in both language and vision to form highly
capable Visual Foundation Agents. These agents are postulated to excel across a
myriad of tasks, potentially approaching general artificial intelligence.
However, existing benchmarks fail to sufficiently challenge or showcase the
full potential of LMMs in complex, real-world environments. To address this
gap, we introduce VisualAgentBench (VAB), a comprehensive and pioneering
benchmark specifically designed to train and evaluate LMMs as visual foundation
agents across diverse scenarios, including Embodied, Graphical User Interface,
and Visual Design, with tasks formulated to probe the depth of LMMs'
understanding and interaction capabilities. Through rigorous testing across
nine proprietary LMM APIs and eight open models, we demonstrate the
considerable yet still developing agent capabilities of these models.
Additionally, VAB constructs a trajectory training set constructed through
hybrid methods including Program-based Solvers, LMM Agent Bootstrapping, and
Human Demonstrations, promoting substantial performance improvements in LMMs
through behavior cloning. Our work not only aims to benchmark existing models
but also provides a solid foundation for future development into visual
foundation agents. Code, train \& test data, and part of fine-tuned open LMMs
are available at \url{https://github.com/THUDM/VisualAgentBench}.
]]></content:encoded>
<pubDate>2024-08-12T17:44:17Z</pubDate>
</item>
<item>
<title>VITA: Towards Open-Source Interactive Omni Multimodal LLM</title>
<link>http://arxiv.org/abs/2408.05211v1</link>
<guid>http://arxiv.org/abs/2408.05211v1</guid>
<content:encoded><![CDATA[
<div> VITA

VITA MLLMMixtral 8x7BVITA VITA VITA VITA https://vita-home.github.io

<br /><br />
VITA VITA VITA VITA  <div>
The remarkable multimodal capabilities and interactive experience of GPT-4o
underscore their necessity in practical applications, yet open-source models
rarely excel in both areas. In this paper, we introduce VITA, the first-ever
open-source Multimodal Large Language Model (MLLM) adept at simultaneous
processing and analysis of Video, Image, Text, and Audio modalities, and
meanwhile has an advanced multimodal interactive experience. Starting from
Mixtral 8x7B as a language foundation, we expand its Chinese vocabulary
followed by bilingual instruction tuning. We further endow the language model
with visual and audio capabilities through two-stage multi-task learning of
multimodal alignment and instruction tuning. VITA demonstrates robust
foundational capabilities of multilingual, vision, and audio understanding, as
evidenced by its strong performance across a range of both unimodal and
multimodal benchmarks. Beyond foundational capabilities, we have made
considerable progress in enhancing the natural multimodal human-computer
interaction experience. To the best of our knowledge, we are the first to
exploit non-awakening interaction and audio interrupt in MLLM. VITA is the
first step for the open-source community to explore the seamless integration of
multimodal understanding and interaction. While there is still lots of work to
be done on VITA to get close to close-source counterparts, we hope that its
role as a pioneer can serve as a cornerstone for subsequent research. Project
Page: https://vita-home.github.io.
]]></content:encoded>
<pubDate>2024-08-09T17:59:49Z</pubDate>
</item>
<item>
<title>Cell Morphology-Guided Small Molecule Generation with GFlowNets</title>
<link>http://arxiv.org/abs/2408.05196v1</link>
<guid>http://arxiv.org/abs/2408.05196v1</guid>
<content:encoded><![CDATA[
<div> <br />
HCIGFlowNets Oracle  <br /><br />: <br />HCIGFlowNets <div>
High-content phenotypic screening, including high-content imaging (HCI), has
gained popularity in the last few years for its ability to characterize novel
therapeutics without prior knowledge of the protein target. When combined with
deep learning techniques to predict and represent molecular-phenotype
interactions, these advancements hold the potential to significantly accelerate
and enhance drug discovery applications. This work focuses on the novel task of
HCI-guided molecular design. Generative models for molecule design could be
guided by HCI data, for example with a supervised model that links molecules to
phenotypes of interest as a reward function. However, limited labeled data,
combined with the high-dimensional readouts, can make training these methods
challenging and impractical. We consider an alternative approach in which we
leverage an unsupervised multimodal joint embedding to define a latent
similarity as a reward for GFlowNets. The proposed model learns to generate new
molecules that could produce phenotypic effects similar to those of the given
image target, without relying on pre-annotated phenotypic labels. We
demonstrate that the proposed method generates molecules with high
morphological and structural similarity to the target, increasing the
likelihood of similar biological activity, as confirmed by an independent
oracle model.
]]></content:encoded>
<pubDate>2024-08-09T17:40:35Z</pubDate>
</item>
<item>
<title>AdapMTL: Adaptive Pruning Framework for Multitask Learning Model</title>
<link>http://arxiv.org/abs/2408.03913v1</link>
<guid>http://arxiv.org/abs/2408.03913v1</guid>
<content:encoded><![CDATA[
<div> AdapMTL<br />
AdapMTLAdapMTLAdapMTL <br /><br />:AdapMTL <div>
In the domain of multimedia and multimodal processing, the efficient handling
of diverse data streams such as images, video, and sensor data is paramount.
Model compression and multitask learning (MTL) are crucial in this field,
offering the potential to address the resource-intensive demands of processing
and interpreting multiple forms of media simultaneously. However, effectively
compressing a multitask model presents significant challenges due to the
complexities of balancing sparsity allocation and accuracy performance across
multiple tasks. To tackle these challenges, we propose AdapMTL, an adaptive
pruning framework for MTL models. AdapMTL leverages multiple learnable soft
thresholds independently assigned to the shared backbone and the task-specific
heads to capture the nuances in different components' sensitivity to pruning.
During training, it co-optimizes the soft thresholds and MTL model weights to
automatically determine the suitable sparsity level at each component to
achieve both high task accuracy and high overall sparsity. It further
incorporates an adaptive weighting mechanism that dynamically adjusts the
importance of task-specific losses based on each task's robustness to pruning.
We demonstrate the effectiveness of AdapMTL through comprehensive experiments
on popular multitask datasets, namely NYU-v2 and Tiny-Taskonomy, with different
architectures, showcasing superior performance compared to state-of-the-art
pruning methods.
]]></content:encoded>
<pubDate>2024-08-07T17:19:15Z</pubDate>
</item>
<item>
<title>LLaVA-OneVision: Easy Visual Task Transfer</title>
<link>http://arxiv.org/abs/2408.03326v1</link>
<guid>http://arxiv.org/abs/2408.03326v1</guid>
<content:encoded><![CDATA[
<div> LLaVA-OneVision, multimodal models, computer vision, transfer learning, video understanding<br />
LLaVA-OneVisionLMMsLLaVA-OneVision/<br /><br />: LLaVA-OneVisionLMMs/ <div>
We present LLaVA-OneVision, a family of open large multimodal models (LMMs)
developed by consolidating our insights into data, models, and visual
representations in the LLaVA-NeXT blog series. Our experimental results
demonstrate that LLaVA-OneVision is the first single model that can
simultaneously push the performance boundaries of open LMMs in three important
computer vision scenarios: single-image, multi-image, and video scenarios.
Importantly, the design of LLaVA-OneVision allows strong transfer learning
across different modalities/scenarios, yielding new emerging capabilities. In
particular, strong video understanding and cross-scenario capabilities are
demonstrated through task transfer from images to videos.
]]></content:encoded>
<pubDate>2024-08-06T17:59:44Z</pubDate>
</item>
<item>
<title>Scaling LLM Test-Time Compute Optimally can be More Effective than
  Scaling Model Parameters</title>
<link>http://arxiv.org/abs/2408.03314v1</link>
<guid>http://arxiv.org/abs/2408.03314v1</guid>
<content:encoded><![CDATA[
<div> LLMtest-time computationscalinginference methodscompute-optimal strategy
<br /><br />:
LLMN4FLOPs14 <div>
Enabling LLMs to improve their outputs by using more test-time computation is
a critical step towards building generally self-improving agents that can
operate on open-ended natural language. In this paper, we study the scaling of
inference-time computation in LLMs, with a focus on answering the question: if
an LLM is allowed to use a fixed but non-trivial amount of inference-time
compute, how much can it improve its performance on a challenging prompt?
Answering this question has implications not only on the achievable performance
of LLMs, but also on the future of LLM pretraining and how one should tradeoff
inference-time and pre-training compute. Despite its importance, little
research attempted to understand the scaling behaviors of various test-time
inference methods. Moreover, current work largely provides negative results for
a number of these strategies. In this work, we analyze two primary mechanisms
to scale test-time computation: (1) searching against dense, process-based
verifier reward models; and (2) updating the model's distribution over a
response adaptively, given the prompt at test time. We find that in both cases,
the effectiveness of different approaches to scaling test-time compute
critically varies depending on the difficulty of the prompt. This observation
motivates applying a "compute-optimal" scaling strategy, which acts to most
effectively allocate test-time compute adaptively per prompt. Using this
compute-optimal strategy, we can improve the efficiency of test-time compute
scaling by more than 4x compared to a best-of-N baseline. Additionally, in a
FLOPs-matched evaluation, we find that on problems where a smaller base model
attains somewhat non-trivial success rates, test-time compute can be used to
outperform a 14x larger model.
]]></content:encoded>
<pubDate>2024-08-06T17:35:05Z</pubDate>
</item>
<item>
<title>Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation
  with Multimodal Generative Pretraining</title>
<link>http://arxiv.org/abs/2408.02657v1</link>
<guid>http://arxiv.org/abs/2408.02657v1</guid>
<content:encoded><![CDATA[
<div> Lumina-mGPT, multimodal autoregressive models, vision and language tasks, photorealistic images, pretrained decoder-only transformer

:
Lumina-mGPTtransformertoken-tokenmGPTFP-SFTOmni-SFTLumina-mGPT- <div>
We present Lumina-mGPT, a family of multimodal autoregressive models capable
of various vision and language tasks, particularly excelling in generating
flexible photorealistic images from text descriptions. Unlike existing
autoregressive image generation approaches, Lumina-mGPT employs a pretrained
decoder-only transformer as a unified framework for modeling multimodal token
sequences. Our key insight is that a simple decoder-only transformer with
multimodal Generative PreTraining (mGPT), utilizing the next-token prediction
objective on massive interleaved text-image sequences, can learn broad and
general multimodal capabilities, thereby illuminating photorealistic
text-to-image generation. Building on these pretrained models, we propose
Flexible Progressive Supervised Finetuning (FP-SFT) on high-quality image-text
pairs to fully unlock their potential for high-aesthetic image synthesis at any
resolution while maintaining their general multimodal capabilities.
Furthermore, we introduce Ominiponent Supervised Finetuning (Omni-SFT),
transforming Lumina-mGPT into a foundation model that seamlessly achieves
omnipotent task unification. The resulting model demonstrates versatile
multimodal capabilities, including visual generation tasks like flexible
text-to-image generation and controllable generation, visual recognition tasks
like segmentation and depth estimation, and vision-language tasks like
multiturn visual question answering. Additionally, we analyze the differences
and similarities between diffusion-based and autoregressive methods in a direct
comparison.
]]></content:encoded>
<pubDate>2024-08-05T17:46:53Z</pubDate>
</item>
<item>
<title>Talk Less, Interact Better: Evaluating In-context Conversational
  Adaptation in Multimodal LLMs</title>
<link>http://arxiv.org/abs/2408.01417v1</link>
<guid>http://arxiv.org/abs/2408.01417v1</guid>
<content:encoded><![CDATA[
<div> ICCA

:<br /><br />ICCAICCAhttps://github.com/lil-lab/ICCA <div>
Humans spontaneously use increasingly efficient language as interactions
progress, by adapting and forming ad-hoc conventions. This phenomenon has been
studied extensively using reference games, showing properties of human language
that go beyond relaying intents. It remains unexplored whether multimodal large
language models (MLLMs) similarly increase communication efficiency during
interactions, and what mechanisms they may adopt for this purpose. We introduce
ICCA, an automated framework to evaluate such conversational adaptation as an
in-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs, and
observe that while they may understand the increasingly efficient language of
their interlocutor, they do not spontaneously make their own language more
efficient over time. This latter ability can only be elicited in some models
(e.g., GPT-4) with heavy-handed prompting. This shows that this property of
linguistic interaction does not arise from current training regimes, even
though it is a common hallmark of human language. ICCA is available at
https://github.com/lil-lab/ICCA.
]]></content:encoded>
<pubDate>2024-08-02T17:51:57Z</pubDate>
</item>
<item>
<title>MM-Vet v2: A Challenging Benchmark to Evaluate Large Multimodal Models
  for Integrated Capabilities</title>
<link>http://arxiv.org/abs/2408.00765v1</link>
<guid>http://arxiv.org/abs/2408.00765v1</guid>
<content:encoded><![CDATA[
<div> MM-Vet v2-Claude 3.5 SonnetGPT-4oInternVL2-Llama3-76B

<br /><br />
MM-Vet v2-MM-Vet v2Claude 3.5 SonnetGPT-4oInternVL2-Llama3-76B68.4 <div>
MM-Vet, with open-ended vision-language questions targeting at evaluating
integrated capabilities, has become one of the most popular benchmarks for
large multimodal model evaluation. MM-Vet assesses six core vision-language
(VL) capabilities: recognition, knowledge, spatial awareness, language
generation, OCR, and math. However, its question format is restricted to single
image-text pairs, lacking the interleaved image and text sequences prevalent in
real-world scenarios. To address this limitation, we introduce MM-Vet v2, which
includes a new VL capability called "image-text sequence understanding",
evaluating models' ability to process VL sequences. Furthermore, we maintain
the high quality of evaluation samples while further expanding the evaluation
set size. Using MM-Vet v2 to benchmark large multimodal models, we found that
Claude 3.5 Sonnet is the best model with a score of 71.8, slightly
outperforming GPT-4o which scored 71.0. Among open-weight models,
InternVL2-Llama3-76B leads with a score of 68.4.
]]></content:encoded>
<pubDate>2024-08-01T17:59:54Z</pubDate>
</item>
<item>
<title>AgentGen: Enhancing Planning Abilities for Large Language Model based
  Agent via Environment and Task Generation</title>
<link>http://arxiv.org/abs/2408.00764v1</link>
<guid>http://arxiv.org/abs/2408.00764v1</guid>
<content:encoded><![CDATA[
<div> agent, LLM, planning, environment, training
:<br /><br />LLMAgentGenLLMAgentGenLLMGPT-4 <div>
Large Language Model (LLM) based agents have garnered significant attention
and are becoming increasingly popular. Furthermore, planning ability is a
crucial component of an LLM-based agent, involving interaction with the
environment and executing actions to complete a planning task, which generally
entails achieving a desired goal from an initial state. This paper investigates
enhancing the planning abilities of LLMs through instruction tuning, referred
to as agent training. Recent studies have demonstrated that utilizing
expert-level trajectory for instruction-tuning LLMs effectively enhances their
planning capabilities. However, existing work primarily focuses on synthesizing
trajectories from manually designed planning tasks and environments. The
labor-intensive nature of creating these environments and tasks impedes the
generation of sufficiently varied and extensive trajectories. To address this
limitation, this paper explores the automated synthesis of diverse environments
and a gradual range of planning tasks, from easy to difficult. We introduce a
framework, AgentGen, that leverages LLMs first to generate environments and
subsequently generate planning tasks conditioned on these environments.
Specifically, to improve environmental diversity, we propose using an
inspiration corpus composed of various domain-specific text segments as the
context for synthesizing environments. Moreover, to increase the difficulty
diversity of generated planning tasks, we propose a bidirectional evolution
method, Bi-Evol, that evolves planning tasks from easier and harder directions
to synthesize a task set with a smoother difficulty curve. The evaluation
results derived from AgentBoard show that AgentGen greatly improves LLMs'
planning ability, e.g., the AgentGen instruction-tuned Llama-3 8B surpasses
GPT-3.5 in overall performance. Moreover, in certain tasks, it even outperforms
GPT-4.
]]></content:encoded>
<pubDate>2024-08-01T17:59:46Z</pubDate>
</item>
<item>
<title>Smoothed Energy Guidance: Guiding Diffusion Models with Reduced Energy
  Curvature of Attention</title>
<link>http://arxiv.org/abs/2408.00760v1</link>
<guid>http://arxiv.org/abs/2408.00760v1</guid>
<content:encoded><![CDATA[
Conditional diffusion models have shown remarkable success in visual content
generation, producing high-quality samples across various domains, largely due
to classifier-free guidance (CFG). Recent attempts to extend guidance to
unconditional models have relied on heuristic techniques, resulting in
suboptimal generation quality and unintended effects. In this work, we propose
Smoothed Energy Guidance (SEG), a novel training- and condition-free approach
that leverages the energy-based perspective of the self-attention mechanism to
enhance image generation. By defining the energy of self-attention, we
introduce a method to reduce the curvature of the energy landscape of attention
and use the output as the unconditional prediction. Practically, we control the
curvature of the energy landscape by adjusting the Gaussian kernel parameter
while keeping the guidance scale parameter fixed. Additionally, we present a
query blurring method that is equivalent to blurring the entire attention
weights without incurring quadratic complexity in the number of tokens. In our
experiments, SEG achieves a Pareto improvement in both quality and the
reduction of side effects. The code is available at
\url{https://github.com/SusungHong/SEG-SDXL}.
]]></content:encoded>
<pubDate>2024-08-01T17:59:09Z</pubDate>
</item>
<item>
<title>Coarse Correspondence Elicit 3D Spacetime Understanding in Multimodal
  Language Model</title>
<link>http://arxiv.org/abs/2408.00754v1</link>
<guid>http://arxiv.org/abs/2408.00754v1</guid>
<content:encoded><![CDATA[
Multimodal language models (MLLMs) are increasingly being implemented in
real-world environments, necessitating their ability to interpret 3D spaces and
comprehend temporal dynamics. Despite their potential, current top models
within our community still fall short in adequately understanding spatial and
temporal dimensions. We introduce Coarse Correspondence, a simple,
training-free, effective, and general-purpose visual prompting method to elicit
3D and temporal understanding in multimodal LLMs. Our method uses a lightweight
tracking model to find object correspondences between frames in a video or
between sets of image viewpoints. It selects the most frequent object instances
and visualizes them with markers with unique IDs in the image. With this simple
approach, we achieve state-of-the-art results on 3D understanding benchmarks
including ScanQA (+20.5\%) and a subset of OpenEQA (+9.7\%), and on long-form
video benchmarks such as EgoSchema (+6.0\%). We also curate a small diagnostic
dataset to evaluate whether MLLMs can reason about space from a described
viewpoint other than the camera viewpoint. Again, Coarse Correspondence
improves spatial perspective-taking abilities but we highlight that MLLMs
struggle with this task. Together, we demonstrate that our simple prompting
method can significantly aid downstream tasks that require 3D or temporal
reasoning.
]]></content:encoded>
<pubDate>2024-08-01T17:57:12Z</pubDate>
</item>
<item>
<title>Tulip Agent -- Enabling LLM-Based Agents to Solve Tasks Using Large Tool
  Libraries</title>
<link>http://arxiv.org/abs/2407.21778v1</link>
<guid>http://arxiv.org/abs/2407.21778v1</guid>
<content:encoded><![CDATA[
<div> agent, architecture, tool library, inference costs, generalizability
<br />
tulip agentLLMtulip agenttulip agenttulip agentGitHub <div>
We introduce tulip agent, an architecture for autonomous LLM-based agents
with Create, Read, Update, and Delete access to a tool library containing a
potentially large number of tools. In contrast to state-of-the-art
implementations, tulip agent does not encode the descriptions of all available
tools in the system prompt, which counts against the model's context window, or
embed the entire prompt for retrieving suitable tools. Instead, the tulip agent
can recursively search for suitable tools in its extensible tool library,
implemented exemplarily as a vector store. The tulip agent architecture
significantly reduces inference costs, allows using even large tool libraries,
and enables the agent to adapt and extend its set of tools. We evaluate the
architecture with several ablation studies in a mathematics context and
demonstrate its generalizability with an application to robotics. A reference
implementation and the benchmark are available at
github.com/HRI-EU/tulip_agent.
]]></content:encoded>
<pubDate>2024-07-31T17:50:54Z</pubDate>
</item>
<item>
<title>FlexAttention for Efficient High-Resolution Vision-Language Models</title>
<link>http://arxiv.org/abs/2407.20228v1</link>
<guid>http://arxiv.org/abs/2407.20228v1</guid>
<content:encoded><![CDATA[
<div> : -, FlexAttention, , , 
:
-FlexAttentionFlexAttention40% <div>
Current high-resolution vision-language models encode images as
high-resolution image tokens and exhaustively take all these tokens to compute
attention, which significantly increases the computational cost. To address
this problem, we propose FlexAttention, a flexible attention mechanism for
efficient high-resolution vision-language models. Specifically, a
high-resolution image is encoded both as high-resolution tokens and
low-resolution tokens, where only the low-resolution tokens and a few selected
high-resolution tokens are utilized to calculate the attention map, which
greatly shrinks the computational cost. The high-resolution tokens are selected
via a high-resolution selection module which could retrieve tokens of relevant
regions based on an input attention map. The selected high-resolution tokens
are then concatenated to the low-resolution tokens and text tokens, and input
to a hierarchical self-attention layer which produces an attention map that
could be used for the next-step high-resolution token selection. The
hierarchical self-attention process and high-resolution token selection process
are performed iteratively for each attention layer. Experiments on multimodal
benchmarks prove that our FlexAttention outperforms existing high-resolution
VLMs (e.g., relatively ~9% in V* Bench, ~7% in TextVQA), while also
significantly reducing the computational cost by nearly 40%.
]]></content:encoded>
<pubDate>2024-07-29T17:59:05Z</pubDate>
</item>
<item>
<title>SOAP-RL: Sequential Option Advantage Propagation for Reinforcement
  Learning in POMDP Environments</title>
<link>http://arxiv.org/abs/2407.18913v1</link>
<guid>http://arxiv.org/abs/2407.18913v1</guid>
<content:encoded><![CDATA[
<div> POMDPs, options, reinforcement learning, SOAP, policy gradient<br />
POMDPsSOAP<br /><br />: POMDPPPOEMSOAPPPOEM-SOAPGAESOAPPOMDPPPOEMLSTMOption-Critic <div>
This work compares ways of extending Reinforcement Learning algorithms to
Partially Observed Markov Decision Processes (POMDPs) with options. One view of
options is as temporally extended action, which can be realized as a memory
that allows the agent to retain historical information beyond the policy's
context window. While option assignment could be handled using heuristics and
hand-crafted objectives, learning temporally consistent options and associated
sub-policies without explicit supervision is a challenge. Two algorithms, PPOEM
and SOAP, are proposed and studied in depth to address this problem. PPOEM
applies the forward-backward algorithm (for Hidden Markov Models) to optimize
the expected returns for an option-augmented policy. However, this learning
approach is unstable during on-policy rollouts. It is also unsuited for
learning causal policies without the knowledge of future trajectories, since
option assignments are optimized for offline sequences where the entire episode
is available. As an alternative approach, SOAP evaluates the policy gradient
for an optimal option assignment. It extends the concept of the generalized
advantage estimation (GAE) to propagate option advantages through time, which
is an analytical equivalent to performing temporal back-propagation of option
policy gradients. This option policy is only conditional on the history of the
agent, not future actions. Evaluated against competing baselines, SOAP
exhibited the most robust performance, correctly discovering options for POMDP
corridor environments, as well as on standard benchmarks including Atari and
MuJoCo, outperforming PPOEM, as well as LSTM and Option-Critic baselines. The
open-sourced code is available at https://github.com/shuishida/SoapRL.
]]></content:encoded>
<pubDate>2024-07-26T17:59:55Z</pubDate>
</item>
<item>
<title>HRP: Human Affordances for Robotic Pre-Training</title>
<link>http://arxiv.org/abs/2407.18911v1</link>
<guid>http://arxiv.org/abs/2407.18911v1</guid>
<content:encoded><![CDATA[
<div> : <br />
<br />
"affordances"affordances5 <br /><br />: <br />"affordances" <div>
In order to *generalize* to various tasks in the wild, robotic agents will
need a suitable representation (i.e., vision network) that enables the robot to
predict optimal actions given high dimensional vision inputs. However, learning
such a representation requires an extreme amount of diverse training data,
which is prohibitively expensive to collect on a real robot. How can we
overcome this problem? Instead of collecting more robot data, this paper
proposes using internet-scale, human videos to extract "affordances," both at
the environment and agent level, and distill them into a pre-trained
representation. We present a simple framework for pre-training representations
on hand, object, and contact "affordance labels" that highlight relevant
objects in images and how to interact with them. These affordances are
automatically extracted from human video data (with the help of off-the-shelf
computer vision modules) and used to fine-tune existing representations. Our
approach can efficiently fine-tune *any* existing representation, and results
in models with stronger downstream robotic performance across the board. We
experimentally demonstrate (using 3000+ robot trials) that this affordance
pre-training scheme boosts performance by a minimum of 15% on 5 real-world
tasks, which consider three diverse robot morphologies (including a dexterous
hand). Unlike prior works in the space, these representations improve
performance across 3 different camera views. Quantitatively, we find that our
approach leads to higher levels of generalization in out-of-distribution
settings. For code, weights, and data check: https://hrp-robot.github.io
]]></content:encoded>
<pubDate>2024-07-26T17:59:52Z</pubDate>
</item>
<item>
<title>Sparse vs Contiguous Adversarial Pixel Perturbations in Multimodal
  Models: An Empirical Analysis</title>
<link>http://arxiv.org/abs/2407.18251v1</link>
<guid>http://arxiv.org/abs/2407.18251v1</guid>
<content:encoded><![CDATA[
<div> multimodal models, adversarial examples, L0-norm perturbation attacks, robustness, unimodal DNNs<br />
<br />
L0DNN0.04%DNNCNNViT0.02%99% <br /><br />: DNNCNN0.02%99% <div>
Assessing the robustness of multimodal models against adversarial examples is
an important aspect for the safety of its users. We craft L0-norm perturbation
attacks on the preprocessed input images. We launch them in a black-box setup
against four multimodal models and two unimodal DNNs, considering both targeted
and untargeted misclassification. Our attacks target less than 0.04% of
perturbed image area and integrate different spatial positioning of perturbed
pixels: sparse positioning and pixels arranged in different contiguous shapes
(row, column, diagonal, and patch). To the best of our knowledge, we are the
first to assess the robustness of three state-of-the-art multimodal models
(ALIGN, AltCLIP, GroupViT) against different sparse and contiguous pixel
distribution perturbations. The obtained results indicate that unimodal DNNs
are more robust than multimodal models. Furthermore, models using CNN-based
Image Encoder are more vulnerable than models with ViT - for untargeted
attacks, we obtain a 99% success rate by perturbing less than 0.02% of the
image area.
]]></content:encoded>
<pubDate>2024-07-25T17:59:48Z</pubDate>
</item>
<item>
<title>SV4D: Dynamic 3D Content Generation with Multi-Frame and Multi-View
  Consistency</title>
<link>http://arxiv.org/abs/2407.17470v1</link>
<guid>http://arxiv.org/abs/2407.17470v1</guid>
<content:encoded><![CDATA[
<div> 3D<br />
<br />
Stable Video 4DSV4D3D3DSV4D4DNeRFSDSObjaverse3DSV4D4D <br /><br />: SV4D3D4D4D <div>
We present Stable Video 4D (SV4D), a latent video diffusion model for
multi-frame and multi-view consistent dynamic 3D content generation. Unlike
previous methods that rely on separately trained generative models for video
generation and novel view synthesis, we design a unified diffusion model to
generate novel view videos of dynamic 3D objects. Specifically, given a
monocular reference video, SV4D generates novel views for each video frame that
are temporally consistent. We then use the generated novel view videos to
optimize an implicit 4D representation (dynamic NeRF) efficiently, without the
need for cumbersome SDS-based optimization used in most prior works. To train
our unified novel view video generation model, we curated a dynamic 3D object
dataset from the existing Objaverse dataset. Extensive experimental results on
multiple datasets and user studies demonstrate SV4D's state-of-the-art
performance on novel-view video synthesis as well as 4D generation compared to
prior works.
]]></content:encoded>
<pubDate>2024-07-24T17:59:43Z</pubDate>
</item>
<item>
<title>Toward human-centered shared autonomy AI paradigms for human-robot
  teaming in healthcare</title>
<link>http://arxiv.org/abs/2407.17464v1</link>
<guid>http://arxiv.org/abs/2407.17464v1</guid>
<content:encoded><![CDATA[
<div> , , , , <br />
:  <div>
With recent advancements in AI and computation tools, intelligent paradigms
emerged to empower different fields such as healthcare robots with new
capabilities. Advanced AI robotic algorithms (e.g., reinforcement learning) can
be trained and developed to autonomously make individual decisions to achieve a
desired and usually fixed goal. However, such independent decisions and goal
achievements might not be ideal for a healthcare robot that usually interacts
with a dynamic end-user or a patient. In such a complex human-robot interaction
(teaming) framework, the dynamic user continuously wants to be involved in
decision-making as well as introducing new goals while interacting with their
present environment in real-time. To address this challenge, an adaptive shared
autonomy AI paradigm is required to be developed for the two interactive agents
(Human & AI agents) with a foundation based on human-centered factors to avoid
any possible ethical issues and guarantee no harm to humanity.
]]></content:encoded>
<pubDate>2024-07-24T17:58:41Z</pubDate>
</item>
<item>
<title>SoNIC: Safe Social Navigation with Adaptive Conformal Inference and
  Constrained Reinforcement Learning</title>
<link>http://arxiv.org/abs/2407.17460v1</link>
<guid>http://arxiv.org/abs/2407.17460v1</guid>
<content:encoded><![CDATA[
Reinforcement Learning (RL) has enabled social robots to generate
trajectories without human-designed rules or interventions, which makes it more
effective than hard-coded systems for generalizing to complex real-world
scenarios. However, social navigation is a safety-critical task that requires
robots to avoid collisions with pedestrians while previous RL-based solutions
fall short in safety performance in complex environments. To enhance the safety
of RL policies, to the best of our knowledge, we propose the first algorithm,
SoNIC, that integrates adaptive conformal inference (ACI) with constrained
reinforcement learning (CRL) to learn safe policies for social navigation. More
specifically, our method augments RL observations with ACI-generated
nonconformity scores and provides explicit guidance for agents to leverage the
uncertainty metrics to avoid safety-critical areas by incorporating safety
constraints with spatial relaxation. Our method outperforms state-of-the-art
baselines in terms of both safety and adherence to social norms by a large
margin and demonstrates much stronger robustness to out-of-distribution
scenarios. Our code and video demos are available on our project website:
https://sonic-social-nav.github.io/.
]]></content:encoded>
<pubDate>2024-07-24T17:57:21Z</pubDate>
</item>
<item>
<title>Can Large Language Models Automatically Jailbreak GPT-4V?</title>
<link>http://arxiv.org/abs/2407.16686v1</link>
<guid>http://arxiv.org/abs/2407.16686v1</guid>
<content:encoded><![CDATA[
<div> , , , , <br />
<br />AutoJailbreak(LLMs)AutoJailbreak(ASR)95.3\%GPT-4VGPT-4V <br /><br />: GPT-4VAutoJailbreakGPT-4V <div>
GPT-4V has attracted considerable attention due to its extraordinary capacity
for integrating and processing multimodal information. At the same time, its
ability of face recognition raises new safety concerns of privacy leakage.
Despite researchers' efforts in safety alignment through RLHF or preprocessing
filters, vulnerabilities might still be exploited. In our study, we introduce
AutoJailbreak, an innovative automatic jailbreak technique inspired by prompt
optimization. We leverage Large Language Models (LLMs) for red-teaming to
refine the jailbreak prompt and employ weak-to-strong in-context learning
prompts to boost efficiency. Furthermore, we present an effective search method
that incorporates early stopping to minimize optimization time and token
expenditure. Our experiments demonstrate that AutoJailbreak significantly
surpasses conventional methods, achieving an Attack Success Rate (ASR)
exceeding 95.3\%. This research sheds light on strengthening GPT-4V security,
underscoring the potential for LLMs to be exploited in compromising GPT-4V
integrity.
]]></content:encoded>
<pubDate>2024-07-23T17:50:45Z</pubDate>
</item>
<item>
<title>T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video
  Generation</title>
<link>http://arxiv.org/abs/2407.14505v1</link>
<guid>http://arxiv.org/abs/2407.14505v1</guid>
<content:encoded><![CDATA[
<div> :
Text-to-video (T2V) generation, compositional, benchmark, metrics, models

:
T2V-CompBench <br /><br />:T2V-CompBench <div>
Text-to-video (T2V) generation models have advanced significantly, yet their
ability to compose different objects, attributes, actions, and motions into a
video remains unexplored. Previous text-to-video benchmarks also neglect this
important ability for evaluation. In this work, we conduct the first systematic
study on compositional text-to-video generation. We propose T2V-CompBench, the
first benchmark tailored for compositional text-to-video generation.
T2V-CompBench encompasses diverse aspects of compositionality, including
consistent attribute binding, dynamic attribute binding, spatial relationships,
motion binding, action binding, object interactions, and generative numeracy.
We further carefully design evaluation metrics of MLLM-based metrics,
detection-based metrics, and tracking-based metrics, which can better reflect
the compositional text-to-video generation quality of seven proposed categories
with 700 text prompts. The effectiveness of the proposed metrics is verified by
correlation with human evaluations. We also benchmark various text-to-video
generative models and conduct in-depth analysis across different models and
different compositional categories. We find that compositional text-to-video
generation is highly challenging for current models, and we hope that our
attempt will shed light on future research in this direction.
]]></content:encoded>
<pubDate>2024-07-19T17:58:36Z</pubDate>
</item>
<item>
<title>Streetscapes: Large-scale Consistent Street View Generation Using
  Autoregressive Video Diffusion</title>
<link>http://arxiv.org/abs/2407.13759v1</link>
<guid>http://arxiv.org/abs/2407.13759v1</guid>
<content:encoded><![CDATA[
<div> : Streetscapes, , , /, <br />
:<br />
/Streetscapes3DGoogle Street ViewStreetscapeshttps://boyangdeng.com/streetscapes<br /> <div>
We present a method for generating Streetscapes-long sequences of views
through an on-the-fly synthesized city-scale scene. Our generation is
conditioned by language input (e.g., city name, weather), as well as an
underlying map/layout hosting the desired trajectory. Compared to recent models
for video generation or 3D view synthesis, our method can scale to much
longer-range camera trajectories, spanning several city blocks, while
maintaining visual quality and consistency. To achieve this goal, we build on
recent work on video diffusion, used within an autoregressive framework that
can easily scale to long sequences. In particular, we introduce a new temporal
imputation method that prevents our autoregressive approach from drifting from
the distribution of realistic city imagery. We train our Streetscapes system on
a compelling source of data-posed imagery from Google Street View, along with
contextual map data-which allows users to generate city views conditioned on
any desired city layout, with controllable camera poses. Please see more
results at our project page at https://boyangdeng.com/streetscapes.
]]></content:encoded>
<pubDate>2024-07-18T17:56:30Z</pubDate>
</item>
<item>
<title>AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge
  Bases</title>
<link>http://arxiv.org/abs/2407.12784v1</link>
<guid>http://arxiv.org/abs/2407.12784v1</guid>
<content:encoded><![CDATA[
<div> :
LLM agents, AgentPoison, backdoor attack, memory, RAG mechanism

:
LLM agentsAgentPoisonRAGLLM agentsAgentPoisonAgentPoisonLLM agentsagentAgentPoison80%(1%)0.1% <br /><br /> <div>
LLM agents have demonstrated remarkable performance across various
applications, primarily due to their advanced capabilities in reasoning,
utilizing external knowledge and tools, calling APIs, and executing actions to
interact with environments. Current agents typically utilize a memory module or
a retrieval-augmented generation (RAG) mechanism, retrieving past knowledge and
instances with similar embeddings from knowledge bases to inform task planning
and execution. However, the reliance on unverified knowledge bases raises
significant concerns about their safety and trustworthiness. To uncover such
vulnerabilities, we propose a novel red teaming approach AgentPoison, the first
backdoor attack targeting generic and RAG-based LLM agents by poisoning their
long-term memory or RAG knowledge base. In particular, we form the trigger
generation process as a constrained optimization to optimize backdoor triggers
by mapping the triggered instances to a unique embedding space, so as to ensure
that whenever a user instruction contains the optimized backdoor trigger, the
malicious demonstrations are retrieved from the poisoned memory or knowledge
base with high probability. In the meantime, benign instructions without the
trigger will still maintain normal performance. Unlike conventional backdoor
attacks, AgentPoison requires no additional model training or fine-tuning, and
the optimized backdoor trigger exhibits superior transferability, in-context
coherence, and stealthiness. Extensive experiments demonstrate AgentPoison's
effectiveness in attacking three types of real-world LLM agents: RAG-based
autonomous driving agent, knowledge-intensive QA agent, and healthcare
EHRAgent. On each agent, AgentPoison achieves an average attack success rate
higher than 80% with minimal impact on benign performance (less than 1%) with a
poison rate less than 0.1%.
]]></content:encoded>
<pubDate>2024-07-17T17:59:47Z</pubDate>
</item>
<item>
<title>VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control</title>
<link>http://arxiv.org/abs/2407.12781v1</link>
<guid>http://arxiv.org/abs/2407.12781v1</guid>
<content:encoded><![CDATA[
<div> Transformer-based video synthesis, camera control, U-Net-based diffusion models, spatial and temporal generation, ControlNet-like conditioning mechanism<br />
<br />:<br />3DU-NettransformerControlNetPlucker3DtransformerRealEstate10Ktransformer <div>
Modern text-to-video synthesis models demonstrate coherent, photorealistic
generation of complex videos from a text description. However, most existing
models lack fine-grained control over camera movement, which is critical for
downstream applications related to content creation, visual effects, and 3D
vision. Recently, new methods demonstrate the ability to generate videos with
controllable camera poses these techniques leverage pre-trained U-Net-based
diffusion models that explicitly disentangle spatial and temporal generation.
Still, no existing approach enables camera control for new, transformer-based
video diffusion models that process spatial and temporal information jointly.
Here, we propose to tame video transformers for 3D camera control using a
ControlNet-like conditioning mechanism that incorporates spatiotemporal camera
embeddings based on Plucker coordinates. The approach demonstrates
state-of-the-art performance for controllable video generation after
fine-tuning on the RealEstate10K dataset. To the best of our knowledge, our
work is the first to enable camera control for transformer-based video
diffusion models.
]]></content:encoded>
<pubDate>2024-07-17T17:59:05Z</pubDate>
</item>
<item>
<title>LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models</title>
<link>http://arxiv.org/abs/2407.12772v1</link>
<guid>http://arxiv.org/abs/2407.12772v1</guid>
<content:encoded><![CDATA[
The advances of large foundation models necessitate wide-coverage, low-cost,
and zero-contamination benchmarks. Despite continuous exploration of language
model evaluations, comprehensive studies on the evaluation of Large Multi-modal
Models (LMMs) remain limited. In this work, we introduce LMMS-EVAL, a unified
and standardized multimodal benchmark framework with over 50 tasks and more
than 10 models to promote transparent and reproducible evaluations. Although
LMMS-EVAL offers comprehensive coverage, we find it still falls short in
achieving low cost and zero contamination. To approach this evaluation
trilemma, we further introduce LMMS-EVAL LITE, a pruned evaluation toolkit that
emphasizes both coverage and efficiency. Additionally, we present Multimodal
LIVEBENCH that utilizes continuously updating news and online forums to assess
models' generalization abilities in the wild, featuring a low-cost and
zero-contamination evaluation approach. In summary, our work highlights the
importance of considering the evaluation trilemma and provides practical
solutions to navigate the trade-offs in evaluating large multi-modal models,
paving the way for more effective and reliable benchmarking of LMMs. We
opensource our codebase and maintain leaderboard of LIVEBENCH at
https://github.com/EvolvingLMMs-Lab/lmms-eval and
https://huggingface.co/spaces/lmms-lab/LiveBench.
]]></content:encoded>
<pubDate>2024-07-17T17:51:53Z</pubDate>
</item>
<item>
<title>Does Refusal Training in LLMs Generalize to the Past Tense?</title>
<link>http://arxiv.org/abs/2407.11969v1</link>
<guid>http://arxiv.org/abs/2407.11969v1</guid>
<content:encoded><![CDATA[
<div> , , , , 

(LLMs)()LLMs <div>
Refusal training is widely used to prevent LLMs from generating harmful,
undesirable, or illegal outputs. We reveal a curious generalization gap in the
current refusal training approaches: simply reformulating a harmful request in
the past tense (e.g., "How to make a Molotov cocktail?" to "How did people make
a Molotov cocktail?") is often sufficient to jailbreak many state-of-the-art
LLMs. We systematically evaluate this method on Llama-3 8B, GPT-3.5 Turbo,
Gemma-2 9B, Phi-3-Mini, GPT-4o, and R2D2 models using GPT-3.5 Turbo as a
reformulation model. For example, the success rate of this simple attack on
GPT-4o increases from 1% using direct requests to 88% using 20 past tense
reformulation attempts on harmful requests from JailbreakBench with GPT-4 as a
jailbreak judge. Interestingly, we also find that reformulations in the future
tense are less effective, suggesting that refusal guardrails tend to consider
past historical questions more benign than hypothetical future questions.
Moreover, our experiments on fine-tuning GPT-3.5 Turbo show that defending
against past reformulations is feasible when past tense examples are explicitly
included in the fine-tuning data. Overall, our findings highlight that the
widely used alignment techniques -- such as SFT, RLHF, and adversarial training
-- employed to align the studied models can be brittle and do not always
generalize as intended. We provide code and jailbreak artifacts at
https://github.com/tml-epfl/llm-past-tense.
]]></content:encoded>
<pubDate>2024-07-16T17:59:55Z</pubDate>
</item>
<item>
<title>Efficient Training with Denoised Neural Weights</title>
<link>http://arxiv.org/abs/2407.11966v1</link>
<guid>http://arxiv.org/abs/2407.11966v1</guid>
<content:encoded><![CDATA[
<div> 

:<br /><br />43.3 <div>
Good weight initialization serves as an effective measure to reduce the
training cost of a deep neural network (DNN) model. The choice of how to
initialize parameters is challenging and may require manual tuning, which can
be time-consuming and prone to human error. To overcome such limitations, this
work takes a novel step towards building a weight generator to synthesize the
neural weights for initialization. We use the image-to-image translation task
with generative adversarial networks (GANs) as an example due to the ease of
collecting model weights spanning a wide range. Specifically, we first collect
a dataset with various image editing concepts and their corresponding trained
weights, which are later used for the training of the weight generator. To
address the different characteristics among layers and the substantial number
of weights to be predicted, we divide the weights into equal-sized blocks and
assign each block an index. Subsequently, a diffusion model is trained with
such a dataset using both text conditions of the concept and the block indexes.
By initializing the image translation model with the denoised weights predicted
by our diffusion model, the training requires only 43.3 seconds. Compared to
training from scratch (i.e., Pix2pix), we achieve a 15x training time
acceleration for a new concept while obtaining even better image generation
quality.
]]></content:encoded>
<pubDate>2024-07-16T17:59:42Z</pubDate>
</item>
<item>
<title>UrbanWorld: An Urban World Model for 3D City Generation</title>
<link>http://arxiv.org/abs/2407.11965v1</link>
<guid>http://arxiv.org/abs/2407.11965v1</guid>
<content:encoded><![CDATA[
Cities, as the most fundamental environment of human life, encompass diverse
physical elements such as buildings, roads and vegetation with complex
interconnection. Crafting realistic, interactive 3D urban environments plays a
crucial role in constructing AI agents capable of perceiving, decision-making,
and acting like humans in real-world environments. However, creating
high-fidelity 3D urban environments usually entails extensive manual labor from
designers, involving intricate detailing and accurate representation of complex
urban features. Therefore, how to accomplish this in an automatical way remains
a longstanding challenge. Toward this problem, we propose UrbanWorld, the first
generative urban world model that can automatically create a customized,
realistic and interactive 3D urban world with flexible control conditions.
UrbanWorld incorporates four key stages in the automatical crafting pipeline:
3D layout generation from openly accessible OSM data, urban scene planning and
designing with a powerful urban multimodal large language model (Urban MLLM),
controllable urban asset rendering with advanced 3D diffusion techniques, and
finally the MLLM-assisted scene refinement. The crafted high-fidelity 3D urban
environments enable realistic feedback and interactions for general AI and
machine perceptual systems in simulations. We are working on contributing
UrbanWorld as an open-source and versatile platform for evaluating and
improving AI abilities in perception, decision-making, and interaction in
realistic urban environments.
]]></content:encoded>
<pubDate>2024-07-16T17:59:29Z</pubDate>
</item>
<item>
<title>Make-An-Agent: A Generalizable Policy Network Generator with
  Behavior-Prompted Diffusion</title>
<link>http://arxiv.org/abs/2407.10973v1</link>
<guid>http://arxiv.org/abs/2407.10973v1</guid>
<content:encoded><![CDATA[
<div> <br />
Make-An-AgentMake-An-Agent <br /><br />: Make-An-AgentOutOfRange context <div>
Can we generate a control policy for an agent using just one demonstration of
desired behaviors as a prompt, as effortlessly as creating an image from a
textual description? In this paper, we present Make-An-Agent, a novel policy
parameter generator that leverages the power of conditional diffusion models
for behavior-to-policy generation. Guided by behavior embeddings that encode
trajectory information, our policy generator synthesizes latent parameter
representations, which can then be decoded into policy networks. Trained on
policy network checkpoints and their corresponding trajectories, our generation
model demonstrates remarkable versatility and scalability on multiple tasks and
has a strong generalization ability on unseen tasks to output well-performed
policies with only few-shot demonstrations as inputs. We showcase its efficacy
and efficiency on various domains and tasks, including varying objectives,
behaviors, and even across different robot manipulators. Beyond simulation, we
directly deploy policies generated by Make-An-Agent onto real-world robots on
locomotion tasks.
]]></content:encoded>
<pubDate>2024-07-15T17:59:57Z</pubDate>
</item>
<item>
<title>Ref-AVS: Refer and Segment Objects in Audio-Visual Scenes</title>
<link>http://arxiv.org/abs/2407.10957v1</link>
<guid>http://arxiv.org/abs/2407.10957v1</guid>
<content:encoded><![CDATA[
<div> Reference Audio-Visual Segmentation, multimodal perception, natural language, benchmark, segmentation guidance
<br />
Ref-AVSRef-AVSRef-AVS<br /><br />: Ref-AVSRef-AVS <div>
Traditional reference segmentation tasks have predominantly focused on silent
visual scenes, neglecting the integral role of multimodal perception and
interaction in human experiences. In this work, we introduce a novel task
called Reference Audio-Visual Segmentation (Ref-AVS), which seeks to segment
objects within the visual domain based on expressions containing multimodal
cues. Such expressions are articulated in natural language forms but are
enriched with multimodal cues, including audio and visual descriptions. To
facilitate this research, we construct the first Ref-AVS benchmark, which
provides pixel-level annotations for objects described in corresponding
multimodal-cue expressions. To tackle the Ref-AVS task, we propose a new method
that adequately utilizes multimodal cues to offer precise segmentation
guidance. Finally, we conduct quantitative and qualitative experiments on three
test subsets to compare our approach with existing methods from related tasks.
The results demonstrate the effectiveness of our method, highlighting its
capability to precisely segment objects using multimodal-cue expressions.
Dataset is available at
\href{https://gewu-lab.github.io/Ref-AVS}{https://gewu-lab.github.io/Ref-AVS}.
]]></content:encoded>
<pubDate>2024-07-15T17:54:45Z</pubDate>
</item>
<item>
<title>FairyLandAI: Personalized Fairy Tales utilizing ChatGPT and DALLE-3</title>
<link>http://arxiv.org/abs/2407.09467v1</link>
<guid>http://arxiv.org/abs/2407.09467v1</guid>
<content:encoded><![CDATA[
<div> : FairyLandAI, OpenAI's API, , , 

: 
FairyLandAIOpenAIAPILLMFairyLandAIFairyLandAIFairyLandAIFairyLandAILLMOpenAIAPI <div>
In the diverse world of AI-driven storytelling, there is a unique opportunity
to engage young audiences with customized, and personalized narratives. This
paper introduces FairyLandAI an innovative Large Language Model (LLM) developed
through OpenAI's API, specifically crafted to create personalized fairytales
for children. The distinctive feature of FairyLandAI is its dual capability: it
not only generates stories that are engaging, age-appropriate, and reflective
of various traditions but also autonomously produces imaginative prompts
suitable for advanced image generation tools like GenAI and Dalle-3, thereby
enriching the storytelling experience. FairyLandAI is expertly tailored to
resonate with the imaginative worlds of children, providing narratives that are
both educational and entertaining and in alignment with the moral values
inherent in different ages. Its unique strength lies in customizing stories to
match individual children's preferences and cultural backgrounds, heralding a
new era in personalized storytelling. Further, its integration with image
generation technology offers a comprehensive narrative experience that
stimulates both verbal and visual creativity. Empirical evaluations of
FairyLandAI demonstrate its effectiveness in crafting captivating stories for
children, which not only entertain but also embody the values and teachings of
diverse traditions. This model serves as an invaluable tool for parents and
educators, supporting them in imparting meaningful moral lessons through
engaging narratives. FairyLandAI represents a pioneering step in using LLMs,
particularly through OpenAI's API, for educational and cultural enrichment,
making complex moral narratives accessible and enjoyable for young, imaginative
minds.
]]></content:encoded>
<pubDate>2024-07-12T17:46:58Z</pubDate>
</item>
<item>
<title>Real-Time Anomaly Detection and Reactive Planning with Large Language
  Models</title>
<link>http://arxiv.org/abs/2407.08735v1</link>
<guid>http://arxiv.org/abs/2407.08735v1</guid>
<content:encoded><![CDATA[
<div> LLM<br />
<br />
LLMLLM <div>
Foundation models, e.g., large language models (LLMs), trained on
internet-scale data possess zero-shot generalization capabilities that make
them a promising technology towards detecting and mitigating
out-of-distribution failure modes of robotic systems. Fully realizing this
promise, however, poses two challenges: (i) mitigating the considerable
computational expense of these models such that they may be applied online, and
(ii) incorporating their judgement regarding potential anomalies into a safe
control framework. In this work, we present a two-stage reasoning framework:
First is a fast binary anomaly classifier that analyzes observations in an LLM
embedding space, which may then trigger a slower fallback selection stage that
utilizes the reasoning capabilities of generative LLMs. These stages correspond
to branch points in a model predictive control strategy that maintains the
joint feasibility of continuing along various fallback plans to account for the
slow reasoner's latency as soon as an anomaly is detected, thus ensuring
safety. We show that our fast anomaly classifier outperforms autoregressive
reasoning with state-of-the-art GPT models, even when instantiated with
relatively small language models. This enables our runtime monitor to improve
the trustworthiness of dynamic robotic systems, such as quadrotors or
autonomous vehicles, under resource and time constraints. Videos illustrating
our approach in both simulation and real-world experiments are available on
this project page: https://sites.google.com/view/aesop-llm.
]]></content:encoded>
<pubDate>2024-07-11T17:59:22Z</pubDate>
</item>
<item>
<title>MetaUrban: A Simulation Platform for Embodied AI in Urban Spaces</title>
<link>http://arxiv.org/abs/2407.08725v1</link>
<guid>http://arxiv.org/abs/2407.08725v1</guid>
<content:encoded><![CDATA[
<div> Urban spaces, Robotics, Embodied AI, MetaUrban, Simulation<br />
<br />
MetaUrbanMetaUrban MetaUrban <br /><br />: Urban spaces, Robotics, Embodied AI, MetaUrban, Simulation <div>
Public urban spaces like streetscapes and plazas serve residents and
accommodate social life in all its vibrant variations. Recent advances in
Robotics and Embodied AI make public urban spaces no longer exclusive to
humans. Food delivery bots and electric wheelchairs have started sharing
sidewalks with pedestrians, while diverse robot dogs and humanoids have
recently emerged in the street. Ensuring the generalizability and safety of
these forthcoming mobile machines is crucial when navigating through the
bustling streets in urban spaces. In this work, we present MetaUrban, a
compositional simulation platform for Embodied AI research in urban spaces.
MetaUrban can construct an infinite number of interactive urban scenes from
compositional elements, covering a vast array of ground plans, object
placements, pedestrians, vulnerable road users, and other mobile agents'
appearances and dynamics. We design point navigation and social navigation
tasks as the pilot study using MetaUrban for embodied AI research and establish
various baselines of Reinforcement Learning and Imitation Learning. Experiments
demonstrate that the compositional nature of the simulated environments can
substantially improve the generalizability and safety of the trained mobile
agents. MetaUrban will be made publicly available to provide more research
opportunities and foster safe and trustworthy embodied AI in urban spaces.
]]></content:encoded>
<pubDate>2024-07-11T17:56:49Z</pubDate>
</item>
<item>
<title>Generative Image as Action Models</title>
<link>http://arxiv.org/abs/2407.07875v1</link>
<guid>http://arxiv.org/abs/2407.07875v1</guid>
<content:encoded><![CDATA[
<div> image-generation, visuomotor control, behavior-cloning, Stable Diffusion, RLbench
:<br /><br />GENIMARGB25RLBench9GENIMA3D <div>
Image-generation diffusion models have been fine-tuned to unlock new
capabilities such as image-editing and novel view synthesis. Can we similarly
unlock image-generation models for visuomotor control? We present GENIMA, a
behavior-cloning agent that fine-tunes Stable Diffusion to 'draw joint-actions'
as targets on RGB images. These images are fed into a controller that maps the
visual targets into a sequence of joint-positions. We study GENIMA on 25
RLBench and 9 real-world manipulation tasks. We find that, by lifting actions
into image-space, internet pre-trained diffusion models can generate policies
that outperform state-of-the-art visuomotor approaches, especially in
robustness to scene perturbations and generalizing to novel objects. Our method
is also competitive with 3D agents, despite lacking priors such as depth,
keypoints, or motion-planners.
]]></content:encoded>
<pubDate>2024-07-10T17:41:10Z</pubDate>
</item>
<item>
<title>Hypothetical Minds: Scaffolding Theory of Mind for Multi-Agent Tasks
  with Large Language Models</title>
<link>http://arxiv.org/abs/2407.07086v1</link>
<guid>http://arxiv.org/abs/2407.07086v1</guid>
<content:encoded><![CDATA[
<div> Melting Pot<br />
<br />
Multi-agent reinforcement learning (MARL)(LLMs)Hypothetical MindsHypothetical MindsMelting PotLLM-agent<br /><br />: Hypothetical MindsMelting Pot <div>
Multi-agent reinforcement learning (MARL) methods struggle with the
non-stationarity of multi-agent systems and fail to adaptively learn online
when tested with novel agents. Here, we leverage large language models (LLMs)
to create an autonomous agent that can handle these challenges. Our agent,
Hypothetical Minds, consists of a cognitively-inspired architecture, featuring
modular components for perception, memory, and hierarchical planning over two
levels of abstraction. We introduce the Theory of Mind module that scaffolds
the high-level planning process by generating hypotheses about other agents'
strategies in natural language. It then evaluates and iteratively refines these
hypotheses by reinforcing hypotheses that make correct predictions about the
other agents' behavior. Hypothetical Minds significantly improves performance
over previous LLM-agent and RL baselines on a range of competitive, mixed
motive, and collaborative domains in the Melting Pot benchmark, including both
dyadic and population-based environments. Additionally, comparisons against
LLM-agent baselines and ablations reveal the importance of hypothesis
evaluation and refinement for succeeding on complex scenarios.
]]></content:encoded>
<pubDate>2024-07-09T17:57:15Z</pubDate>
</item>
<item>
<title>Tailor3D: Customized 3D Assets Editing and Generation with Dual-Side
  Images</title>
<link>http://arxiv.org/abs/2407.06191v1</link>
<guid>http://arxiv.org/abs/2407.06191v1</guid>
<content:encoded><![CDATA[
<div> pipeline, 3D assets, dual-side images, editing, Tailor3D
: 
3D AIGC3D3DTailor3D3DLRMLoRA Triplane TransformerTailor3D3D <div>
Recent advances in 3D AIGC have shown promise in directly creating 3D objects
from text and images, offering significant cost savings in animation and
product design. However, detailed edit and customization of 3D assets remains a
long-standing challenge. Specifically, 3D Generation methods lack the ability
to follow finely detailed instructions as precisely as their 2D image creation
counterparts. Imagine you can get a toy through 3D AIGC but with undesired
accessories and dressing. To tackle this challenge, we propose a novel pipeline
called Tailor3D, which swiftly creates customized 3D assets from editable
dual-side images. We aim to emulate a tailor's ability to locally change
objects or perform overall style transfer. Unlike creating 3D assets from
multiple views, using dual-side images eliminates conflicts on overlapping
areas that occur when editing individual views. Specifically, it begins by
editing the front view, then generates the back view of the object through
multi-view diffusion. Afterward, it proceeds to edit the back views. Finally, a
Dual-sided LRM is proposed to seamlessly stitch together the front and back 3D
features, akin to a tailor sewing together the front and back of a garment. The
Dual-sided LRM rectifies imperfect consistencies between the front and back
views, enhancing editing capabilities and reducing memory burdens while
seamlessly integrating them into a unified 3D representation with the LoRA
Triplane Transformer. Experimental results demonstrate Tailor3D's effectiveness
across various 3D generation and editing tasks, including 3D generative fill
and style transfer. It provides a user-friendly, efficient solution for editing
3D assets, with each editing step taking only seconds to complete.
]]></content:encoded>
<pubDate>2024-07-08T17:59:55Z</pubDate>
</item>
<item>
<title>JeDi: Joint-Image Diffusion Models for Finetuning-Free Personalized
  Text-to-Image Generation</title>
<link>http://arxiv.org/abs/2407.06187v1</link>
<guid>http://arxiv.org/abs/2407.06187v1</guid>
<content:encoded><![CDATA[
<div> personalization, text-to-image generation, finetuning-free, Joint-Image Diffusion, synthetic dataset generation

:<br /><br />Joint-Image DiffusionJEDI- <div>
Personalized text-to-image generation models enable users to create images
that depict their individual possessions in diverse scenes, finding
applications in various domains. To achieve the personalization capability,
existing methods rely on finetuning a text-to-image foundation model on a
user's custom dataset, which can be non-trivial for general users,
resource-intensive, and time-consuming. Despite attempts to develop
finetuning-free methods, their generation quality is much lower compared to
their finetuning counterparts. In this paper, we propose Joint-Image Diffusion
(\jedi), an effective technique for learning a finetuning-free personalization
model. Our key idea is to learn the joint distribution of multiple related
text-image pairs that share a common subject. To facilitate learning, we
propose a scalable synthetic dataset generation technique. Once trained, our
model enables fast and easy personalization at test time by simply using
reference images as input during the sampling process. Our approach does not
require any expensive optimization process or additional modules and can
faithfully preserve the identity represented by any number of reference images.
Experimental results show that our model achieves state-of-the-art generation
quality, both quantitatively and qualitatively, significantly outperforming
both the prior finetuning-based and finetuning-free personalization baselines.
]]></content:encoded>
<pubDate>2024-07-08T17:59:02Z</pubDate>
</item>
<item>
<title>VCoME: Verbal Video Composition with Multimodal Editing Effects</title>
<link>http://arxiv.org/abs/2407.04697v1</link>
<guid>http://arxiv.org/abs/2407.04697v1</guid>
<content:encoded><![CDATA[
<div> VCoME<br />
:<br />
VCoMEVCoME85 <div>
Verbal videos, featuring voice-overs or text overlays, provide valuable
content but present significant challenges in composition, especially when
incorporating editing effects to enhance clarity and visual appeal. In this
paper, we introduce the novel task of verbal video composition with editing
effects. This task aims to generate coherent and visually appealing verbal
videos by integrating multimodal editing effects across textual, visual, and
audio categories. To achieve this, we curate a large-scale dataset of video
effects compositions from publicly available sources. We then formulate this
task as a generative problem, involving the identification of appropriate
positions in the verbal content and the recommendation of editing effects for
these positions. To address this task, we propose VCoME, a general framework
that employs a large multimodal model to generate editing effects for video
composition. Specifically, VCoME takes in the multimodal video context and
autoregressively outputs where to apply effects within the verbal content and
which effects are most appropriate for each position. VCoME also supports
prompt-based control of composition density and style, providing substantial
flexibility for diverse applications. Through extensive quantitative and
qualitative evaluations, we clearly demonstrate the effectiveness of VCoME. A
comprehensive user study shows that our method produces videos of professional
quality while being 85$\times$ more efficient than professional editors.
]]></content:encoded>
<pubDate>2024-07-05T17:59:02Z</pubDate>
</item>
<item>
<title>Fair Division of Indivisible Chores via Earning Restricted Equilibria</title>
<link>http://arxiv.org/abs/2407.03318v1</link>
<guid>http://arxiv.org/abs/2407.03318v1</guid>
<content:encoded><![CDATA[
<div> envy-freeness, Pareto optimality, earning-restricted, competitive equilibrium, algorithms
<br /><br />:
mO(n^2)-EFX5-EFX3-EFXPOO(n)-EFX2-EF2 + POEFm + POLCPERER <div>
We study fair division of $m$ indivisible chores among $n$ agents with
additive preferences. We consider the desirable fairness notions of
envy-freeness up to any chore (EFX) and envy-freeness up to $k$ chores (EF$k$),
alongside the efficiency notion of Pareto optimality (PO). We present the first
constant approximations of these notions, showing the existence of:
  - 5-EFX allocations, which improve the best-known factor of $O(n^2)$-EFX.
  - 3-EFX and PO allocations for the special case of bivalued instances, which
improve the best-known factor of $O(n)$-EFX without any efficiency guarantees.
  - 2-EF2 + PO allocations, which improve the best-known factor of EF$m$ + PO.
  A notable contribution of our work is the introduction of the novel concept
of earning-restricted (ER) competitive equilibrium for fractional allocations,
which limits agents' earnings from each chore. Technically, our work addresses
two main challenges: proving the existence of an ER equilibrium and designing
algorithms that leverage ER equilibria to achieve the above results. To tackle
the first challenge, we formulate a linear complementarity problem (LCP)
formulation that captures all ER equilibria and show that the classic
complementary pivot algorithm on the LCP must terminate at an ER equilibrium.
For the second challenge, we carefully set the earning limits and use
properties of ER equilibria to design sophisticated procedures that involve
swapping and merging bundles to meet the desired fairness and efficiency
criteria. We expect that the concept of ER equilibrium will be instrumental in
deriving further results on related problems.
]]></content:encoded>
<pubDate>2024-07-03T17:58:22Z</pubDate>
</item>
<item>
<title>BACON: Supercharge Your VLM with Bag-of-Concept Graph to Mitigate
  Hallucinations</title>
<link>http://arxiv.org/abs/2407.03314v1</link>
<guid>http://arxiv.org/abs/2407.03314v1</guid>
<content:encoded><![CDATA[
<div> : BACON, VLMs, , , <br />
<br />:
Bag-of-Concept GraphBACONVLMsVQAVLMsBACON100KVLMsBACONBACONBACONBACONVQABACON <div>
This paper presents Bag-of-Concept Graph (BACON) to gift models with limited
linguistic abilities to taste the privilege of Vision Language Models (VLMs)
and boost downstream tasks such as detection, visual question answering (VQA),
and image generation. Since the visual scenes in physical worlds are structured
with complex relations between objects, BACON breaks down annotations into
basic minimum elements and presents them in a graph structure. Element-wise
style enables easy understanding, and structural composition liberates
difficult locating. Careful prompt design births the BACON captions with the
help of public-available VLMs and segmentation methods. In this way, we gather
a dataset with 100K annotated images, which endow VLMs with remarkable
capabilities, such as accurately generating BACON, transforming prompts into
BACON format, envisioning scenarios in the style of BACONr, and dynamically
modifying elements within BACON through interactive dialogue and more. Wide
representative experiments, including detection, VQA, and image generation
tasks, tell BACON as a lifeline to achieve previous out-of-reach tasks or excel
in their current cutting-edge solutions.
]]></content:encoded>
<pubDate>2024-07-03T17:55:27Z</pubDate>
</item>
<item>
<title>MMedAgent: Learning to Use Medical Tools with Multi-modal Agent</title>
<link>http://arxiv.org/abs/2407.02483v1</link>
<guid>http://arxiv.org/abs/2407.02483v1</guid>
<content:encoded><![CDATA[
<div> , , MMedAgent, , 
<br /><br />:
MMedAgentMMedAgentGPT-4oMMedAgent <div>
Multi-Modal Large Language Models (MLLMs), despite being successful, exhibit
limited generality and often fall short when compared to specialized models.
Recently, LLM-based agents have been developed to address these challenges by
selecting appropriate specialized models as tools based on user inputs.
However, such advancements have not been extensively explored within the
medical domain. To bridge this gap, this paper introduces the first agent
explicitly designed for the medical field, named \textbf{M}ulti-modal
\textbf{Med}ical \textbf{Agent} (MMedAgent). We curate an instruction-tuning
dataset comprising six medical tools solving seven tasks, enabling the agent to
choose the most suitable tools for a given task. Comprehensive experiments
demonstrate that MMedAgent achieves superior performance across a variety of
medical tasks compared to state-of-the-art open-source methods and even the
closed-source model, GPT-4o. Furthermore, MMedAgent exhibits efficiency in
updating and integrating new medical tools.
]]></content:encoded>
<pubDate>2024-07-02T17:58:23Z</pubDate>
</item>
<item>
<title>Understanding Alignment in Multimodal LLMs: A Comprehensive Study</title>
<link>http://arxiv.org/abs/2407.02477v1</link>
<guid>http://arxiv.org/abs/2407.02477v1</guid>
<content:encoded><![CDATA[
<div> MLLMs, , , , <br />
BDHS<br /><br />:BDHS <div>
Preference alignment has become a crucial component in enhancing the
performance of Large Language Models (LLMs), yet its impact in Multimodal Large
Language Models (MLLMs) remains comparatively underexplored. Similar to
language models, MLLMs for image understanding tasks encounter challenges like
hallucination. In MLLMs, hallucination can occur not only by stating incorrect
facts but also by producing responses that are inconsistent with the image
content. A primary objective of alignment for MLLMs is to encourage these
models to align responses more closely with image information. Recently,
multiple works have introduced preference datasets for MLLMs and examined
different alignment methods, including Direct Preference Optimization (DPO) and
Proximal Policy Optimization (PPO). However, due to variations in datasets,
base model types, and alignment methods, it remains unclear which specific
elements contribute most significantly to the reported improvements in these
works. In this paper, we independently analyze each aspect of preference
alignment in MLLMs. We start by categorizing the alignment algorithms into two
groups, offline (such as DPO), and online (such as online-DPO), and show that
combining offline and online methods can improve the performance of the model
in certain scenarios. We review a variety of published multimodal preference
datasets and discuss how the details of their construction impact model
performance. Based on these insights, we introduce a novel way of creating
multimodal preference data called Bias-Driven Hallucination Sampling (BDHS)
that needs neither additional annotation nor external models, and show that it
can achieve competitive performance to previously published alignment work for
multimodal models across a range of benchmarks.
]]></content:encoded>
<pubDate>2024-07-02T17:55:03Z</pubDate>
</item>
<item>
<title>PoliFormer: Scaling On-Policy RL with Transformers Results in Masterful
  Navigators</title>
<link>http://arxiv.org/abs/2406.20083v1</link>
<guid>http://arxiv.org/abs/2406.20083v1</guid>
<content:encoded><![CDATA[
<div> <br />
PoliFormerAgentPoliFormerPoliFormerLoCoBotStretch RE-1CHORES-S85.5%28.5%PoliFormer<br /><br />: PoliFormerAgentCHORES-S <div>
We present PoliFormer (Policy Transformer), an RGB-only indoor navigation
agent trained end-to-end with reinforcement learning at scale that generalizes
to the real-world without adaptation despite being trained purely in
simulation. PoliFormer uses a foundational vision transformer encoder with a
causal transformer decoder enabling long-term memory and reasoning. It is
trained for hundreds of millions of interactions across diverse environments,
leveraging parallelized, multi-machine rollouts for efficient training with
high throughput. PoliFormer is a masterful navigator, producing
state-of-the-art results across two distinct embodiments, the LoCoBot and
Stretch RE-1 robots, and four navigation benchmarks. It breaks through the
plateaus of previous work, achieving an unprecedented 85.5% success rate in
object goal navigation on the CHORES-S benchmark, a 28.5% absolute improvement.
PoliFormer can also be trivially extended to a variety of downstream
applications such as object tracking, multi-object navigation, and
open-vocabulary navigation with no finetuning.
]]></content:encoded>
<pubDate>2024-06-28T17:51:10Z</pubDate>
</item>
<item>
<title>ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos</title>
<link>http://arxiv.org/abs/2406.19392v1</link>
<guid>http://arxiv.org/abs/2406.19392v1</guid>
<content:encoded><![CDATA[
<div> : ReXTime, temporal reasoning, video events, benchmark, large language models <br />
<br /> 
ReXTimeAIReXTime-9212,14314.3%9695<br /><br />: ReXTime <div>
We introduce ReXTime, a benchmark designed to rigorously test AI models'
ability to perform temporal reasoning within video events. Specifically,
ReXTime focuses on reasoning across time, i.e. human-like understanding when
the question and its corresponding answer occur in different video segments.
This form of reasoning, requiring advanced understanding of cause-and-effect
relationships across video segments, poses significant challenges to even the
frontier multimodal large language models. To facilitate this evaluation, we
develop an automated pipeline for generating temporal reasoning question-answer
pairs, significantly reducing the need for labor-intensive manual annotations.
Our benchmark includes 921 carefully vetted validation samples and 2,143 test
samples, each manually curated for accuracy and relevance. Evaluation results
show that while frontier large language models outperform academic models, they
still lag behind human performance by a significant 14.3% accuracy gap.
Additionally, our pipeline creates a training dataset of 9,695 machine
generated samples without manual effort, which empirical studies suggest can
enhance the across-time reasoning via fine-tuning.
]]></content:encoded>
<pubDate>2024-06-27T17:59:45Z</pubDate>
</item>
<item>
<title>OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and
  Understanding</title>
<link>http://arxiv.org/abs/2406.19389v1</link>
<guid>http://arxiv.org/abs/2406.19389v1</guid>
<content:encoded><![CDATA[
<div> OMG-LLaVA
<br /><br />
:
OMG-LLaVALLMLLMOMG-LLaVALLMLLM <div>
Current universal segmentation methods demonstrate strong capabilities in
pixel-level image and video understanding. However, they lack reasoning
abilities and cannot be controlled via text instructions. In contrast, large
vision-language multimodal models exhibit powerful vision-based conversation
and reasoning capabilities but lack pixel-level understanding and have
difficulty accepting visual prompts for flexible user interaction. This paper
proposes OMG-LLaVA, a new and elegant framework combining powerful pixel-level
vision understanding with reasoning abilities. It can accept various visual and
text prompts for flexible user interaction. Specifically, we use a universal
segmentation method as the visual encoder, integrating image information,
perception priors, and visual prompts into visual tokens provided to the LLM.
The LLM is responsible for understanding the user's text instructions and
providing text responses and pixel-level segmentation results based on the
visual information. We propose perception prior embedding to better integrate
perception priors with image features. OMG-LLaVA achieves image-level,
object-level, and pixel-level reasoning and understanding in a single model,
matching or surpassing the performance of specialized methods on multiple
benchmarks. Rather than using LLM to connect each specialist, our work aims at
end-to-end training on one encoder, one decoder, and one LLM. The code and
model have been released for further research.
]]></content:encoded>
<pubDate>2024-06-27T17:59:01Z</pubDate>
</item>
<item>
<title>Symbolic Learning Enables Self-Evolving Agents</title>
<link>http://arxiv.org/abs/2406.18532v1</link>
<guid>http://arxiv.org/abs/2406.18532v1</guid>
<content:encoded><![CDATA[
<div> agent symbolic learning, language agents, data-centric, artificial general intelligence, back-propagation and gradient descent
<br /><br />
agent symbolic learning  <br />agent symbolic learning, data-centric, <br />:
agent symbolic learning  <div>
The AI community has been exploring a pathway to artificial general
intelligence (AGI) by developing "language agents", which are complex large
language models (LLMs) pipelines involving both prompting techniques and tool
usage methods. While language agents have demonstrated impressive capabilities
for many real-world tasks, a fundamental limitation of current language agents
research is that they are model-centric, or engineering-centric. That's to say,
the progress on prompts, tools, and pipelines of language agents requires
substantial manual engineering efforts from human experts rather than
automatically learning from data. We believe the transition from model-centric,
or engineering-centric, to data-centric, i.e., the ability of language agents
to autonomously learn and evolve in environments, is the key for them to
possibly achieve AGI.
  In this work, we introduce agent symbolic learning, a systematic framework
that enables language agents to optimize themselves on their own in a
data-centric way using symbolic optimizers. Specifically, we consider agents as
symbolic networks where learnable weights are defined by prompts, tools, and
the way they are stacked together. Agent symbolic learning is designed to
optimize the symbolic network within language agents by mimicking two
fundamental algorithms in connectionist learning: back-propagation and gradient
descent. Instead of dealing with numeric weights, agent symbolic learning works
with natural language simulacrums of weights, loss, and gradients. We conduct
proof-of-concept experiments on both standard benchmarks and complex real-world
tasks and show that agent symbolic learning enables language agents to update
themselves after being created and deployed in the wild, resulting in
"self-evolving agents".
]]></content:encoded>
<pubDate>2024-06-26T17:59:18Z</pubDate>
</item>
<item>
<title>MultiDiff: Consistent Novel View Synthesis from a Single Image</title>
<link>http://arxiv.org/abs/2406.18524v1</link>
<guid>http://arxiv.org/abs/2406.18524v1</guid>
<content:encoded><![CDATA[
<div> 
<br />
MultiDiffRGBMultiDiffMultiDiffMultiDiffRealEstate10KScanNet
<br /><br />: MultiDiff <div>
We introduce MultiDiff, a novel approach for consistent novel view synthesis
of scenes from a single RGB image. The task of synthesizing novel views from a
single reference image is highly ill-posed by nature, as there exist multiple,
plausible explanations for unobserved areas. To address this issue, we
incorporate strong priors in form of monocular depth predictors and
video-diffusion models. Monocular depth enables us to condition our model on
warped reference images for the target views, increasing geometric stability.
The video-diffusion prior provides a strong proxy for 3D scenes, allowing the
model to learn continuous and pixel-accurate correspondences across generated
images. In contrast to approaches relying on autoregressive image generation
that are prone to drifts and error accumulation, MultiDiff jointly synthesizes
a sequence of frames yielding high-quality and multi-view consistent results --
even for long-term scene generation with large camera movements, while reducing
inference time by an order of magnitude. For additional consistency and image
quality improvements, we introduce a novel, structured noise distribution. Our
experimental results demonstrate that MultiDiff outperforms state-of-the-art
methods on the challenging, real-world datasets RealEstate10K and ScanNet.
Finally, our model naturally supports multi-view consistent editing without the
need for further tuning.
]]></content:encoded>
<pubDate>2024-06-26T17:53:51Z</pubDate>
</item>
<item>
<title>ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of
  Text-to-Time-lapse Video Generation</title>
<link>http://arxiv.org/abs/2406.18522v1</link>
<guid>http://arxiv.org/abs/2406.18522v1</guid>
<content:encoded><![CDATA[
We propose a novel text-to-video (T2V) generation benchmark,
ChronoMagic-Bench, to evaluate the temporal and metamorphic capabilities of the
T2V models (e.g. Sora and Lumiere) in time-lapse video generation. In contrast
to existing benchmarks that focus on the visual quality and textual relevance
of generated videos, ChronoMagic-Bench focuses on the model's ability to
generate time-lapse videos with significant metamorphic amplitude and temporal
coherence. The benchmark probes T2V models for their physics, biology, and
chemistry capabilities, in a free-form text query. For these purposes,
ChronoMagic-Bench introduces 1,649 prompts and real-world videos as references,
categorized into four major types of time-lapse videos: biological,
human-created, meteorological, and physical phenomena, which are further
divided into 75 subcategories. This categorization comprehensively evaluates
the model's capacity to handle diverse and complex transformations. To
accurately align human preference with the benchmark, we introduce two new
automatic metrics, MTScore and CHScore, to evaluate the videos' metamorphic
attributes and temporal coherence. MTScore measures the metamorphic amplitude,
reflecting the degree of change over time, while CHScore assesses the temporal
coherence, ensuring the generated videos maintain logical progression and
continuity. Based on the ChronoMagic-Bench, we conduct comprehensive manual
evaluations of ten representative T2V models, revealing their strengths and
weaknesses across different categories of prompts, and providing a thorough
evaluation framework that addresses current gaps in video generation research.
Moreover, we create a large-scale ChronoMagic-Pro dataset, containing 460k
high-quality pairs of 720p time-lapse videos and detailed captions ensuring
high physical pertinence and large metamorphic amplitude.
]]></content:encoded>
<pubDate>2024-06-26T17:50:47Z</pubDate>
</item>
<item>
<title>Text-Animator: Controllable Visual Text Video Generation</title>
<link>http://arxiv.org/abs/2406.17777v1</link>
<guid>http://arxiv.org/abs/2406.17777v1</guid>
<content:encoded><![CDATA[
<div> Text-to-Video, Visualization, Text-Animator, Video generation, Visual text
<br /><br />Text-Animator Text-AnimatorText-Animator: <br />Text-Animator <div>
Video generation is a challenging yet pivotal task in various industries,
such as gaming, e-commerce, and advertising. One significant unresolved aspect
within T2V is the effective visualization of text within generated videos.
Despite the progress achieved in Text-to-Video~(T2V) generation, current
methods still cannot effectively visualize texts in videos directly, as they
mainly focus on summarizing semantic scene information, understanding, and
depicting actions. While recent advances in image-level visual text generation
show promise, transitioning these techniques into the video domain faces
problems, notably in preserving textual fidelity and motion coherence. In this
paper, we propose an innovative approach termed Text-Animator for visual text
video generation. Text-Animator contains a text embedding injection module to
precisely depict the structures of visual text in generated videos. Besides, we
develop a camera control module and a text refinement module to improve the
stability of generated visual text by controlling the camera movement as well
as the motion of visualized text. Quantitative and qualitative experimental
results demonstrate the superiority of our approach to the accuracy of
generated visual text over state-of-the-art video generation methods. The
project page can be found at https://laulampaul.github.io/text-animator.html.
]]></content:encoded>
<pubDate>2024-06-25T17:59:41Z</pubDate>
</item>
<item>
<title>MG-LLaVA: Towards Multi-Granularity Visual Instruction Tuning</title>
<link>http://arxiv.org/abs/2406.17770v1</link>
<guid>http://arxiv.org/abs/2406.17770v1</guid>
<content:encoded><![CDATA[
<div> MG-LLaVA
<br />
MG-LLaVAConv-GateMG-LLaVAMG-LLaVA<br /><br />: MG-LLaVAMG-LLaVA <div>
Multi-modal large language models (MLLMs) have made significant strides in
various visual understanding tasks. However, the majority of these models are
constrained to process low-resolution images, which limits their effectiveness
in perception tasks that necessitate detailed visual information. In our study,
we present MG-LLaVA, an innovative MLLM that enhances the model's visual
processing capabilities by incorporating a multi-granularity vision flow, which
includes low-resolution, high-resolution, and object-centric features. We
propose the integration of an additional high-resolution visual encoder to
capture fine-grained details, which are then fused with base visual features
through a Conv-Gate fusion network. To further refine the model's object
recognition abilities, we incorporate object-level features derived from
bounding boxes identified by offline detectors. Being trained solely on
publicly available multimodal data through instruction tuning, MG-LLaVA
demonstrates exceptional perception skills. We instantiate MG-LLaVA with a wide
variety of language encoders, ranging from 3.8B to 34B, to evaluate the model's
performance comprehensively. Extensive evaluations across multiple benchmarks
demonstrate that MG-LLaVA outperforms existing MLLMs of comparable parameter
sizes, showcasing its remarkable efficacy. The code will be available at
https://github.com/PhoenixZ810/MG-LLaVA.
]]></content:encoded>
<pubDate>2024-06-25T17:55:11Z</pubDate>
</item>
<item>
<title>EXTRACT: Efficient Policy Learning by Extracting Transferrable Robot
  Skills from Offline Data</title>
<link>http://arxiv.org/abs/2406.17768v1</link>
<guid>http://arxiv.org/abs/2406.17768v1</guid>
<content:encoded><![CDATA[
Most reinforcement learning (RL) methods focus on learning optimal policies
over low-level action spaces. While these methods can perform well in their
training environments, they lack the flexibility to transfer to new tasks.
Instead, RL agents that can act over useful, temporally extended skills rather
than low-level actions can learn new tasks more easily. Prior work in
skill-based RL either requires expert supervision to define useful skills,
which is hard to scale, or learns a skill-space from offline data with
heuristics that limit the adaptability of the skills, making them difficult to
transfer during downstream RL. Our approach, EXTRACT, instead utilizes
pre-trained vision language models to extract a discrete set of semantically
meaningful skills from offline data, each of which is parameterized by
continuous arguments, without human supervision. This skill parameterization
allows robots to learn new tasks by only needing to learn when to select a
specific skill and how to modify its arguments for the specific task. We
demonstrate through experiments in sparse-reward, image-based, robot
manipulation environments that EXTRACT can more quickly learn new tasks than
prior works, with major gains in sample efficiency and performance over prior
skill-based RL. Website at https://www.jessezhang.net/projects/extract/.
]]></content:encoded>
<pubDate>2024-06-25T17:50:03Z</pubDate>
</item>
<item>
<title>Revisiting Referring Expression Comprehension Evaluation in the Era of
  Large Multimodal Models</title>
<link>http://arxiv.org/abs/2406.16866v1</link>
<guid>http://arxiv.org/abs/2406.16866v1</guid>
<content:encoded><![CDATA[
<div> : Referring expression comprehension, large multimodal models, benchmarks, labeling error rates, Ref-L4

:<br /><br />Referring expression comprehension (REC)LMMsCogVLMREC92.44%RefCOCORefCOCO+RefCOCOgLMMs14%24%RECLMMsRef-L4RECRef-L424RefCOCORefCOCO+RefCOCOgRef-L4https://github.com/JierunChen/Ref-L4 <div>
Referring expression comprehension (REC) involves localizing a target
instance based on a textual description. Recent advancements in REC have been
driven by large multimodal models (LMMs) like CogVLM, which achieved 92.44%
accuracy on RefCOCO. However, this study questions whether existing benchmarks
such as RefCOCO, RefCOCO+, and RefCOCOg, capture LMMs' comprehensive
capabilities. We begin with a manual examination of these benchmarks, revealing
high labeling error rates: 14% in RefCOCO, 24% in RefCOCO+, and 5% in RefCOCOg,
which undermines the authenticity of evaluations. We address this by excluding
problematic instances and reevaluating several LMMs capable of handling the REC
task, showing significant accuracy improvements, thus highlighting the impact
of benchmark noise. In response, we introduce Ref-L4, a comprehensive REC
benchmark, specifically designed to evaluate modern REC models. Ref-L4 is
distinguished by four key features: 1) a substantial sample size with 45,341
annotations; 2) a diverse range of object categories with 365 distinct types
and varying instance scales from 30 to 3,767; 3) lengthy referring expressions
averaging 24.2 words; and 4) an extensive vocabulary comprising 22,813 unique
words. We evaluate a total of 24 large models on Ref-L4 and provide valuable
insights. The cleaned versions of RefCOCO, RefCOCO+, and RefCOCOg, as well as
our Ref-L4 benchmark and evaluation code, are available at
https://github.com/JierunChen/Ref-L4.
]]></content:encoded>
<pubDate>2024-06-24T17:59:58Z</pubDate>
</item>
<item>
<title>FreeTraj: Tuning-Free Trajectory Control in Video Diffusion Models</title>
<link>http://arxiv.org/abs/2406.16863v1</link>
<guid>http://arxiv.org/abs/2406.16863v1</guid>
<content:encoded><![CDATA[
<div> : , , , , 

:<br /><br />
 FreeTraj  FreeTraj  <div>
Diffusion model has demonstrated remarkable capability in video generation,
which further sparks interest in introducing trajectory control into the
generation process. While existing works mainly focus on training-based methods
(e.g., conditional adapter), we argue that diffusion model itself allows decent
control over the generated content without requiring any training. In this
study, we introduce a tuning-free framework to achieve trajectory-controllable
video generation, by imposing guidance on both noise construction and attention
computation. Specifically, 1) we first show several instructive phenomenons and
analyze how initial noises influence the motion trajectory of generated
content. 2) Subsequently, we propose FreeTraj, a tuning-free approach that
enables trajectory control by modifying noise sampling and attention
mechanisms. 3) Furthermore, we extend FreeTraj to facilitate longer and larger
video generation with controllable trajectories. Equipped with these designs,
users have the flexibility to provide trajectories manually or opt for
trajectories automatically generated by the LLM trajectory planner. Extensive
experiments validate the efficacy of our approach in enhancing the trajectory
controllability of video diffusion models.
]]></content:encoded>
<pubDate>2024-06-24T17:59:56Z</pubDate>
</item>
<item>
<title>Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs</title>
<link>http://arxiv.org/abs/2406.16860v1</link>
<guid>http://arxiv.org/abs/2406.16860v1</guid>
<content:encoded><![CDATA[
We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a
vision-centric approach. While stronger language models can enhance multimodal
capabilities, the design choices for vision components are often insufficiently
explored and disconnected from visual representation learning research. This
gap hinders accurate sensory grounding in real-world scenarios. Our study uses
LLMs and visual instruction tuning as an interface to evaluate various visual
representations, offering new insights into different models and architectures
-- self-supervised, strongly supervised, or combinations thereof -- based on
experiments with over 20 vision encoders. We critically examine existing MLLM
benchmarks, addressing the difficulties involved in consolidating and
interpreting results from various tasks, and introduce a new vision-centric
benchmark, CV-Bench. To further improve visual grounding, we propose the
Spatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that
integrates high-resolution vision features with LLMs while reducing the number
of tokens. Additionally, we discuss the curation of high-quality visual
instruction-tuning data from publicly available sources, emphasizing the
importance of data source balancing and distribution ratio. Collectively,
Cambrian-1 not only achieves state-of-the-art performance but also serves as a
comprehensive, open cookbook for instruction-tuned MLLMs. We provide model
weights, code, supporting tools, datasets, and detailed instruction-tuning and
evaluation recipes. We hope our release will inspire and accelerate
advancements in multimodal systems and visual representation learning.
]]></content:encoded>
<pubDate>2024-06-24T17:59:42Z</pubDate>
</item>
<item>
<title>DreamBench++: A Human-Aligned Benchmark for Personalized Image
  Generation</title>
<link>http://arxiv.org/abs/2406.16855v1</link>
<guid>http://arxiv.org/abs/2406.16855v1</guid>
<content:encoded><![CDATA[
Personalized image generation holds great promise in assisting humans in
everyday work and life due to its impressive function in creatively generating
personalized content. However, current evaluations either are automated but
misalign with humans or require human evaluations that are time-consuming and
expensive. In this work, we present DreamBench++, a human-aligned benchmark
automated by advanced multimodal GPT models. Specifically, we systematically
design the prompts to let GPT be both human-aligned and self-aligned, empowered
with task reinforcement. Further, we construct a comprehensive dataset
comprising diverse images and prompts. By benchmarking 7 modern generative
models, we demonstrate that DreamBench++ results in significantly more
human-aligned evaluation, helping boost the community with innovative findings.
]]></content:encoded>
<pubDate>2024-06-24T17:58:47Z</pubDate>
</item>
<item>
<title>Long Context Transfer from Language to Vision</title>
<link>http://arxiv.org/abs/2406.16852v1</link>
<guid>http://arxiv.org/abs/2406.16852v1</guid>
<content:encoded><![CDATA[
Video sequences offer valuable temporal information, but existing large
multimodal models (LMMs) fall short in understanding extremely long videos.
Many works address this by reducing the number of visual tokens using visual
resamplers. Alternatively, in this paper, we approach this problem from the
perspective of the language model. By simply extrapolating the context length
of the language backbone, we enable LMMs to comprehend orders of magnitude more
visual tokens without any video training. We call this phenomenon long context
transfer and carefully ablate its properties. To effectively measure LMMs'
ability to generalize to long contexts in the vision modality, we develop
V-NIAH (Visual Needle-In-A-Haystack), a purely synthetic long vision benchmark
inspired by the language model's NIAH test. Our proposed Long Video Assistant
(LongVA) can process 2000 frames or over 200K visual tokens without additional
complexities. With its extended context length, LongVA achieves
state-of-the-art performance on Video-MME among 7B-scale models by densely
sampling more input frames. Our work is open-sourced at
https://github.com/EvolvingLMMs-Lab/LongVA.
]]></content:encoded>
<pubDate>2024-06-24T17:58:06Z</pubDate>
</item>
<item>
<title>GenoTEX: A Benchmark for Evaluating LLM-Based Exploration of Gene
  Expression Data in Alignment with Bioinformaticians</title>
<link>http://arxiv.org/abs/2406.15341v1</link>
<guid>http://arxiv.org/abs/2406.15341v1</guid>
<content:encoded><![CDATA[
<div> 
<br /><br />
GenoTEXGenoTEXGenoAgentsGenoTEXGitHub <div>
Recent advancements in machine learning have significantly improved the
identification of disease-associated genes from gene expression datasets.
However, these processes often require extensive expertise and manual effort,
limiting their scalability. Large Language Model (LLM)-based agents have shown
promise in automating these tasks due to their increasing problem-solving
abilities. To support the evaluation and development of such methods, we
introduce GenoTEX, a benchmark dataset for the automatic exploration of gene
expression data, involving the tasks of dataset selection, preprocessing, and
statistical analysis. GenoTEX provides annotated code and results for solving a
wide range of gene identification problems, in a full analysis pipeline that
follows the standard of computational genomics. These annotations are curated
by human bioinformaticians who carefully analyze the datasets to ensure
accuracy and reliability. To provide baselines for these tasks, we present
GenoAgents, a team of LLM-based agents designed with context-aware planning,
iterative correction, and domain expert consultation to collaboratively explore
gene datasets. Our experiments with GenoAgents demonstrate the potential of
LLM-based approaches in genomics data analysis, while error analysis highlights
the challenges and areas for future improvement. We propose GenoTEX as a
promising resource for benchmarking and enhancing AI-driven methods for
genomics data analysis. We make our benchmark publicly available at
\url{https://github.com/Liu-Hy/GenoTex}.
]]></content:encoded>
<pubDate>2024-06-21T17:55:24Z</pubDate>
</item>
<item>
<title>Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning</title>
<link>http://arxiv.org/abs/2406.15334v1</link>
<guid>http://arxiv.org/abs/2406.15334v1</guid>
<content:encoded><![CDATA[
<div> , , , , 

MTVMTVMTVLMMs<br /><br />: MTVMTV <div>
The recent success of interleaved Large Multimodal Models (LMMs) in few-shot
learning suggests that in-context learning (ICL) with many examples can be
promising for learning new tasks. However, this many-shot multimodal ICL
setting has one crucial problem: it is fundamentally limited by the model's
context length set at pretraining. The problem is especially prominent in the
multimodal domain, which processes both text and images, requiring additional
tokens. This motivates the need for a multimodal method to compress many shots
into fewer tokens without finetuning. In this work, we enable LMMs to perform
multimodal, many-shot in-context learning by leveraging Multimodal Task Vectors
(MTV)--compact implicit representations of in-context examples compressed in
the model's attention heads. Specifically, we first demonstrate the existence
of such MTV in LMMs and then leverage these extracted MTV to enable many-shot
in-context learning for various vision-and-language tasks. Our experiments
suggest that MTV can scale in performance with the number of compressed shots
and generalize to similar out-of-domain tasks without additional context length
for inference.
]]></content:encoded>
<pubDate>2024-06-21T17:50:02Z</pubDate>
</item>
<item>
<title>GeoLRM: Geometry-Aware Large Reconstruction Model for High-Quality 3D
  Gaussian Generation</title>
<link>http://arxiv.org/abs/2406.15333v1</link>
<guid>http://arxiv.org/abs/2406.15333v1</guid>
<content:encoded><![CDATA[
In this work, we introduce the Geometry-Aware Large Reconstruction Model
(GeoLRM), an approach which can predict high-quality assets with 512k Gaussians
and 21 input images in only 11 GB GPU memory. Previous works neglect the
inherent sparsity of 3D structure and do not utilize explicit geometric
relationships between 3D and 2D images. This limits these methods to a
low-resolution representation and makes it difficult to scale up to the dense
views for better quality. GeoLRM tackles these issues by incorporating a novel
3D-aware transformer structure that directly processes 3D points and uses
deformable cross-attention mechanisms to effectively integrate image features
into 3D representations. We implement this solution through a two-stage
pipeline: initially, a lightweight proposal network generates a sparse set of
3D anchor points from the posed image inputs; subsequently, a specialized
reconstruction transformer refines the geometry and retrieves textural details.
Extensive experimental results demonstrate that GeoLRM significantly
outperforms existing models, especially for dense view inputs. We also
demonstrate the practical applicability of our model with 3D generation tasks,
showcasing its versatility and potential for broader adoption in real-world
applications.
]]></content:encoded>
<pubDate>2024-06-21T17:49:31Z</pubDate>
</item>
<item>
<title>Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities</title>
<link>http://arxiv.org/abs/2406.14562v1</link>
<guid>http://arxiv.org/abs/2406.14562v1</guid>
<content:encoded><![CDATA[
<div> <br />
:<br />
  <div>
When presented with questions involving visual thinking, humans naturally
switch reasoning modalities, often forming mental images or drawing visual
aids. Large language models have shown promising results in arithmetic and
symbolic reasoning by expressing intermediate reasoning in text as a chain of
thought, yet struggle to extend this capability to answer text queries that are
easily solved by visual reasoning, even with extensive multimodal pretraining.
We introduce a simple method, whiteboard-of-thought prompting, to unlock the
visual reasoning capabilities of multimodal large language models across
modalities. Whiteboard-of-thought prompting provides multimodal large language
models with a metaphorical `whiteboard' to draw out reasoning steps as images,
then returns these images back to the model for further processing. We find
this can be accomplished with no demonstrations or specialized modules, instead
leveraging models' existing ability to write code with libraries such as
Matplotlib and Turtle. This simple approach shows state-of-the-art results on
four difficult natural language tasks that involve visual and spatial
reasoning. We identify multiple settings where GPT-4o using chain-of-thought
fails dramatically, including more than one where it achieves $0\%$ accuracy,
while whiteboard-of-thought enables up to $92\%$ accuracy in these same
settings. We present a detailed exploration of where the technique succeeds as
well as its sources of error.
]]></content:encoded>
<pubDate>2024-06-20T17:59:45Z</pubDate>
</item>
<item>
<title>CooHOI: Learning Cooperative Human-Object Interaction with Manipulated
  Object Dynamics</title>
<link>http://arxiv.org/abs/2406.14558v1</link>
<guid>http://arxiv.org/abs/2406.14558v1</guid>
<content:encoded><![CDATA[
<div> CooHOIAMP

CooHOIAMPMAPPOHOICooHOI<br /><br />: CoHOIAMPMAPPO <div>
Recent years have seen significant advancements in humanoid control, largely
due to the availability of large-scale motion capture data and the application
of reinforcement learning methodologies. However, many real-world tasks, such
as moving large and heavy furniture, require multi-character collaboration.
Given the scarcity of data on multi-character collaboration and the efficiency
challenges associated with multi-agent learning, these tasks cannot be
straightforwardly addressed using training paradigms designed for single-agent
scenarios. In this paper, we introduce Cooperative Human-Object Interaction
(CooHOI), a novel framework that addresses multi-character objects transporting
through a two-phase learning paradigm: individual skill acquisition and
subsequent transfer. Initially, a single agent learns to perform tasks using
the Adversarial Motion Priors (AMP) framework. Following this, the agent learns
to collaborate with others by considering the shared dynamics of the
manipulated object during parallel training using Multi Agent Proximal Policy
Optimization (MAPPO). When one agent interacts with the object, resulting in
specific object dynamics changes, the other agents learn to respond
appropriately, thereby achieving implicit communication and coordination
between teammates. Unlike previous approaches that relied on tracking-based
methods for multi-character HOI, CooHOI is inherently efficient, does not
depend on motion capture data of multi-character interactions, and can be
seamlessly extended to include more participants and a wide range of object
types
]]></content:encoded>
<pubDate>2024-06-20T17:59:22Z</pubDate>
</item>
<item>
<title>A Survey of Multimodal-Guided Image Editing with Text-to-Image Diffusion
  Models</title>
<link>http://arxiv.org/abs/2406.14555v1</link>
<guid>http://arxiv.org/abs/2406.14555v1</guid>
<content:encoded><![CDATA[
Image editing aims to edit the given synthetic or real image to meet the
specific requirements from users. It is widely studied in recent years as a
promising and challenging field of Artificial Intelligence Generative Content
(AIGC). Recent significant advancement in this field is based on the
development of text-to-image (T2I) diffusion models, which generate images
according to text prompts. These models demonstrate remarkable generative
capabilities and have become widely used tools for image editing. T2I-based
image editing methods significantly enhance editing performance and offer a
user-friendly interface for modifying content guided by multimodal inputs. In
this survey, we provide a comprehensive review of multimodal-guided image
editing techniques that leverage T2I diffusion models. First, we define the
scope of image editing from a holistic perspective and detail various control
signals and editing scenarios. We then propose a unified framework to formalize
the editing process, categorizing it into two primary algorithm families. This
framework offers a design space for users to achieve specific goals.
Subsequently, we present an in-depth analysis of each component within this
framework, examining the characteristics and applicable scenarios of different
combinations. Given that training-based methods learn to directly map the
source image to target one under user guidance, we discuss them separately, and
introduce injection schemes of source image in different scenarios.
Additionally, we review the application of 2D techniques to video editing,
highlighting solutions for inter-frame inconsistency. Finally, we discuss open
challenges in the field and suggest potential future research directions. We
keep tracing related works at
https://github.com/xinchengshuai/Awesome-Image-Editing.
]]></content:encoded>
<pubDate>2024-06-20T17:58:52Z</pubDate>
</item>
<item>
<title>DrVideo: Document Retrieval Based Long Video Understanding</title>
<link>http://arxiv.org/abs/2406.12846v1</link>
<guid>http://arxiv.org/abs/2406.12846v1</guid>
<content:encoded><![CDATA[
<div> document-retrieval-based system, large language models, agent-based iterative loop, <br /><br />DrVideoDrVideoDrVideoEgoSchema33.8MovieChat-1K break17.9MovieChat-1K1038.0LLama-Vid QA6030.2 <br /><br />: DrVideo <div>
Existing methods for long video understanding primarily focus on videos only
lasting tens of seconds, with limited exploration of techniques for handling
longer videos. The increased number of frames in longer videos presents two
main challenges: difficulty in locating key information and performing
long-range reasoning. Thus, we propose DrVideo, a document-retrieval-based
system designed for long video understanding. Our key idea is to convert the
long-video understanding problem into a long-document understanding task so as
to effectively leverage the power of large language models. Specifically,
DrVideo transforms a long video into a text-based long document to initially
retrieve key frames and augment the information of these frames, which is used
this as the system's starting point. It then employs an agent-based iterative
loop to continuously search for missing information, augment relevant data, and
provide final predictions in a chain-of-thought manner once sufficient
question-related information is gathered. Extensive experiments on long video
benchmarks confirm the effectiveness of our method. DrVideo outperforms
existing state-of-the-art methods with +3.8 accuracy on EgoSchema benchmark (3
minutes), +17.9 in MovieChat-1K break mode, +38.0 in MovieChat-1K global mode
(10 minutes), and +30.2 on the LLama-Vid QA dataset (over 60 minutes).
]]></content:encoded>
<pubDate>2024-06-18T17:59:03Z</pubDate>
</item>
<item>
<title>Synergizing Foundation Models and Federated Learning: A Survey</title>
<link>http://arxiv.org/abs/2406.12844v1</link>
<guid>http://arxiv.org/abs/2406.12844v1</guid>
<content:encoded><![CDATA[
<div> : Foundation Models, Federated Learning, , , 
<br />
:<br />
FMsFLFMsFLFLFMsFM-FL <div>
The recent development of Foundation Models (FMs), represented by large
language models, vision transformers, and multimodal models, has been making a
significant impact on both academia and industry. Compared with small-scale
models, FMs have a much stronger demand for high-volume data during the
pre-training phase. Although general FMs can be pre-trained on data collected
from open sources such as the Internet, domain-specific FMs need proprietary
data, posing a practical challenge regarding the amount of data available due
to privacy concerns. Federated Learning (FL) is a collaborative learning
paradigm that breaks the barrier of data availability from different
participants. Therefore, it provides a promising solution to customize and
adapt FMs to a wide range of domain-specific tasks using distributed datasets
whilst preserving privacy. This survey paper discusses the potentials and
challenges of synergizing FL and FMs and summarizes core techniques, future
directions, and applications. A periodically updated paper collection on FM-FL
is available at https://github.com/lishenghui/awesome-fm-fl.
]]></content:encoded>
<pubDate>2024-06-18T17:58:09Z</pubDate>
</item>
<item>
<title>Can Go AIs be adversarially robust?</title>
<link>http://arxiv.org/abs/2406.12843v1</link>
<guid>http://arxiv.org/abs/2406.12843v1</guid>
<content:encoded><![CDATA[
Prior work found that superhuman Go AIs like KataGo can be defeated by simple
adversarial strategies. In this paper, we study if simple defenses can improve
KataGo's worst-case performance. We test three natural defenses: adversarial
training on hand-constructed positions, iterated adversarial training, and
changing the network architecture. We find that some of these defenses are able
to protect against previously discovered attacks. Unfortunately, we also find
that none of these defenses are able to withstand adaptive attacks. In
particular, we are able to train new adversaries that reliably defeat our
defended agents by causing them to blunder in ways humans would not. Our
results suggest that building robust AI systems is challenging even in narrow
domains such as Go. For interactive examples of attacks and a link to our
codebase, see https://goattack.far.ai.
]]></content:encoded>
<pubDate>2024-06-18T17:57:49Z</pubDate>
</item>
<item>
<title>Autoregressive Image Generation without Vector Quantization</title>
<link>http://arxiv.org/abs/2406.11838v1</link>
<guid>http://arxiv.org/abs/2406.11838v1</guid>
<content:encoded><![CDATA[
<div> Diffusion, Autoregressive modeling, Continuous-valued space, Diffusion Loss function, Image generation
: MAR <div>
Conventional wisdom holds that autoregressive models for image generation are
typically accompanied by vector-quantized tokens. We observe that while a
discrete-valued space can facilitate representing a categorical distribution,
it is not a necessity for autoregressive modeling. In this work, we propose to
model the per-token probability distribution using a diffusion procedure, which
allows us to apply autoregressive models in a continuous-valued space. Rather
than using categorical cross-entropy loss, we define a Diffusion Loss function
to model the per-token probability. This approach eliminates the need for
discrete-valued tokenizers. We evaluate its effectiveness across a wide range
of cases, including standard autoregressive models and generalized masked
autoregressive (MAR) variants. By removing vector quantization, our image
generator achieves strong results while enjoying the speed advantage of
sequence modeling. We hope this work will motivate the use of autoregressive
generation in other continuous-valued domains and applications.
]]></content:encoded>
<pubDate>2024-06-17T17:59:58Z</pubDate>
</item>
<item>
<title>mDPO: Conditional Preference Optimization for Multimodal Large Language
  Models</title>
<link>http://arxiv.org/abs/2406.11839v1</link>
<guid>http://arxiv.org/abs/2406.11839v1</guid>
<content:encoded><![CDATA[
<div> : mDPO<br />
<br />
DPOLLMDPOmDPODPOLLMmDPO<br /><br />:DPOmDPO <div>
Direct preference optimization (DPO) has shown to be an effective method for
large language model (LLM) alignment. Recent works have attempted to apply DPO
to multimodal scenarios but have found it challenging to achieve consistent
improvement. Through a comparative experiment, we identify the unconditional
preference problem in multimodal preference optimization, where the model
overlooks the image condition. To address this problem, we propose mDPO, a
multimodal DPO objective that prevents the over-prioritization of language-only
preferences by also optimizing image preference. Moreover, we introduce a
reward anchor that forces the reward to be positive for chosen responses,
thereby avoiding the decrease in their likelihood -- an intrinsic problem of
relative preference optimization. Experiments on two multimodal LLMs of
different sizes and three widely used benchmarks demonstrate that mDPO
effectively addresses the unconditional preference problem in multimodal
preference optimization and significantly improves model performance,
particularly in reducing hallucination.
]]></content:encoded>
<pubDate>2024-06-17T17:59:58Z</pubDate>
</item>
<item>
<title>Scaling the Codebook Size of VQGAN to 100,000 with a Utilization Rate of
  99%</title>
<link>http://arxiv.org/abs/2406.11837v1</link>
<guid>http://arxiv.org/abs/2406.11837v1</guid>
<content:encoded><![CDATA[
In the realm of image quantization exemplified by VQGAN, the process encodes
images into discrete tokens drawn from a codebook with a predefined size.
Recent advancements, particularly with LLAMA 3, reveal that enlarging the
codebook significantly enhances model performance. However, VQGAN and its
derivatives, such as VQGAN-FC (Factorized Codes) and VQGAN-EMA, continue to
grapple with challenges related to expanding the codebook size and enhancing
codebook utilization. For instance, VQGAN-FC is restricted to learning a
codebook with a maximum size of 16,384, maintaining a typically low utilization
rate of less than 12% on ImageNet. In this work, we propose a novel image
quantization model named VQGAN-LC (Large Codebook), which extends the codebook
size to 100,000, achieving an utilization rate exceeding 99%. Unlike previous
methods that optimize each codebook entry, our approach begins with a codebook
initialized with 100,000 features extracted by a pre-trained vision encoder.
Optimization then focuses on training a projector that aligns the entire
codebook with the feature distributions of the encoder in VQGAN-LC. We
demonstrate the superior performance of our model over its counterparts across
a variety of tasks, including image reconstruction, image classification,
auto-regressive image generation using GPT, and image creation with diffusion-
and flow-based generative models. Code and models are available at
https://github.com/zh460045050/VQGAN-LC.
]]></content:encoded>
<pubDate>2024-06-17T17:59:57Z</pubDate>
</item>
<item>
<title>Exploring the Role of Large Language Models in Prompt Encoding for
  Diffusion Models</title>
<link>http://arxiv.org/abs/2406.11831v1</link>
<guid>http://arxiv.org/abs/2406.11831v1</guid>
<content:encoded><![CDATA[
Large language models (LLMs) based on decoder-only transformers have
demonstrated superior text understanding capabilities compared to CLIP and
T5-series models. However, the paradigm for utilizing current advanced LLMs in
text-to-image diffusion models remains to be explored. We observed an unusual
phenomenon: directly using a large language model as the prompt encoder
significantly degrades the prompt-following ability in image generation. We
identified two main obstacles behind this issue. One is the misalignment
between the next token prediction training in LLM and the requirement for
discriminative prompt features in diffusion models. The other is the intrinsic
positional bias introduced by the decoder-only architecture. To deal with this
issue, we propose a novel framework to fully harness the capabilities of LLMs.
Through the carefully designed usage guidance, we effectively enhance the text
representation capability for prompt encoding and eliminate its inherent
positional bias. This allows us to integrate state-of-the-art LLMs into the
text-to-image generation model flexibly. Furthermore, we also provide an
effective manner to fuse multiple LLMs into our framework. Considering the
excellent performance and scaling capabilities demonstrated by the transformer
architecture, we further design an LLM-Infused Diffusion Transformer (LI-DiT)
based on the framework. We conduct extensive experiments to validate LI-DiT
across model size and data size. Benefiting from the inherent ability of the
LLMs and our innovative designs, the prompt understanding performance of LI-DiT
easily surpasses state-of-the-art open-source models as well as mainstream
closed-source commercial models including Stable Diffusion 3, DALL-E 3, and
Midjourney V6. The powerful LI-DiT-10B will be available after further
optimization and security checks.
]]></content:encoded>
<pubDate>2024-06-17T17:59:43Z</pubDate>
</item>
<item>
<title>VideoGUI: A Benchmark for GUI Automation from Instructional Videos</title>
<link>http://arxiv.org/abs/2406.10227v1</link>
<guid>http://arxiv.org/abs/2406.10227v1</guid>
<content:encoded><![CDATA[
<div> GUI automation, VideoGUI, multi-modal benchmark, visual-centric tasks, evaluation metrics
<br /><br />VideoGUIGUIGUIAdobe PhotoshopStable Diffusion WebUIVideoGUIGUIiiiiiiVideoGUIGPT4oGUI
<br /><br />: VideoGUIGUIGUIGUI <div>
Graphical User Interface (GUI) automation holds significant promise for
enhancing human productivity by assisting with computer tasks. Existing task
formulations primarily focus on simple tasks that can be specified by a single,
language-only instruction, such as "Insert a new slide." In this work, we
introduce VideoGUI, a novel multi-modal benchmark designed to evaluate GUI
assistants on visual-centric GUI tasks. Sourced from high-quality web
instructional videos, our benchmark focuses on tasks involving professional and
novel software (e.g., Adobe Photoshop or Stable Diffusion WebUI) and complex
activities (e.g., video editing). VideoGUI evaluates GUI assistants through a
hierarchical process, allowing for identification of the specific levels at
which they may fail: (i) high-level planning: reconstruct procedural subtasks
from visual conditions without language descriptions; (ii) middle-level
planning: generate sequences of precise action narrations based on visual state
(i.e., screenshot) and goals; (iii) atomic action execution: perform specific
actions such as accurately clicking designated elements. For each level, we
design evaluation metrics across individual dimensions to provide clear
signals, such as individual performance in clicking, dragging, typing, and
scrolling for atomic action execution. Our evaluation on VideoGUI reveals that
even the SoTA large multimodal model GPT4o performs poorly on visual-centric
GUI tasks, especially for high-level planning.
]]></content:encoded>
<pubDate>2024-06-14T17:59:08Z</pubDate>
</item>
<item>
<title>Alleviating Distortion in Image Generation via Multi-Resolution
  Diffusion Models</title>
<link>http://arxiv.org/abs/2406.09416v1</link>
<guid>http://arxiv.org/abs/2406.09416v1</guid>
<content:encoded><![CDATA[
<div> , , , , Transformer<br />
<br />
U-NetTransformerTransformerDiMRTD-LNImageNetImageNet 256x256512x512FID <br /><br />: <br />DiMRTD-LNTransformerImageNetFID <div>
This paper presents innovative enhancements to diffusion models by
integrating a novel multi-resolution network and time-dependent layer
normalization. Diffusion models have gained prominence for their effectiveness
in high-fidelity image generation. While conventional approaches rely on
convolutional U-Net architectures, recent Transformer-based designs have
demonstrated superior performance and scalability. However, Transformer
architectures, which tokenize input data (via "patchification"), face a
trade-off between visual fidelity and computational complexity due to the
quadratic nature of self-attention operations concerning token length. While
larger patch sizes enable attention computation efficiency, they struggle to
capture fine-grained visual details, leading to image distortions. To address
this challenge, we propose augmenting the Diffusion model with the
Multi-Resolution network (DiMR), a framework that refines features across
multiple resolutions, progressively enhancing detail from low to high
resolution. Additionally, we introduce Time-Dependent Layer Normalization
(TD-LN), a parameter-efficient approach that incorporates time-dependent
parameters into layer normalization to inject time information and achieve
superior performance. Our method's efficacy is demonstrated on the
class-conditional ImageNet generation benchmark, where DiMR-XL variants
outperform prior diffusion models, setting new state-of-the-art FID scores of
1.70 on ImageNet 256 x 256 and 2.89 on ImageNet 512 x 512. Project page:
https://qihao067.github.io/projects/DiMR
]]></content:encoded>
<pubDate>2024-06-13T17:59:58Z</pubDate>
</item>
<item>
<title>Explore the Limits of Omni-modal Pretraining at Scale</title>
<link>http://arxiv.org/abs/2406.09412v1</link>
<guid>http://arxiv.org/abs/2406.09412v1</guid>
<content:encoded><![CDATA[
<div> MiCoGitHub
<br /><br />
:
MiCo10251837https://github.com/invictus717/MiCo <div>
We propose to build omni-modal intelligence, which is capable of
understanding any modality and learning universal representations. In specific,
we propose a scalable pretraining paradigm, named Multimodal Context (MiCo),
which can scale up the numbers of modalities and amount of data, together with
the model parameters, in the pretraining process. With MiCo, the pretrained
models show significant emergent abilities in multimodal learning, which are
evaluated on the following tasks: i) single-modality perception benchmarks of
10 different modalities, ii) 25 cross-modality understanding tasks of
retrieval, question-answering, captioning, and iii) 18 multimodal large
language model benchmarks. Our models establish 37 new records for
state-of-the-art performance. We hope that our research could contribute to the
development of omni-modal intelligence. Code and Models are at
https://github.com/invictus717/MiCo
]]></content:encoded>
<pubDate>2024-06-13T17:59:53Z</pubDate>
</item>
<item>
<title>MuirBench: A Comprehensive Benchmark for Robust Multi-image
  Understanding</title>
<link>http://arxiv.org/abs/2406.09411v1</link>
<guid>http://arxiv.org/abs/2406.09411v1</guid>
<content:encoded><![CDATA[
We introduce MuirBench, a comprehensive benchmark that focuses on robust
multi-image understanding capabilities of multimodal LLMs. MuirBench consists
of 12 diverse multi-image tasks (e.g., scene understanding, ordering) that
involve 10 categories of multi-image relations (e.g., multiview, temporal
relations). Comprising 11,264 images and 2,600 multiple-choice questions,
MuirBench is created in a pairwise manner, where each standard instance is
paired with an unanswerable variant that has minimal semantic differences, in
order for a reliable assessment. Evaluated upon 20 recent multi-modal LLMs, our
results reveal that even the best-performing models like GPT-4o and Gemini Pro
find it challenging to solve MuirBench, achieving 68.0% and 49.3% in accuracy.
Open-source multimodal LLMs trained on single images can hardly generalize to
multi-image questions, hovering below 33.3% in accuracy. These results
highlight the importance of MuirBench in encouraging the community to develop
multimodal LLMs that can look beyond a single image, suggesting potential
pathways for future improvements.
]]></content:encoded>
<pubDate>2024-06-13T17:59:52Z</pubDate>
</item>
<item>
<title>Words Worth a Thousand Pictures: Measuring and Understanding Perceptual
  Variability in Text-to-Image Generation</title>
<link>http://arxiv.org/abs/2406.08482v1</link>
<guid>http://arxiv.org/abs/2406.08482v1</guid>
<content:encoded><![CDATA[
<div> W1KP<br />
W1KP1878%W1KPImagen10-50Stable Diffusion XLDALL-E 350-20056CLIPhttp://w1kp.com<br /><br />: W1KP <div>
Diffusion models are the state of the art in text-to-image generation, but
their perceptual variability remains understudied. In this paper, we examine
how prompts affect image variability in black-box diffusion-based models. We
propose W1KP, a human-calibrated measure of variability in a set of images,
bootstrapped from existing image-pair perceptual distances. Current datasets do
not cover recent diffusion models, thus we curate three test sets for
evaluation. Our best perceptual distance outperforms nine baselines by up to 18
points in accuracy, and our calibration matches graded human judgements 78% of
the time. Using W1KP, we study prompt reusability and show that Imagen prompts
can be reused for 10-50 random seeds before new images become too similar to
already generated images, while Stable Diffusion XL and DALL-E 3 can be reused
50-200 times. Lastly, we analyze 56 linguistic features of real prompts,
finding that the prompt's length, CLIP embedding norm, concreteness, and word
senses influence variability most. As far as we are aware, we are the first to
analyze diffusion variability from a visuolinguistic perspective. Our project
page is at http://w1kp.com
]]></content:encoded>
<pubDate>2024-06-12T17:59:27Z</pubDate>
</item>
<item>
<title>What If We Recaption Billions of Web Images with LLaMA-3?</title>
<link>http://arxiv.org/abs/2406.08478v1</link>
<guid>http://arxiv.org/abs/2406.08478v1</guid>
<content:encoded><![CDATA[
<div> LLaMA-3, -, , , 
<br /> 
LLaMA-3-Recap-DataComp-1B-CLIP-shotDiffusion Transformers -LLaMA-3
<br /><br />: 
LLaMA-3Recap-DataComp-1B <div>
Web-crawled image-text pairs are inherently noisy. Prior studies demonstrate
that semantically aligning and enriching textual descriptions of these pairs
can significantly enhance model training across various vision-language tasks,
particularly text-to-image generation. However, large-scale investigations in
this area remain predominantly closed-source. Our paper aims to bridge this
community effort, leveraging the powerful and \textit{open-sourced} LLaMA-3, a
GPT-4 level LLM. Our recaptioning pipeline is simple: first, we fine-tune a
LLaMA-3-8B powered LLaVA-1.5 and then employ it to recaption 1.3 billion images
from the DataComp-1B dataset. Our empirical results confirm that this enhanced
dataset, Recap-DataComp-1B, offers substantial benefits in training advanced
vision-language models. For discriminative models like CLIP, we observe
enhanced zero-shot performance in cross-modal retrieval tasks. For generative
models like text-to-image Diffusion Transformers, the generated images exhibit
a significant improvement in alignment with users' text instructions,
especially in following complex queries. Our project page is
https://www.haqtu.me/Recap-Datacomp-1B/
]]></content:encoded>
<pubDate>2024-06-12T17:59:07Z</pubDate>
</item>
<item>
<title>Commonsense-T2I Challenge: Can Text-to-Image Generation Models
  Understand Commonsense?</title>
<link>http://arxiv.org/abs/2406.07546v1</link>
<guid>http://arxiv.org/abs/2406.07546v1</guid>
<content:encoded><![CDATA[
<div> -<br />
<br />
Commonsense-T2I-DALL-E 3Commonsense-T2I48.92%Diffusion XL24.92%GPTCommonsense-T2IT2I<br /><br />: Commonsense-T2I-GPT-enriched <div>
We present a novel task and benchmark for evaluating the ability of
text-to-image(T2I) generation models to produce images that fit commonsense in
real life, which we call Commonsense-T2I. Given two adversarial text prompts
containing an identical set of action words with minor differences, such as "a
lightbulb without electricity" v.s. "a lightbulb with electricity", we evaluate
whether T2I models can conduct visual-commonsense reasoning, e.g. produce
images that fit "the lightbulb is unlit" vs. "the lightbulb is lit"
correspondingly. Commonsense-T2I presents an adversarial challenge, providing
pairwise text prompts along with expected outputs. The dataset is carefully
hand-curated by experts and annotated with fine-grained labels, such as
commonsense type and likelihood of the expected outputs, to assist analyzing
model behavior. We benchmark a variety of state-of-the-art (sota) T2I models
and surprisingly find that, there is still a large gap between image synthesis
and real life photos--even the DALL-E 3 model could only achieve 48.92% on
Commonsense-T2I, and the stable diffusion XL model only achieves 24.92%
accuracy. Our experiments show that GPT-enriched prompts cannot solve this
challenge, and we include a detailed analysis about possible reasons for such
deficiency. We aim for Commonsense-T2I to serve as a high-quality evaluation
benchmark for T2I commonsense checking, fostering advancements in real life
image generation.
]]></content:encoded>
<pubDate>2024-06-11T17:59:48Z</pubDate>
</item>
<item>
<title>Situational Awareness Matters in 3D Vision Language Reasoning</title>
<link>http://arxiv.org/abs/2406.07544v1</link>
<guid>http://arxiv.org/abs/2406.07544v1</guid>
<content:encoded><![CDATA[
<div> 3D vision language reasoning, household robots, embodied AI, SIG3D, situational awareness<br />
<br />
3DAISIG3DSQA3DScanQASIG3D30%3D<br /><br />: 3DSIG3DSIG3D3D <div>
Being able to carry out complicated vision language reasoning tasks in 3D
space represents a significant milestone in developing household robots and
human-centered embodied AI. In this work, we demonstrate that a critical and
distinct challenge in 3D vision language reasoning is situational awareness,
which incorporates two key components: (1) The autonomous agent grounds its
self-location based on a language prompt. (2) The agent answers open-ended
questions from the perspective of its calculated position. To address this
challenge, we introduce SIG3D, an end-to-end Situation-Grounded model for 3D
vision language reasoning. We tokenize the 3D scene into sparse voxel
representation and propose a language-grounded situation estimator, followed by
a situated question answering module. Experiments on the SQA3D and ScanQA
datasets show that SIG3D outperforms state-of-the-art models in situation
estimation and question answering by a large margin (e.g., an enhancement of
over 30% on situation estimation accuracy). Subsequent analysis corroborates
our architectural design choices, explores the distinct functions of visual and
textual tokens, and highlights the importance of situational awareness in the
domain of 3D question answering.
]]></content:encoded>
<pubDate>2024-06-11T17:59:45Z</pubDate>
</item>
<item>
<title>Cognitive Insights Across Languages: Enhancing Multimodal Interview
  Analysis</title>
<link>http://arxiv.org/abs/2406.07542v1</link>
<guid>http://arxiv.org/abs/2406.07542v1</guid>
<content:encoded><![CDATA[
Cognitive decline is a natural process that occurs as individuals age. Early
diagnosis of anomalous decline is crucial for initiating professional treatment
that can enhance the quality of life of those affected. To address this issue,
we propose a multimodal model capable of predicting Mild Cognitive Impairment
and cognitive scores. The TAUKADIAL dataset is used to conduct the evaluation,
which comprises audio recordings of clinical interviews. The proposed model
demonstrates the ability to transcribe and differentiate between languages used
in the interviews. Subsequently, the model extracts audio and text features,
combining them into a multimodal architecture to achieve robust and generalized
results. Our approach involves in-depth research to implement various features
obtained from the proposed modalities.
]]></content:encoded>
<pubDate>2024-06-11T17:59:31Z</pubDate>
</item>
<item>
<title>GaussianCity: Generative Gaussian Splatting for Unbounded 3D City
  Generation</title>
<link>http://arxiv.org/abs/2406.06526v1</link>
<guid>http://arxiv.org/abs/2406.06526v1</guid>
<content:encoded><![CDATA[
<div> GaussianCity, 3D city generation, NeRF, 3D-GS, unbounded 3D scenes

GaussianCity3D13DBEV-PointVRAM2BEV-Point3DPoint SerializerBEVGaussianCity3DCityDreamerGaussianCity6010.72 FPS0.18 FPS<br /><br />: GaussianCity 3D3D <div>
3D city generation with NeRF-based methods shows promising generation results
but is computationally inefficient. Recently 3D Gaussian Splatting (3D-GS) has
emerged as a highly efficient alternative for object-level 3D generation.
However, adapting 3D-GS from finite-scale 3D objects and humans to
infinite-scale 3D cities is non-trivial. Unbounded 3D city generation entails
significant storage overhead (out-of-memory issues), arising from the need to
expand points to billions, often demanding hundreds of Gigabytes of VRAM for a
city scene spanning 10km^2. In this paper, we propose GaussianCity, a
generative Gaussian Splatting framework dedicated to efficiently synthesizing
unbounded 3D cities with a single feed-forward pass. Our key insights are
two-fold: 1) Compact 3D Scene Representation: We introduce BEV-Point as a
highly compact intermediate representation, ensuring that the growth in VRAM
usage for unbounded scenes remains constant, thus enabling unbounded city
generation. 2) Spatial-aware Gaussian Attribute Decoder: We present
spatial-aware BEV-Point decoder to produce 3D Gaussian attributes, which
leverages Point Serializer to integrate the structural and contextual
characteristics of BEV points. Extensive experiments demonstrate that
GaussianCity achieves state-of-the-art results in both drone-view and
street-view 3D city generation. Notably, compared to CityDreamer, GaussianCity
exhibits superior performance with a speedup of 60 times (10.72 FPS v.s. 0.18
FPS).
]]></content:encoded>
<pubDate>2024-06-10T17:59:55Z</pubDate>
</item>
<item>
<title>Autoregressive Model Beats Diffusion: Llama for Scalable Image
  Generation</title>
<link>http://arxiv.org/abs/2406.06525v1</link>
<guid>http://arxiv.org/abs/2406.06525v1</guid>
<content:encoded><![CDATA[
<div> : LlamaGen, , , , <br />
: <br />LlamaGenLlama1160.94 rFIDImageNet97%21.1131ImageNet 256x2562.18 FIDLDMDiT37.75LAION-COCOLLM326% - 414% <div>
We introduce LlamaGen, a new family of image generation models that apply
original ``next-token prediction'' paradigm of large language models to visual
generation domain. It is an affirmative answer to whether vanilla
autoregressive models, e.g., Llama, without inductive biases on visual signals
can achieve state-of-the-art image generation performance if scaling properly.
We reexamine design spaces of image tokenizers, scalability properties of image
generation models, and their training data quality. The outcome of this
exploration consists of: (1) An image tokenizer with downsample ratio of 16,
reconstruction quality of 0.94 rFID and codebook usage of 97% on ImageNet
benchmark. (2) A series of class-conditional image generation models ranging
from 111M to 3.1B parameters, achieving 2.18 FID on ImageNet 256x256
benchmarks, outperforming the popular diffusion models such as LDM, DiT. (3) A
text-conditional image generation model with 775M parameters, from two-stage
training on LAION-COCO and high aesthetics quality images, demonstrating
competitive performance of visual quality and text alignment. (4) We verify the
effectiveness of LLM serving frameworks in optimizing the inference speed of
image generation models and achieve 326% - 414% speedup. We release all models
and codes to facilitate open-source community of visual generation and
multimodal foundation models.
]]></content:encoded>
<pubDate>2024-06-10T17:59:52Z</pubDate>
</item>
<item>
<title>3D-GRAND: Towards Better Grounding and Less Hallucination for 3D-LLMs</title>
<link>http://arxiv.org/abs/2406.05132v1</link>
<guid>http://arxiv.org/abs/2406.05132v1</guid>
<content:encoded><![CDATA[
<div> : , 3D, , 3D-LLMs, <br />
:<br />
3D-3D-GRAND3D-LLMs3D-POPE3D-LLMs3D-LLM3D3D-GRAND3D-POPE3D-LLMs <div>
The integration of language and 3D perception is crucial for developing
embodied agents and robots that comprehend and interact with the physical
world. While large language models (LLMs) have demonstrated impressive language
understanding and generation capabilities, their adaptation to 3D environments
(3D-LLMs) remains in its early stages. A primary challenge is the absence of
large-scale datasets that provide dense grounding between language and 3D
scenes. In this paper, we introduce 3D-GRAND, a pioneering large-scale dataset
comprising 40,087 household scenes paired with 6.2 million densely-grounded
scene-language instructions. Our results show that instruction tuning with
3D-GRAND significantly enhances grounding capabilities and reduces
hallucinations in 3D-LLMs. As part of our contributions, we propose a
comprehensive benchmark 3D-POPE to systematically evaluate hallucination in
3D-LLMs, enabling fair comparisons among future models. Our experiments
highlight a scaling effect between dataset size and 3D-LLM performance,
emphasizing the critical role of large-scale 3D-text datasets in advancing
embodied AI research. Notably, our results demonstrate early signals for
effective sim-to-real transfer, indicating that models trained on large
synthetic data can perform well on real-world 3D scans. Through 3D-GRAND and
3D-POPE, we aim to equip the embodied AI community with essential resources and
insights, setting the stage for more reliable and better-grounded 3D-LLMs.
Project website: https://3d-grand.github.io
]]></content:encoded>
<pubDate>2024-06-07T17:59:59Z</pubDate>
</item>
<item>
<title>An Empirical Study on Parameter-Efficient Fine-Tuning for MultiModal
  Large Language Models</title>
<link>http://arxiv.org/abs/2406.05130v1</link>
<guid>http://arxiv.org/abs/2406.05130v1</guid>
<content:encoded><![CDATA[
<div> -efficient fine-tuning, MLLM, PEFT methods, multimodal instruction datasets, 

-efficient fine-tuningPEFTMLLMsMLLMsLLMPEFTPEFTPEFTMLLMadapterPEFTMLLMPEFTMLLMadapter <div>
Multimodal large language models (MLLMs) fine-tuned with multimodal
instruction datasets have demonstrated remarkable capabilities in multimodal
tasks. However, fine-tuning all parameters of MLLMs has become challenging as
they usually contain billions of parameters. To address this issue, we study
parameter-efficient fine-tuning (PEFT) methods for MLLMs. We aim to identify
effective methods for enhancing the performance of MLLMs in scenarios where
only a limited number of parameters are trained. This paper conducts empirical
studies using four popular PEFT methods to fine-tune the LLM component of
open-source MLLMs. We present a comprehensive analysis that encompasses various
aspects, including the impact of PEFT methods on various models, parameters and
location of the PEFT module, size of fine-tuning data, model stability based on
PEFT methods, MLLM's generalization, and hallucination. We evaluated four PEFT
methods on seven datasets from two different categories: unseen and seen
datasets. Across all experiments, we show that the adapter is the
best-performing PEFT method. At the same time, fine-tuning the connector layers
leads to improved performance in most MLLMs. Code and data are available at
https://github.com/alenai97/PEFT-MLLM.git.
]]></content:encoded>
<pubDate>2024-06-07T17:58:11Z</pubDate>
</item>
<item>
<title>Physics3D: Learning Physical Properties of 3D Gaussians via Video
  Diffusion</title>
<link>http://arxiv.org/abs/2406.04338v2</link>
<guid>http://arxiv.org/abs/2406.04338v2</guid>
<content:encoded><![CDATA[
<div> 3D

<br /><br />: 
3D3D3DPhysics3D3DPhysics3D <div>
In recent years, there has been rapid development in 3D generation models,
opening up new possibilities for applications such as simulating the dynamic
movements of 3D objects and customizing their behaviors. However, current 3D
generative models tend to focus only on surface features such as color and
shape, neglecting the inherent physical properties that govern the behavior of
objects in the real world. To accurately simulate physics-aligned dynamics, it
is essential to predict the physical properties of materials and incorporate
them into the behavior prediction process. Nonetheless, predicting the diverse
materials of real-world objects is still challenging due to the complex nature
of their physical attributes. In this paper, we propose \textbf{Physics3D}, a
novel method for learning various physical properties of 3D objects through a
video diffusion model. Our approach involves designing a highly generalizable
physical simulation system based on a viscoelastic material model, which
enables us to simulate a wide range of materials with high-fidelity
capabilities. Moreover, we distill the physical priors from a video diffusion
model that contains more understanding of realistic object materials. Extensive
experiments demonstrate the effectiveness of our method with both elastic and
plastic materials. Physics3D shows great potential for bridging the gap between
the physical world and virtual neural space, providing a better integration and
application of realistic physical principles in virtual environments. Project
page: https://liuff19.github.io/Physics3D.
]]></content:encoded>
<pubDate>2024-06-07T01:30:11Z</pubDate>
</item>
<item>
<title>Physics3D: Learning Physical Properties of 3D Gaussians via Video
  Diffusion</title>
<link>http://arxiv.org/abs/2406.04338v1</link>
<guid>http://arxiv.org/abs/2406.04338v1</guid>
<content:encoded><![CDATA[
<div> 3D<br />
:<br />
3D3DPhysics3D <div>
In recent years, there has been rapid development in 3D generation models,
opening up new possibilities for applications such as simulating the dynamic
movements of 3D objects and customizing their behaviors. However, current 3D
generative models tend to focus only on surface features such as color and
shape, neglecting the inherent physical properties that govern the behavior of
objects in the real world. To accurately simulate physics-aligned dynamics, it
is essential to predict the physical properties of materials and incorporate
them into the behavior prediction process. Nonetheless, predicting the diverse
materials of real-world objects is still challenging due to the complex nature
of their physical attributes. In this paper, we propose \textbf{Physics3D}, a
novel method for learning various physical properties of 3D objects through a
video diffusion model. Our approach involves designing a highly generalizable
physical simulation system based on a viscoelastic material model, which
enables us to simulate a wide range of materials with high-fidelity
capabilities. Moreover, we distill the physical priors from a video diffusion
model that contains more understanding of realistic object materials. Extensive
experiments demonstrate the effectiveness of our method with both elastic and
plastic materials. Physics3D shows great potential for bridging the gap between
the physical world and virtual neural space, providing a better integration and
application of realistic physical principles in virtual environments. Project
page: https://liuff19.github.io/Physics3D.
]]></content:encoded>
<pubDate>2024-06-06T17:59:47Z</pubDate>
</item>
<item>
<title>Coherent Zero-Shot Visual Instruction Generation</title>
<link>http://arxiv.org/abs/2406.04337v1</link>
<guid>http://arxiv.org/abs/2406.04337v1</guid>
<content:encoded><![CDATA[
<div> : , , , , 

<br /><br />:  <div>
Despite the advances in text-to-image synthesis, particularly with diffusion
models, generating visual instructions that require consistent representation
and smooth state transitions of objects across sequential steps remains a
formidable challenge. This paper introduces a simple, training-free framework
to tackle the issues, capitalizing on the advancements in diffusion models and
large language models (LLMs). Our approach systematically integrates text
comprehension and image generation to ensure visual instructions are visually
appealing and maintain consistency and accuracy throughout the instruction
sequence. We validate the effectiveness by testing multi-step instructions and
comparing the text alignment and consistency with several baselines. Our
experiments show that our approach can visualize coherent and visually pleasing
instructions
]]></content:encoded>
<pubDate>2024-06-06T17:59:44Z</pubDate>
</item>
<item>
<title>Wings: Learning Multimodal LLMs without Text-only Forgetting</title>
<link>http://arxiv.org/abs/2406.03496v1</link>
<guid>http://arxiv.org/abs/2406.03496v1</guid>
<content:encoded><![CDATA[
<div> Wings, MLLMs, attention, text-only forgetting, multimodal comprehension<br />
<br />
WingsMLLMMLLMLoRRAWingsMLLMIITWings<br /><br />:WingsMLLMMLLMWingsMLLM <div>
Multimodal large language models (MLLMs), initiated with a trained LLM, first
align images with text and then fine-tune on multimodal mixed inputs. However,
the MLLM catastrophically forgets the text-only instructions, which do not
include images and can be addressed within the initial LLM. In this paper, we
present Wings, a novel MLLM that excels in both text-only dialogues and
multimodal comprehension. Analyzing MLLM attention in multimodal instructions
reveals that text-only forgetting is related to the attention shifts from
pre-image to post-image text. From that, we construct extra modules that act as
the boosted learner to compensate for the attention shift. The complementary
visual and textual learners, like "wings" on either side, are connected in
parallel within each layer's attention block. Initially, image and text inputs
are aligned with visual learners operating alongside the main attention,
balancing focus on visual elements. Textual learners are later collaboratively
integrated with attention-based routing to blend the outputs of the visual and
textual learners. We design the Low-Rank Residual Attention (LoRRA) to
guarantee high efficiency for learners. Our experimental results demonstrate
that Wings outperforms equally-scaled MLLMs in both text-only and visual
question-answering tasks. On a newly constructed Interleaved Image-Text (IIT)
benchmark, Wings exhibits superior performance from text-only-rich to
multimodal-rich question-answering tasks.
]]></content:encoded>
<pubDate>2024-06-05T17:59:40Z</pubDate>
</item>
<item>
<title>Leveraging Visual Tokens for Extended Text Contexts in Multi-Modal
  Learning</title>
<link>http://arxiv.org/abs/2406.02547v1</link>
<guid>http://arxiv.org/abs/2406.02547v1</guid>
<content:encoded><![CDATA[
<div> GPU<br />
FLOPs<br />
VisInContextGPUFLOPs<br />
VisInContext<br />
VisInContext

<br /><br />:
VisInContextGPUFLOPs2562048FLOPsVisInContext <div>
Training models with longer in-context lengths is a significant challenge for
multimodal model due to substantial GPU memory and computational costs. This
exploratory study does not present state-of-the-art models; rather, it
introduces an innovative method designed to increase in-context text length in
multi-modality large language models (MLLMs) efficiently. We present Visualized
In-Context Text Processing (VisInContext), which processes long in-context text
using visual tokens. This technique significantly reduces GPU memory usage and
floating point operations (FLOPs) for both training and inferenceing stage. For
instance, our method expands the pre-training in-context text length from 256
to 2048 tokens with nearly same FLOPs for a 56 billion parameter MOE model.
Experimental results demonstrate that model trained with VisInContext delivers
superior performance on common downstream benchmarks for in-context few-shot
evaluation. Additionally, VisInContext is complementary to existing methods for
increasing in-context text length and enhances document understanding
capabilities, showing great potential in document QA tasks and sequential
document retrieval.
]]></content:encoded>
<pubDate>2024-06-04T17:59:25Z</pubDate>
</item>
<item>
<title>Robust and highly scalable estimation of directional couplings from
  time-shifted signals</title>
<link>http://arxiv.org/abs/2406.02545v1</link>
<guid>http://arxiv.org/abs/2406.02545v1</guid>
<content:encoded><![CDATA[
<div> 

KL lossDCM <br /><br />:  <div>
The estimation of directed couplings between the nodes of a network from
indirect measurements is a central methodological challenge in scientific
fields such as neuroscience, systems biology and economics. Unfortunately, the
problem is generally ill-posed due to the possible presence of unknown delays
in the measurements. In this paper, we offer a solution of this problem by
using a variational Bayes framework, where the uncertainty over the delays is
marginalized in order to obtain conservative coupling estimates. To overcome
the well-known overconfidence of classical variational methods, we use a
hybrid-VI scheme where the (possibly flat or multimodal) posterior over the
measurement parameters is estimated using a forward KL loss while the (nearly
convex) conditional posterior over the couplings is estimated using the highly
scalable gradient-based VI. In our ground-truth experiments, we show that the
network provides reliable and conservative estimates of the couplings, greatly
outperforming similar methods such as regression DCM.
]]></content:encoded>
<pubDate>2024-06-04T17:58:33Z</pubDate>
</item>
<item>
<title>ViDiT-Q: Efficient and Accurate Quantization of Diffusion Transformers
  for Image and Video Generation</title>
<link>http://arxiv.org/abs/2406.02540v1</link>
<guid>http://arxiv.org/abs/2406.02540v1</guid>
<content:encoded><![CDATA[
Diffusion transformers (DiTs) have exhibited remarkable performance in visual
generation tasks, such as generating realistic images or videos based on
textual instructions. However, larger model sizes and multi-frame processing
for video generation lead to increased computational and memory costs, posing
challenges for practical deployment on edge devices. Post-Training Quantization
(PTQ) is an effective method for reducing memory costs and computational
complexity. When quantizing diffusion transformers, we find that applying
existing diffusion quantization methods designed for U-Net faces challenges in
preserving quality. After analyzing the major challenges for quantizing
diffusion transformers, we design an improved quantization scheme: "ViDiT-Q":
Video and Image Diffusion Transformer Quantization) to address these issues.
Furthermore, we identify highly sensitive layers and timesteps hinder
quantization for lower bit-widths. To tackle this, we improve ViDiT-Q with a
novel metric-decoupled mixed-precision quantization method (ViDiT-Q-MP). We
validate the effectiveness of ViDiT-Q across a variety of text-to-image and
video models. While baseline quantization methods fail at W8A8 and produce
unreadable content at W4A8, ViDiT-Q achieves lossless W8A8 quantization.
ViDiTQ-MP achieves W4A8 with negligible visual quality degradation, resulting
in a 2.5x memory optimization and a 1.5x latency speedup.
]]></content:encoded>
<pubDate>2024-06-04T17:57:10Z</pubDate>
</item>
<item>
<title>RapVerse: Coherent Vocals and Whole-Body Motions Generations from Text</title>
<link>http://arxiv.org/abs/2405.20336v1</link>
<guid>http://arxiv.org/abs/2405.20336v1</guid>
<content:encoded><![CDATA[
<div> : 3D holistic body motions, singing vocals, RapVerse dataset, autoregressive multimodal transformers, vector-quantized variational autoencoder

RapVerse- https://vis-www.cs.umass.edu/RapVerse<br /><br />: RapVerse3D <div>
In this work, we introduce a challenging task for simultaneously generating
3D holistic body motions and singing vocals directly from textual lyrics
inputs, advancing beyond existing works that typically address these two
modalities in isolation. To facilitate this, we first collect the RapVerse
dataset, a large dataset containing synchronous rapping vocals, lyrics, and
high-quality 3D holistic body meshes. With the RapVerse dataset, we investigate
the extent to which scaling autoregressive multimodal transformers across
language, audio, and motion can enhance the coherent and realistic generation
of vocals and whole-body human motions. For modality unification, a
vector-quantized variational autoencoder is employed to encode whole-body
motion sequences into discrete motion tokens, while a vocal-to-unit model is
leveraged to obtain quantized audio tokens preserving content, prosodic
information, and singer identity. By jointly performing transformer modeling on
these three modalities in a unified way, our framework ensures a seamless and
realistic blend of vocals and human motions. Extensive experiments demonstrate
that our unified generation framework not only produces coherent and realistic
singing vocals alongside human motions directly from textual inputs but also
rivals the performance of specialized single-modality generation systems,
establishing new benchmarks for joint vocal-motion generation. The project page
is available for research purposes at https://vis-www.cs.umass.edu/RapVerse.
]]></content:encoded>
<pubDate>2024-05-30T17:59:39Z</pubDate>
</item>
<item>
<title>LLMs Meet Multimodal Generation and Editing: A Survey</title>
<link>http://arxiv.org/abs/2405.19334v1</link>
<guid>http://arxiv.org/abs/2405.19334v1</guid>
<content:encoded><![CDATA[
<div> <br />
3DAIAIGC <div>
With the recent advancement in large language models (LLMs), there is a
growing interest in combining LLMs with multimodal learning. Previous surveys
of multimodal large language models (MLLMs) mainly focus on understanding. This
survey elaborates on multimodal generation across different domains, including
image, video, 3D, and audio, where we highlight the notable advancements with
milestone works in these fields. Specifically, we exhaustively investigate the
key technical components behind methods and multimodal datasets utilized in
these studies. Moreover, we dig into tool-augmented multimodal agents that can
use existing generative models for human-computer interaction. Lastly, we also
comprehensively discuss the advancement in AI safety and investigate emerging
applications as well as future prospects. Our work provides a systematic and
insightful overview of multimodal generation, which is expected to advance the
development of Artificial Intelligence for Generative Content (AIGC) and world
models. A curated list of all related papers can be found at
https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation
]]></content:encoded>
<pubDate>2024-05-29T17:59:20Z</pubDate>
</item>
<item>
<title>Multi-Modal Generative Embedding Model</title>
<link>http://arxiv.org/abs/2405.19333v1</link>
<guid>http://arxiv.org/abs/2405.19333v1</guid>
<content:encoded><![CDATA[
<div> Multi-Modal Generative Embedding Model (MM-GEM)PoolAggregatorViT-Large

(MM-GEM)PoolAggregatorMM-GEMMM-GEMMM-GEM5% <br /><br />: (MM-GEM)PoolAggregator <div>
Most multi-modal tasks can be formulated into problems of either generation
or embedding. Existing models usually tackle these two types of problems by
decoupling language modules into a text decoder for generation, and a text
encoder for embedding. To explore the minimalism of multi-modal paradigms, we
attempt to achieve only one model per modality in this work. We propose a
Multi-Modal Generative Embedding Model (MM-GEM), whereby the generative and
embedding objectives are encapsulated in one Large Language Model. We also
propose a PoolAggregator to boost efficiency and enable the ability of
fine-grained embedding and generation. A surprising finding is that these two
objectives do not significantly conflict with each other. For example, MM-GEM
instantiated from ViT-Large and TinyLlama shows competitive performance on
benchmarks for multimodal embedding models such as cross-modal retrieval and
zero-shot classification, while has good ability of image captioning.
Additionally, MM-GEM can seamlessly execute region-level image caption
generation and retrieval tasks. Besides, the advanced text model in MM-GEM
brings over 5% improvement in Recall@1 for long text and image retrieval.
]]></content:encoded>
<pubDate>2024-05-29T17:59:10Z</pubDate>
</item>
<item>
<title>Normative Modules: A Generative Agent Architecture for Learning Norms
  that Supports Multi-Agent Cooperation</title>
<link>http://arxiv.org/abs/2405.19328v1</link>
<guid>http://arxiv.org/abs/2405.19328v1</guid>
<content:encoded><![CDATA[
Generative agents, which implement behaviors using a large language model
(LLM) to interpret and evaluate an environment, has demonstrated the capacity
to solve complex tasks across many social and technological domains. However,
when these agents interact with other agents and humans in presence of social
structures such as existing norms, fostering cooperation between them is a
fundamental challenge. In this paper, we develop the framework of a 'Normative
Module': an architecture designed to enhance cooperation by enabling agents to
recognize and adapt to the normative infrastructure of a given environment. We
focus on the equilibrium selection aspect of the cooperation problem and inform
our agent design based on the existence of classification institutions that
implement correlated equilibrium to provide effective resolution of the
equilibrium selection problem. Specifically, the normative module enables
agents to learn through peer interactions which of multiple candidate
institutions in the environment, does a group treat as authoritative. By
enabling normative competence in this sense, agents gain ability to coordinate
their sanctioning behaviour; coordinated sanctioning behaviour in turn shapes
primary behaviour within a social environment, leading to higher average
welfare. We design a new environment that supports institutions and evaluate
the proposed framework based on two key criteria derived from agent
interactions with peers and institutions: (i) the agent's ability to disregard
non-authoritative institutions and (ii) the agent's ability to identify
authoritative institutions among several options. We show that these
capabilities allow the agent to achieve more stable cooperative outcomes
compared to baseline agents without the normative module, paving the way for
research in a new avenue of designing environments and agents that account for
normative infrastructure.
]]></content:encoded>
<pubDate>2024-05-29T17:57:30Z</pubDate>
</item>
<item>
<title>Hierarchical World Models as Visual Whole-Body Humanoid Controllers</title>
<link>http://arxiv.org/abs/2405.18418v1</link>
<guid>http://arxiv.org/abs/2405.18418v1</guid>
<content:encoded><![CDATA[
<div> <br />8https://nicklashansen.com/rlpuppeteer <br /><br />:  <div>
Whole-body control for humanoids is challenging due to the high-dimensional
nature of the problem, coupled with the inherent instability of a bipedal
morphology. Learning from visual observations further exacerbates this
difficulty. In this work, we explore highly data-driven approaches to visual
whole-body humanoid control based on reinforcement learning, without any
simplifying assumptions, reward design, or skill primitives. Specifically, we
propose a hierarchical world model in which a high-level agent generates
commands based on visual observations for a low-level agent to execute, both of
which are trained with rewards. Our approach produces highly performant control
policies in 8 tasks with a simulated 56-DoF humanoid, while synthesizing
motions that are broadly preferred by humans. Code and videos:
https://nicklashansen.com/rlpuppeteer
]]></content:encoded>
<pubDate>2024-05-28T17:57:23Z</pubDate>
</item>
<item>
<title>Reason3D: Searching and Reasoning 3D Segmentation via Large Language
  Model</title>
<link>http://arxiv.org/abs/2405.17427v1</link>
<guid>http://arxiv.org/abs/2405.17427v1</guid>
<content:encoded><![CDATA[
<div> 3DReason3D<br />
Reason3D3DReason3D3DReason3DScanNetMatterport3D3D3D3Dhttps://github.com/KuanchihHuang/Reason3D<br /><br />: Reason3D3DReason3D3DReason3D <div>
Recent advancements in multimodal large language models (LLMs) have shown
their potential in various domains, especially concept reasoning. Despite these
developments, applications in understanding 3D environments remain limited.
This paper introduces Reason3D, a novel LLM designed for comprehensive 3D
understanding. Reason3D takes point cloud data and text prompts as input to
produce textual responses and segmentation masks, facilitating advanced tasks
like 3D reasoning segmentation, hierarchical searching, express referring, and
question answering with detailed mask outputs. Specifically, we propose a
hierarchical mask decoder to locate small objects within expansive scenes. This
decoder initially generates a coarse location estimate covering the object's
general area. This foundational estimation facilitates a detailed,
coarse-to-fine segmentation strategy that significantly enhances the precision
of object identification and segmentation. Experiments validate that Reason3D
achieves remarkable results on large-scale ScanNet and Matterport3D datasets
for 3D express referring, 3D question answering, and 3D reasoning segmentation
tasks. Code and models are available at:
https://github.com/KuanchihHuang/Reason3D.
]]></content:encoded>
<pubDate>2024-05-27T17:59:41Z</pubDate>
</item>
<item>
<title>LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence</title>
<link>http://arxiv.org/abs/2405.17424v1</link>
<guid>http://arxiv.org/abs/2405.17424v1</guid>
<content:encoded><![CDATA[
<div> Minecraft

:<br />
LARMLARMLARMMinecraftLARM6.8LARM <div>
Due to the need to interact with the real world, embodied agents are required
to possess comprehensive prior knowledge, long-horizon planning capability, and
a swift response speed. Despite recent large language model (LLM) based agents
achieving promising performance, they still exhibit several limitations. For
instance, the output of LLMs is a descriptive sentence, which is ambiguous when
determining specific actions. To address these limitations, we introduce the
large auto-regressive model (LARM). LARM leverages both text and multi-view
images as input and predicts subsequent actions in an auto-regressive manner.
To train LARM, we develop a novel data format named auto-regressive node
transmission structure and assemble a corresponding dataset. Adopting a
two-phase training regimen, LARM successfully harvests enchanted equipment in
Minecraft, which demands significantly more complex decision-making chains than
the highest achievements of prior best methods. Besides, the speed of LARM is
6.8x faster.
]]></content:encoded>
<pubDate>2024-05-27T17:59:32Z</pubDate>
</item>
<item>
<title>Improved Distribution Matching Distillation for Fast Image Synthesis</title>
<link>http://arxiv.org/abs/2405.14867v1</link>
<guid>http://arxiv.org/abs/2405.14867v1</guid>
<content:encoded><![CDATA[
<div> Distribution Matching Distillation, DMD, regression loss, GAN loss, multi-step sampling <br />
: 
DMD2DMDDMDGAN-500FIDImageNet-64x641.28COCO 20148.35SDXL <div>
Recent approaches have shown promises distilling diffusion models into
efficient one-step generators. Among them, Distribution Matching Distillation
(DMD) produces one-step generators that match their teacher in distribution,
without enforcing a one-to-one correspondence with the sampling trajectories of
their teachers. However, to ensure stable training, DMD requires an additional
regression loss computed using a large set of noise-image pairs generated by
the teacher with many steps of a deterministic sampler. This is costly for
large-scale text-to-image synthesis and limits the student's quality, tying it
too closely to the teacher's original sampling paths. We introduce DMD2, a set
of techniques that lift this limitation and improve DMD training. First, we
eliminate the regression loss and the need for expensive dataset construction.
We show that the resulting instability is due to the fake critic not estimating
the distribution of generated samples accurately and propose a two time-scale
update rule as a remedy. Second, we integrate a GAN loss into the distillation
procedure, discriminating between generated samples and real images. This lets
us train the student model on real data, mitigating the imperfect real score
estimation from the teacher model, and enhancing quality. Lastly, we modify the
training procedure to enable multi-step sampling. We identify and address the
training-inference input mismatch problem in this setting, by simulating
inference-time generator samples during training time. Taken together, our
improvements set new benchmarks in one-step image generation, with FID scores
of 1.28 on ImageNet-64x64 and 8.35 on zero-shot COCO 2014, surpassing the
original teacher despite a 500X reduction in inference cost. Further, we show
our approach can generate megapixel images by distilling SDXL, demonstrating
exceptional visual quality among few-step methods.
]]></content:encoded>
<pubDate>2024-05-23T17:59:49Z</pubDate>
</item>
<item>
<title>Comprehensive Multimodal Deep Learning Survival Prediction Enabled by a
  Transformer Architecture: A Multicenter Study in Glioblastoma</title>
<link>http://arxiv.org/abs/2405.12963v1</link>
<guid>http://arxiv.org/abs/2405.12963v1</guid>
<content:encoded><![CDATA[
<div> , , , , <br />
<br />
transformerMRI3D-CNNtransformer <br /><br />: <br /><br />transformer<br />3D-CNN<br />transformer <div>
Background: This research aims to improve glioblastoma survival prediction by
integrating MR images, clinical and molecular-pathologic data in a
transformer-based deep learning model, addressing data heterogeneity and
performance generalizability. Method: We propose and evaluate a
transformer-based non-linear and non-proportional survival prediction model.
The model employs self-supervised learning techniques to effectively encode the
high-dimensional MRI input for integration with non-imaging data using
cross-attention. To demonstrate model generalizability, the model is assessed
with the time-dependent concordance index (Cdt) in two training setups using
three independent public test sets: UPenn-GBM, UCSF-PDGM, and RHUH-GBM, each
comprising 378, 366, and 36 cases, respectively. Results: The proposed
transformer model achieved promising performance for imaging as well as
non-imaging data, effectively integrating both modalities for enhanced
performance (UPenn-GBM test-set, imaging Cdt 0.645, multimodal Cdt 0.707) while
outperforming state-of-the-art late-fusion 3D-CNN-based models. Consistent
performance was observed across the three independent multicenter test sets
with Cdt values of 0.707 (UPenn-GBM, internal test set), 0.672 (UCSF-PDGM,
first external test set) and 0.618 (RHUH-GBM, second external test set). The
model achieved significant discrimination between patients with favorable and
unfavorable survival for all three datasets (logrank p 1.9\times{10}^{-8},
9.7\times{10}^{-3}, and 1.2\times{10}^{-2}). Conclusions: The proposed
transformer-based survival prediction model integrates complementary
information from diverse input modalities, contributing to improved
glioblastoma survival prediction compared to state-of-the-art methods.
Consistent performance was observed across institutions supporting model
generalizability.
]]></content:encoded>
<pubDate>2024-05-21T17:44:48Z</pubDate>
</item>
<item>
<title>Adapting Large Multimodal Models to Distribution Shifts: The Role of
  In-Context Learning</title>
<link>http://arxiv.org/abs/2405.12217v1</link>
<guid>http://arxiv.org/abs/2405.12217v1</guid>
<content:encoded><![CDATA[
<div> LMMs, adaptability, in-context learning, TopKNearestPR, InvariantSelectPR
LMM(ICL)LMMICLTopKNearestPRInvariantSelectPRClass-conditioned Contrastive InvarianceCCIInvariantSelectPRLMM: LMMICLLMMTopKNearestPRInvariantSelectPRCCILMM <div>
Recent studies indicate that large multimodal models (LMMs) are highly robust
against natural distribution shifts, often surpassing previous baselines.
Despite this, domain-specific adaptation is still necessary, particularly in
specialized areas like healthcare. Due to the impracticality of fine-tuning
LMMs given their vast parameter space, this work investigates in-context
learning (ICL) as an effective alternative for enhancing LMMs' adaptability. We
find that the success of ICL heavily relies on the choice of demonstration,
mirroring challenges seen in large language models but introducing unique
complexities for LMMs facing distribution shifts. Our study addresses this by
evaluating an unsupervised ICL method, TopKNearestPR, which selects in-context
examples through a nearest example search based on feature similarity. We
uncover that its effectiveness is limited by the deficiencies of pre-trained
vision encoders under distribution shift scenarios. To address these
challenges, we propose InvariantSelectPR, a novel method leveraging
Class-conditioned Contrastive Invariance (CCI) for more robust demonstration
selection. Specifically, CCI enhances pre-trained vision encoders by improving
their discriminative capabilities across different classes and ensuring
invariance to domain-specific variations. This enhancement allows the encoders
to effectively identify and retrieve the most informative examples, which are
then used to guide LMMs in adapting to new query samples under varying
distributions. Our experiments show that InvariantSelectPR substantially
improves the adaptability of LMMs, achieving significant performance gains on
benchmark datasets, with a 34.2%$\uparrow$ accuracy increase in 7-shot on
Camelyon17 and 16.9%$\uparrow$ increase in 7-shot on HAM10000 compared to the
baseline zero-shot performance.
]]></content:encoded>
<pubDate>2024-05-20T17:59:21Z</pubDate>
</item>
<item>
<title>Observational Scaling Laws and the Predictability of Language Model
  Performance</title>
<link>http://arxiv.org/abs/2405.10938v1</link>
<guid>http://arxiv.org/abs/2405.10938v1</guid>
<content:encoded><![CDATA[
<div> scaling laws, language model performance, observational approach, training compute efficiencies, predictability<br />
<br />:<br />80S <div>
Understanding how language model performance varies with scale is critical to
benchmark and algorithm development. Scaling laws are one approach to building
this understanding, but the requirement of training models across many
different scales has limited their use. We propose an alternative,
observational approach that bypasses model training and instead builds scaling
laws from ~80 publically available models. Building a single scaling law from
multiple model families is challenging due to large variations in their
training compute efficiencies and capabilities. However, we show that these
variations are consistent with a simple, generalized scaling law where language
model performance is a function of a low-dimensional capability space, and
model families only vary in their efficiency in converting training compute to
capabilities. Using this approach, we show the surprising predictability of
complex scaling phenomena: we show that several emergent phenomena follow a
smooth, sigmoidal behavior and are predictable from small models; we show that
the agent performance of models such as GPT-4 can be precisely predicted from
simpler non-agentic benchmarks; and we show how to predict the impact of
post-training interventions like Chain-of-Thought and Self-Consistency as
language model capabilities continue to improve.
]]></content:encoded>
<pubDate>2024-05-17T17:49:44Z</pubDate>
</item>
<item>
<title>CinePile: A Long Video Question Answering Dataset and Benchmark</title>
<link>http://arxiv.org/abs/2405.08813v1</link>
<guid>http://arxiv.org/abs/2405.08813v1</guid>
<content:encoded><![CDATA[
<div> CinePile-LLMs
<br /><br />
CinePile30.5LLMs-LLMsLLMshttps://hf.co/datasets/tomg-group-umd/cinepile
<br /><br />: CinePileLLMs <div>
Current datasets for long-form video understanding often fall short of
providing genuine long-form comprehension challenges, as many tasks derived
from these datasets can be successfully tackled by analyzing just one or a few
random frames from a video. To address this issue, we present a novel dataset
and benchmark, CinePile, specifically designed for authentic long-form video
understanding. This paper details our innovative approach for creating a
question-answer dataset, utilizing advanced LLMs with human-in-the-loop and
building upon human-generated raw data. Our comprehensive dataset comprises
305,000 multiple-choice questions (MCQs), covering various visual and
multimodal aspects, including temporal comprehension, understanding
human-object interactions, and reasoning about events or actions within a
scene. Additionally, we evaluate recent video-centric LLMs, both open-source
and proprietary, on the test split of our dataset. The findings reveal that
even state-of-the-art video-centric LLMs significantly lag behind human
performance in these tasks, highlighting the complexity and challenge inherent
in video understanding. The dataset is available at
https://hf.co/datasets/tomg-group-umd/cinepile
]]></content:encoded>
<pubDate>2024-05-14T17:59:02Z</pubDate>
</item>
<item>
<title>SciFIBench: Benchmarking Large Multimodal Models for Scientific Figure
  Interpretation</title>
<link>http://arxiv.org/abs/2405.08807v1</link>
<guid>http://arxiv.org/abs/2405.08807v1</guid>
<content:encoded><![CDATA[
<div> , , , , CS arXiv paper

SciFIBench10001226SciFIBench <br /><br />: <div>
Large multimodal models (LMMs) have proven flexible and generalisable across
many tasks and fields. Although they have strong potential to aid scientific
research, their capabilities in this domain are not well characterised. A key
aspect of scientific research is the ability to understand and interpret
figures, which serve as a rich, compressed source of complex information. In
this work, we present SciFIBench, a scientific figure interpretation benchmark.
Our main benchmark consists of a 1000-question gold set of multiple-choice
questions split between two tasks across 12 categories. The questions are
curated from CS arXiv paper figures and captions, using adversarial filtering
to find hard negatives and human verification for quality control. We evaluate
26 LMMs on SciFIBench, finding it to be a challenging benchmark. Finally, we
investigate the alignment and reasoning faithfulness of the LMMs on augmented
question sets from our benchmark. We release SciFIBench to encourage progress
in this domain.
]]></content:encoded>
<pubDate>2024-05-14T17:54:17Z</pubDate>
</item>
<item>
<title>SPIN: Simultaneous Perception, Interaction and Navigation</title>
<link>http://arxiv.org/abs/2405.07991v1</link>
<guid>http://arxiv.org/abs/2405.07991v1</guid>
<content:encoded><![CDATA[
<div> 
<br />
 <div>
While there has been remarkable progress recently in the fields of
manipulation and locomotion, mobile manipulation remains a long-standing
challenge. Compared to locomotion or static manipulation, a mobile system must
make a diverse range of long-horizon tasks feasible in unstructured and dynamic
environments. While the applications are broad and interesting, there are a
plethora of challenges in developing these systems such as coordination between
the base and arm, reliance on onboard perception for perceiving and interacting
with the environment, and most importantly, simultaneously integrating all
these parts together. Prior works approach the problem using disentangled
modular skills for mobility and manipulation that are trivially tied together.
This causes several limitations such as compounding errors, delays in
decision-making, and no whole-body coordination. In this work, we present a
reactive mobile manipulation framework that uses an active visual system to
consciously perceive and react to its environment. Similar to how humans
leverage whole-body and hand-eye coordination, we develop a mobile manipulator
that exploits its ability to move and see, more specifically -- to move in
order to see and to see in order to move. This allows it to not only move
around and interact with its environment but also, choose "when" to perceive
"what" using an active visual system. We observe that such an agent learns to
navigate around complex cluttered scenarios while displaying agile whole-body
coordination using only ego-vision without needing to create environment maps.
Results visualizations and videos at https://spin-robot.github.io/
]]></content:encoded>
<pubDate>2024-05-13T17:59:36Z</pubDate>
</item>
<item>
<title>A Generalist Learner for Multifaceted Medical Image Interpretation</title>
<link>http://arxiv.org/abs/2405.07988v1</link>
<guid>http://arxiv.org/abs/2405.07988v1</guid>
<content:encoded><![CDATA[
<div> MedVersa
<br /><br />
MedVersaMedInterp1300113MedVersaMedVersa910 
<br />MedVersa <div>
Current medical artificial intelligence systems are often limited to narrow
applications, hindering their widespread adoption in clinical practice. To
address this limitation, we propose MedVersa, a generalist learner that enables
flexible learning and tasking for medical image interpretation. By leveraging a
large language model as a learnable orchestrator, MedVersa can learn from both
visual and linguistic supervision, support multimodal inputs, and perform
real-time task specification. This versatility allows MedVersa to adapt to
various clinical scenarios and perform multifaceted medical image analysis. We
introduce MedInterp, the largest multimodal dataset to date for medical image
interpretation, consisting of over 13 million annotated instances spanning 11
tasks across 3 modalities, to support the development of MedVersa. Our
experiments demonstrate that MedVersa achieves state-of-the-art performance in
9 tasks, sometimes outperforming specialist counterparts by over 10%. MedVersa
is the first to showcase the viability of multimodal generative medical AI in
implementing multimodal outputs, inputs, and dynamic task specification,
highlighting its potential as a multifunctional system for comprehensive
medical image analysis. This generalist approach to medical image
interpretation paves the way for more adaptable and efficient AI-assisted
clinical decision-making.
]]></content:encoded>
<pubDate>2024-05-13T17:58:51Z</pubDate>
</item>
<item>
<title>Conformal Validity Guarantees Exist for Any Data Distribution</title>
<link>http://arxiv.org/abs/2405.06627v1</link>
<guid>http://arxiv.org/abs/2405.06627v1</guid>
<content:encoded><![CDATA[
<div> , , , , 
:
\textit{} <div>
As machine learning (ML) gains widespread adoption, practitioners are
increasingly seeking means to quantify and control the risk these systems
incur. This challenge is especially salient when ML systems have autonomy to
collect their own data, such as in black-box optimization and active learning,
where their actions induce sequential feedback-loop shifts in the data
distribution. Conformal prediction has emerged as a promising approach to
uncertainty and risk quantification, but existing variants either fail to
accommodate sequences of data-dependent shifts, or do not fully exploit the
fact that agent-induced shift is under our control. In this work we prove that
conformal prediction can theoretically be extended to \textit{any} joint data
distribution, not just exchangeable or quasi-exchangeable ones, although it is
exceedingly impractical to compute in the most general case. For practical
applications, we outline a procedure for deriving specific conformal algorithms
for any data distribution, and we use this procedure to derive tractable
algorithms for a series of agent-induced covariate shifts. We evaluate the
proposed algorithms empirically on synthetic black-box optimization and active
learning tasks.
]]></content:encoded>
<pubDate>2024-05-10T17:40:24Z</pubDate>
</item>
<item>
<title>Characterizing the Accuracy - Efficiency Trade-off of Low-rank
  Decomposition in Language Models</title>
<link>http://arxiv.org/abs/2405.06626v1</link>
<guid>http://arxiv.org/abs/2405.06626v1</guid>
<content:encoded><![CDATA[
<div> -
<br /><br />
TuckerBERTLlama 29% 
<br />
: <br />
1. 
2. 
3. TuckerBERTLlama 2 <div>
Large language models (LLMs) have emerged and presented their general
problem-solving capabilities with one model. However, the model size has
increased dramatically with billions of parameters to enable such broad
problem-solving capabilities. In addition, due to the dominance of
matrix-matrix and matrix-vector multiplications in LLMs, the compute-to-model
size ratio is significantly lower than that of CNNs. This shift pushes LLMs
from a computation-bound regime to a memory-bound regime. Therefore, optimizing
the memory footprint and traffic is an important optimization direction for
LLMs today.
  Model compression methods such as quantization and parameter pruning have
been actively explored for achieving the memory footprint and traffic
optimization. However, the accuracy-efficiency trade-off of rank pruning for
LLMs is not well-understood yet. Therefore, we characterize the
accuracy-efficiency trade-off of a low-rank decomposition method, specifically
Tucker decomposition, on recent language models, including an open-source LLM,
Llama 2.
  We formalize the low-rank decomposition design space and show that the
decomposition design space is enormous (e.g., O($2^{37}$) for Llama2-7B). To
navigate such a vast design space, we formulate the design space and perform
thorough case studies of accuracy-efficiency trade-offs using six widely used
LLM benchmarks on BERT and Llama 2 models. Our results show that we can achieve
a 9\% model size reduction with minimal accuracy drops, which range from 4\%p
to 10\%p, depending on the difficulty of the benchmark, without any retraining
to recover accuracy after decomposition. The results show that low-rank
decomposition can be a promising direction for LLM-based applications that
require real-time service in scale (e.g., AI agent assist and real-time coding
assistant), where the latency is as important as the model accuracy.
]]></content:encoded>
<pubDate>2024-05-10T17:40:02Z</pubDate>
</item>
<item>
<title>Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency
  for Tool Planning</title>
<link>http://arxiv.org/abs/2405.05955v1</link>
<guid>http://arxiv.org/abs/2405.05955v1</guid>
<content:encoded><![CDATA[
<div> Smurfs<br />
"Smurfs"mistral-7b-instructSmurfsToolBench I2I3Smurfs84.4%ChatGPT-ReACTGPT-4 <br /><br />: <br />Smurfs<br />mistral-7b-instructSmurfs<br /><br />SmurfsToolBench I2I3Smurfs<br /> <div>
The emergence of large language models (LLMs) has opened up unprecedented
possibilities for automating complex tasks that are often comparable to human
performance. Despite their capabilities, LLMs still encounter difficulties in
completing tasks that require high levels of accuracy and complexity due to
their inherent limitations in handling multifaceted problems single-handedly.
This paper introduces "Smurfs", a cutting-edge multi-agent framework designed
to revolutionize the application of LLMs. By transforming a conventional LLM
into a synergistic multi-agent ensemble, Smurfs enhances task decomposition and
execution without necessitating extra training. This is achieved through
innovative prompting strategies that allocate distinct roles within the model,
thereby facilitating collaboration among specialized agents. The framework
gives access to external tools to efficiently solve complex tasks. Our
empirical investigation, featuring the mistral-7b-instruct model as a case
study, showcases Smurfs' superior capability in intricate tool utilization
scenarios. Notably, Smurfs outmatches the ChatGPT-ReACT in the ToolBench I2 and
I3 benchmark with a remarkable 84.4% win rate, surpassing the highest recorded
performance of a GPT-4 model at 73.5%. Furthermore, through comprehensive
ablation studies, we dissect the contribution of the core components of the
multi-agent framework to its overall efficacy. This not only verifies the
effectiveness of the framework, but also sets a route for future exploration of
multi-agent LLM systems.
]]></content:encoded>
<pubDate>2024-05-09T17:49:04Z</pubDate>
</item>
<item>
<title>Frame Interpolation with Consecutive Brownian Bridge Diffusion</title>
<link>http://arxiv.org/abs/2405.05953v1</link>
<guid>http://arxiv.org/abs/2405.05953v1</guid>
<content:encoded><![CDATA[
<div> Latent Diffusion Models, Video Frame Interpolation, Conditional Image Generation, Random Generation, Deterministic Output
:  <div>
Recent work in Video Frame Interpolation (VFI) tries to formulate VFI as a
diffusion-based conditional image generation problem, synthesizing the
intermediate frame given a random noise and neighboring frames. Due to the
relatively high resolution of videos, Latent Diffusion Models (LDMs) are
employed as the conditional generation model, where the autoencoder compresses
images into latent representations for diffusion and then reconstructs images
from these latent representations. Such a formulation poses a crucial
challenge: VFI expects that the output is deterministically equal to the ground
truth intermediate frame, but LDMs randomly generate a diverse set of different
images when the model runs multiple times. The reason for the diverse
generation is that the cumulative variance (variance accumulated at each step
of generation) of generated latent representations in LDMs is large. This makes
the sampling trajectory random, resulting in diverse rather than deterministic
generations. To address this problem, we propose our unique solution: Frame
Interpolation with Consecutive Brownian Bridge Diffusion. Specifically, we
propose consecutive Brownian Bridge diffusion that takes a deterministic
initial value as input, resulting in a much smaller cumulative variance of
generated latent representations. Our experiments suggest that our method can
improve together with the improvement of the autoencoder and achieve
state-of-the-art performance in VFI, leaving strong potential for further
enhancement.
]]></content:encoded>
<pubDate>2024-05-09T17:46:22Z</pubDate>
</item>
<item>
<title>LLMs with Personalities in Multi-issue Negotiation Games</title>
<link>http://arxiv.org/abs/2405.05248v2</link>
<guid>http://arxiv.org/abs/2405.05248v2</guid>
<content:encoded><![CDATA[
<div> , , , , 
<br />
LLMs1500ShapleyLLMs""
<br /><br />: LLMs <div>
Powered by large language models (LLMs), AI agents have become capable of
many human tasks. Using the most canonical definitions of the Big Five
personality, we measure the ability of LLMs to negotiate within a
game-theoretical framework, as well as methodological challenges to measuring
notions of fairness and risk. Simulations (n=1,500) for both single-issue and
multi-issue negotiation reveal increase in domain complexity with asymmetric
issue valuations improve agreement rates but decrease surplus from aggressive
negotiation. Through gradient-boosted regression and Shapley explainers, we
find high openness, conscientiousness, and neuroticism are associated with fair
tendencies; low agreeableness and low openness are associated with rational
tendencies. Low conscientiousness is associated with high toxicity. These
results indicate that LLMs may have built-in guardrails that default to fair
behavior, but can be "jail broken" to exploit agreeable opponents. We also
offer pragmatic insight in how negotiation bots can be designed, and a
framework of assessing negotiation behavior based on game theory and
computational social science.
]]></content:encoded>
<pubDate>2024-05-09T01:09:09Z</pubDate>
</item>
<item>
<title>Diffusion-HMC: Parameter Inference with Diffusion Model driven
  Hamiltonian Monte Carlo</title>
<link>http://arxiv.org/abs/2405.05255v1</link>
<guid>http://arxiv.org/abs/2405.05255v1</guid>
<content:encoded><![CDATA[
<div> <br />
<br />
 <br /><br />: <br /> <div>
Diffusion generative models have excelled at diverse image generation and
reconstruction tasks across fields. A less explored avenue is their application
to discriminative tasks involving regression or classification problems. The
cornerstone of modern cosmology is the ability to generate predictions for
observed astrophysical fields from theory and constrain physical models from
observations using these predictions. This work uses a single diffusion
generative model to address these interlinked objectives -- as a surrogate
model or emulator for cold dark matter density fields conditional on input
cosmological parameters, and as a parameter inference model that solves the
inverse problem of constraining the cosmological parameters of an input field.
The model is able to emulate fields with summary statistics consistent with
those of the simulated target distribution. We then leverage the approximate
likelihood of the diffusion generative model to derive tight constraints on
cosmology by using the Hamiltonian Monte Carlo method to sample the posterior
on cosmological parameters for a given test image. Finally, we demonstrate that
this parameter inference approach is more robust to the addition of noise than
baseline parameter inference networks.
]]></content:encoded>
<pubDate>2024-05-08T17:59:03Z</pubDate>
</item>
<item>
<title>LLMs with Personalities in Multi-issue Negotiation Games</title>
<link>http://arxiv.org/abs/2405.05248v1</link>
<guid>http://arxiv.org/abs/2405.05248v1</guid>
<content:encoded><![CDATA[
<div> Negotiation, Large Language Models, Personality, Fairness, Risk

AILLMLLM1500ShapleyLLM"" <br /><br />: LLM <div>
Powered by large language models (LLMs), AI agents have become capable of
many human tasks. Using the most canonical definitions of the Big Five
personality, we measure the ability of LLMs to negotiate within a
game-theoretical framework, as well as methodological challenges to measuring
notions of fairness and risk. Simulations (n=1,500) for both single-issue and
multi-issue negotiation reveal increase in domain complexity with asymmetric
issue valuations improve agreement rates but decrease surplus from aggressive
negotiation. Through gradient-boosted regression and Shapley explainers, we
find high openness, conscientiousness, and neuroticism are associated with fair
tendencies; low agreeableness and low openness are associated with rational
tendencies. Low conscientiousness is associated with high toxicity. These
results indicate that LLMs may have built-in guardrails that default to fair
behavior, but can be "jail broken" to exploit agreeable opponents. We also
offer pragmatic insight in how negotiation bots can be designed, and a
framework of assessing negotiation behavior based on game theory and
computational social science.
]]></content:encoded>
<pubDate>2024-05-08T17:51:53Z</pubDate>
</item>
<item>
<title>Pose Priors from Language Models</title>
<link>http://arxiv.org/abs/2405.03689v1</link>
<guid>http://arxiv.org/abs/2405.03689v1</guid>
<content:encoded><![CDATA[
<div> : -shot, 3D, , , 

: 
-shot3D3D <div>
We present a zero-shot pose optimization method that enforces accurate
physical contact constraints when estimating the 3D pose of humans. Our central
insight is that since language is often used to describe physical interaction,
large pretrained text-based models can act as priors on pose estimation.
  We can thus leverage this insight to improve pose estimation by converting
natural language descriptors, generated by a large multimodal model (LMM), into
tractable losses to constrain the 3D pose optimization. Despite its simplicity,
our method produces surprisingly compelling pose reconstructions of people in
close contact, correctly capturing the semantics of the social and physical
interactions. We demonstrate that our method rivals more complex
state-of-the-art approaches that require expensive human annotation of contact
points and training specialized models. Moreover, unlike previous approaches,
our method provides a unified framework for resolving self-contact and
person-to-person contact.
]]></content:encoded>
<pubDate>2024-05-06T17:59:36Z</pubDate>
</item>
<item>
<title>Vibe-Eval: A hard evaluation suite for measuring progress of multimodal
  language models</title>
<link>http://arxiv.org/abs/2405.02287v1</link>
<guid>http://arxiv.org/abs/2405.02287v1</guid>
<content:encoded><![CDATA[
<div> , , , , 

Vibe-Eval26910050%Reka CoreAPIVibe-Evalhttps://github.com/reka-ai/reka-vibe-eval<br /><br />: Vibe-EvalAPI <div>
We introduce Vibe-Eval: a new open benchmark and framework for evaluating
multimodal chat models. Vibe-Eval consists of 269 visual understanding prompts,
including 100 of hard difficulty, complete with gold-standard responses
authored by experts. Vibe-Eval is open-ended and challenging with dual
objectives: (i) vibe checking multimodal chat models for day-to-day tasks and
(ii) rigorously testing and probing the capabilities of present frontier
models. Notably, our hard set contains >50% questions that all frontier models
answer incorrectly. We explore the nuances of designing, evaluating, and
ranking models on ultra challenging prompts. We also discuss trade-offs between
human and automatic evaluation, and show that automatic model evaluation using
Reka Core roughly correlates to human judgment. We offer free API access for
the purpose of lightweight evaluation and plan to conduct formal human
evaluations for public models that perform well on the Vibe-Eval's automatic
scores. We release the evaluation code and data, see
https://github.com/reka-ai/reka-vibe-eval
]]></content:encoded>
<pubDate>2024-05-03T17:59:55Z</pubDate>
</item>
<item>
<title>Geometric Fabrics: a Safe Guiding Medium for Policy Learning</title>
<link>http://arxiv.org/abs/2405.02250v1</link>
<guid>http://arxiv.org/abs/2405.02250v1</guid>
<content:encoded><![CDATA[
<div> , , , , 

 <div>
Robotics policies are always subjected to complex, second order dynamics that
entangle their actions with resulting states. In reinforcement learning (RL)
contexts, policies have the burden of deciphering these complicated
interactions over massive amounts of experience and complex reward functions to
learn how to accomplish tasks. Moreover, policies typically issue actions
directly to controllers like Operational Space Control (OSC) or joint PD
control, which induces straightline motion towards these action targets in task
or joint space. However, straightline motion in these spaces for the most part
do not capture the rich, nonlinear behavior our robots need to exhibit,
shifting the burden of discovering these behaviors more completely to the
agent. Unlike these simpler controllers, geometric fabrics capture a much
richer and desirable set of behaviors via artificial, second order dynamics
grounded in nonlinear geometry. These artificial dynamics shift the
uncontrolled dynamics of a robot via an appropriate control law to form
behavioral dynamics. Behavioral dynamics unlock a new action space and safe,
guiding behavior over which RL policies are trained. Behavioral dynamics enable
bang-bang-like RL policy actions that are still safe for real robots, simplify
reward engineering, and help sequence real-world, high-performance policies. We
describe the framework more generally and create a specific instantiation for
the problem of dexterous, in-hand reorientation of a cube by a highly actuated
robot hand.
]]></content:encoded>
<pubDate>2024-05-03T17:07:45Z</pubDate>
</item>
<item>
<title>Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon
  Robotics Tasks</title>
<link>http://arxiv.org/abs/2405.01534v1</link>
<guid>http://arxiv.org/abs/2405.01534v1</guid>
<content:encoded><![CDATA[
<div> Plan-Seq-Learn

LLMsLLMLLMsRLPlan-Seq-LearnPSLPSL2510PSL85%https://mihdalal.github.io/planseqlearn/<br /><br />: Plan-Seq-Learn <div>
Large Language Models (LLMs) have been shown to be capable of performing
high-level planning for long-horizon robotics tasks, yet existing methods
require access to a pre-defined skill library (e.g. picking, placing, pulling,
pushing, navigating). However, LLM planning does not address how to design or
learn those behaviors, which remains challenging particularly in long-horizon
settings. Furthermore, for many tasks of interest, the robot needs to be able
to adjust its behavior in a fine-grained manner, requiring the agent to be
capable of modifying low-level control actions. Can we instead use the
internet-scale knowledge from LLMs for high-level policies, guiding
reinforcement learning (RL) policies to efficiently solve robotic control tasks
online without requiring a pre-determined set of skills? In this paper, we
propose Plan-Seq-Learn (PSL): a modular approach that uses motion planning to
bridge the gap between abstract language and learned low-level control for
solving long-horizon robotics tasks from scratch. We demonstrate that PSL
achieves state-of-the-art results on over 25 challenging robotics tasks with up
to 10 stages. PSL solves long-horizon tasks from raw visual input spanning four
benchmarks at success rates of over 85%, out-performing language-based,
classical, and end-to-end approaches. Video results and code at
https://mihdalal.github.io/planseqlearn/
]]></content:encoded>
<pubDate>2024-05-02T17:59:31Z</pubDate>
</item>
<item>
<title>OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D
  Perception, Reasoning and Planning</title>
<link>http://arxiv.org/abs/2405.01533v1</link>
<guid>http://arxiv.org/abs/2405.01533v1</guid>
<content:encoded><![CDATA[
<div> 3D MLLMautonomous driving agentsOmniDrive-nuScenesvisual question-answering3D situational awareness<br />
<br />
3D3D3D-OmniDrive-nuScenes3D3D<br /><br />: <div>
The advances in multimodal large language models (MLLMs) have led to growing
interests in LLM-based autonomous driving agents to leverage their strong
reasoning capabilities. However, capitalizing on MLLMs' strong reasoning
capabilities for improved planning behavior is challenging since planning
requires full 3D situational awareness beyond 2D reasoning. To address this
challenge, our work proposes a holistic framework for strong alignment between
agent models and 3D driving tasks. Our framework starts with a novel 3D MLLM
architecture that uses sparse queries to lift and compress visual
representations into 3D before feeding them into an LLM. This query-based
representation allows us to jointly encode dynamic objects and static map
elements (e.g., traffic lanes), providing a condensed world model for
perception-action alignment in 3D. We further propose OmniDrive-nuScenes, a new
visual question-answering dataset challenging the true 3D situational awareness
of a model with comprehensive visual question-answering (VQA) tasks, including
scene description, traffic regulation, 3D grounding, counterfactual reasoning,
decision making and planning. Extensive studies show the effectiveness of the
proposed architecture as well as the importance of the VQA tasks for reasoning
and planning in complex 3D scenes.
]]></content:encoded>
<pubDate>2024-05-02T17:59:24Z</pubDate>
</item>
<item>
<title>No Representation, No Trust: Connecting Representation, Collapse, and
  Trust Issues in PPO</title>
<link>http://arxiv.org/abs/2405.00662v1</link>
<guid>http://arxiv.org/abs/2405.00662v1</guid>
<content:encoded><![CDATA[
<div> Proximal Policy Optimization<br />
Proximal Policy OptimizationPPOAtariMuJoCoPPOactorPPOProximal Feature OptimizationPFOPPO<br /><br />: Proximal Policy OptimizationProximal Feature Optimization <div>
Reinforcement learning (RL) is inherently rife with non-stationarity since
the states and rewards the agent observes during training depend on its
changing policy. Therefore, networks in deep RL must be capable of adapting to
new observations and fitting new targets. However, previous works have observed
that networks in off-policy deep value-based methods exhibit a decrease in
representation rank, often correlated with an inability to continue learning or
a collapse in performance. Although this phenomenon has generally been
attributed to neural network learning under non-stationarity, it has been
overlooked in on-policy policy optimization methods which are often thought
capable of training indefinitely. In this work, we empirically study
representation dynamics in Proximal Policy Optimization (PPO) on the Atari and
MuJoCo environments, revealing that PPO agents are also affected by feature
rank deterioration and loss of plasticity. We show that this is aggravated with
stronger non-stationarity, ultimately driving the actor's performance to
collapse, regardless of the performance of the critic. We draw connections
between representation collapse, performance collapse, and trust region issues
in PPO, and present Proximal Feature Optimization (PFO), a novel auxiliary
loss, that along with other interventions shows that regularizing the
representation dynamics improves the performance of PPO agents.
]]></content:encoded>
<pubDate>2024-05-01T17:50:16Z</pubDate>
</item>
<item>
<title>DOCCI: Descriptions of Connected and Contrasting Images</title>
<link>http://arxiv.org/abs/2404.19753v1</link>
<guid>http://arxiv.org/abs/2404.19753v1</guid>
<content:encoded><![CDATA[
<div> -DOCCI

<br /><br />:
-T2II2TDOCCIDescriptions of Connected and Contrasting Images1.5136DOCCIDOCCI <div>
Vision-language datasets are vital for both text-to-image (T2I) and
image-to-text (I2T) research. However, current datasets lack descriptions with
fine-grained detail that would allow for richer associations to be learned by
models. To fill the gap, we introduce Descriptions of Connected and Contrasting
Images (DOCCI), a dataset with long, human-annotated English descriptions for
15k images that were taken, curated and donated by a single researcher intent
on capturing key challenges such as spatial relations, counting, text
rendering, world knowledge, and more. We instruct human annotators to create
comprehensive descriptions for each image; these average 136 words in length
and are crafted to clearly distinguish each image from those that are related
or similar. Each description is highly compositional and typically encompasses
multiple challenges. Through both quantitative and qualitative analyses, we
demonstrate that DOCCI serves as an effective training resource for
image-to-text generation -- a PaLI 5B model finetuned on DOCCI shows equal or
superior results compared to highly-performant larger models like LLaVA-1.5 7B
and InstructBLIP 7B. Furthermore, we show that DOCCI is a useful testbed for
text-to-image generation, highlighting the limitations of current text-to-image
models in capturing long descriptions and fine details.
]]></content:encoded>
<pubDate>2024-04-30T17:56:24Z</pubDate>
</item>
<item>
<title>Hallucination of Multimodal Large Language Models: A Survey</title>
<link>http://arxiv.org/abs/2404.18930v1</link>
<guid>http://arxiv.org/abs/2404.18930v1</guid>
<content:encoded><![CDATA[
<div> -

MLLMs-LVLMsMLLMsMLLMs <div>
This survey presents a comprehensive analysis of the phenomenon of
hallucination in multimodal large language models (MLLMs), also known as Large
Vision-Language Models (LVLMs), which have demonstrated significant
advancements and remarkable abilities in multimodal tasks. Despite these
promising developments, MLLMs often generate outputs that are inconsistent with
the visual content, a challenge known as hallucination, which poses substantial
obstacles to their practical deployment and raises concerns regarding their
reliability in real-world applications. This problem has attracted increasing
attention, prompting efforts to detect and mitigate such inaccuracies. We
review recent advances in identifying, evaluating, and mitigating these
hallucinations, offering a detailed overview of the underlying causes,
evaluation benchmarks, metrics, and strategies developed to address this issue.
Additionally, we analyze the current challenges and limitations, formulating
open questions that delineate potential pathways for future research. By
drawing the granular classification and landscapes of hallucination causes,
evaluation benchmarks, and mitigation methods, this survey aims to deepen the
understanding of hallucinations in MLLMs and inspire further advancements in
the field. Through our thorough and in-depth review, we contribute to the
ongoing dialogue on enhancing the robustness and reliability of MLLMs,
providing valuable insights and resources for researchers and practitioners
alike. Resources are available at:
https://github.com/showlab/Awesome-MLLM-Hallucination.
]]></content:encoded>
<pubDate>2024-04-29T17:59:41Z</pubDate>
</item>
<item>
<title>Stylus: Automatic Adapter Selection for Diffusion Models</title>
<link>http://arxiv.org/abs/2404.18928v1</link>
<guid>http://arxiv.org/abs/2404.18928v1</guid>
<content:encoded><![CDATA[
<div> Stylus, adapters, fine-tuned, matching, prompt

: StylusStylus75KStylusDocsStable DiffusionStylusCLIP-FID Pareto <div>
Beyond scaling base models with more data or parameters, fine-tuned adapters
provide an alternative way to generate high fidelity, custom images at reduced
costs. As such, adapters have been widely adopted by open-source communities,
accumulating a database of over 100K adapters-most of which are highly
customized with insufficient descriptions. This paper explores the problem of
matching the prompt to a set of relevant adapters, built on recent work that
highlight the performance gains of composing adapters. We introduce Stylus,
which efficiently selects and automatically composes task-specific adapters
based on a prompt's keywords. Stylus outlines a three-stage approach that first
summarizes adapters with improved descriptions and embeddings, retrieves
relevant adapters, and then further assembles adapters based on prompts'
keywords by checking how well they fit the prompt. To evaluate Stylus, we
developed StylusDocs, a curated dataset featuring 75K adapters with
pre-computed adapter embeddings. In our evaluation on popular Stable Diffusion
checkpoints, Stylus achieves greater CLIP-FID Pareto efficiency and is twice as
preferred, with humans and multimodal models as evaluators, over the base
model. See stylus-diffusion.github.io for more.
]]></content:encoded>
<pubDate>2024-04-29T17:59:16Z</pubDate>
</item>
<item>
<title>TheaterGen: Character Management with LLM for Consistent Multi-turn
  Image Generation</title>
<link>http://arxiv.org/abs/2404.18919v1</link>
<guid>http://arxiv.org/abs/2404.18919v1</guid>
<content:encoded><![CDATA[
Recent advances in diffusion models can generate high-quality and stunning
images from text. However, multi-turn image generation, which is of high demand
in real-world scenarios, still faces challenges in maintaining semantic
consistency between images and texts, as well as contextual consistency of the
same subject across multiple interactive turns. To address this issue, we
introduce TheaterGen, a training-free framework that integrates large language
models (LLMs) and text-to-image (T2I) models to provide the capability of
multi-turn image generation. Within this framework, LLMs, acting as a
"Screenwriter", engage in multi-turn interaction, generating and managing a
standardized prompt book that encompasses prompts and layout designs for each
character in the target image. Based on these, Theatergen generate a list of
character images and extract guidance information, akin to the "Rehearsal".
Subsequently, through incorporating the prompt book and guidance information
into the reverse denoising process of T2I diffusion models, Theatergen generate
the final image, as conducting the "Final Performance". With the effective
management of prompt books and character images, TheaterGen significantly
improves semantic and contextual consistency in synthesized images.
Furthermore, we introduce a dedicated benchmark, CMIGBench (Consistent
Multi-turn Image Generation Benchmark) with 8000 multi-turn instructions.
Different from previous multi-turn benchmarks, CMIGBench does not define
characters in advance. Both the tasks of story generation and multi-turn
editing are included on CMIGBench for comprehensive evaluation. Extensive
experimental results show that TheaterGen outperforms state-of-the-art methods
significantly. It raises the performance bar of the cutting-edge Mini DALLE 3
model by 21% in average character-character similarity and 19% in average
text-image similarity.
]]></content:encoded>
<pubDate>2024-04-29T17:58:14Z</pubDate>
</item>
<item>
<title>Sample-Efficient Robust Multi-Agent Reinforcement Learning in the Face
  of Environmental Uncertainty</title>
<link>http://arxiv.org/abs/2404.18909v1</link>
<guid>http://arxiv.org/abs/2404.18909v1</guid>
<content:encoded><![CDATA[
To overcome the sim-to-real gap in reinforcement learning (RL), learned
policies must maintain robustness against environmental uncertainties. While
robust RL has been widely studied in single-agent regimes, in multi-agent
environments, the problem remains understudied -- despite the fact that the
problems posed by environmental uncertainties are often exacerbated by
strategic interactions. This work focuses on learning in distributionally
robust Markov games (RMGs), a robust variant of standard Markov games, wherein
each agent aims to learn a policy that maximizes its own worst-case performance
when the deployed environment deviates within its own prescribed uncertainty
set. This results in a set of robust equilibrium strategies for all agents that
align with classic notions of game-theoretic equilibria. Assuming a
non-adaptive sampling mechanism from a generative model, we propose a
sample-efficient model-based algorithm (DRNVI) with finite-sample complexity
guarantees for learning robust variants of various notions of game-theoretic
equilibria. We also establish an information-theoretic lower bound for solving
RMGs, which confirms the near-optimal sample complexity of DRNVI with respect
to problem-dependent factors such as the size of the state space, the target
accuracy, and the horizon length.
]]></content:encoded>
<pubDate>2024-04-29T17:51:47Z</pubDate>
</item>
<item>
<title>Learning Visuotactile Skills with Two Multifingered Hands</title>
<link>http://arxiv.org/abs/2404.16823v1</link>
<guid>http://arxiv.org/abs/2404.16823v1</guid>
<content:encoded><![CDATA[
<div> : 
:
-HATO <div>
Aiming to replicate human-like dexterity, perceptual experiences, and motion
patterns, we explore learning from human demonstrations using a bimanual system
with multifingered hands and visuotactile data. Two significant challenges
exist: the lack of an affordable and accessible teleoperation system suitable
for a dual-arm setup with multifingered hands, and the scarcity of
multifingered hand hardware equipped with touch sensing. To tackle the first
challenge, we develop HATO, a low-cost hands-arms teleoperation system that
leverages off-the-shelf electronics, complemented with a software suite that
enables efficient data collection; the comprehensive software suite also
supports multimodal data processing, scalable policy learning, and smooth
policy deployment. To tackle the latter challenge, we introduce a novel
hardware adaptation by repurposing two prosthetic hands equipped with touch
sensors for research. Using visuotactile data collected from our system, we
learn skills to complete long-horizon, high-precision tasks which are difficult
to achieve without multifingered dexterity and touch feedback. Furthermore, we
empirically investigate the effects of dataset size, sensing modality, and
visual input preprocessing on policy learning. Our results mark a promising
step forward in bimanual multifingered manipulation from visuotactile data.
Videos, code, and datasets can be found at https://toruowo.github.io/hato/ .
]]></content:encoded>
<pubDate>2024-04-25T17:59:41Z</pubDate>
</item>
<item>
<title>How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal
  Models with Open-Source Suites</title>
<link>http://arxiv.org/abs/2404.16821v1</link>
<guid>http://arxiv.org/abs/2404.16821v1</guid>
<content:encoded><![CDATA[
<div> : InternVL 1.5, , , , 

:
InternVL 1.5InternViT-6BLLM1404484484KOCRInternVL 1.5188Codehttps://github.com/OpenGVLab/InternVL<br /><br /> <div>
In this report, we introduce InternVL 1.5, an open-source multimodal large
language model (MLLM) to bridge the capability gap between open-source and
proprietary commercial models in multimodal understanding. We introduce three
simple improvements: (1) Strong Vision Encoder: we explored a continuous
learning strategy for the large-scale vision foundation model -- InternViT-6B,
boosting its visual understanding capabilities, and making it can be
transferred and reused in different LLMs. (2) Dynamic High-Resolution: we
divide images into tiles ranging from 1 to 40 of 448$\times$448 pixels
according to the aspect ratio and resolution of the input images, which
supports up to 4K resolution input. (3) High-Quality Bilingual Dataset: we
carefully collected a high-quality bilingual dataset that covers common scenes,
document images, and annotated them with English and Chinese question-answer
pairs, significantly enhancing performance in OCR- and Chinese-related tasks.
We evaluate InternVL 1.5 through a series of benchmarks and comparative
studies. Compared to both open-source and proprietary models, InternVL 1.5
shows competitive performance, achieving state-of-the-art results in 8 of 18
benchmarks. Code has been released at https://github.com/OpenGVLab/InternVL.
]]></content:encoded>
<pubDate>2024-04-25T17:59:19Z</pubDate>
</item>
<item>
<title>DPO: Differential reinforcement learning with application to optimal
  configuration search</title>
<link>http://arxiv.org/abs/2404.15617v1</link>
<guid>http://arxiv.org/abs/2404.15617v1</guid>
<content:encoded><![CDATA[
<div> Differential Policy Optimization (DPO)
<br />
DPODPODPODPOLagrangianDPO
<br /><br />: <br />DPODPO <div>
Reinforcement learning (RL) with continuous state and action spaces remains
one of the most challenging problems within the field. Most current learning
methods focus on integral identities such as value functions to derive an
optimal strategy for the learning agent. In this paper, we instead study the
dual form of the original RL formulation to propose the first differential RL
framework that can handle settings with limited training samples and
short-length episodes. Our approach introduces Differential Policy Optimization
(DPO), a pointwise and stage-wise iteration method that optimizes policies
encoded by local-movement operators. We prove a pointwise convergence estimate
for DPO and provide a regret bound comparable with current theoretical works.
Such pointwise estimate ensures that the learned policy matches the optimal
path uniformly across different steps. We then apply DPO to a class of
practical RL problems which search for optimal configurations with Lagrangian
rewards. DPO is easy to implement, scalable, and shows competitive results on
benchmarking experiments against several popular RL methods.
]]></content:encoded>
<pubDate>2024-04-24T03:11:12Z</pubDate>
</item>
<item>
<title>ID-Animator: Zero-Shot Identity-Preserving Human Video Generation</title>
<link>http://arxiv.org/abs/2404.15275v1</link>
<guid>http://arxiv.org/abs/2404.15275v1</guid>
<content:encoded><![CDATA[
<div> 
<br /><br />
:
ID-AnimatorID-AnimatorT2Vanimatediffhttps://github.com/ID-Animator/ID-Animator <div>
Generating high fidelity human video with specified identities has attracted
significant attention in the content generation community. However, existing
techniques struggle to strike a balance between training efficiency and
identity preservation, either requiring tedious case-by-case finetuning or
usually missing the identity details in video generation process. In this
study, we present ID-Animator, a zero-shot human-video generation approach that
can perform personalized video generation given single reference facial image
without further training. ID-Animator inherits existing diffusion-based video
generation backbones with a face adapter to encode the ID-relevant embeddings
from learnable facial latent queries. To facilitate the extraction of identity
information in video generation, we introduce an ID-oriented dataset
construction pipeline, which incorporates decoupled human attribute and action
captioning technique from a constructed facial image pool. Based on this
pipeline, a random face reference training method is further devised to
precisely capture the ID-relevant embeddings from reference images, thus
improving the fidelity and generalization capacity of our model for ID-specific
video generation. Extensive experiments demonstrate the superiority of
ID-Animator to generate personalized human videos over previous models.
Moreover, our method is highly compatible with popular pre-trained T2V models
like animatediff and various community backbone models, showing high
extendability in real-world applications for video generation where identity
preservation is highly desired. Our codes and checkpoints will be released at
https://github.com/ID-Animator/ID-Animator.
]]></content:encoded>
<pubDate>2024-04-23T17:59:43Z</pubDate>
</item>
<item>
<title>Estimation Network Design framework for efficient distributed
  optimization</title>
<link>http://arxiv.org/abs/2404.15273v1</link>
<guid>http://arxiv.org/abs/2404.15273v1</guid>
<content:encoded><![CDATA[
<div> , , , , 

(END)ENDENDADMMAugDGMPush-Sum DGDEND

<br /><br />: ENDEND <div>
Distributed decision problems features a group of agents that can only
communicate over a peer-to-peer network, without a central memory. In
applications such as network control and data ranking, each agent is only
affected by a small portion of the decision vector: this sparsity is typically
ignored in distributed algorithms, while it could be leveraged to improve
efficiency and scalability. To address this issue, our recent paper introduces
Estimation Network Design (END), a graph theoretical language for the analysis
and design of distributed iterations. END algorithms can be tuned to exploit
the sparsity of specific problem instances, reducing communication overhead and
minimizing redundancy, yet without requiring case-by-case convergence analysis.
In this paper, we showcase the flexility of END in the context of distributed
optimization. In particular, we study the sparsity-aware version of many
established methods, including ADMM, AugDGM and Push-Sum DGD. Simulations on an
estimation problem in sensor networks demonstrate that END algorithms can boost
convergence speed and greatly reduce the communication and memory cost.
]]></content:encoded>
<pubDate>2024-04-23T17:59:09Z</pubDate>
</item>
<item>
<title>CT-GLIP: 3D Grounded Language-Image Pretraining with CT Scans and
  Radiology Reports for Full-Body Scenarios</title>
<link>http://arxiv.org/abs/2404.15272v1</link>
<guid>http://arxiv.org/abs/2404.15272v1</guid>
<content:encoded><![CDATA[
Medical Vision-Language Pretraining (Med-VLP) establishes a connection
between visual content from medical images and the relevant textual
descriptions. Existing Med-VLP methods primarily focus on 2D images depicting a
single body part, notably chest X-rays. In this paper, we extend the scope of
Med-VLP to encompass 3D images, specifically targeting full-body scenarios, by
using a multimodal dataset of CT images and reports. Compared with the 2D
counterpart, 3D VLP is required to effectively capture essential semantics from
significantly sparser representation in 3D imaging. In this paper, we introduce
CT-GLIP (Grounded Language-Image Pretraining with CT scans), a novel method
that constructs organ-level image-text pairs to enhance multimodal contrastive
learning, aligning grounded visual features with precise diagnostic text.
Additionally, we developed an abnormality dictionary to augment contrastive
learning with diverse negative samples. Our method, trained on a multimodal CT
dataset comprising 44,011 organ-level vision-text pairs from 17,702 patients
across 104 organs, demonstrates it can identify organs and abnormalities in a
zero-shot manner using natural languages. The performance of CT-GLIP is
validated on a separate test set of 1,130 patients, focusing on the 16 most
frequent abnormalities across 7 organs. The experimental results show our
model's superior performance over the standard CLIP framework across zero-shot
and fine-tuning scenarios, using both CNN and ViT architectures.
]]></content:encoded>
<pubDate>2024-04-23T17:59:01Z</pubDate>
</item>
<item>
<title>Automatic Layout Planning for Visually-Rich Documents with
  Instruction-Following Models</title>
<link>http://arxiv.org/abs/2404.15271v1</link>
<guid>http://arxiv.org/abs/2404.15271v1</guid>
<content:encoded><![CDATA[
Recent advancements in instruction-following models have made user
interactions with models more user-friendly and efficient, broadening their
applicability. In graphic design, non-professional users often struggle to
create visually appealing layouts due to limited skills and resources. In this
work, we introduce a novel multimodal instruction-following framework for
layout planning, allowing users to easily arrange visual elements into tailored
layouts by specifying canvas size and design purpose, such as for book covers,
posters, brochures, or menus. We developed three layout reasoning tasks to
train the model in understanding and executing layout instructions. Experiments
on two benchmarks show that our method not only simplifies the design process
for non-professionals but also surpasses the performance of few-shot GPT-4V
models, with mIoU higher by 12% on Crello. This progress highlights the
potential of multimodal instruction-following models to automate and simplify
the design process, providing an approachable solution for a wide range of
design tasks on visually-rich documents.
]]></content:encoded>
<pubDate>2024-04-23T17:58:33Z</pubDate>
</item>
<item>
<title>Aligning LLM Agents by Learning Latent Preference from User Edits</title>
<link>http://arxiv.org/abs/2404.15269v1</link>
<guid>http://arxiv.org/abs/2404.15269v1</guid>
<content:encoded><![CDATA[
We study interactive learning of language agents based on user edits made to
the agent's output. In a typical setting such as writing assistants, the user
interacts with a language agent to generate a response given a context, and may
optionally edit the agent response to personalize it based on their latent
preference, in addition to improving the correctness. The edit feedback is
naturally generated, making it a suitable candidate for improving the agent's
alignment with the user's preference, and for reducing the cost of user edits
over time. We propose a learning framework, PRELUDE that infers a description
of the user's latent preference based on historic edit data and using it to
define a prompt policy that drives future response generation. This avoids
fine-tuning the agent, which is costly, challenging to scale with the number of
users, and may even degrade its performance on other tasks. Furthermore,
learning descriptive preference improves interpretability, allowing the user to
view and modify the learned preference. However, user preference can be complex
and vary based on context, making it challenging to learn. To address this, we
propose a simple yet effective algorithm named CIPHER that leverages a large
language model (LLM) to infer the user preference for a given context based on
user edits. In the future, CIPHER retrieves inferred preferences from the
k-closest contexts in the history, and forms an aggregate preference for
response generation. We introduce two interactive environments -- summarization
and email writing, for evaluation using a GPT-4 simulated user. We compare with
algorithms that directly retrieve user edits but do not learn descriptive
preference, and algorithms that learn context-agnostic preference. On both
tasks, CIPHER achieves the lowest edit distance cost and learns preferences
that show significant similarity to the ground truth preferences
]]></content:encoded>
<pubDate>2024-04-23T17:57:47Z</pubDate>
</item>
<item>
<title>From Parts to Whole: A Unified Reference Framework for Controllable
  Human Image Generation</title>
<link>http://arxiv.org/abs/2404.15267v1</link>
<guid>http://arxiv.org/abs/2404.15267v1</guid>
<content:encoded><![CDATA[
Recent advancements in controllable human image generation have led to
zero-shot generation using structural signals (e.g., pose, depth) or facial
appearance. Yet, generating human images conditioned on multiple parts of human
appearance remains challenging. Addressing this, we introduce Parts2Whole, a
novel framework designed for generating customized portraits from multiple
reference images, including pose images and various aspects of human
appearance. To achieve this, we first develop a semantic-aware appearance
encoder to retain details of different human parts, which processes each image
based on its textual label to a series of multi-scale feature maps rather than
one image token, preserving the image dimension. Second, our framework supports
multi-image conditioned generation through a shared self-attention mechanism
that operates across reference and target features during the diffusion
process. We enhance the vanilla attention mechanism by incorporating mask
information from the reference human images, allowing for the precise selection
of any part. Extensive experiments demonstrate the superiority of our approach
over existing alternatives, offering advanced capabilities for multi-part
controllable human image customization. See our project page at
https://huanngzh.github.io/Parts2Whole/.
]]></content:encoded>
<pubDate>2024-04-23T17:56:08Z</pubDate>
</item>
<item>
<title>MoVA: Adapting Mixture of Vision Experts to Multimodal Context</title>
<link>http://arxiv.org/abs/2404.13046v1</link>
<guid>http://arxiv.org/abs/2404.13046v1</guid>
<content:encoded><![CDATA[
<div> MoVA<br />
MoVALoRAMoV-AdapterMoVAMoVAhttps://github.com/TempleX98/MoVA <br /><br />: MoVA <div>
As the key component in multimodal large language models (MLLMs), the ability
of the visual encoder greatly affects MLLM's understanding on diverse image
content. Although some large-scale pretrained vision encoders such as vision
encoders in CLIP and DINOv2 have brought promising performance, we found that
there is still no single vision encoder that can dominate various image content
understanding, e.g., the CLIP vision encoder leads to outstanding results on
general image understanding but poor performance on document or chart content.
To alleviate the bias of CLIP vision encoder, we first delve into the inherent
behavior of different pre-trained vision encoders and then propose the MoVA, a
powerful and novel MLLM, adaptively routing and fusing task-specific vision
experts with a coarse-to-fine mechanism. In the coarse-grained stage, we design
a context-aware expert routing strategy to dynamically select the most suitable
vision experts according to the user instruction, input image, and expertise of
vision experts. This benefits from the powerful model function understanding
ability of the large language model (LLM) equipped with expert-routing low-rank
adaptation (LoRA). In the fine-grained stage, we elaborately conduct the
mixture-of-vision-expert adapter (MoV-Adapter) to extract and fuse
task-specific knowledge from various experts. This coarse-to-fine paradigm
effectively leverages representations from experts based on multimodal context
and model expertise, further enhancing the generalization ability. We conduct
extensive experiments to evaluate the effectiveness of the proposed approach.
Without any bells and whistles, MoVA can achieve significant performance gains
over current state-of-the-art methods in a wide range of challenging multimodal
benchmarks. Codes and models will be available at
https://github.com/TempleX98/MoVA.
]]></content:encoded>
<pubDate>2024-04-19T17:59:48Z</pubDate>
</item>
<item>
<title>BLINK: Multimodal Large Language Models Can See but Not Perceive</title>
<link>http://arxiv.org/abs/2404.12390v1</link>
<guid>http://arxiv.org/abs/2404.12390v1</guid>
<content:encoded><![CDATA[
<div> benchmark, multimodal language models, visual perception, challenges, improvement
<br /><br />:
Blink(LLMs)BlinkLLMsBlink143,80795.70%LLMsGPT-4VGemini51.26%45.72%13.17%7.63%LLMsCVBlinkLLMs <div>
We introduce Blink, a new benchmark for multimodal language models (LLMs)
that focuses on core visual perception abilities not found in other
evaluations. Most of the Blink tasks can be solved by humans "within a blink"
(e.g., relative depth estimation, visual correspondence, forensics detection,
and multi-view reasoning). However, we find these perception-demanding tasks
cast significant challenges for current multimodal LLMs because they resist
mediation through natural language. Blink reformats 14 classic computer vision
tasks into 3,807 multiple-choice questions, paired with single or multiple
images and visual prompting. While humans get 95.70% accuracy on average, Blink
is surprisingly challenging for existing multimodal LLMs: even the
best-performing GPT-4V and Gemini achieve accuracies of 51.26% and 45.72%, only
13.17% and 7.63% higher than random guessing, indicating that such perception
abilities have not "emerged" yet in recent multimodal LLMs. Our analysis also
highlights that specialist CV models could solve these problems much better,
suggesting potential pathways for future improvements. We believe Blink will
stimulate the community to help multimodal LLMs catch up with human-level
visual perception.
]]></content:encoded>
<pubDate>2024-04-18T17:59:54Z</pubDate>
</item>
<item>
<title>Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language
  Models</title>
<link>http://arxiv.org/abs/2404.12387v1</link>
<guid>http://arxiv.org/abs/2404.12387v1</guid>
<content:encoded><![CDATA[
<div> , , , , Reka
<br /><br />
: 
Reka Core, FlashEdgeReka EdgeReka FlashRekaReka CoreMMMUVQAv2CoreGPT4-VCoreClaude 3 OpusCoreMMLUGSM8KGPT4-0613Perception-TestCoreGemini Ultrahttp://chat.reka.ai http://showcase.reka.ai  <div>
We introduce Reka Core, Flash, and Edge, a series of powerful multimodal
language models trained from scratch by Reka. Reka models are able to process
and reason with text, images, video, and audio inputs. This technical report
discusses details of training some of these models and provides comprehensive
evaluation results. We show that Reka Edge and Reka Flash are not only
state-of-the-art but also outperform many much larger models, delivering
outsized values for their respective compute class. Meanwhile, our most capable
and largest model, Reka Core, approaches the best frontier models on both
automatic evaluations and blind human evaluations. On image question answering
benchmarks (e.g. MMMU, VQAv2), Core performs competitively to GPT4-V.
Meanwhile, on multimodal chat, Core ranks as the second most preferred model
under a blind third-party human evaluation setup, outperforming other models
such as Claude 3 Opus. On text benchmarks, Core not only performs competitively
to other frontier models on a set of well-established benchmarks (e.g. MMLU,
GSM8K) but also outperforms GPT4-0613 on human evaluation. On video question
answering (Perception-Test), Core outperforms Gemini Ultra. Models are shipped
in production at http://chat.reka.ai . A showcase of non cherry picked
qualitative examples can also be found at http://showcase.reka.ai .
]]></content:encoded>
<pubDate>2024-04-18T17:59:48Z</pubDate>
</item>
<item>
<title>MeshLRM: Large Reconstruction Model for High-Quality Mesh</title>
<link>http://arxiv.org/abs/2404.12385v1</link>
<guid>http://arxiv.org/abs/2404.12385v1</guid>
<content:encoded><![CDATA[
We propose MeshLRM, a novel LRM-based approach that can reconstruct a
high-quality mesh from merely four input images in less than one second.
Different from previous large reconstruction models (LRMs) that focus on
NeRF-based reconstruction, MeshLRM incorporates differentiable mesh extraction
and rendering within the LRM framework. This allows for end-to-end mesh
reconstruction by fine-tuning a pre-trained NeRF LRM with mesh rendering.
Moreover, we improve the LRM architecture by simplifying several complex
designs in previous LRMs. MeshLRM's NeRF initialization is sequentially trained
with low- and high-resolution images; this new LRM training strategy enables
significantly faster convergence and thereby leads to better quality with less
compute. Our approach achieves state-of-the-art mesh reconstruction from
sparse-view inputs and also allows for many downstream applications, including
text-to-3D and single-image-to-3D generation. Project page:
https://sarahweiii.github.io/meshlrm/
]]></content:encoded>
<pubDate>2024-04-18T17:59:41Z</pubDate>
</item>
<item>
<title>COMBO: Compositional World Models for Embodied Multi-Agent Cooperation</title>
<link>http://arxiv.org/abs/2404.10775v1</link>
<guid>http://arxiv.org/abs/2404.10775v1</guid>
<content:encoded><![CDATA[
<div> , , , , 
<br />
ThreeDWorld2-4 <div>
In this paper, we investigate the problem of embodied multi-agent
cooperation, where decentralized agents must cooperate given only partial
egocentric views of the world. To effectively plan in this setting, in contrast
to learning world dynamics in a single-agent scenario, we must simulate world
dynamics conditioned on an arbitrary number of agents' actions given only
partial egocentric visual observations of the world. To address this issue of
partial observability, we first train generative models to estimate the overall
world state given partial egocentric observations. To enable accurate
simulation of multiple sets of actions on this world state, we then propose to
learn a compositional world model for multi-agent cooperation by factorizing
the naturally composable joint actions of multiple agents and compositionally
generating the video. By leveraging this compositional world model, in
combination with Vision Language Models to infer the actions of other agents,
we can use a tree search procedure to integrate these modules and facilitate
online cooperative planning. To evaluate the efficacy of our methods, we create
two challenging embodied multi-agent long-horizon cooperation tasks using the
ThreeDWorld simulator and conduct experiments with 2-4 agents. The results show
our compositional world model is effective and the framework enables the
embodied agents to cooperate efficiently with different agents across various
tasks and an arbitrary number of agents, showing the promising future of our
proposed framework. More videos can be found at
https://vis-www.cs.umass.edu/combo/.
]]></content:encoded>
<pubDate>2024-04-16T17:59:11Z</pubDate>
</item>
<item>
<title>A Conceptual Framework for Conversational Search and Recommendation:
  Conceptualizing Agent-Human Interactions During the Conversational Search
  Process</title>
<link>http://arxiv.org/abs/2404.08630v1</link>
<guid>http://arxiv.org/abs/2404.08630v1</guid>
<content:encoded><![CDATA[
<div> : , , , , <br /> 
: 
/<br /> <div>
The conversational search task aims to enable a user to resolve information
needs via natural language dialogue with an agent. In this paper, we aim to
develop a conceptual framework of the actions and intents of users and agents
explaining how these actions enable the user to explore the search space and
resolve their information need. We outline the different actions and intents,
before discussing key decision points in the conversation where the agent needs
to decide how to steer the conversational search process to a successful and/or
satisfactory conclusion. Essentially, this paper provides a conceptualization
of the conversational search process between an agent and user, which provides
a framework and a starting point for research, development and evaluation of
conversational search agents.
]]></content:encoded>
<pubDate>2024-04-12T17:48:18Z</pubDate>
</item>
<item>
<title>Connecting NeRFs, Images, and Text</title>
<link>http://arxiv.org/abs/2404.07993v1</link>
<guid>http://arxiv.org/abs/2404.07993v1</guid>
<content:encoded><![CDATA[
<div> Neural Radiance Fields, 3D scenes, multimodal representation learning, NeRF embeddings, bidirectional mapping<br />
<br />
NeRFNeRFNeRFNeRFNeRF <br /><br />: <br />NeRFNeRFNeRFNeRFNeRF <div>
Neural Radiance Fields (NeRFs) have emerged as a standard framework for
representing 3D scenes and objects, introducing a novel data type for
information exchange and storage. Concurrently, significant progress has been
made in multimodal representation learning for text and image data. This paper
explores a novel research direction that aims to connect the NeRF modality with
other modalities, similar to established methodologies for images and text. To
this end, we propose a simple framework that exploits pre-trained models for
NeRF representations alongside multimodal models for text and image processing.
Our framework learns a bidirectional mapping between NeRF embeddings and those
obtained from corresponding images and text. This mapping unlocks several novel
and useful applications, including NeRF zero-shot classification and NeRF
retrieval from images or text.
]]></content:encoded>
<pubDate>2024-04-11T17:59:59Z</pubDate>
</item>
<item>
<title>UMBRAE: Unified Multimodal Decoding of Brain Signals</title>
<link>http://arxiv.org/abs/2404.07202v1</link>
<guid>http://arxiv.org/abs/2404.07202v1</guid>
<content:encoded><![CDATA[
<div> UMBRAE
:
UMBRACEUMBRAEBrainHub <div>
We address prevailing challenges of the brain-powered research, departing
from the observation that the literature hardly recover accurate spatial
information and require subject-specific models. To address these challenges,
we propose UMBRAE, a unified multimodal decoding of brain signals. First, to
extract instance-level conceptual and spatial details from neural signals, we
introduce an efficient universal brain encoder for multimodal-brain alignment
and recover object descriptions at multiple levels of granularity from
subsequent multimodal large language model (MLLM). Second, we introduce a
cross-subject training strategy mapping subject-specific features to a common
feature space. This allows a model to be trained on multiple subjects without
extra resources, even yielding superior results compared to subject-specific
models. Further, we demonstrate this supports weakly-supervised adaptation to
new subjects, with only a fraction of the total training data. Experiments
demonstrate that UMBRAE not only achieves superior results in the newly
introduced tasks but also outperforms methods in well established tasks. To
assess our method, we construct and share with the community a comprehensive
brain understanding benchmark BrainHub. Our code and benchmark are available at
https://weihaox.github.io/UMBRAE.
]]></content:encoded>
<pubDate>2024-04-10T17:59:20Z</pubDate>
</item>
<item>
<title>MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video
  Understanding</title>
<link>http://arxiv.org/abs/2404.05726v1</link>
<guid>http://arxiv.org/abs/2404.05726v1</guid>
<content:encoded><![CDATA[
<div>     

LLMsGPULLMshttps://boheumd.github.io/MA-LMM/ <br /><br />: <br /> <div>
With the success of large language models (LLMs), integrating the vision
model into LLMs to build vision-language foundation models has gained much more
interest recently. However, existing LLM-based large multimodal models (e.g.,
Video-LLaMA, VideoChat) can only take in a limited number of frames for short
video understanding. In this study, we mainly focus on designing an efficient
and effective model for long-term video understanding. Instead of trying to
process more frames simultaneously like most existing work, we propose to
process videos in an online manner and store past video information in a memory
bank. This allows our model to reference historical video content for long-term
analysis without exceeding LLMs' context length constraints or GPU memory
limits. Our memory bank can be seamlessly integrated into current multimodal
LLMs in an off-the-shelf manner. We conduct extensive experiments on various
video understanding tasks, such as long-video understanding, video question
answering, and video captioning, and our model can achieve state-of-the-art
performances across multiple datasets. Code available at
https://boheumd.github.io/MA-LMM/.
]]></content:encoded>
<pubDate>2024-04-08T17:59:24Z</pubDate>
</item>
<item>
<title>Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs</title>
<link>http://arxiv.org/abs/2404.05719v1</link>
<guid>http://arxiv.org/abs/2404.05719v1</guid>
<content:encoded><![CDATA[
<div> : Multimodal large language models, Ferret-UI, user interface, training samples, model evaluation

Ferret-UIMultimodal large language model"any resolution"UI/Ferret-UIUIFerret-UIUI MLLMsUIGPT-4V

<br /><br />:
Ferret-UIMultimodal large language modelUIFerret-UIUIUI MLLMsGPT-4V <div>
Recent advancements in multimodal large language models (MLLMs) have been
noteworthy, yet, these general-domain MLLMs often fall short in their ability
to comprehend and interact effectively with user interface (UI) screens. In
this paper, we present Ferret-UI, a new MLLM tailored for enhanced
understanding of mobile UI screens, equipped with referring, grounding, and
reasoning capabilities. Given that UI screens typically exhibit a more
elongated aspect ratio and contain smaller objects of interest (e.g., icons,
texts) than natural images, we incorporate "any resolution" on top of Ferret to
magnify details and leverage enhanced visual features. Specifically, each
screen is divided into 2 sub-images based on the original aspect ratio (i.e.,
horizontal division for portrait screens and vertical division for landscape
screens). Both sub-images are encoded separately before being sent to LLMs. We
meticulously gather training samples from an extensive range of elementary UI
tasks, such as icon recognition, find text, and widget listing. These samples
are formatted for instruction-following with region annotations to facilitate
precise referring and grounding. To augment the model's reasoning ability, we
further compile a dataset for advanced tasks, including detailed description,
perception/interaction conversations, and function inference. After training on
the curated datasets, Ferret-UI exhibits outstanding comprehension of UI
screens and the capability to execute open-ended instructions. For model
evaluation, we establish a comprehensive benchmark encompassing all the
aforementioned tasks. Ferret-UI excels not only beyond most open-source UI
MLLMs, but also surpasses GPT-4V on all the elementary UI tasks.
]]></content:encoded>
<pubDate>2024-04-08T17:55:44Z</pubDate>
</item>
<item>
<title>SwapAnything: Enabling Arbitrary Object Swapping in Personalized Visual
  Editing</title>
<link>http://arxiv.org/abs/2404.05717v1</link>
<guid>http://arxiv.org/abs/2404.05717v1</guid>
<content:encoded><![CDATA[
Effective editing of personal content holds a pivotal role in enabling
individuals to express their creativity, weaving captivating narratives within
their visual stories, and elevate the overall quality and impact of their
visual content. Therefore, in this work, we introduce SwapAnything, a novel
framework that can swap any objects in an image with personalized concepts
given by the reference, while keeping the context unchanged. Compared with
existing methods for personalized subject swapping, SwapAnything has three
unique advantages: (1) precise control of arbitrary objects and parts rather
than the main subject, (2) more faithful preservation of context pixels, (3)
better adaptation of the personalized concept to the image. First, we propose
targeted variable swapping to apply region control over latent feature maps and
swap masked variables for faithful context preservation and initial semantic
concept swapping. Then, we introduce appearance adaptation, to seamlessly adapt
the semantic concept into the original image in terms of target location,
shape, style, and content during the image generation process. Extensive
results on both human and automatic evaluation demonstrate significant
improvements of our approach over baseline methods on personalized swapping.
Furthermore, SwapAnything shows its precise and faithful swapping abilities
across single object, multiple objects, partial object, and cross-domain
swapping tasks. SwapAnything also achieves great performance on text-based
swapping and tasks beyond swapping such as object insertion.
]]></content:encoded>
<pubDate>2024-04-08T17:52:29Z</pubDate>
</item>
<item>
<title>Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation</title>
<link>http://arxiv.org/abs/2404.04256v1</link>
<guid>http://arxiv.org/abs/2404.04256v1</guid>
<content:encoded><![CDATA[
<div> , AI, , , Sigma, Siamese Mamba<br />
<br />
SigmaSelective Structured State Space Model, Mamba, SiameseMambaRGB-RGB-State Space Models (SSMs) https://github.com/zifuwan/Sigma<br />
<br />: SigmaSiamese MambaState Space Models (SSMs)RGB-RGB- <div>
Multi-modal semantic segmentation significantly enhances AI agents'
perception and scene understanding, especially under adverse conditions like
low-light or overexposed environments. Leveraging additional modalities
(X-modality) like thermal and depth alongside traditional RGB provides
complementary information, enabling more robust and reliable segmentation. In
this work, we introduce Sigma, a Siamese Mamba network for multi-modal semantic
segmentation, utilizing the Selective Structured State Space Model, Mamba.
Unlike conventional methods that rely on CNNs, with their limited local
receptive fields, or Vision Transformers (ViTs), which offer global receptive
fields at the cost of quadratic complexity, our model achieves global receptive
fields coverage with linear complexity. By employing a Siamese encoder and
innovating a Mamba fusion mechanism, we effectively select essential
information from different modalities. A decoder is then developed to enhance
the channel-wise modeling ability of the model. Our method, Sigma, is
rigorously evaluated on both RGB-Thermal and RGB-Depth segmentation tasks,
demonstrating its superiority and marking the first successful application of
State Space Models (SSMs) in multi-modal perception tasks. Code is available at
https://github.com/zifuwan/Sigma.
]]></content:encoded>
<pubDate>2024-04-05T17:59:44Z</pubDate>
</item>
<item>
<title>CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept
  Matching</title>
<link>http://arxiv.org/abs/2404.03653v1</link>
<guid>http://arxiv.org/abs/2404.03653v1</guid>
<content:encoded><![CDATA[
<div> : Diffusion models, text-to-image generation, CoMat, fine-tuning strategy, image captioning

:
DiffusionCoMatSDXLCoMat-SDXLSDXL<br /><br />:  <div>
Diffusion models have demonstrated great success in the field of
text-to-image generation. However, alleviating the misalignment between the
text prompts and images is still challenging. The root reason behind the
misalignment has not been extensively investigated. We observe that the
misalignment is caused by inadequate token attention activation. We further
attribute this phenomenon to the diffusion model's insufficient condition
utilization, which is caused by its training paradigm. To address the issue, we
propose CoMat, an end-to-end diffusion model fine-tuning strategy with an
image-to-text concept matching mechanism. We leverage an image captioning model
to measure image-to-text alignment and guide the diffusion model to revisit
ignored tokens. A novel attribute concentration module is also proposed to
address the attribute binding problem. Without any image or human preference
data, we use only 20K text prompts to fine-tune SDXL to obtain CoMat-SDXL.
Extensive experiments show that CoMat-SDXL significantly outperforms the
baseline model SDXL in two text-to-image alignment benchmarks and achieves
start-of-the-art performance.
]]></content:encoded>
<pubDate>2024-04-04T17:59:46Z</pubDate>
</item>
<item>
<title>AutoWebGLM: Bootstrap And Reinforce A Large Language Model-based Web
  Navigating Agent</title>
<link>http://arxiv.org/abs/2404.03648v1</link>
<guid>http://arxiv.org/abs/2404.03648v1</guid>
<content:encoded><![CDATA[
<div> GPT-4,Automated Web Navigation, AutoWebGLM, ChatGLM3-6B, Reinforcement Learning
<br /><br />:
HTMLAutoWebGLMChatGLM3-6BGPT-4HTMLAutoWebBenchAutoWebGLM <div>
Large language models (LLMs) have fueled many intelligent agent tasks, such
as web navigation -- but most existing agents perform far from satisfying in
real-world webpages due to three factors: (1) the versatility of actions on
webpages, (2) HTML text exceeding model processing capacity, and (3) the
complexity of decision-making due to the open-domain nature of web. In light of
the challenge, we develop AutoWebGLM, a GPT-4-outperforming automated web
navigation agent built upon ChatGLM3-6B. Inspired by human browsing patterns,
we design an HTML simplification algorithm to represent webpages, preserving
vital information succinctly. We employ a hybrid human-AI method to build web
browsing data for curriculum training. Then, we bootstrap the model by
reinforcement learning and rejection sampling to further facilitate webpage
comprehension, browser operations, and efficient task decomposition by itself.
For testing, we establish a bilingual benchmark -- AutoWebBench -- for
real-world web browsing tasks. We evaluate AutoWebGLM across diverse web
navigation benchmarks, revealing its improvements but also underlying
challenges to tackle real environments. Related code, model, and data will be
released at \url{https://github.com/THUDM/AutoWebGLM}.
]]></content:encoded>
<pubDate>2024-04-04T17:58:40Z</pubDate>
</item>
<item>
<title>Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale
  Prediction</title>
<link>http://arxiv.org/abs/2404.02905v1</link>
<guid>http://arxiv.org/abs/2404.02905v1</guid>
<content:encoded><![CDATA[
<div> : Visual AutoRegressive modeling, next-scale prediction, image generation, scalability, zero-shot generalization<br />
<br />
Visual AutoRegressive modeling (VAR)(AR)VARARImageNet 256x256VARFrechet inception(FID)18.651.80Inception(IS)80.4356.420ARVAR(DiT)VARLLMs-0.998VARVARLLMsAR/VAR<br /><br />:Visual AutoRegressive modeling (VAR)VARVARAR/VAR <div>
We present Visual AutoRegressive modeling (VAR), a new generation paradigm
that redefines the autoregressive learning on images as coarse-to-fine
"next-scale prediction" or "next-resolution prediction", diverging from the
standard raster-scan "next-token prediction". This simple, intuitive
methodology allows autoregressive (AR) transformers to learn visual
distributions fast and generalize well: VAR, for the first time, makes AR
models surpass diffusion transformers in image generation. On ImageNet 256x256
benchmark, VAR significantly improve AR baseline by improving Frechet inception
distance (FID) from 18.65 to 1.80, inception score (IS) from 80.4 to 356.4,
with around 20x faster inference speed. It is also empirically verified that
VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions
including image quality, inference speed, data efficiency, and scalability.
Scaling up VAR models exhibits clear power-law scaling laws similar to those
observed in LLMs, with linear correlation coefficients near -0.998 as solid
evidence. VAR further showcases zero-shot generalization ability in downstream
tasks including image in-painting, out-painting, and editing. These results
suggest VAR has initially emulated the two important properties of LLMs:
Scaling Laws and zero-shot task generalization. We have released all models and
codes to promote the exploration of AR/VAR models for visual generation and
unified learning.
]]></content:encoded>
<pubDate>2024-04-03T17:59:53Z</pubDate>
</item>
<item>
<title>ALOHa: A New Measure for Hallucination in Captioning Models</title>
<link>http://arxiv.org/abs/2404.02904v1</link>
<guid>http://arxiv.org/abs/2404.02904v1</guid>
<content:encoded><![CDATA[
<div> multimodal pre-training, visual description, object hallucination, ALOHa, large language modelsLLMs

ALOHa ALohaHATnocapsCHIAR

<br /><br />: ALOHaALOHaHATnocapsCHIAR <div>
Despite recent advances in multimodal pre-training for visual description,
state-of-the-art models still produce captions containing errors, such as
hallucinating objects not present in a scene. The existing prominent metric for
object hallucination, CHAIR, is limited to a fixed set of MS COCO objects and
synonyms. In this work, we propose a modernized open-vocabulary metric, ALOHa,
which leverages large language models (LLMs) to measure object hallucinations.
Specifically, we use an LLM to extract groundable objects from a candidate
caption, measure their semantic similarity to reference objects from captions
and object detections, and use Hungarian matching to produce a final
hallucination score. We show that ALOHa correctly identifies 13.6% more
hallucinated objects than CHAIR on HAT, a new gold-standard subset of MS COCO
Captions annotated for hallucinations, and 30.8% more on nocaps, where objects
extend beyond MS COCO categories. Our code is available at
https://davidmchan.github.io/aloha/.
]]></content:encoded>
<pubDate>2024-04-03T17:59:36Z</pubDate>
</item>
<item>
<title>MatAtlas: Text-driven Consistent Geometry Texturing and Material
  Assignment</title>
<link>http://arxiv.org/abs/2404.02899v1</link>
<guid>http://arxiv.org/abs/2404.02899v1</guid>
<content:encoded><![CDATA[
We present MatAtlas, a method for consistent text-guided 3D model texturing.
Following recent progress we leverage a large scale text-to-image generation
model (e.g., Stable Diffusion) as a prior to texture a 3D model. We carefully
design an RGB texturing pipeline that leverages a grid pattern diffusion,
driven by depth and edges. By proposing a multi-step texture refinement
process, we significantly improve the quality and 3D consistency of the
texturing output. To further address the problem of baked-in lighting, we move
beyond RGB colors and pursue assigning parametric materials to the assets.
Given the high-quality initial RGB texture, we propose a novel material
retrieval method capitalized on Large Language Models (LLM), enabling
editabiliy and relightability. We evaluate our method on a wide variety of
geometries and show that our method significantly outperform prior arts. We
also analyze the role of each component through a detailed ablation study.
]]></content:encoded>
<pubDate>2024-04-03T17:57:15Z</pubDate>
</item>
<item>
<title>A Mean Field Game Model for Timely Computation in Edge Computing Systems</title>
<link>http://arxiv.org/abs/2404.02898v1</link>
<guid>http://arxiv.org/abs/2404.02898v1</guid>
<content:encoded><![CDATA[
We consider the problem of task offloading in multi-access edge computing
(MEC) systems constituting $N$ devices assisted by an edge server (ES), where
the devices can split task execution between a local processor and the ES.
Since the local task execution and communication with the ES both consume
power, each device must judiciously choose between the two. We model the
problem as a large population non-cooperative game among the $N$ devices. Since
computation of an equilibrium in this scenario is difficult due to the presence
of a large number of devices, we employ the mean-field game framework to reduce
the finite-agent game problem to a generic user's multi-objective optimization
problem, with a coupled consistency condition. By leveraging the novel age of
information (AoI) metric, we invoke techniques from stochastic hybrid systems
(SHS) theory and study the tradeoffs between increasing information freshness
and reducing power consumption. In numerical simulations, we validate that a
higher load at the ES may lead devices to upload their task to the ES less
often.
]]></content:encoded>
<pubDate>2024-04-03T17:55:20Z</pubDate>
</item>
<item>
<title>Segment Any 3D Object with Language</title>
<link>http://arxiv.org/abs/2404.02157v1</link>
<guid>http://arxiv.org/abs/2404.02157v1</guid>
<content:encoded><![CDATA[
<div> Open-Vocabulary 3D Instance Segmentation, free-form language instructions, semantic-aware, geometric-aware, multimodal fusion network<br />
<br />
3DOV-3DIS2D3D3DSegment any 3D Object with LanguagESOLE-3D3DSOLEScanNetv2ScanNet200ReplicaSOLE<br /><br />: 3DOV-3DISSOLE-SOLESOLE <div>
In this paper, we investigate Open-Vocabulary 3D Instance Segmentation
(OV-3DIS) with free-form language instructions. Earlier works that rely on only
annotated base categories for training suffer from limited generalization to
unseen novel categories. Recent works mitigate poor generalizability to novel
categories by generating class-agnostic masks or projecting generalized masks
from 2D to 3D, but disregard semantic or geometry information, leading to
sub-optimal performance. Instead, generating generalizable but semantic-related
masks directly from 3D point clouds would result in superior outcomes. In this
paper, we introduce Segment any 3D Object with LanguagE (SOLE), which is a
semantic and geometric-aware visual-language learning framework with strong
generalizability by generating semantic-related masks directly from 3D point
clouds. Specifically, we propose a multimodal fusion network to incorporate
multimodal semantics in both backbone and decoder. In addition, to align the 3D
segmentation model with various language instructions and enhance the mask
quality, we introduce three types of multimodal associations as supervision.
Our SOLE outperforms previous methods by a large margin on ScanNetv2,
ScanNet200, and Replica benchmarks, and the results are even close to the
fully-supervised counterpart despite the absence of class annotations in the
training. Furthermore, extensive qualitative results demonstrate the
versatility of our SOLE to language instructions.
]]></content:encoded>
<pubDate>2024-04-02T17:59:10Z</pubDate>
</item>
<item>
<title>Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks</title>
<link>http://arxiv.org/abs/2404.02151v1</link>
<guid>http://arxiv.org/abs/2404.02151v1</guid>
<content:encoded><![CDATA[
<div> : LLMs, adaptive attacks, jailbreaking, logprobs, vulnerabilities

: 
LLMslogprobslogprobsClaudeAPIhttps://github.com/tml-epfl/llm-adaptive-attacks<br /><br /> <div>
We show that even the most recent safety-aligned LLMs are not robust to
simple adaptive jailbreaking attacks. First, we demonstrate how to successfully
leverage access to logprobs for jailbreaking: we initially design an
adversarial prompt template (sometimes adapted to the target LLM), and then we
apply random search on a suffix to maximize the target logprob (e.g., of the
token "Sure"), potentially with multiple restarts. In this way, we achieve
nearly 100\% attack success rate -- according to GPT-4 as a judge -- on
GPT-3.5/4, Llama-2-Chat-7B/13B/70B, Gemma-7B, and R2D2 from HarmBench that was
adversarially trained against the GCG attack. We also show how to jailbreak all
Claude models -- that do not expose logprobs -- via either a transfer or
prefilling attack with 100\% success rate. In addition, we show how to use
random search on a restricted set of tokens for finding trojan strings in
poisoned models -- a task that shares many similarities with jailbreaking --
which is the algorithm that brought us the first place in the SaTML'24 Trojan
Detection Competition. The common theme behind these attacks is that adaptivity
is crucial: different models are vulnerable to different prompting templates
(e.g., R2D2 is very sensitive to in-context learning prompts), some models have
unique vulnerabilities based on their APIs (e.g., prefilling for Claude), and
in some settings it is crucial to restrict the token search space based on
prior knowledge (e.g., for trojan detection). We provide the code, prompts, and
logs of the attacks at https://github.com/tml-epfl/llm-adaptive-attacks.
]]></content:encoded>
<pubDate>2024-04-02T17:58:27Z</pubDate>
</item>
<item>
<title>Diffusion$^2$: Dynamic 3D Content Generation via Score Composition of
  Orthogonal Diffusion Models</title>
<link>http://arxiv.org/abs/2404.02148v1</link>
<guid>http://arxiv.org/abs/2404.02148v1</guid>
<content:encoded><![CDATA[
Recent advancements in 3D generation are predominantly propelled by
improvements in 3D-aware image diffusion models which are pretrained on
Internet-scale image data and fine-tuned on massive 3D data, offering the
capability of producing highly consistent multi-view images. However, due to
the scarcity of synchronized multi-view video data, it is impractical to adapt
this paradigm to 4D generation directly. Despite that, the available video and
3D data are adequate for training video and multi-view diffusion models that
can provide satisfactory dynamic and geometric priors respectively. In this
paper, we present Diffusion$^2$, a novel framework for dynamic 3D content
creation that leverages the knowledge about geometric consistency and temporal
smoothness from these models to directly sample dense multi-view and
multi-frame images which can be employed to optimize continuous 4D
representation. Specifically, we design a simple yet effective denoising
strategy via score composition of video and multi-view diffusion models based
on the probability structure of the images to be generated. Owing to the high
parallelism of the image generation and the efficiency of the modern 4D
reconstruction pipeline, our framework can generate 4D content within few
minutes. Furthermore, our method circumvents the reliance on 4D data, thereby
having the potential to benefit from the scalability of the foundation video
and multi-view diffusion models. Extensive experiments demonstrate the efficacy
of our proposed framework and its capability to flexibly adapt to various types
of prompts.
]]></content:encoded>
<pubDate>2024-04-02T17:58:03Z</pubDate>
</item>
<item>
<title>Iterated Learning Improves Compositionality in Large Vision-Language
  Models</title>
<link>http://arxiv.org/abs/2404.02145v1</link>
<guid>http://arxiv.org/abs/2404.02145v1</guid>
<content:encoded><![CDATA[
A fundamental characteristic common to both human vision and natural language
is their compositional nature. Yet, despite the performance gains contributed
by large vision and language pretraining, recent investigations find that
most-if not all-our state-of-the-art vision-language models struggle at
compositionality. They are unable to distinguish between images of " a girl in
white facing a man in black" and "a girl in black facing a man in white".
Moreover, prior work suggests that compositionality doesn't arise with scale:
larger model sizes or training data don't help. This paper develops a new
iterated training algorithm that incentivizes compositionality. We draw on
decades of cognitive science research that identifies cultural transmission-the
need to teach a new generation-as a necessary inductive prior that incentivizes
humans to develop compositional languages. Specifically, we reframe
vision-language contrastive learning as the Lewis Signaling Game between a
vision agent and a language agent, and operationalize cultural transmission by
iteratively resetting one of the agent's weights during training. After every
iteration, this training paradigm induces representations that become "easier
to learn", a property of compositional languages: e.g. our model trained on
CC3M and CC12M improves standard CLIP by 4.7%, 4.0% respectfully in the
SugarCrepe benchmark.
]]></content:encoded>
<pubDate>2024-04-02T17:57:31Z</pubDate>
</item>
<item>
<title>Detecting Image Attribution for Text-to-Image Diffusion Models in RGB
  and Beyond</title>
<link>http://arxiv.org/abs/2403.19653v1</link>
<guid>http://arxiv.org/abs/2403.19653v1</guid>
<content:encoded><![CDATA[
<div> T2I

T2I12T2IRGB <br /><br />: T2I <div>
Modern text-to-image (T2I) diffusion models can generate images with
remarkable realism and creativity. These advancements have sparked research in
fake image detection and attribution, yet prior studies have not fully explored
the practical and scientific dimensions of this task. In addition to
attributing images to 12 state-of-the-art T2I generators, we provide extensive
analyses on what inference stage hyperparameters and image modifications are
discernible. Our experiments reveal that initialization seeds are highly
detectable, along with other subtle variations in the image generation process
to some extent. We further investigate what visual traces are leveraged in
image attribution by perturbing high-frequency details and employing mid-level
representations of image style and structure. Notably, altering high-frequency
information causes only slight reductions in accuracy, and training an
attributor on style representations outperforms training on RGB images. Our
analyses underscore that fake images are detectable and attributable at various
levels of visual granularity than previously explored.
]]></content:encoded>
<pubDate>2024-03-28T17:59:42Z</pubDate>
</item>
<item>
<title>MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions</title>
<link>http://arxiv.org/abs/2403.19651v1</link>
<guid>http://arxiv.org/abs/2403.19651v1</guid>
<content:encoded><![CDATA[
<div> MagicLens
<br /><br />:
MagicLensMagicLens360050140MagicLens <div>
Image retrieval, i.e., finding desired images given a reference image,
inherently encompasses rich, multi-faceted search intents that are difficult to
capture solely using image-based measures. Recent work leverages text
instructions to allow users to more freely express their search intents.
However, existing work primarily focuses on image pairs that are visually
similar and/or can be characterized by a small set of pre-defined relations.
The core thesis of this paper is that text instructions can enable retrieving
images with richer relations beyond visual similarity. To show this, we
introduce MagicLens, a series of self-supervised image retrieval models that
support open-ended instructions. MagicLens is built on a key novel insight:
image pairs that naturally occur on the same web pages contain a wide range of
implicit relations (e.g., inside view of), and we can bring those implicit
relations explicit by synthesizing instructions via large multimodal models
(LMMs) and large language models (LLMs). Trained on 36.7M (query image,
instruction, target image) triplets with rich semantic relations mined from the
web, MagicLens achieves comparable or better results on eight benchmarks of
various image retrieval tasks than prior state-of-the-art (SOTA) methods.
Remarkably, it outperforms previous SOTA but with a 50X smaller model size on
multiple benchmarks. Additional human analyses on a 1.4M-image unseen corpus
further demonstrate the diversity of search intents supported by MagicLens.
]]></content:encoded>
<pubDate>2024-03-28T17:59:20Z</pubDate>
</item>
<item>
<title>Human-compatible driving partners through data-regularized self-play
  reinforcement learning</title>
<link>http://arxiv.org/abs/2403.19648v1</link>
<guid>http://arxiv.org/abs/2403.19648v1</guid>
<content:encoded><![CDATA[
A central challenge for autonomous vehicles is coordinating with humans.
Therefore, incorporating realistic human agents is essential for scalable
training and evaluation of autonomous driving systems in simulation. Simulation
agents are typically developed by imitating large-scale, high-quality datasets
of human driving. However, pure imitation learning agents empirically have high
collision rates when executed in a multi-agent closed-loop setting. To build
agents that are realistic and effective in closed-loop settings, we propose
Human-Regularized PPO (HR-PPO), a multi-agent algorithm where agents are
trained through self-play with a small penalty for deviating from a human
reference policy. In contrast to prior work, our approach is RL-first and only
uses 30 minutes of imperfect human demonstrations. We evaluate agents in a
large set of multi-agent traffic scenes. Results show our HR-PPO agents are
highly effective in achieving goals, with a success rate of 93%, an off-road
rate of 3.5%, and a collision rate of 3%. At the same time, the agents drive in
a human-like manner, as measured by their similarity to existing human driving
logs. We also find that HR-PPO agents show considerable improvements on proxy
measures for coordination with human driving, particularly in highly
interactive scenarios. We open-source our code and trained agents at
https://github.com/Emerge-Lab/nocturne_lab and provide demonstrations of agent
behaviors at https://sites.google.com/view/driving-partners.
]]></content:encoded>
<pubDate>2024-03-28T17:56:56Z</pubDate>
</item>
<item>
<title>GANTASTIC: GAN-based Transfer of Interpretable Directions for
  Disentangled Image Editing in Text-to-Image Diffusion Models</title>
<link>http://arxiv.org/abs/2403.19645v1</link>
<guid>http://arxiv.org/abs/2403.19645v1</guid>
<content:encoded><![CDATA[
The rapid advancement in image generation models has predominantly been
driven by diffusion models, which have demonstrated unparalleled success in
generating high-fidelity, diverse images from textual prompts. Despite their
success, diffusion models encounter substantial challenges in the domain of
image editing, particularly in executing disentangled edits-changes that target
specific attributes of an image while leaving irrelevant parts untouched. In
contrast, Generative Adversarial Networks (GANs) have been recognized for their
success in disentangled edits through their interpretable latent spaces. We
introduce GANTASTIC, a novel framework that takes existing directions from
pre-trained GAN models-representative of specific, controllable attributes-and
transfers these directions into diffusion-based models. This novel approach not
only maintains the generative quality and diversity that diffusion models are
known for but also significantly enhances their capability to perform precise,
targeted image edits, thereby leveraging the best of both worlds.
]]></content:encoded>
<pubDate>2024-03-28T17:55:16Z</pubDate>
</item>
<item>
<title>SLEDGE: Synthesizing Simulation Environments for Driving Agents with
  Generative Models</title>
<link>http://arxiv.org/abs/2403.17933v1</link>
<guid>http://arxiv.org/abs/2403.17933v1</guid>
<content:encoded><![CDATA[
<div> 
<br /><br />SLEDGE(RVAE)RVAESLEDGEnuPlanSLEDGE500(<4GB)SLEDGEnuPlanPDM40% <div>
SLEDGE is the first generative simulator for vehicle motion planning trained
on real-world driving logs. Its core component is a learned model that is able
to generate agent bounding boxes and lane graphs. The model's outputs serve as
an initial state for traffic simulation. The unique properties of the entities
to be generated for SLEDGE, such as their connectivity and variable count per
scene, render the naive application of most modern generative models to this
task non-trivial. Therefore, together with a systematic study of existing lane
graph representations, we introduce a novel raster-to-vector autoencoder
(RVAE). It encodes agents and the lane graph into distinct channels in a
rasterized latent map. This facilitates both lane-conditioned agent generation
and combined generation of lanes and agents with a Diffusion Transformer. Using
generated entities in SLEDGE enables greater control over the simulation, e.g.
upsampling turns or increasing traffic density. Further, SLEDGE can support
500m long routes, a capability not found in existing data-driven simulators
like nuPlan. It presents new challenges for planning algorithms, evidenced by
failure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge,
when tested on hard routes and dense traffic generated by our model. Compared
to nuPlan, SLEDGE requires 500$\times$ less storage to set up (<4GB), making it
a more accessible option and helping with democratizing future research in this
field.
]]></content:encoded>
<pubDate>2024-03-26T17:58:29Z</pubDate>
</item>
<item>
<title>MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution</title>
<link>http://arxiv.org/abs/2403.17927v1</link>
<guid>http://arxiv.org/abs/2403.17927v1</guid>
<content:encoded><![CDATA[
<div> GitHub, LLMs, MAGIS, software evolution, agents
:<br />
GitHubLLMsLLMsGitHubLLMAgentMAGISSWE-benchMAGISLLMsGPT-3.5GPT-4Claude-2MAGIS13.94%GitHubMAGISGPT-4GitHub <div>
In software evolution, resolving the emergent issues within GitHub
repositories is a complex challenge that involves not only the incorporation of
new code but also the maintenance of existing functionalities. Large Language
Models (LLMs) have shown promise in code generation and understanding but face
difficulties in code change, particularly at the repository level. To overcome
these challenges, we empirically study the reason why LLMs mostly fail to
resolve GitHub issues and analyze some impact factors. Motivated by the
empirical findings, we propose a novel LLM-based Multi-Agent framework for
GitHub Issue reSolution, MAGIS, consisting of four kinds of agents customized
for the software evolution: Manager, Repository Custodian, Developer, and
Quality Assurance Engineer agents. This framework leverages the collaboration
of various agents in the planning and coding process to unlock the potential of
LLMs to resolve GitHub issues. In experiments, we employ the SWE-bench
benchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4, and
Claude-2. MAGIS can resolve 13.94% GitHub issues, which significantly
outperforms the baselines. Specifically, MAGIS achieves an eight-fold increase
in resolved ratio over the direct application of GPT-4, the based LLM of our
method. We also analyze the factors for improving GitHub issue resolution
rates, such as line location, task allocation, etc.
]]></content:encoded>
<pubDate>2024-03-26T17:57:57Z</pubDate>
</item>
<item>
<title>An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems</title>
<link>http://arxiv.org/abs/2403.16809v1</link>
<guid>http://arxiv.org/abs/2403.16809v1</guid>
<content:encoded><![CDATA[
<div> : CPS-IoT, (LLMs), , , <br />
<br />
(LLMs)Cyber-Physical Systems and the Internet of Things (CPS-IoT) LLMsLLMsCPS-IoTFoundation ModelsLLMsCPS-IoT <div>
The increasing prevalence of Cyber-Physical Systems and the Internet of
Things (CPS-IoT) applications and Foundation Models are enabling new
applications that leverage real-time control of the environment. For example,
real-time control of Heating, Ventilation and Air-Conditioning (HVAC) systems
can reduce its usage when not needed for the comfort of human occupants, hence
reducing energy consumption. Collecting real-time feedback on human preferences
in such human-in-the-loop (HITL) systems, however, is difficult in practice. We
propose the use of large language models (LLMs) to deal with the challenges of
dynamic environments and difficult-to-obtain data in CPS optimization. In this
paper, we present a case study that employs LLM agents to mimic the behaviors
and thermal preferences of various population groups (e.g. young families, the
elderly) in a shopping mall. The aggregated thermal preferences are integrated
into an agent-in-the-loop based reinforcement learning algorithm AitL-RL, which
employs the LLM as a dynamic simulation of the physical environment to learn
how to balance between energy savings and occupant comfort. Our results show
that LLMs are capable of simulating complex population movements within large
open spaces. Besides, AitL-RL demonstrates superior performance compared to the
popular existing policy of set point control, suggesting that adaptive and
personalized decision-making is critical for efficient optimization in CPS-IoT
applications. Through this case study, we demonstrate the potential of
integrating advanced Foundation Models like LLMs into CPS-IoT to enhance system
adaptability and efficiency. The project's code can be found on our GitHub
repository.
]]></content:encoded>
<pubDate>2024-03-25T14:32:28Z</pubDate>
</item>
<item>
<title>Exploiting Priors from 3D Diffusion Models for RGB-Based One-Shot View
  Planning</title>
<link>http://arxiv.org/abs/2403.16803v1</link>
<guid>http://arxiv.org/abs/2403.16803v1</guid>
<content:encoded><![CDATA[
<div> , , , 3D, 

3DRGB<br /><br />: 3D <div>
Object reconstruction is relevant for many autonomous robotic tasks that
require interaction with the environment. A key challenge in such scenarios is
planning view configurations to collect informative measurements for
reconstructing an initially unknown object. One-shot view planning enables
efficient data collection by predicting view configurations and planning the
globally shortest path connecting all views at once. However, geometric priors
about the object are required to conduct one-shot view planning. In this work,
we propose a novel one-shot view planning approach that utilizes the powerful
3D generation capabilities of diffusion models as priors. By incorporating such
geometric priors into our pipeline, we achieve effective one-shot view planning
starting with only a single RGB image of the object to be reconstructed. Our
planning experiments in simulation and real-world setups indicate that our
approach balances well between object reconstruction quality and movement cost.
]]></content:encoded>
<pubDate>2024-03-25T14:21:49Z</pubDate>
</item>
<item>
<title>LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis</title>
<link>http://arxiv.org/abs/2403.15385v1</link>
<guid>http://arxiv.org/abs/2403.15385v1</guid>
<content:encoded><![CDATA[
<div> : 3D, LATTE3D, , 3D, 

:<br /><br />
LATTE3D3D3DLATTE3D400LATTE3D <div>
Recent text-to-3D generation approaches produce impressive 3D results but
require time-consuming optimization that can take up to an hour per prompt.
Amortized methods like ATT3D optimize multiple prompts simultaneously to
improve efficiency, enabling fast text-to-3D synthesis. However, they cannot
capture high-frequency geometry and texture details and struggle to scale to
large prompt sets, so they generalize poorly. We introduce LATTE3D, addressing
these limitations to achieve fast, high-quality generation on a significantly
larger prompt set. Key to our method is 1) building a scalable architecture and
2) leveraging 3D data during optimization through 3D-aware diffusion priors,
shape regularization, and model initialization to achieve robustness to diverse
and complex training prompts. LATTE3D amortizes both neural field and textured
surface generation to produce highly detailed textured meshes in a single
forward pass. LATTE3D generates 3D objects in 400ms, and can be further
enhanced with fast test-time optimization.
]]></content:encoded>
<pubDate>2024-03-22T17:59:37Z</pubDate>
</item>
<item>
<title>ThemeStation: Generating Theme-Aware 3D Assets from Few Exemplars</title>
<link>http://arxiv.org/abs/2403.15383v1</link>
<guid>http://arxiv.org/abs/2403.15383v1</guid>
<content:encoded><![CDATA[
<div> 3D assets, ThemeStation, generation, diversity, quality
<br /><br />
ThemeStation3D3D3D3DDSDThemeStation3DThemeStation3D3D
<br /> 
:ThemeStation3D3D3DDSDThemeStation3D <div>
Real-world applications often require a large gallery of 3D assets that share
a consistent theme. While remarkable advances have been made in general 3D
content creation from text or image, synthesizing customized 3D assets
following the shared theme of input 3D exemplars remains an open and
challenging problem. In this work, we present ThemeStation, a novel approach
for theme-aware 3D-to-3D generation. ThemeStation synthesizes customized 3D
assets based on given few exemplars with two goals: 1) unity for generating 3D
assets that thematically align with the given exemplars and 2) diversity for
generating 3D assets with a high degree of variations. To this end, we design a
two-stage framework that draws a concept image first, followed by a
reference-informed 3D modeling stage. We propose a novel dual score
distillation (DSD) loss to jointly leverage priors from both the input
exemplars and the synthesized concept image. Extensive experiments and user
studies confirm that ThemeStation surpasses prior works in producing diverse
theme-aware 3D models with impressive quality. ThemeStation also enables
various applications such as controllable 3D-to-3D generation.
]]></content:encoded>
<pubDate>2024-03-22T17:59:01Z</pubDate>
</item>
<item>
<title>Long-CLIP: Unlocking the Long-Text Capability of CLIP</title>
<link>http://arxiv.org/abs/2403.15378v1</link>
<guid>http://arxiv.org/abs/2403.15378v1</guid>
<content:encoded><![CDATA[
Contrastive Language-Image Pre-training (CLIP) has been the cornerstone for
zero-shot classification, text-image retrieval, and text-image generation by
aligning image and text modalities. Despite its widespread adoption, a
significant limitation of CLIP lies in the inadequate length of text input. The
length of the text token is restricted to 77, and an empirical study shows the
actual effective length is even less than 20. This prevents CLIP from handling
detailed descriptions, limiting its applications for image retrieval and
text-to-image generation with extensive prerequisites. To this end, we propose
Long-CLIP as a plug-and-play alternative to CLIP that supports long-text input,
retains or even surpasses its zero-shot generalizability, and aligns the CLIP
latent space, making it readily replace CLIP without any further adaptation in
downstream frameworks. Nevertheless, achieving this goal is far from
straightforward, as simplistic fine-tuning can result in a significant
degradation of CLIP's performance. Moreover, substituting the text encoder with
a language model supporting longer contexts necessitates pretraining with vast
amounts of data, incurring significant expenses. Accordingly, Long-CLIP
introduces an efficient fine-tuning solution on CLIP with two novel strategies
designed to maintain the original capabilities, including (1) a
knowledge-preserved stretching of positional embedding and (2) a primary
component matching of CLIP features. With leveraging just one million extra
long text-image pairs, Long-CLIP has shown the superiority to CLIP for about
20% in long caption text-image retrieval and 6% in traditional text-image
retrieval tasks, e.g., COCO and Flickr30k. Furthermore, Long-CLIP offers
enhanced capabilities for generating images from detailed text descriptions by
replacing CLIP in a plug-and-play manner.
]]></content:encoded>
<pubDate>2024-03-22T17:58:16Z</pubDate>
</item>
<item>
<title>Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language
  Models</title>
<link>http://arxiv.org/abs/2403.12966v1</link>
<guid>http://arxiv.org/abs/2403.12966v1</guid>
<content:encoded><![CDATA[
<div> --

<br /><br />-Chain-of-Spot (CoS)Chain-of-SpotLLaVA-1.5- <div>
In the realm of vision-language understanding, the proficiency of models in
interpreting and reasoning over visual content has become a cornerstone for
numerous applications. However, it is challenging for the visual encoder in
Large Vision-Language Models (LVLMs) to extract useful features tailored to
questions that aid the language model's response. Furthermore, a common
practice among existing LVLMs is to utilize lower-resolution images, which
restricts the ability for visual recognition. Our work introduces the
Chain-of-Spot (CoS) method, which we describe as Interactive Reasoning, a novel
approach that enhances feature extraction by focusing on key regions of
interest (ROI) within the image, corresponding to the posed questions or
instructions. This technique allows LVLMs to access more detailed visual
information without altering the original image resolution, thereby offering
multi-granularity image features. By integrating Chain-of-Spot with
instruct-following LLaVA-1.5 models, the process of image reasoning
consistently improves performance across a wide range of multimodal datasets
and benchmarks without bells and whistles and achieves new state-of-the-art
results. Our empirical findings demonstrate a significant improvement in LVLMs'
ability to understand and reason about visual content, paving the way for more
sophisticated visual instruction-following applications. Code and models are
available at https://github.com/dongyh20/Chain-of-Spot
]]></content:encoded>
<pubDate>2024-03-19T17:59:52Z</pubDate>
</item>
<item>
<title>LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images</title>
<link>http://arxiv.org/abs/2403.11703v1</link>
<guid>http://arxiv.org/abs/2403.11703v1</guid>
<content:encoded><![CDATA[
<div> : Visual encoding, Large multimodal model (LMM), LLaVA-UHD, , 

LLaVA-UHD9LMMLLaVA-UHD94%LLaVA-1.5 336x3366TextVQA6.4%8A100 GPU23LLaVA-1.526https://github.com/thunlp/LLaVA-UHD<br /><br />: LLaVA-UHDLLaVA-UHDLMM <div>
Visual encoding constitutes the basis of large multimodal models (LMMs) in
understanding the visual world. Conventional LMMs process images in fixed sizes
and limited resolutions, while recent explorations in this direction are
limited in adaptivity, efficiency, and even correctness. In this work, we first
take GPT-4V and LLaVA-1.5 as representative examples and expose systematic
flaws rooted in their visual encoding strategy. To address the challenges, we
present LLaVA-UHD, a large multimodal model that can efficiently perceive
images in any aspect ratio and high resolution. LLaVA-UHD includes three key
components: (1) An image modularization strategy that divides native-resolution
images into smaller variable-sized slices for efficient and extensible
encoding, (2) a compression module that further condenses image tokens from
visual encoders, and (3) a spatial schema to organize slice tokens for LLMs.
Comprehensive experiments show that LLaVA-UHD outperforms established LMMs
trained with 2-3 orders of magnitude more data on 9 benchmarks. Notably, our
model built on LLaVA-1.5 336x336 supports 6 times larger (i.e., 672x1088)
resolution images using only 94% inference computation, and achieves 6.4
accuracy improvement on TextVQA. Moreover, the model can be efficiently trained
in academic settings, within 23 hours on 8 A100 GPUs (vs. 26 hours of
LLaVA-1.5). We make the data and code publicly available at
https://github.com/thunlp/LLaVA-UHD.
]]></content:encoded>
<pubDate>2024-03-18T12:04:11Z</pubDate>
</item>
<item>
<title>Virbo: Multimodal Multilingual Avatar Video Generation in Digital
  Marketing</title>
<link>http://arxiv.org/abs/2403.11700v1</link>
<guid>http://arxiv.org/abs/2403.11700v1</guid>
<content:encoded><![CDATA[
<div> 
<br /><br />:
VirboVirboVirbo <div>
With the widespread popularity of internet celebrity marketing all over the
world, short video production has gradually become a popular way of presenting
products information. However, the traditional video production industry
usually includes series of procedures as script writing, video filming in a
professional studio, video clipping, special effects rendering, customized
post-processing, and so forth. Not to mention that multilingual videos is not
accessible for those who could not speak multilingual languages. These
complicated procedures usually needs a professional team to complete, and this
made short video production costly in both time and money. This paper presents
an intelligent system that supports the automatic generation of talking avatar
videos, namely Virbo. With simply a user-specified script, Virbo could use a
deep generative model to generate a target talking videos. Meanwhile, the
system also supports multimodal inputs to customize the video with specified
face, specified voice and special effects. This system also integrated a
multilingual customization module that supports generate multilingual talking
avatar videos in a batch with hundreds of delicate templates and creative
special effects. Through a series of user studies and demo tests, we found that
Virbo can generate talking avatar videos that maintained a high quality of
videos as those from a professional team while reducing the entire production
costs significantly. This intelligent system will effectively promote the video
production industry and facilitate the internet marketing neglecting of
language barriers and cost challenges.
]]></content:encoded>
<pubDate>2024-03-18T11:56:35Z</pubDate>
</item>
<item>
<title>VideoAgent: Long-form Video Understanding with Large Language Model as
  Agent</title>
<link>http://arxiv.org/abs/2403.10517v1</link>
<guid>http://arxiv.org/abs/2403.10517v1</guid>
<content:encoded><![CDATA[
<div> VideoAgentEgoSchemaNExT-QA benchmarks<br />
:<br />
VideoAgent-EgoSchemaNExT-QAVideoAgent <div>
Long-form video understanding represents a significant challenge within
computer vision, demanding a model capable of reasoning over long multi-modal
sequences. Motivated by the human cognitive process for long-form video
understanding, we emphasize interactive reasoning and planning over the ability
to process lengthy visual inputs. We introduce a novel agent-based system,
VideoAgent, that employs a large language model as a central agent to
iteratively identify and compile crucial information to answer a question, with
vision-language foundation models serving as tools to translate and retrieve
visual information. Evaluated on the challenging EgoSchema and NExT-QA
benchmarks, VideoAgent achieves 54.1% and 71.3% zero-shot accuracy with only
8.4 and 8.2 frames used on average. These results demonstrate superior
effectiveness and efficiency of our method over the current state-of-the-art
methods, highlighting the potential of agent-based approaches in advancing
long-form video understanding.
]]></content:encoded>
<pubDate>2024-03-15T17:57:52Z</pubDate>
</item>
<item>
<title>SCP-Diff: Photo-Realistic Semantic Image Synthesis with
  Spatial-Categorical Joint Prior</title>
<link>http://arxiv.org/abs/2403.09638v1</link>
<guid>http://arxiv.org/abs/2403.09638v1</guid>
<content:encoded><![CDATA[
<div> : Semantic image synthesis, ControlNet, latent diffusion models, noise priors, SCP-Diff

:
GANControlNetControlNetSIS-SCP-DiffCityscapes10.53FIDADE20K12.66FID <br /><br /> <div>
Semantic image synthesis (SIS) shows good promises for sensor simulation.
However, current best practices in this field, based on GANs, have not yet
reached the desired level of quality. As latent diffusion models make
significant strides in image generation, we are prompted to evaluate
ControlNet, a notable method for its dense control capabilities. Our
investigation uncovered two primary issues with its results: the presence of
weird sub-structures within large semantic areas and the misalignment of
content with the semantic mask. Through empirical study, we pinpointed the
cause of these problems as a mismatch between the noised training data
distribution and the standard normal prior applied at the inference stage. To
address this challenge, we developed specific noise priors for SIS,
encompassing spatial, categorical, and a novel spatial-categorical joint prior
for inference. This approach, which we have named SCP-Diff, has yielded
exceptional results, achieving an FID of 10.53 on Cityscapes and 12.66 on
ADE20K.The code and models can be accessed via the project page.
]]></content:encoded>
<pubDate>2024-03-14T17:59:55Z</pubDate>
</item>
<item>
<title>3D-VLA: A 3D Vision-Language-Action Generative World Model</title>
<link>http://arxiv.org/abs/2403.09631v1</link>
<guid>http://arxiv.org/abs/2403.09631v1</guid>
<content:encoded><![CDATA[
<div> 3D-VLA, perception, action, world model, embodied foundation model

3D-VLA--2D3D3D3D3D-VLA<br /><br />: <div>
Recent vision-language-action (VLA) models rely on 2D inputs, lacking
integration with the broader realm of the 3D physical world. Furthermore, they
perform action prediction by learning a direct mapping from perception to
action, neglecting the vast dynamics of the world and the relations between
actions and dynamics. In contrast, human beings are endowed with world models
that depict imagination about future scenarios to plan actions accordingly. To
this end, we propose 3D-VLA by introducing a new family of embodied foundation
models that seamlessly link 3D perception, reasoning, and action through a
generative world model. Specifically, 3D-VLA is built on top of a 3D-based
large language model (LLM), and a set of interaction tokens is introduced to
engage with the embodied environment. Furthermore, to inject generation
abilities into the model, we train a series of embodied diffusion models and
align them into the LLM for predicting the goal images and point clouds. To
train our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by
extracting vast 3D-related information from existing robotics datasets. Our
experiments on held-in datasets demonstrate that 3D-VLA significantly improves
the reasoning, multimodal generation, and planning capabilities in embodied
environments, showcasing its potential in real-world applications.
]]></content:encoded>
<pubDate>2024-03-14T17:58:41Z</pubDate>
</item>
<item>
<title>Quiet-STaR: Language Models Can Teach Themselves to Think Before
  Speaking</title>
<link>http://arxiv.org/abs/2403.09629v1</link>
<guid>http://arxiv.org/abs/2403.09629v1</guid>
<content:encoded><![CDATA[
When writing and talking, people sometimes pause to think. Although
reasoning-focused works have often framed reasoning as a method of answering
questions or completing agentic tasks, reasoning is implicit in almost all
written text. For example, this applies to the steps not stated between the
lines of a proof or to the theory of mind underlying a conversation. In the
Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned
by inferring rationales from few-shot examples in question-answering and
learning from those that lead to a correct answer. This is a highly constrained
setting -- ideally, a language model could instead learn to infer unstated
rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR
in which LMs learn to generate rationales at each token to explain future text,
improving their predictions. We address key challenges, including 1) the
computational cost of generating continuations, 2) the fact that the LM does
not initially know how to generate or use internal thoughts, and 3) the need to
predict beyond individual next tokens. To resolve these, we propose a tokenwise
parallel sampling algorithm, using learnable tokens indicating a thought's
start and end, and an extended teacher-forcing technique. Encouragingly,
generated rationales disproportionately help model difficult-to-predict tokens
and improve the LM's ability to directly answer difficult questions. In
particular, after continued pretraining of an LM on a corpus of internet text
with Quiet-STaR, we find zero-shot improvements on GSM8K
(5.9%$\rightarrow$10.9%) and CommonsenseQA (36.3%$\rightarrow$47.2%) and
observe a perplexity improvement of difficult tokens in natural text.
Crucially, these improvements require no fine-tuning on these tasks. Quiet-STaR
marks a step towards LMs that can learn to reason in a more general and
scalable way.
]]></content:encoded>
<pubDate>2024-03-14T17:58:16Z</pubDate>
</item>
<item>
<title>VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis</title>
<link>http://arxiv.org/abs/2403.08764v1</link>
<guid>http://arxiv.org/abs/2403.08764v1</guid>
<content:encoded><![CDATA[
<div> VLOGGER, audio-driven, human video generation, diffusion model, MENTOR

VLOGGER12MENTOR3D800,000VLOGGERVLOGGERMENTOR

<br /><br />: VLOGGERMENTOR <div>
We propose VLOGGER, a method for audio-driven human video generation from a
single input image of a person, which builds on the success of recent
generative diffusion models. Our method consists of 1) a stochastic
human-to-3d-motion diffusion model, and 2) a novel diffusion-based architecture
that augments text-to-image models with both spatial and temporal controls.
This supports the generation of high quality video of variable length, easily
controllable through high-level representations of human faces and bodies. In
contrast to previous work, our method does not require training for each
person, does not rely on face detection and cropping, generates the complete
image (not just the face or the lips), and considers a broad spectrum of
scenarios (e.g. visible torso or diverse subject identities) that are critical
to correctly synthesize humans who communicate. We also curate MENTOR, a new
and diverse dataset with 3d pose and expression annotations, one order of
magnitude larger than previous ones (800,000 identities) and with dynamic
gestures, on which we train and ablate our main technical contributions.
  VLOGGER outperforms state-of-the-art methods in three public benchmarks,
considering image quality, identity preservation and temporal consistency while
also generating upper-body gestures. We analyze the performance of VLOGGER with
respect to multiple diversity metrics, showing that our architectural choices
and the use of MENTOR benefit training a fair and unbiased model at scale.
Finally we show applications in video editing and personalization.
]]></content:encoded>
<pubDate>2024-03-13T17:59:02Z</pubDate>
</item>
<item>
<title>Bridging Different Language Models and Generative Vision Models for
  Text-to-Image Generation</title>
<link>http://arxiv.org/abs/2403.07860v1</link>
<guid>http://arxiv.org/abs/2403.07860v1</guid>
<content:encoded><![CDATA[
<div> text-to-image generation, language model, vision model, integration, LaVi-Bridge

: 
LaVi-BridgeLoRALaVi-BridgeLaVi-Bridge https://github.com/ShihaoZhaoZSH/LaVi-Bridge  <div>
Text-to-image generation has made significant advancements with the
introduction of text-to-image diffusion models. These models typically consist
of a language model that interprets user prompts and a vision model that
generates corresponding images. As language and vision models continue to
progress in their respective domains, there is a great potential in exploring
the replacement of components in text-to-image diffusion models with more
advanced counterparts. A broader research objective would therefore be to
investigate the integration of any two unrelated language and generative vision
models for text-to-image generation. In this paper, we explore this objective
and propose LaVi-Bridge, a pipeline that enables the integration of diverse
pre-trained language models and generative vision models for text-to-image
generation. By leveraging LoRA and adapters, LaVi-Bridge offers a flexible and
plug-and-play approach without requiring modifications to the original weights
of the language and vision models. Our pipeline is compatible with various
language models and generative vision models, accommodating different
structures. Within this framework, we demonstrate that incorporating superior
modules, such as more advanced language models or generative vision models,
results in notable improvements in capabilities like text alignment or image
quality. Extensive evaluations have been conducted to verify the effectiveness
of LaVi-Bridge. Code is available at
https://github.com/ShihaoZhaoZSH/LaVi-Bridge.
]]></content:encoded>
<pubDate>2024-03-12T17:50:11Z</pubDate>
</item>
<item>
<title>Gemini 1.5: Unlocking multimodal understanding across millions of tokens
  of context</title>
<link>http://arxiv.org/abs/2403.05530v1</link>
<guid>http://arxiv.org/abs/2403.05530v1</guid>
<content:encoded><![CDATA[
<div> Gemini 1.5 Pro, multimodal, long-context, recall, state-of-the-art

Gemini 1.5 ProGeminiGemini 1.5 ProQAQAASRGemini 1.0 UltraGemini 1.5 Pro>99%10MClaude 2.120GPT-4 Turbo12.8KalamangKalamang <br /><br />: Gemimi 1.5 ProQAQAASR <div>
In this report, we present the latest model of the Gemini family, Gemini 1.5
Pro, a highly compute-efficient multimodal mixture-of-experts model capable of
recalling and reasoning over fine-grained information from millions of tokens
of context, including multiple long documents and hours of video and audio.
Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks
across modalities, improves the state-of-the-art in long-document QA,
long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's
state-of-the-art performance across a broad set of benchmarks. Studying the
limits of Gemini 1.5 Pro's long-context ability, we find continued improvement
in next-token prediction and near-perfect retrieval (>99%) up to at least 10M
tokens, a generational leap over existing models such as Claude 2.1 (200k) and
GPT-4 Turbo (128k). Finally, we highlight surprising new capabilities of large
language models at the frontier; when given a grammar manual for Kalamang, a
language with fewer than 200 speakers worldwide, the model learns to translate
English to Kalamang at a similar level to a person who learned from the same
content.
]]></content:encoded>
<pubDate>2024-03-08T18:54:20Z</pubDate>
</item>
<item>
<title>Mechanism for Decision-aware Collaborative Federated Learning: A Pitfall
  of Shapley Values</title>
<link>http://arxiv.org/abs/2403.04753v1</link>
<guid>http://arxiv.org/abs/2403.04753v1</guid>
<content:encoded><![CDATA[
<div> : , , , , Shapley value

: MCFLShapley valueShapley valueShapley value<br /><br />: Shapley value <div>
This paper investigates mechanism design for decision-aware collaboration via
federated learning (FL) platforms. Our framework consists of a digital platform
and multiple decision-aware agents, each endowed with proprietary data sets.
The platform offers an infrastructure that enables access to the data, creates
incentives for collaborative learning aimed at operational decision-making, and
conducts FL to avoid direct raw data sharing. The computation and communication
efficiency of the FL process is inherently influenced by the agent
participation equilibrium induced by the mechanism. Therefore, assessing the
system's efficiency involves two critical factors: the surplus created by
coalition formation and the communication costs incurred across the coalition
during FL. To evaluate the system efficiency under the intricate interplay
between mechanism design, agent participation, operational decision-making, and
the performance of FL algorithms, we introduce a multi-action collaborative
federated learning (MCFL) framework for decision-aware agents. Under this
framework, we further analyze the equilibrium for the renowned Shapley value
based mechanisms. Specifically, we examine the issue of false-name
manipulation, a form of dishonest behavior where participating agents create
duplicate fake identities to split their original data among these identities.
By solving the agent participation equilibrium, we demonstrate that while
Shapley value effectively maximizes coalition-generated surplus by encouraging
full participation, it inadvertently promotes false-name manipulation. This
further significantly increases the communication costs when the platform
conducts FL. Thus, we highlight a significant pitfall of Shapley value based
mechanisms, which implicitly incentivizes data splitting and identity
duplication, ultimately impairing the overall efficiency in FL systems.
]]></content:encoded>
<pubDate>2024-03-07T18:54:59Z</pubDate>
</item>
<item>
<title>Stop Regressing: Training Value Functions via Classification for
  Scalable Deep RL</title>
<link>http://arxiv.org/abs/2403.03950v1</link>
<guid>http://arxiv.org/abs/2403.03950v1</guid>
<content:encoded><![CDATA[
<div> : , , , , 

TransformerAtari 2600SoftMoEsResNetsAtariQ-transformersTransformerWordle
<br /><br />: Atari <div>
Value functions are a central component of deep reinforcement learning (RL).
These functions, parameterized by neural networks, are trained using a mean
squared error regression objective to match bootstrapped target values.
However, scaling value-based RL methods that use regression to large networks,
such as high-capacity Transformers, has proven challenging. This difficulty is
in stark contrast to supervised learning: by leveraging a cross-entropy
classification loss, supervised methods have scaled reliably to massive
networks. Observing this discrepancy, in this paper, we investigate whether the
scalability of deep RL can also be improved simply by using classification in
place of regression for training value functions. We demonstrate that value
functions trained with categorical cross-entropy significantly improves
performance and scalability in a variety of domains. These include: single-task
RL on Atari 2600 games with SoftMoEs, multi-task RL on Atari with large-scale
ResNets, robotic manipulation with Q-transformers, playing Chess without
search, and a language-agent Wordle task with high-capacity Transformers,
achieving state-of-the-art results on these domains. Through careful analysis,
we show that the benefits of categorical cross-entropy primarily stem from its
ability to mitigate issues inherent to value-based RL, such as noisy targets
and non-stationarity. Overall, we argue that a simple shift to training value
functions with categorical cross-entropy can yield substantial improvements in
the scalability of deep RL at little-to-no cost.
]]></content:encoded>
<pubDate>2024-03-06T18:55:47Z</pubDate>
</item>
<item>
<title>Can Audio Reveal Music Performance Difficulty? Insights from the Piano
  Syllabus Dataset</title>
<link>http://arxiv.org/abs/2403.03947v1</link>
<guid>http://arxiv.org/abs/2403.03947v1</guid>
<content:encoded><![CDATA[
<div> , , , , 
<br /><br />:
 <div>
Automatically estimating the performance difficulty of a music piece
represents a key process in music education to create tailored curricula
according to the individual needs of the students. Given its relevance, the
Music Information Retrieval (MIR) field depicts some proof-of-concept works
addressing this task that mainly focuses on high-level music abstractions such
as machine-readable scores or music sheet images. In this regard, the potential
of directly analyzing audio recordings has been generally neglected, which
prevents students from exploring diverse music pieces that may not have a
formal symbolic-level transcription. This work pioneers in the automatic
estimation of performance difficulty of music pieces on audio recordings with
two precise contributions: (i) the first audio-based difficulty estimation
dataset -- namely, Piano Syllabus (PSyllabus) dataset -- featuring 7,901 piano
pieces across 11 difficulty levels from 1,233 composers; and (ii) a recognition
framework capable of managing different input representations -- both unimodal
and multimodal manners -- directly derived from audio to perform the difficulty
estimation task. The comprehensive experimentation comprising different
pre-training schemes, input modalities, and multi-task scenarios prove the
validity of the proposal and establishes PSyllabus as a reference dataset for
audio-based difficulty estimation in the MIR field. The dataset as well as the
developed code and trained models are publicly shared to promote further
research in the field.
]]></content:encoded>
<pubDate>2024-03-06T18:54:13Z</pubDate>
</item>
<item>
<title>Scaling Rectified Flow Transformers for High-Resolution Image Synthesis</title>
<link>http://arxiv.org/abs/2403.03206v1</link>
<guid>http://arxiv.org/abs/2403.03206v1</guid>
<content:encoded><![CDATA[
<div> 
<br />
 
<br /><br />: 
<br /> <div>
Diffusion models create data from noise by inverting the forward paths of
data towards noise and have emerged as a powerful generative modeling technique
for high-dimensional, perceptual data such as images and videos. Rectified flow
is a recent generative model formulation that connects data and noise in a
straight line. Despite its better theoretical properties and conceptual
simplicity, it is not yet decisively established as standard practice. In this
work, we improve existing noise sampling techniques for training rectified flow
models by biasing them towards perceptually relevant scales. Through a
large-scale study, we demonstrate the superior performance of this approach
compared to established diffusion formulations for high-resolution
text-to-image synthesis. Additionally, we present a novel transformer-based
architecture for text-to-image generation that uses separate weights for the
two modalities and enables a bidirectional flow of information between image
and text tokens, improving text comprehension, typography, and human preference
ratings. We demonstrate that this architecture follows predictable scaling
trends and correlates lower validation loss to improved text-to-image synthesis
as measured by various metrics and human evaluations. Our largest models
outperform state-of-the-art models, and we will make our experimental data,
code, and model weights publicly available.
]]></content:encoded>
<pubDate>2024-03-05T18:45:39Z</pubDate>
</item>
<item>
<title>Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers</title>
<link>http://arxiv.org/abs/2402.19479v1</link>
<guid>http://arxiv.org/abs/2402.19479v1</guid>
<content:encoded><![CDATA[
<div> Panda-70M

HD-VILA-100M3807000Panda-70M<br /><br />: Panda-70M <div>
The quality of the data and annotation upper-bounds the quality of a
downstream model. While there exist large text corpora and image-text pairs,
high-quality video-text data is much harder to collect. First of all, manual
labeling is more time-consuming, as it requires an annotator to watch an entire
video. Second, videos have a temporal dimension, consisting of several scenes
stacked together, and showing multiple actions. Accordingly, to establish a
video dataset with high-quality captions, we propose an automatic approach
leveraging multimodal inputs, such as textual video description, subtitles, and
individual video frames. Specifically, we curate 3.8M high-resolution videos
from the publicly available HD-VILA-100M dataset. We then split them into
semantically consistent video clips, and apply multiple cross-modality teacher
models to obtain captions for each video. Next, we finetune a retrieval model
on a small subset where the best caption of each video is manually selected and
then employ the model in the whole dataset to select the best caption as the
annotation. In this way, we get 70M videos paired with high-quality text
captions. We dub the dataset as Panda-70M. We show the value of the proposed
dataset on three downstream tasks: video captioning, video and text retrieval,
and text-driven video generation. The models trained on the proposed data score
substantially better on the majority of metrics across all the tasks.
]]></content:encoded>
<pubDate>2024-02-29T18:59:50Z</pubDate>
</item>
<item>
<title>Trajectory Prediction for Autonomous Driving Using a Transformer Network</title>
<link>http://arxiv.org/abs/2402.16501v1</link>
<guid>http://arxiv.org/abs/2402.16501v1</guid>
<content:encoded><![CDATA[
<div> : autonomous driving, multi-modal trajectory prediction, transformer network, convolutional networks, Lyft l5kit dataset

TransformerLyft l5kit

:
1. 
2. Transformer
3. 
4. 
5. Lyft l5kit <div>
Predicting the trajectories of surrounding agents is still considered one of
the most challenging tasks for autonomous driving. In this paper, we introduce
a multi-modal trajectory prediction framework based on the transformer network.
The semantic maps of each agent are used as inputs to convolutional networks to
automatically derive relevant contextual information. A novel auxiliary loss
that penalizes unfeasible off-road predictions is also proposed in this study.
Experiments on the Lyft l5kit dataset show that the proposed model achieves
state-of-the-art performance, substantially improving the accuracy and
feasibility of the prediction outcomes.
]]></content:encoded>
<pubDate>2024-02-26T11:35:23Z</pubDate>
</item>
<item>
<title>LLMArena: Assessing Capabilities of Large Language Models in Dynamic
  Multi-Agent Environments</title>
<link>http://arxiv.org/abs/2402.16499v1</link>
<guid>http://arxiv.org/abs/2402.16499v1</guid>
<content:encoded><![CDATA[
<div> LLMArena, , , , 

LLMArena  Trueskill LLMArena  <br /><br />: <br />LLMArena  Trueskill LLMArena <div>
Recent advancements in large language models (LLMs) have revealed their
potential for achieving autonomous agents possessing human-level intelligence.
However, existing benchmarks for evaluating LLM Agents either use static
datasets, potentially leading to data leakage or focus only on single-agent
scenarios, overlooking the complexities of multi-agent interactions. There is a
lack of a benchmark that evaluates the diverse capabilities of LLM agents in
multi-agent, dynamic environments. To this end, we introduce LLMArena, a novel
and easily extensible framework for evaluating the diverse capabilities of LLM
in multi-agent dynamic environments. LLMArena encompasses seven distinct gaming
environments, employing Trueskill scoring to assess crucial abilities in LLM
agents, including spatial reasoning, strategic planning, numerical reasoning,
risk assessment, communication, opponent modeling, and team collaboration. We
conduct an extensive experiment and human evaluation among different sizes and
types of LLMs, showing that LLMs still have a significant journey ahead in
their development towards becoming fully autonomous agents, especially in
opponent modeling and team collaboration. We hope LLMArena could guide future
research towards enhancing these capabilities in LLMs, ultimately leading to
more sophisticated and practical applications in dynamic, multi-agent settings.
The code and data will be available.
]]></content:encoded>
<pubDate>2024-02-26T11:31:48Z</pubDate>
</item>
<item>
<title>Q-FOX Learning: Breaking Tradition in Reinforcement Learning</title>
<link>http://arxiv.org/abs/2402.16562v1</link>
<guid>http://arxiv.org/abs/2402.16562v1</guid>
<content:encoded><![CDATA[
<div> , , , Q-FOX, OpenAI Gym
:<br /><br /> Q-FOXFOXQ-learningQ-FOXOpenAI GymQ-FOXQ-FOX <div>
Reinforcement learning (RL) is a subset of artificial intelligence (AI) where
agents learn the best action by interacting with the environment, making it
suitable for tasks that do not require labeled data or direct supervision.
Hyperparameters (HP) tuning refers to choosing the best parameter that leads to
optimal solutions in RL algorithms. Manual or random tuning of the HP may be a
crucial process because variations in this parameter lead to changes in the
overall learning aspects and different rewards. In this paper, a novel and
automatic HP-tuning method called Q-FOX is proposed. This uses both the FOX
optimizer, a new optimization method inspired by nature that mimics red foxes'
hunting behavior, and the commonly used, easy-to-implement RL Q-learning
algorithm to solve the problem of HP tuning. Moreover, a new objective function
is proposed which prioritizes the reward over the mean squared error (MSE) and
learning time (steps). Q-FOX has been evaluated on two OpenAI Gym environment
control tasks: Cart Pole and Frozen Lake. It exposed greater cumulative rewards
than HP tuning with other optimizers, such as PSO, GA, Bee, or randomly
selected HP. The cumulative reward for the Cart Pole task was 32.08, and for
the Frozen Lake task was 0.95. Despite the robustness of Q-FOX, it has
limitations. It cannot be used directly in real-word problems before choosing
the HP in a simulation environment because its processes work iteratively,
making it time-consuming. The results indicate that Q-FOX has played an
essential role in HP tuning for RL algorithms to effectively solve different
control tasks.
]]></content:encoded>
<pubDate>2024-02-26T13:39:04Z</pubDate>
</item>
<item>
<title>Contracts with Inspections</title>
<link>http://arxiv.org/abs/2402.16553v1</link>
<guid>http://arxiv.org/abs/2402.16553v1</guid>
<content:encoded><![CDATA[
<div> hidden-action model, principal-agent, incentive, inspection, deterministic<br />
<br />
XOS <br /><br />: - <div>
In the classical principal-agent hidden-action model, a principal delegates
the execution of a costly task to an agent for which he can choose among
actions with different costs and different success probabilities to accomplish
the task. To incentivize the agent to exert effort, the principal can commit to
a contract, which is the amount of payment based on the task's success. A
crucial assumption of this model is that the principal can only base the
payment on the outcome but not on the agent's chosen action.
  In this work, we relax the hidden-action assumption and introduce a new model
where the principal is allowed to inspect subsets of actions at some cost that
depends on the inspected subset. If the principal discovers that the agent did
not select the agreed-upon action through the inspection, the principal can
withhold payment. This relaxation of the model introduces a broader strategy
space for the principal, who now faces a tradeoff between positive incentives
(increasing payment) and negative incentives (increasing inspection).
  We show how to find the best deterministic incentive-compatible inspection
scheme for all monotone inspection cost functions. We then turn to randomized
inspection schemes and show that one can efficiently find the best randomized
incentive-compatible inspection scheme when the inspection cost function is
submodular. We complement this result by showing that it is impossible to
efficiently find the optimal randomized inspection scheme for the more general
case of XOS inspection cost functions.
]]></content:encoded>
<pubDate>2024-02-26T13:26:34Z</pubDate>
</item>
<item>
<title>AgentOhana: Design Unified Data and Training Pipeline for Effective
  Agent Learning</title>
<link>http://arxiv.org/abs/2402.15506v1</link>
<guid>http://arxiv.org/abs/2402.15506v1</guid>
<content:encoded><![CDATA[
<div> AgentOhana, LLMs, agent trajectories, data loader, xLAM-v0.1
:<br /><br />AgentOhanaLLMsagent-basedAgentOhanaagentagentxLAM-v0.1AI Agent <div>
Autonomous agents powered by large language models (LLMs) have garnered
significant research attention. However, fully harnessing the potential of LLMs
for agent-based tasks presents inherent challenges due to the heterogeneous
nature of diverse data sources featuring multi-turn trajectories. In this
paper, we introduce \textbf{AgentOhana} as a comprehensive solution to address
these challenges. \textit{AgentOhana} aggregates agent trajectories from
distinct environments, spanning a wide array of scenarios. It meticulously
standardizes and unifies these trajectories into a consistent format,
streamlining the creation of a generic data loader optimized for agent
training. Leveraging the data unification, our training pipeline maintains
equilibrium across different data sources and preserves independent randomness
across devices during dataset partitioning and model training. Additionally, we
present \textbf{xLAM-v0.1}, a large action model tailored for AI agents, which
demonstrates exceptional performance across various benchmarks.
]]></content:encoded>
<pubDate>2024-02-23T18:56:26Z</pubDate>
</item>
<item>
<title>PALO: A Polyglot Large Multimodal Model for 5B People</title>
<link>http://arxiv.org/abs/2402.14818v1</link>
<guid>http://arxiv.org/abs/2402.14818v1</guid>
<content:encoded><![CDATA[
<div> Palobenchmark<br />
Palo105065%1.7B7B13B <div>
In pursuit of more inclusive Vision-Language Models (VLMs), this study
introduces a Large Multilingual Multimodal Model called \textsc{Palo}.
\textsc{Palo} offers visual reasoning capabilities in 10 major languages,
including English, Chinese, Hindi, Spanish, French, Arabic, Bengali, Russian,
Urdu, and Japanese, that span a total of $\sim$5B people (65\% of the world
population). Our approach involves a semi-automated translation approach to
adapt the multimodal instruction dataset from English to the target languages
using a fine-tuned Large Language Model, thereby ensuring high linguistic
fidelity while allowing scalability due to minimal manual effort. The
incorporation of diverse instruction sets helps us boost overall performance
across multiple languages especially those that are underrepresented like
Hindi, Arabic, Bengali, and Urdu. The resulting models are trained across three
scales (1.7B, 7B and 13B parameters) to show the generalization and scalability
where we observe substantial improvements compared to strong baselines. We also
propose the first multilingual multimodal benchmark for the forthcoming
approaches to evaluate their vision-language reasoning capabilities across
languages. Code: https://github.com/mbzuai-oryx/PALO.
]]></content:encoded>
<pubDate>2024-02-22T18:59:58Z</pubDate>
</item>
<item>
<title>A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit
  Tasks in Public Health</title>
<link>http://arxiv.org/abs/2402.14807v1</link>
<guid>http://arxiv.org/abs/2402.14807v1</guid>
<content:encoded><![CDATA[
<div> RMAB<br />
DLMLLMRMABARMMANDLMDLM <br /><br />: <br />DLMDLM <div>
Efforts to reduce maternal mortality rate, a key UN Sustainable Development
target (SDG Target 3.1), rely largely on preventative care programs to spread
critical health information to high-risk populations. These programs face two
important challenges: efficiently allocating limited health resources to large
beneficiary populations, and adapting to evolving policy priorities. While
prior works in restless multi-armed bandit (RMAB) demonstrated success in
public health allocation tasks, they lack flexibility to adapt to evolving
policy priorities. Concurrently, Large Language Models (LLMs) have emerged as
adept, automated planners in various domains, including robotic control and
navigation. In this paper, we propose DLM: a Decision Language Model for RMABs.
To enable dynamic fine-tuning of RMAB policies for challenging public health
settings using human-language commands, we propose using LLMs as automated
planners to (1) interpret human policy preference prompts, (2) propose code
reward functions for a multi-agent RL environment for RMABs, and (3) iterate on
the generated reward using feedback from RMAB simulations to effectively adapt
policy outcomes. In collaboration with ARMMAN, an India-based public health
organization promoting preventative care for pregnant mothers, we conduct a
simulation study, showing DLM can dynamically shape policy outcomes using only
human language commands as input.
]]></content:encoded>
<pubDate>2024-02-22T18:58:27Z</pubDate>
</item>
<item>
<title>OlympiadBench: A Challenging Benchmark for Promoting AGI with
  Olympiad-Level Bilingual Multimodal Scientific Problems</title>
<link>http://arxiv.org/abs/2402.14008v1</link>
<guid>http://arxiv.org/abs/2402.14008v1</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Multimodal Models, OlympiadBench, GPT-4V, 

- Large Language ModelsLLMs Large Multimodal ModelsLMMs
- OlympiadBench
- OlympiadBench GPT-4V 11.28% 
-  GPT-4V 
- 

<br /><br />:
LLMsLMMsOlympiadBench 8,952 OlympiadBench GPT-4V OlympiadBench 17.23% 11.28%  GPT-4V  <div>
Recent advancements have seen Large Language Models (LLMs) and Large
Multimodal Models (LMMs) surpassing general human capabilities in various
tasks, approaching the proficiency level of human experts across multiple
domains. With traditional benchmarks becoming less challenging for these
models, new rigorous challenges are essential to gauge their advanced
abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual
multimodal scientific benchmark, featuring 8,952 problems from Olympiad-level
mathematics and physics competitions, including the Chinese college entrance
exam. Each problem is detailed with expert-level annotations for step-by-step
reasoning. Evaluating top-tier models on OlympiadBench, we implement a
comprehensive assessment methodology to accurately evaluate model responses.
Notably, the best-performing model, GPT-4V, attains an average score of 17.23%
on OlympiadBench, with a mere 11.28% in physics, highlighting the benchmark
rigor and the intricacy of physical reasoning. Our analysis orienting GPT-4V
points out prevalent issues with hallucinations, knowledge omissions, and
logical fallacies. We hope that our challenging benchmark can serve as a
valuable resource for helping future AGI research endeavors.
]]></content:encoded>
<pubDate>2024-02-21T18:49:26Z</pubDate>
</item>
<item>
<title>Information Elicitation in Agency Games</title>
<link>http://arxiv.org/abs/2402.14005v1</link>
<guid>http://arxiv.org/abs/2402.14005v1</guid>
<content:encoded><![CDATA[
<div> <br />
<br />
 <div>
Rapid progress in scalable, commoditized tools for data collection and data
processing has made it possible for firms and policymakers to employ ever more
complex metrics as guides for decision-making. These developments have
highlighted a prevailing challenge -- deciding *which* metrics to compute. In
particular, a firm's ability to compute a wider range of existing metrics does
not address the problem of *unknown unknowns*, which reflects informational
limitations on the part of the firm. To guide the choice of metrics in the face
of this informational problem, we turn to the evaluated agents themselves, who
may have more information than a principal about how to measure outcomes
effectively. We model this interaction as a simple agency game, where we ask:
*When does an agent have an incentive to reveal the observability of a
cost-correlated variable to the principal?* There are two effects: better
information reduces the agent's information rents but also makes some projects
go forward that otherwise would fail. We show that the agent prefers to reveal
information that exposes a strong enough differentiation between high and low
costs. Expanding the agent's action space to include the ability to *garble*
their information, we show that the agent often prefers to garble over full
revelation. Still, giving the agent the ability to garble can lead to higher
total welfare. Our model has analogies with price discrimination, and we
leverage some of these synergies to analyze total welfare.
]]></content:encoded>
<pubDate>2024-02-21T18:44:38Z</pubDate>
</item>
<item>
<title>CounterCurate: Enhancing Physical and Semantic Visio-Linguistic
  Compositional Reasoning via Counterfactual Examples</title>
<link>http://arxiv.org/abs/2402.13254v1</link>
<guid>http://arxiv.org/abs/2402.13254v1</guid>
<content:encoded><![CDATA[
<div> : CounterCurate, visio-linguistic compositional reasoning, physically grounded reasoning, data augmentation, semantic counterfactuals

CounterCurate-GLIGENFlickr30k-PositionsCLIPLLaVA33%37%GPT-4VDALLE-3SugarCrepeCounterCurateGPT-4V<br /><br />: CounterCurate <div>
We propose CounterCurate, a framework to comprehensively improve the
visio-linguistic compositional reasoning capability for both contrastive and
generative multimodal models. In particular, we identify two under-explored
critical problems: the neglect of the physically grounded reasoning (counting
and position understanding) and the potential of using highly capable text and
image generation models for semantic counterfactual fine-tuning. Our work
pioneers an approach that addresses these gaps. We first spotlight the
near-chance performance of multimodal models like CLIP and LLaVA in physically
grounded compositional reasoning. We then apply simple data augmentation using
a grounded image generation model, GLIGEN, to generate finetuning data,
resulting in significant performance improvements: +33% and +37% for CLIP and
LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark.
Moreover, we exploit the capabilities of high-performing text generation and
image generation models, specifically GPT-4V and DALLE-3, to curate challenging
semantic counterfactuals, thereby further enhancing compositional reasoning
capabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms
GPT-4V.
]]></content:encoded>
<pubDate>2024-02-20T18:59:55Z</pubDate>
</item>
<item>
<title>Fusion of Diffusion Weighted MRI and Clinical Data for Predicting
  Functional Outcome after Acute Ischemic Stroke with Deep Contrastive Learning</title>
<link>http://arxiv.org/abs/2402.10894v1</link>
<guid>http://arxiv.org/abs/2402.10894v1</guid>
<content:encoded><![CDATA[
<div> MRI

:<br />
MRI3AUCF1MRINIHSS <div>
Stroke is a common disabling neurological condition that affects about
one-quarter of the adult population over age 25; more than half of patients
still have poor outcomes, such as permanent functional dependence or even
death, after the onset of acute stroke. The aim of this study is to investigate
the efficacy of diffusion-weighted MRI modalities combining with structured
health profile on predicting the functional outcome to facilitate early
intervention. A deep fusion learning network is proposed with two-stage
training: the first stage focuses on cross-modality representation learning and
the second stage on classification. Supervised contrastive learning is
exploited to learn discriminative features that separate the two classes of
patients from embeddings of individual modalities and from the fused multimodal
embedding. The network takes as the input DWI and ADC images, and structured
health profile data. The outcome is the prediction of the patient needing
long-term care at 3 months after the onset of stroke. Trained and evaluated
with a dataset of 3297 patients, our proposed fusion model achieves 0.87, 0.80
and 80.45% for AUC, F1-score and accuracy, respectively, outperforming existing
models that consolidate both imaging and structured data in the medical domain.
If trained with comprehensive clinical variables, including NIHSS and
comorbidities, the gain from images on making accurate prediction is not
considered substantial, but significant. However, diffusion-weighted MRI can
replace NIHSS to achieve comparable level of accuracy combining with other
readily available clinical variables for better generalization.
]]></content:encoded>
<pubDate>2024-02-16T18:51:42Z</pubDate>
</item>
<item>
<title>When is Tree Search Useful for LLM Planning? It Depends on the
  Discriminator</title>
<link>http://arxiv.org/abs/2402.10890v1</link>
<guid>http://arxiv.org/abs/2402.10890v1</guid>
<content:encoded><![CDATA[
<div> <br />
90%10-20<br /><br />90%10-20 <div>
In this paper, we examine how large language models (LLMs) solve multi-step
problems under a language agent framework with three components: a generator, a
discriminator, and a planning method. We investigate the practical utility of
two advanced planning methods, iterative correction and tree search. We present
a comprehensive analysis of how discrimination accuracy affects the overall
performance of agents when using these two methods or a simpler method,
re-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical
reasoning, show that: (1) advanced planning methods demand discriminators with
at least 90% accuracy to achieve significant improvements over re-ranking; (2)
current LLMs' discrimination abilities have not met the needs of advanced
planning methods to achieve such improvements; (3) with LLM-based
discriminators, advanced planning methods may not adequately balance accuracy
and efficiency. For example, compared to the other two methods, tree search is
at least 10--20 times slower but leads to negligible performance gains, which
hinders its real-world applications. Code and data will be released at
https://github.com/OSU-NLP-Group/llm-planning-eval.
]]></content:encoded>
<pubDate>2024-02-16T18:45:58Z</pubDate>
</item>
<item>
<title>A Trembling House of Cards? Mapping Adversarial Attacks against Language
  Agents</title>
<link>http://arxiv.org/abs/2402.10196v1</link>
<guid>http://arxiv.org/abs/2402.10196v1</guid>
<content:encoded><![CDATA[
<div> : 
: 
12 <br /><br /> <div>
Language agents powered by large language models (LLMs) have seen exploding
development. Their capability of using language as a vehicle for thought and
communication lends an incredible level of flexibility and versatility. People
have quickly capitalized on this capability to connect LLMs to a wide range of
external components and environments: databases, tools, the Internet, robotic
embodiment, etc. Many believe an unprecedentedly powerful automation technology
is emerging. However, new automation technologies come with new safety risks,
especially for intricate systems like language agents. There is a surprisingly
large gap between the speed and scale of their development and deployment and
our understanding of their safety risks. Are we building a house of cards? In
this position paper, we present the first systematic effort in mapping
adversarial attacks against language agents. We first present a unified
conceptual framework for agents with three major components: Perception, Brain,
and Action. Under this framework, we present a comprehensive discussion and
propose 12 potential attack scenarios against different components of an agent,
covering different attack strategies (e.g., input manipulation, adversarial
demonstrations, jailbreaking, backdoors). We also draw connections to
successful attack strategies previously applied to LLMs. We emphasize the
urgency to gain a thorough understanding of language agent risks before their
widespread deployment.
]]></content:encoded>
<pubDate>2024-02-15T18:51:32Z</pubDate>
</item>
<item>
<title>Rec-GPT4V: Multimodal Recommendation with Large Vision-Language Models</title>
<link>http://arxiv.org/abs/2402.08670v1</link>
<guid>http://arxiv.org/abs/2402.08670v1</guid>
<content:encoded><![CDATA[
<div> : large vision-language models, multimodal recommendations, Rec-GPT4V, user preferences, image sequence dynamics

: <br /><br />Rec-GPT4VVisual-Summary ThoughtVSTLVLMsLVLMsLVLMsVST <div>
The development of large vision-language models (LVLMs) offers the potential
to address challenges faced by traditional multimodal recommendations thanks to
their proficient understanding of static images and textual dynamics. However,
the application of LVLMs in this field is still limited due to the following
complexities: First, LVLMs lack user preference knowledge as they are trained
from vast general datasets. Second, LVLMs suffer setbacks in addressing
multiple image dynamics in scenarios involving discrete, noisy, and redundant
image sequences. To overcome these issues, we propose the novel reasoning
scheme named Rec-GPT4V: Visual-Summary Thought (VST) of leveraging large
vision-language models for multimodal recommendation. We utilize user history
as in-context user preferences to address the first challenge. Next, we prompt
LVLMs to generate item image summaries and utilize image comprehension in
natural language space combined with item titles to query the user preferences
over candidate items. We conduct comprehensive experiments across four datasets
with three LVLMs: GPT4-V, LLaVa-7b, and LLaVa-13b. The numerical results
indicate the efficacy of VST.
]]></content:encoded>
<pubDate>2024-02-13T18:51:18Z</pubDate>
</item>
<item>
<title>MODIPHY: Multimodal Obscured Detection for IoT using PHantom
  Convolution-Enabled Faster YOLO</title>
<link>http://arxiv.org/abs/2402.07894v1</link>
<guid>http://arxiv.org/abs/2402.07894v1</guid>
<content:encoded><![CDATA[
<div> , YOLO Phantom, , , 

:<br />
"YOLO Phantom"YOLOPhantom43%19%GFLOPsYOLO PhantomRGB-RGBAWSYOLOv8nRGB17%14%GitHub <div>
Low-light conditions and occluded scenarios impede object detection in
real-world Internet of Things (IoT) applications like autonomous vehicles and
security systems. While advanced machine learning models strive for accuracy,
their computational demands clash with the limitations of resource-constrained
devices, hampering real-time performance. In our current research, we tackle
this challenge, by introducing "YOLO Phantom", one of the smallest YOLO models
ever conceived. YOLO Phantom utilizes the novel Phantom Convolution block,
achieving comparable accuracy to the latest YOLOv8n model while simultaneously
reducing both parameters and model size by 43%, resulting in a significant 19%
reduction in Giga Floating Point Operations (GFLOPs). YOLO Phantom leverages
transfer learning on our multimodal RGB-infrared dataset to address low-light
and occlusion issues, equipping it with robust vision under adverse conditions.
Its real-world efficacy is demonstrated on an IoT platform with advanced
low-light and RGB cameras, seamlessly connecting to an AWS-based notification
endpoint for efficient real-time object detection. Benchmarks reveal a
substantial boost of 17% and 14% in frames per second (FPS) for thermal and RGB
detection, respectively, compared to the baseline YOLOv8n model. For community
contribution, both the code and the multimodal dataset are available on GitHub.
]]></content:encoded>
<pubDate>2024-02-12T18:56:53Z</pubDate>
</item>
<item>
<title>MAIDCRL: Semi-centralized Multi-Agent Influence Dense-CNN Reinforcement
  Learning</title>
<link>http://arxiv.org/abs/2402.07890v1</link>
<guid>http://arxiv.org/abs/2402.07890v1</guid>
<content:encoded><![CDATA[
<div> Distributed decision-making, multi-agent systems, interactive behavior learning, Dense Reinforcement Learning, agent influence maps<br />
<br />
DenseAIMsStarCraftSMACMAIDRLDenseNetDense-CNNMAIDCRLCNNMAIDCRLSMAC <br /><br />: <br />Dense-CNNMAIDCRLAIMsStarCraftSMACCNN-enabled MAIDCRL <div>
Distributed decision-making in multi-agent systems presents difficult
challenges for interactive behavior learning in both cooperative and
competitive systems. To mitigate this complexity, MAIDRL presents a
semi-centralized Dense Reinforcement Learning algorithm enhanced by agent
influence maps (AIMs), for learning effective multi-agent control on StarCraft
Multi-Agent Challenge (SMAC) scenarios. In this paper, we extend the DenseNet
in MAIDRL and introduce semi-centralized Multi-Agent Dense-CNN Reinforcement
Learning, MAIDCRL, by incorporating convolutional layers into the deep model
architecture, and evaluate the performance on both homogeneous and
heterogeneous scenarios. The results show that the CNN-enabled MAIDCRL
significantly improved the learning performance and achieved a faster learning
rate compared to the existing MAIDRL, especially on more complicated
heterogeneous SMAC scenarios. We further investigate the stability and
robustness of our model. The statistics reflect that our model not only
achieves higher winning rate in all the given scenarios but also boosts the
agent's learning process in fine-grained decision-making.
]]></content:encoded>
<pubDate>2024-02-12T18:53:20Z</pubDate>
</item>
<item>
<title>Feedback Loops With Language Models Drive In-Context Reward Hacking</title>
<link>http://arxiv.org/abs/2402.06627v1</link>
<guid>http://arxiv.org/abs/2402.06627v1</guid>
<content:encoded><![CDATA[
<div> , , , , 
:
 <div>
Language models influence the external world: they query APIs that read and
write to web pages, generate content that shapes human behavior, and run system
commands as autonomous agents. These interactions form feedback loops: LLM
outputs affect the world, which in turn affect subsequent LLM outputs. In this
work, we show that feedback loops can cause in-context reward hacking (ICRH),
where the LLM at test-time optimizes a (potentially implicit) objective but
creates negative side effects in the process. For example, consider an LLM
agent deployed to increase Twitter engagement; the LLM may retrieve its
previous tweets into the context window and make them more controversial,
increasing engagement but also toxicity. We identify and study two processes
that lead to ICRH: output-refinement and policy-refinement. For these
processes, evaluations on static datasets are insufficient -- they miss the
feedback effects and thus cannot capture the most harmful behavior. In
response, we provide three recommendations for evaluation to capture more
instances of ICRH. As AI development accelerates, the effects of feedback loops
will proliferate, increasing the need to understand their role in shaping LLM
behavior.
]]></content:encoded>
<pubDate>2024-02-09T18:59:29Z</pubDate>
</item>
<item>
<title>SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large
  Language Models</title>
<link>http://arxiv.org/abs/2402.05935v1</link>
<guid>http://arxiv.org/abs/2402.05935v1</guid>
<content:encoded><![CDATA[
<div> SPHINX-X, Multimodality, Large Language Model, dataset, training efficiency
<br /><br />
:SPHINX-XSPHINXMLLM-TinyLlama1.1BInternLM2-7BLLaMA2-13BMixtral8x7BLLMMLLMhttps://github.com/Alpha-VLLM/LLaMA2-Accessory <div>
We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM)
series developed upon SPHINX. To improve the architecture and training
efficiency, we modify the SPHINX framework by removing redundant visual
encoders, bypassing fully-padded sub-images with skip tokens, and simplifying
multi-stage training into a one-stage all-in-one paradigm. To fully unleash the
potential of MLLMs, we assemble a comprehensive multi-domain and multimodal
dataset covering publicly available resources in language, vision, and
vision-language tasks. We further enrich this collection with our curated OCR
intensive and Set-of-Mark datasets, extending the diversity and generality. By
training over different base LLMs including TinyLlama1.1B, InternLM2-7B,
LLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in
parameter size and multilingual capabilities. Comprehensive benchmarking
reveals a strong correlation between the multi-modal performance with the data
and parameter scales. Code and models are released at
https://github.com/Alpha-VLLM/LLaMA2-Accessory
]]></content:encoded>
<pubDate>2024-02-08T18:59:48Z</pubDate>
</item>
<item>
<title>An Interactive Agent Foundation Model</title>
<link>http://arxiv.org/abs/2402.05929v1</link>
<guid>http://arxiv.org/abs/2402.05929v1</guid>
<content:encoded><![CDATA[
<div> Agent-based systems, multi-task agent training paradigm, versatile AI framework, Robotics, Gaming AI, Healthcare

Agent FoundationAIAI--<br /><br />: AI <div>
The development of artificial intelligence systems is transitioning from
creating static, task-specific models to dynamic, agent-based systems capable
of performing well in a wide range of applications. We propose an Interactive
Agent Foundation Model that uses a novel multi-task agent training paradigm for
training AI agents across a wide range of domains, datasets, and tasks. Our
training paradigm unifies diverse pre-training strategies, including visual
masked auto-encoders, language modeling, and next-action prediction, enabling a
versatile and adaptable AI framework. We demonstrate the performance of our
framework across three separate domains -- Robotics, Gaming AI, and Healthcare.
Our model demonstrates its ability to generate meaningful and contextually
relevant outputs in each area. The strength of our approach lies in its
generality, leveraging a variety of data sources such as robotics sequences,
gameplay data, large-scale video datasets, and textual information for
effective multimodal and multi-task learning. Our approach provides a promising
avenue for developing generalist, action-taking, multimodal systems.
]]></content:encoded>
<pubDate>2024-02-08T18:58:02Z</pubDate>
</item>
<item>
<title>WebLINX: Real-World Website Navigation with Multi-Turn Dialogue</title>
<link>http://arxiv.org/abs/2402.05930v1</link>
<guid>http://arxiv.org/abs/2402.05930v1</guid>
<content:encoded><![CDATA[
We propose the problem of conversational web navigation, where a digital
agent controls a web browser and follows user instructions to solve real-world
tasks in a multi-turn dialogue fashion. To support this problem, we introduce
WEBLINX - a large-scale benchmark of 100K interactions across 2300 expert
demonstrations of conversational web navigation. Our benchmark covers a broad
range of patterns on over 150 real-world websites and can be used to train and
evaluate agents in diverse scenarios. Due to the magnitude of information
present, Large Language Models (LLMs) cannot process entire web pages in
real-time. To solve this bottleneck, we design a retrieval-inspired model that
efficiently prunes HTML pages by ranking relevant elements. We use the selected
elements, along with screenshots and action history, to assess a variety of
models for their ability to replicate human behavior when navigating the web.
Our experiments span from small text-only to proprietary multimodal LLMs. We
find that smaller finetuned decoders surpass the best zero-shot LLMs (including
GPT-4V), but also larger finetuned multimodal models which were explicitly
pretrained on screenshots. However, all finetuned models struggle to generalize
to unseen websites. Our findings highlight the need for large multimodal models
that can generalize to novel settings. Our code, data and models are available
for research: https://mcgill-nlp.github.io/weblinx
]]></content:encoded>
<pubDate>2024-02-08T18:58:02Z</pubDate>
</item>
<item>
<title>Language-Based Augmentation to Address Shortcut Learning in Object Goal
  Navigation</title>
<link>http://arxiv.org/abs/2402.05090v1</link>
<guid>http://arxiv.org/abs/2402.05090v1</guid>
<content:encoded><![CDATA[
<div> DRL, Object-Goal Navigation, Shortcut learning, Language-Based (L-B) augmentation, Vision-Language Model (VLM)
<br /><br />:
- <div>
Deep Reinforcement Learning (DRL) has shown great potential in enabling
robots to find certain objects (e.g., `find a fridge') in environments like
homes or schools. This task is known as Object-Goal Navigation (ObjectNav). DRL
methods are predominantly trained and evaluated using environment simulators.
Although DRL has shown impressive results, the simulators may be biased or
limited. This creates a risk of shortcut learning, i.e., learning a policy
tailored to specific visual details of training environments. We aim to deepen
our understanding of shortcut learning in ObjectNav, its implications and
propose a solution. We design an experiment for inserting a shortcut bias in
the appearance of training environments. As a proof-of-concept, we associate
room types to specific wall colors (e.g., bedrooms with green walls), and
observe poor generalization of a state-of-the-art (SOTA) ObjectNav method to
environments where this is not the case (e.g., bedrooms with blue walls). We
find that shortcut learning is the root cause: the agent learns to navigate to
target objects, by simply searching for the associated wall color of the target
object's room. To solve this, we propose Language-Based (L-B) augmentation. Our
key insight is that we can leverage the multimodal feature space of a
Vision-Language Model (VLM) to augment visual representations directly at the
feature-level, requiring no changes to the simulator, and only an addition of
one layer to the model. Where the SOTA ObjectNav method's success rate drops
69%, our proposal has only a drop of 23%.
]]></content:encoded>
<pubDate>2024-02-07T18:44:27Z</pubDate>
</item>
<item>
<title>AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls</title>
<link>http://arxiv.org/abs/2402.04253v1</link>
<guid>http://arxiv.org/abs/2402.04253v1</guid>
<content:encoded><![CDATA[
<div> APIs, AnyTool, GPT-4, evaluation protocol, benchmark<br />
<br />:
AnyTool16,000Rapid APIAPIAPIAPIAnyToolAnyToolGPT-4AnyToolBenchAnyToolToolLLMGPT-4ToolBenchAnyToolToolLLM 35.4%https://github.com/dyabel/AnyTool  <div>
We introduce AnyTool, a large language model agent designed to revolutionize
the utilization of a vast array of tools in addressing user queries. We utilize
over 16,000 APIs from Rapid API, operating under the assumption that a subset
of these APIs could potentially resolve the queries. AnyTool primarily
incorporates three elements: an API retriever with a hierarchical structure, a
solver aimed at resolving user queries using a selected set of API candidates,
and a self-reflection mechanism, which re-activates AnyTool if the initial
solution proves impracticable. AnyTool is powered by the function calling
feature of GPT-4, eliminating the need for training external modules. We also
revisit the evaluation protocol introduced by previous works and identify a
limitation in this protocol that leads to an artificially high pass rate. By
revising the evaluation protocol to better reflect practical application
scenarios, we introduce an additional benchmark, termed AnyToolBench.
Experiments across various datasets demonstrate the superiority of our AnyTool
over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool
utilization. For instance, AnyTool outperforms ToolLLM by +35.4% in terms of
average pass rate on ToolBench. Code will be available at
https://github.com/dyabel/AnyTool.
]]></content:encoded>
<pubDate>2024-02-06T18:59:57Z</pubDate>
</item>
<item>
<title>EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters</title>
<link>http://arxiv.org/abs/2402.04252v1</link>
<guid>http://arxiv.org/abs/2402.04252v1</guid>
<content:encoded><![CDATA[
<div> : CLIP, EVA-CLIP-18B, 18-billion, , EVA-style<br />
<br />
EVA-CLIP-18BCLIP18027top-180.7%EVA-CLIP50CLIPLAION-2BCOYO-700M20EVA-CLIPEVA-CLIP-18BEVA<br /><br />: EVA-CLIP-18BCLIP180 <div>
Scaling up contrastive language-image pretraining (CLIP) is critical for
empowering both vision and multimodal models. We present EVA-CLIP-18B, the
largest and most powerful open-source CLIP model to date, with 18-billion
parameters. With only 6-billion training samples seen, EVA-CLIP-18B achieves an
exceptional 80.7% zero-shot top-1 accuracy averaged across 27 widely recognized
image classification benchmarks, outperforming its forerunner EVA-CLIP
(5-billion parameters) and other open-source CLIP models by a large margin.
Remarkably, we observe a consistent performance improvement with the model size
scaling of EVA-CLIP, despite maintaining a constant training dataset of
2-billion image-text pairs from LAION-2B and COYO-700M. This dataset is openly
available and much smaller than the in-house datasets (e.g., DFN-5B, WebLI-10B)
employed in other state-of-the-art CLIP models. EVA-CLIP-18B demonstrates the
potential of EVA-style weak-to-strong visual model scaling. With our model
weights made publicly available, we hope to facilitate future research in
vision and multimodal foundation models.
]]></content:encoded>
<pubDate>2024-02-06T18:59:48Z</pubDate>
</item>
<item>
<title>Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science</title>
<link>http://arxiv.org/abs/2402.04247v1</link>
<guid>http://arxiv.org/abs/2402.04247v1</guid>
<content:encoded><![CDATA[
Intelligent agents powered by large language models (LLMs) have demonstrated
substantial promise in autonomously conducting experiments and facilitating
scientific discoveries across various disciplines. While their capabilities are
promising, they also introduce novel vulnerabilities that demand careful
consideration for safety. However, there exists a notable gap in the
literature, as there has been no comprehensive exploration of these
vulnerabilities. This position paper fills this gap by conducting a thorough
examination of vulnerabilities in LLM-based agents within scientific domains,
shedding light on potential risks associated with their misuse and emphasizing
the need for safety measures. We begin by providing a comprehensive overview of
the potential risks inherent to scientific LLM agents, taking into account user
intent, the specific scientific domain, and their potential impact on the
external environment. Then, we delve into the origins of these vulnerabilities
and provide a scoping review of the limited existing works. Based on our
analysis, we propose a triadic framework involving human regulation, agent
alignment, and an understanding of environmental feedback (agent regulation) to
mitigate these identified risks. Furthermore, we highlight the limitations and
challenges associated with safeguarding scientific agents and advocate for the
development of improved models, robust benchmarks, and comprehensive
regulations to address these issues effectively.
]]></content:encoded>
<pubDate>2024-02-06T18:54:07Z</pubDate>
</item>
<item>
<title>V-IRL: Grounding Virtual Intelligence in Real Life</title>
<link>http://arxiv.org/abs/2402.03310v1</link>
<guid>http://arxiv.org/abs/2402.03310v1</guid>
<content:encoded><![CDATA[
<div> : , AI, , , 
:<br /><br />V-IRLAI <div>
There is a sensory gulf between the Earth that humans inhabit and the digital
realms in which modern AI agents are created. To develop AI agents that can
sense, think, and act as flexibly as humans in real-world settings, it is
imperative to bridge the realism gap between the digital and physical worlds.
How can we embody agents in an environment as rich and diverse as the one we
inhabit, without the constraints imposed by real hardware and control? Towards
this end, we introduce V-IRL: a platform that enables agents to scalably
interact with the real world in a virtual yet realistic environment. Our
platform serves as a playground for developing agents that can accomplish
various practical tasks and as a vast testbed for measuring progress in
capabilities spanning perception, decision-making, and interaction with
real-world data across the entire globe.
]]></content:encoded>
<pubDate>2024-02-05T18:59:36Z</pubDate>
</item>
<item>
<title>AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion</title>
<link>http://arxiv.org/abs/2402.03309v1</link>
<guid>http://arxiv.org/abs/2402.03309v1</guid>
<content:encoded><![CDATA[
<div> acoustic-optical neural surface reconstruction, underwater perception, 3D surface reconstruction, baselines, multimodal fusion<br />
<br />
AONeuSRGBAONeuSRGBhttps://aoneus.github.io/ <br /><br />: AONeuS <div>
Underwater perception and 3D surface reconstruction are challenging problems
with broad applications in construction, security, marine archaeology, and
environmental monitoring. Treacherous operating conditions, fragile
surroundings, and limited navigation control often dictate that submersibles
restrict their range of motion and, thus, the baseline over which they can
capture measurements. In the context of 3D scene reconstruction, it is
well-known that smaller baselines make reconstruction more challenging. Our
work develops a physics-based multimodal acoustic-optical neural surface
reconstruction framework (AONeuS) capable of effectively integrating
high-resolution RGB measurements with low-resolution depth-resolved imaging
sonar measurements. By fusing these complementary modalities, our framework can
reconstruct accurate high-resolution 3D surfaces from measurements captured
over heavily-restricted baselines. Through extensive simulations and in-lab
experiments, we demonstrate that AONeuS dramatically outperforms recent
RGB-only and sonar-only inverse-differentiable-rendering--based surface
reconstruction methods. A website visualizing the results of our paper is
located at this address: https://aoneus.github.io/
]]></content:encoded>
<pubDate>2024-02-05T18:59:31Z</pubDate>
</item>
<item>
<title>Do Diffusion Models Learn Semantically Meaningful and Efficient
  Representations?</title>
<link>http://arxiv.org/abs/2402.03305v1</link>
<guid>http://arxiv.org/abs/2402.03305v1</guid>
<content:encoded><![CDATA[
Diffusion models are capable of impressive feats of image generation with
uncommon juxtapositions such as astronauts riding horses on the moon with
properly placed shadows. These outputs indicate the ability to perform
compositional generalization, but how do the models do so? We perform
controlled experiments on conditional DDPMs learning to generate 2D spherical
Gaussian bumps centered at specified $x$- and $y$-positions. Our results show
that the emergence of semantically meaningful latent representations is key to
achieving high performance. En route to successful performance over learning,
the model traverses three distinct phases of latent representations: (phase A)
no latent structure, (phase B) a 2D manifold of disordered states, and (phase
C) a 2D ordered manifold. Corresponding to each of these phases, we identify
qualitatively different generation behaviors: 1) multiple bumps are generated,
2) one bump is generated but at inaccurate $x$ and $y$ locations, 3) a bump is
generated at the correct $x$ and y location. Furthermore, we show that even
under imbalanced datasets where features ($x$- versus $y$-positions) are
represented with skewed frequencies, the learning process for $x$ and $y$ is
coupled rather than factorized, demonstrating that simple vanilla-flavored
diffusion models cannot learn efficient representations in which localization
in $x$ and $y$ are factorized into separate 1D tasks. These findings suggest
the need for future work to find inductive biases that will push generative
models to discover and exploit factorizable independent structures in their
inputs, which will be required to vault these models into more data-efficient
regimes.
]]></content:encoded>
<pubDate>2024-02-05T18:58:38Z</pubDate>
</item>
<item>
<title>TravelPlanner: A Benchmark for Real-World Planning with Language Agents</title>
<link>http://arxiv.org/abs/2402.01622v1</link>
<guid>http://arxiv.org/abs/2402.01622v1</guid>
<content:encoded><![CDATA[
<div> <br />
<br />
LLMTravelPlanner4001225GPT-40.6TravelPlanner <br /><br />: <div>
Planning has been part of the core pursuit for artificial intelligence since
its conception, but earlier AI agents mostly focused on constrained settings
because many of the cognitive substrates necessary for human-level planning
have been lacking. Recently, language agents powered by large language models
(LLMs) have shown interesting capabilities such as tool use and reasoning. Are
these language agents capable of planning in more complex settings that are out
of the reach of prior AI agents? To advance this investigation, we propose
TravelPlanner, a new planning benchmark that focuses on travel planning, a
common real-world planning scenario. It provides a rich sandbox environment,
various tools for accessing nearly four million data records, and 1,225
meticulously curated planning intents and reference plans. Comprehensive
evaluations show that the current language agents are not yet capable of
handling such complex planning tasks-even GPT-4 only achieves a success rate of
0.6%. Language agents struggle to stay on task, use the right tools to collect
information, or keep track of multiple constraints. However, we note that the
mere possibility for language agents to tackle such a complex problem is in
itself non-trivial progress. TravelPlanner provides a challenging yet
meaningful testbed for future language agents.
]]></content:encoded>
<pubDate>2024-02-02T18:39:51Z</pubDate>
</item>
<item>
<title>MAGDi: Structured Distillation of Multi-Agent Interaction Graphs
  Improves Reasoning in Smaller Language Models</title>
<link>http://arxiv.org/abs/2402.01620v1</link>
<guid>http://arxiv.org/abs/2402.01620v1</guid>
<content:encoded><![CDATA[
<div> ; ; ; ; <br />
<br />
MAGDiMAGDiMAGDiMAGDiMAGDi<br /><br />: <br />MAGDiMAGDiMAGDiMAGDi <div>
Multi-agent interactions between Large Language Model (LLM) agents have shown
major improvements on diverse reasoning tasks. However, these involve long
generations from multiple models across several rounds, making them expensive.
Moreover, these multi-agent approaches fail to provide a final, single model
for efficient inference. To address this, we introduce MAGDi, a new method for
structured distillation of the reasoning interactions between multiple LLMs
into smaller LMs. MAGDi teaches smaller models by representing multi-agent
interactions as graphs, augmenting a base student model with a graph encoder,
and distilling knowledge using three objective functions: next-token
prediction, a contrastive loss between correct and incorrect reasoning, and a
graph-based objective to model the interaction structure. Experiments on seven
widely-used commonsense and math reasoning benchmarks show that MAGDi improves
the reasoning capabilities of smaller models, outperforming several methods
that distill from a single teacher and multiple teachers. Moreover, MAGDi also
demonstrates an order of magnitude higher efficiency over its teachers. We
conduct extensive analyses to show that MAGDi (1) enhances the generalizability
to out-of-domain tasks, (2) scales positively with the size and strength of the
base student model, and (3) obtains larger improvements (via our multi-teacher
training) when applying self-consistency - an inference technique that relies
on model diversity.
]]></content:encoded>
<pubDate>2024-02-02T18:35:14Z</pubDate>
</item>
<item>
<title>Binding Touch to Everything: Learning Unified Multimodal Tactile
  Representations</title>
<link>http://arxiv.org/abs/2401.18084v1</link>
<guid>http://arxiv.org/abs/2401.18084v1</guid>
<content:encoded><![CDATA[
<div> tactile, multimodal learning, UniTouch, vision-based touch sensors, zero-shot setting
<br /><br />
1. UniTouch
2. UniTouch
3. UniTouch-shot
4. 
5. UniTouch
<br /><br />: <br />UniTouch <div>
The ability to associate touch with other modalities has huge implications
for humans and computational systems. However, multimodal learning with touch
remains challenging due to the expensive data collection process and
non-standardized sensor outputs. We introduce UniTouch, a unified tactile model
for vision-based touch sensors connected to multiple modalities, including
vision, language, and sound. We achieve this by aligning our UniTouch
embeddings to pretrained image embeddings already associated with a variety of
other modalities. We further propose learnable sensor-specific tokens, allowing
the model to learn from a set of heterogeneous tactile sensors, all at the same
time. UniTouch is capable of conducting various touch sensing tasks in the
zero-shot setting, from robot grasping prediction to touch image question
answering. To the best of our knowledge, UniTouch is the first to demonstrate
such capabilities. Project page: https://cfeng16.github.io/UniTouch/
]]></content:encoded>
<pubDate>2024-01-31T18:59:57Z</pubDate>
</item>
<item>
<title>CARFF: Conditional Auto-encoded Radiance Field for 3D Scene Forecasting</title>
<link>http://arxiv.org/abs/2401.18075v1</link>
<guid>http://arxiv.org/abs/2401.18075v1</guid>
<content:encoded><![CDATA[
<div> : CARFF, Conditional Auto-encoded Radiance Field, 3D Scene Forecasting, Neural Radiance Field, CARLA driving simulator

CARFF3D2D3DNeRF3DVAENeRF3DCARLACARFF<br /><br />: CARFF3D3D3DCARLA <div>
We propose CARFF: Conditional Auto-encoded Radiance Field for 3D Scene
Forecasting, a method for predicting future 3D scenes given past observations,
such as 2D ego-centric images. Our method maps an image to a distribution over
plausible 3D latent scene configurations using a probabilistic encoder, and
predicts the evolution of the hypothesized scenes through time. Our latent
scene representation conditions a global Neural Radiance Field (NeRF) to
represent a 3D scene model, which enables explainable predictions and
straightforward downstream applications. This approach extends beyond previous
neural rendering work by considering complex scenarios of uncertainty in
environmental states and dynamics. We employ a two-stage training of
Pose-Conditional-VAE and NeRF to learn 3D representations. Additionally, we
auto-regressively predict latent scene representations as a partially
observable Markov decision process, utilizing a mixture density network. We
demonstrate the utility of our method in realistic scenarios using the CARLA
driving simulator, where CARFF can be used to enable efficient trajectory and
contingency planning in complex multi-agent autonomous driving scenarios
involving visual occlusions.
]]></content:encoded>
<pubDate>2024-01-31T18:56:09Z</pubDate>
</item>
<item>
<title>Weaver: Foundation Models for Creative Writing</title>
<link>http://arxiv.org/abs/2401.17268v1</link>
<guid>http://arxiv.org/abs/2401.17268v1</guid>
<content:encoded><![CDATA[
<div> : Weaver, , , , <br />
: <br />
WeaverWeaverWeaverWeaverWeaverWeaverAI <div>
This work introduces Weaver, our first family of large language models (LLMs)
dedicated to content creation. Weaver is pre-trained on a carefully selected
corpus that focuses on improving the writing capabilities of large language
models. We then fine-tune Weaver for creative and professional writing purposes
and align it to the preference of professional writers using a suit of novel
methods for instruction data synthesis and LLM alignment, making it able to
produce more human-like texts and follow more diverse instructions for content
creation. The Weaver family consists of models of Weaver Mini (1.8B), Weaver
Base (6B), Weaver Pro (14B), and Weaver Ultra (34B) sizes, suitable for
different applications and can be dynamically dispatched by a routing agent
according to query complexity to balance response quality and computation cost.
Evaluation on a carefully curated benchmark for assessing the writing
capabilities of LLMs shows Weaver models of all sizes outperform generalist
LLMs several times larger than them. Notably, our most-capable Weaver Ultra
model surpasses GPT-4, a state-of-the-art generalist LLM, on various writing
scenarios, demonstrating the advantage of training specialized LLMs for writing
purposes. Moreover, Weaver natively supports retrieval-augmented generation
(RAG) and function calling (tool usage). We present various use cases of these
abilities for improving AI-assisted writing systems, including integration of
external knowledge bases, tools, or APIs, and providing personalized writing
assistance. Furthermore, we discuss and summarize a guideline and best
practices for pre-training and fine-tuning domain-specific LLMs.
]]></content:encoded>
<pubDate>2024-01-30T18:58:43Z</pubDate>
</item>
<item>
<title>ReacLLaMA: Merging chemical and textual information in chemical
  reactivity AI models</title>
<link>http://arxiv.org/abs/2401.17267v1</link>
<guid>http://arxiv.org/abs/2401.17267v1</guid>
<content:encoded><![CDATA[
<div> Graphormer, reactivity model, chemical information, procedural text, accuracy

:<br />
GraphormerGraphormerGPT-2ReacLLaMA-AdapterLLaMA 2GraphormerZero-Shot Labeling ReacLLaMA <div>
Chemical reactivity models are developed to predict chemical reaction
outcomes in the form of classification (success/failure) or regression (product
yield) tasks. The vast majority of the reported models are trained solely on
chemical information such as reactants, products, reagents, and solvents, but
not on the details of a synthetic protocol. Herein incorporation of procedural
text with the aim to augment the Graphormer reactivity model and improve its
accuracy is presented. Two major approaches are used: training an adapter
Graphormer model that is provided with a GPT-2-derived latent representation of
the text procedure (ReacLLaMA-Adapter) and labeling an unlabeled part of a
dataset with the LLaMA 2 model followed by training the Graphormer on an
extended dataset (Zero-Shot Labeling ReacLLaMA). Both methodologies enhance the
discernment of unpromising reactions, thereby providing more accurate models
with improved specificity.
]]></content:encoded>
<pubDate>2024-01-30T18:57:08Z</pubDate>
</item>
<item>
<title>InternLM-XComposer2: Mastering Free-form Text-Image Composition and
  Comprehension in Vision-Language Large Model</title>
<link>http://arxiv.org/abs/2401.16420v1</link>
<guid>http://arxiv.org/abs/2401.16420v1</guid>
<content:encoded><![CDATA[
<div> : InternLM-XComposer2, PLoRA, , , GitHub
:
InternLM-XComposer2-Partial LoRA (PLoRA)LoRAInternLM-XComposer2GPT-4VGemini ProInternLM-XComposer27Bhttps://github.com/InternLM/InternLM-XComposer <div>
We introduce InternLM-XComposer2, a cutting-edge vision-language model
excelling in free-form text-image composition and comprehension. This model
goes beyond conventional vision-language understanding, adeptly crafting
interleaved text-image content from diverse inputs like outlines, detailed
textual specifications, and reference images, enabling highly customizable
content creation. InternLM-XComposer2 proposes a Partial LoRA (PLoRA) approach
that applies additional LoRA parameters exclusively to image tokens to preserve
the integrity of pre-trained language knowledge, striking a balance between
precise vision understanding and text composition with literary talent.
Experimental results demonstrate the superiority of InternLM-XComposer2 based
on InternLM2-7B in producing high-quality long-text multi-modal content and its
exceptional vision-language understanding performance across various
benchmarks, where it not only significantly outperforms existing multimodal
models but also matches or even surpasses GPT-4V and Gemini Pro in certain
assessments. This highlights its remarkable proficiency in the realm of
multimodal understanding. The InternLM-XComposer2 model series with 7B
parameters are publicly available at
https://github.com/InternLM/InternLM-XComposer.
]]></content:encoded>
<pubDate>2024-01-29T18:59:02Z</pubDate>
</item>
<item>
<title>Annotated Hands for Generative Models</title>
<link>http://arxiv.org/abs/2401.15075v1</link>
<guid>http://arxiv.org/abs/2401.15075v1</guid>
<content:encoded><![CDATA[
<div> , , , , 
<br />
GANs <br /><br />: <br /> <div>
Generative models such as GANs and diffusion models have demonstrated
impressive image generation capabilities. Despite these successes, these
systems are surprisingly poor at creating images with hands. We propose a novel
training framework for generative models that substantially improves the
ability of such systems to create hand images. Our approach is to augment the
training images with three additional channels that provide annotations to
hands in the image. These annotations provide additional structure that coax
the generative model to produce higher quality hand images. We demonstrate this
approach on two different generative models: a generative adversarial network
and a diffusion model. We demonstrate our method both on a new synthetic
dataset of hand images and also on real photographs that contain hands. We
measure the improved quality of the generated hands through higher confidence
in finger joint identification using an off-the-shelf hand detector.
]]></content:encoded>
<pubDate>2024-01-26T18:57:54Z</pubDate>
</item>
<item>
<title>Fully Independent Communication in Multi-Agent Reinforcement Learning</title>
<link>http://arxiv.org/abs/2401.15059v1</link>
<guid>http://arxiv.org/abs/2401.15059v1</guid>
<content:encoded><![CDATA[
<div> , , , , 
<br />
MARL
<br /><br />: 
 <div>
Multi-Agent Reinforcement Learning (MARL) comprises a broad area of research
within the field of multi-agent systems. Several recent works have focused
specifically on the study of communication approaches in MARL. While multiple
communication methods have been proposed, these might still be too complex and
not easily transferable to more practical contexts. One of the reasons for that
is due to the use of the famous parameter sharing trick. In this paper, we
investigate how independent learners in MARL that do not share parameters can
communicate. We demonstrate that this setting might incur into some problems,
to which we propose a new learning scheme as a solution. Our results show that,
despite the challenges, independent agents can still learn communication
strategies following our method. Additionally, we use this method to
investigate how communication in MARL is affected by different network
capacities, both for sharing and not sharing parameters. We observe that
communication may not always be needed and that the chosen agent network sizes
need to be considered when used together with communication in order to achieve
efficient learning.
]]></content:encoded>
<pubDate>2024-01-26T18:42:01Z</pubDate>
</item>
<item>
<title>LongFin: A Multimodal Document Understanding Model for Long Financial
  Domain Documents</title>
<link>http://arxiv.org/abs/2401.15050v1</link>
<guid>http://arxiv.org/abs/2401.15050v1</guid>
<content:encoded><![CDATA[
Document AI is a growing research field that focuses on the comprehension and
extraction of information from scanned and digital documents to make everyday
business operations more efficient. Numerous downstream tasks and datasets have
been introduced to facilitate the training of AI models capable of parsing and
extracting information from various document types such as receipts and scanned
forms. Despite these advancements, both existing datasets and models fail to
address critical challenges that arise in industrial contexts. Existing
datasets primarily comprise short documents consisting of a single page, while
existing models are constrained by a limited maximum length, often set at 512
tokens. Consequently, the practical application of these methods in financial
services, where documents can span multiple pages, is severely impeded. To
overcome these challenges, we introduce LongFin, a multimodal document AI model
capable of encoding up to 4K tokens. We also propose the LongForms dataset, a
comprehensive financial dataset that encapsulates several industrial challenges
in financial documents. Through an extensive evaluation, we demonstrate the
effectiveness of the LongFin model on the LongForms dataset, surpassing the
performance of existing public models while maintaining comparable results on
existing single-page benchmarks.
]]></content:encoded>
<pubDate>2024-01-26T18:23:45Z</pubDate>
</item>
<item>
<title>Deconstructing Denoising Diffusion Models for Self-Supervised Learning</title>
<link>http://arxiv.org/abs/2401.14404v1</link>
<guid>http://arxiv.org/abs/2401.14404v1</guid>
<content:encoded><![CDATA[
<div> Denoising Diffusion Models, representation learning, self-supervised learning, deconstructive procedure, classical methods
<br />
DDMDDMDAEDDMDAE
<br /><br />: Denoising Diffusion ModelsDDMDDMsDAE <div>
In this study, we examine the representation learning abilities of Denoising
Diffusion Models (DDM) that were originally purposed for image generation. Our
philosophy is to deconstruct a DDM, gradually transforming it into a classical
Denoising Autoencoder (DAE). This deconstructive procedure allows us to explore
how various components of modern DDMs influence self-supervised representation
learning. We observe that only a very few modern components are critical for
learning good representations, while many others are nonessential. Our study
ultimately arrives at an approach that is highly simplified and to a large
extent resembles a classical DAE. We hope our study will rekindle interest in a
family of classical methods within the realm of modern self-supervised
learning.
]]></content:encoded>
<pubDate>2024-01-25T18:59:57Z</pubDate>
</item>
<item>
<title>VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web
  Tasks</title>
<link>http://arxiv.org/abs/2401.13649v1</link>
<guid>http://arxiv.org/abs/2401.13649v1</guid>
<content:encoded><![CDATA[
<div> benchmark, autonomous agents, multimodal, VisualWebArena, web

VisualWebArena-LLMVisualWebArena <div>
Autonomous agents capable of planning, reasoning, and executing actions on
the web offer a promising avenue for automating computer tasks. However, the
majority of existing benchmarks primarily focus on text-based agents,
neglecting many natural tasks that require visual information to effectively
solve. Given that most computer interfaces cater to human perception, visual
information often augments textual data in ways that text-only models struggle
to harness effectively. To bridge this gap, we introduce VisualWebArena, a
benchmark designed to assess the performance of multimodal web agents on
realistic \textit{visually grounded tasks}. VisualWebArena comprises of a set
of diverse and complex web-based tasks that evaluate various capabilities of
autonomous multimodal agents. To perform on this benchmark, agents need to
accurately process image-text inputs, interpret natural language instructions,
and execute actions on websites to accomplish user-defined objectives. We
conduct an extensive evaluation of state-of-the-art LLM-based autonomous
agents, including several multimodal models. Through extensive quantitative and
qualitative analysis, we identify several limitations of text-only LLM agents,
and reveal gaps in the capabilities of state-of-the-art multimodal language
agents. VisualWebArena provides a framework for evaluating multimodal
autonomous language agents, and offers insights towards building stronger
autonomous agents for the web. Our code, baseline models, and data is publicly
available at https://jykoh.com/vwa.
]]></content:encoded>
<pubDate>2024-01-24T18:35:21Z</pubDate>
</item>
<item>
<title>HAZARD Challenge: Embodied Decision Making in Dynamically Changing
  Environments</title>
<link>http://arxiv.org/abs/2401.12975v1</link>
<guid>http://arxiv.org/abs/2401.12975v1</guid>
<content:encoded><![CDATA[
<div> , , HAZARD, , 

HAZARDHAZARDLLMsRLLLMHAZARDhttps://vis-www.cs.umass.edu/hazard/<br /><br />HAZARDLLM <div>
Recent advances in high-fidelity virtual environments serve as one of the
major driving forces for building intelligent embodied agents to perceive,
reason and interact with the physical world. Typically, these environments
remain unchanged unless agents interact with them. However, in real-world
scenarios, agents might also face dynamically changing environments
characterized by unexpected events and need to rapidly take action accordingly.
To remedy this gap, we propose a new simulated embodied benchmark, called
HAZARD, specifically designed to assess the decision-making abilities of
embodied agents in dynamic situations. HAZARD consists of three unexpected
disaster scenarios, including fire, flood, and wind, and specifically supports
the utilization of large language models (LLMs) to assist common sense
reasoning and decision-making. This benchmark enables us to evaluate autonomous
agents' decision-making capabilities across various pipelines, including
reinforcement learning (RL), rule-based, and search-based methods in
dynamically changing environments. As a first step toward addressing this
challenge using large language models, we further develop an LLM-based agent
and perform an in-depth analysis of its promise and challenge of solving these
challenging tasks. HAZARD is available at https://vis-www.cs.umass.edu/hazard/.
]]></content:encoded>
<pubDate>2024-01-23T18:59:43Z</pubDate>
</item>
<item>
<title>CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation</title>
<link>http://arxiv.org/abs/2401.12208v1</link>
<guid>http://arxiv.org/abs/2401.12208v1</guid>
<content:encoded><![CDATA[
<div> CheXinstruct, CheXagent, vision-language, CXR, CheXbench
<br />
XCXRCheXagentCheXinstructCheXagentLLMCheXbenchFMs8CXRCheXagentCheXbenchFMshttps://stanford-aimi.github.io/chexagent.html
<br /><br />: CXRCheXagentCheXagent <div>
Chest X-rays (CXRs) are the most frequently performed imaging test in
clinical practice. Recent advances in the development of vision-language
foundation models (FMs) give rise to the possibility of performing automated
CXR interpretation, which can assist physicians with clinical decision-making
and improve patient outcomes. However, developing FMs that can accurately
interpret CXRs is challenging due to the (1) limited availability of
large-scale vision-language datasets in the medical image domain, (2) lack of
vision and language encoders that can capture the complexities of medical data,
and (3) absence of evaluation frameworks for benchmarking the abilities of FMs
on CXR interpretation. In this work, we address these challenges by first
introducing \emph{CheXinstruct} - a large-scale instruction-tuning dataset
curated from 28 publicly-available datasets. We then present \emph{CheXagent} -
an instruction-tuned FM capable of analyzing and summarizing CXRs. To build
CheXagent, we design a clinical large language model (LLM) for parsing
radiology reports, a vision encoder for representing CXR images, and a network
to bridge the vision and language modalities. Finally, we introduce
\emph{CheXbench} - a novel benchmark designed to systematically evaluate FMs
across 8 clinically-relevant CXR interpretation tasks. Extensive quantitative
evaluations and qualitative reviews with five expert radiologists demonstrate
that CheXagent outperforms previously-developed general- and medical-domain FMs
on CheXbench tasks. Furthermore, in an effort to improve model transparency, we
perform a fairness evaluation across factors of sex, race and age to highlight
potential performance disparities. Our project is at
\url{https://stanford-aimi.github.io/chexagent.html}.
]]></content:encoded>
<pubDate>2024-01-22T18:51:07Z</pubDate>
</item>
<item>
<title>Retrieval-Guided Reinforcement Learning for Boolean Circuit Minimization</title>
<link>http://arxiv.org/abs/2401.12205v1</link>
<guid>http://arxiv.org/abs/2401.12205v1</guid>
<content:encoded><![CDATA[
<div> logic synthesis, chip design, hardware description languages, ABC-RL, quality-of-result

VerilogABC-RLABC-RLQoR24.8%ABC-RL9<br /><br />: ABC-RL24.8%ABC-RL9 <div>
Logic synthesis, a pivotal stage in chip design, entails optimizing chip
specifications encoded in hardware description languages like Verilog into
highly efficient implementations using Boolean logic gates. The process
involves a sequential application of logic minimization heuristics (``synthesis
recipe"), with their arrangement significantly impacting crucial metrics such
as area and delay. Addressing the challenge posed by the broad spectrum of
design complexities - from variations of past designs (e.g., adders and
multipliers) to entirely novel configurations (e.g., innovative processor
instructions) - requires a nuanced `synthesis recipe` guided by human expertise
and intuition. This study conducts a thorough examination of learning and
search techniques for logic synthesis, unearthing a surprising revelation:
pre-trained agents, when confronted with entirely novel designs, may veer off
course, detrimentally affecting the search trajectory. We present ABC-RL, a
meticulously tuned $\alpha$ parameter that adeptly adjusts recommendations from
pre-trained agents during the search process. Computed based on similarity
scores through nearest neighbor retrieval from the training dataset, ABC-RL
yields superior synthesis recipes tailored for a wide array of hardware
designs. Our findings showcase substantial enhancements in the
Quality-of-result (QoR) of synthesized circuits, boasting improvements of up to
24.8% compared to state-of-the-art techniques. Furthermore, ABC-RL achieves an
impressive up to 9x reduction in runtime (iso-QoR) when compared to current
state-of-the-art methodologies.
]]></content:encoded>
<pubDate>2024-01-22T18:46:30Z</pubDate>
</item>
<item>
<title>Applications of flow models to the generation of correlated lattice QCD
  ensembles</title>
<link>http://arxiv.org/abs/2401.10874v1</link>
<guid>http://arxiv.org/abs/2401.10874v1</guid>
<content:encoded><![CDATA[
<div> : normalizing flows, lattice quantum field theory, variance reduction, gauge theories, QCD observables
: 
QCD- <br /><br /> <div>
Machine-learned normalizing flows can be used in the context of lattice
quantum field theory to generate statistically correlated ensembles of lattice
gauge fields at different action parameters. This work demonstrates how these
correlations can be exploited for variance reduction in the computation of
observables. Three different proof-of-concept applications are demonstrated
using a novel residual flow architecture: continuum limits of gauge theories,
the mass dependence of QCD observables, and hadronic matrix elements based on
the Feynman-Hellmann approach. In all three cases, it is shown that statistical
uncertainties are significantly reduced when machine-learned flows are
incorporated as compared with the same calculations performed with uncorrelated
ensembles or direct reweighting.
]]></content:encoded>
<pubDate>2024-01-19T18:33:52Z</pubDate>
</item>
<item>
<title>Vlogger: Make Your Dream A Vlog</title>
<link>http://arxiv.org/abs/2401.09414v1</link>
<guid>http://arxiv.org/abs/2401.09414v1</guid>
<content:encoded><![CDATA[
<div> Vlogger, AI, video blog, Large Language Model, <br />
Script, Actor, ShowMaker, Voicer, vlog<br />

:<br />
VloggerAIvlogVloggerLLMvlogvlog123ShowMaker4VoicerVloggervlogShowMakerVloggerShowMakerT2VT2VVlogger5vloghttps://github.com/zhuangshaobin/Vlogger <div>
In this work, we present Vlogger, a generic AI system for generating a
minute-level video blog (i.e., vlog) of user descriptions. Different from short
videos with a few seconds, vlog often contains a complex storyline with
diversified scenes, which is challenging for most existing video generation
approaches. To break through this bottleneck, our Vlogger smartly leverages
Large Language Model (LLM) as Director and decomposes a long video generation
task of vlog into four key stages, where we invoke various foundation models to
play the critical roles of vlog professionals, including (1) Script, (2) Actor,
(3) ShowMaker, and (4) Voicer. With such a design of mimicking human beings,
our Vlogger can generate vlogs through explainable cooperation of top-down
planning and bottom-up shooting. Moreover, we introduce a novel video diffusion
model, ShowMaker, which serves as a videographer in our Vlogger for generating
the video snippet of each shooting scene. By incorporating Script and Actor
attentively as textual and visual prompts, it can effectively enhance
spatial-temporal coherence in the snippet. Besides, we design a concise mixed
training paradigm for ShowMaker, boosting its capacity for both T2V generation
and prediction. Finally, the extensive experiments show that our method
achieves state-of-the-art performance on zero-shot T2V generation and
prediction tasks. More importantly, Vlogger can generate over 5-minute vlogs
from open-world descriptions, without loss of video coherence on script and
actor. The code and model is all available at
https://github.com/zhuangshaobin/Vlogger.
]]></content:encoded>
<pubDate>2024-01-17T18:55:12Z</pubDate>
</item>
<item>
<title>Multimodal assessment of best possible self as a self-regulatory
  activity for the classroom</title>
<link>http://arxiv.org/abs/2401.08424v1</link>
<guid>http://arxiv.org/abs/2401.08424v1</guid>
<content:encoded><![CDATA[
<div> : 

BPSBPSNA-STAIASHRVNABPSHRVBPSBPSBPS<br /><br />:BPSBPS BPS <div>
Best possible self (BPS) is a positive psychological intervention shown to
enhance well-being which involves writing a description of an ideal future
scenario. This paper presents a comparison of psychophysiological effects of a
BPS activity that has been adapted for classroom settings and a time-matched
control activity (NA). Thirty-three undergraduate students participated in the
study that assessed state anxiety (State-Trait Anxiety Inventory, STAI), affect
(Affective Slider, AS), and cardiac vagal activity (heart-rate variability,
HRV) as an indicator of self-regulatory resource usage, at three time periods
(PRE, DURING, POST). Results show that BPS led to a significantly greater
increase in positive valence (DURING) and overall higher levels of cardiac
vagal activity (HRV) compared to NA. These findings suggest that BPS has
promising characteristics as a self-regulatory technique aimed at fostering
positive affect and positively impacting self-regulatory resources. As BPS does
not require expert knowledge nor specialized technology to administer, it may
be a suitable activity for educators to use when teaching and having students
practice self-regulation. This study presents evidence collected in a
replicable multimodal approach of the self-regulatory effects of a brief BPS
activity on undergraduate students.
]]></content:encoded>
<pubDate>2024-01-16T15:11:12Z</pubDate>
</item>
<item>
<title>AGG: Amortized Generative 3D Gaussians for Single Image to 3D</title>
<link>http://arxiv.org/abs/2401.04099v1</link>
<guid>http://arxiv.org/abs/2401.04099v1</guid>
<content:encoded><![CDATA[
<div> 3D content creation, 3D Gaussian splatting, Amortized Generative 3D Gaussian framework, optimization-based, super-resolution

3D3D3D3D3D3D3DAGG3DAGG3D3D3D3D3DAGGhttps://ir1d.github.io/AGG/ <br /><br />: 3D3D3D3D3D3DAGG3D3D3D <div>
Given the growing need for automatic 3D content creation pipelines, various
3D representations have been studied to generate 3D objects from a single
image. Due to its superior rendering efficiency, 3D Gaussian splatting-based
models have recently excelled in both 3D reconstruction and generation. 3D
Gaussian splatting approaches for image to 3D generation are often
optimization-based, requiring many computationally expensive score-distillation
steps. To overcome these challenges, we introduce an Amortized Generative 3D
Gaussian framework (AGG) that instantly produces 3D Gaussians from a single
image, eliminating the need for per-instance optimization. Utilizing an
intermediate hybrid representation, AGG decomposes the generation of 3D
Gaussian locations and other appearance attributes for joint optimization.
Moreover, we propose a cascaded pipeline that first generates a coarse
representation of the 3D data and later upsamples it with a 3D Gaussian
super-resolution module. Our method is evaluated against existing
optimization-based 3D Gaussian frameworks and sampling-based pipelines
utilizing other 3D representations, where AGG showcases competitive generation
abilities both qualitatively and quantitatively while being several orders of
magnitude faster. Project page: https://ir1d.github.io/AGG/
]]></content:encoded>
<pubDate>2024-01-08T18:56:33Z</pubDate>
</item>
<item>
<title>The Tactician's Web of Large-Scale Formal Knowledge</title>
<link>http://arxiv.org/abs/2401.02950v1</link>
<guid>http://arxiv.org/abs/2401.02950v1</guid>
<content:encoded><![CDATA[
<div> Coq proof assistant, formal mathematical knowledge, machine learning, interconnected web, proof engineering
<br /><br />:
Tactician's WebCoqCoqCoq <div>
The Tactician's Web is a platform offering a large web of strongly
interconnected, machine-checked, formal mathematical knowledge conveniently
packaged for machine learning, analytics, and proof engineering. Built on top
of the Coq proof assistant, the platform exports a dataset containing a wide
variety of formal theories, presented as a web of definitions, theorems, proof
terms, tactics, and proof states. Theories are encoded both as a semantic graph
(rendered below) and as human-readable text, each with a unique set of
advantages and disadvantages. Proving agents may interact with Coq through the
same rich data representation and can be automatically benchmarked on a set of
theorems. Tight integration with Coq provides the unique possibility to make
agents available to proof engineers as practical tools.
]]></content:encoded>
<pubDate>2024-01-05T18:52:35Z</pubDate>
</item>
<item>
<title>Graph2Tac: Learning Hierarchical Representations of Math Concepts in
  Theorem proving</title>
<link>http://arxiv.org/abs/2401.02949v1</link>
<guid>http://arxiv.org/abs/2401.02949v1</guid>
<content:encoded><![CDATA[
<div> : , AI, Coq, , 

: <br /><br />AICoqGraph2Tac(G2T)k <div>
Concepts abound in mathematics and its applications. They vary greatly
between subject areas, and new ones are introduced in each mathematical paper
or application. A formal theory builds a hierarchy of definitions, theorems and
proofs that reference each other. When an AI agent is proving a new theorem,
most of the mathematical concepts and lemmas relevant to that theorem may have
never been seen during training. This is especially true in the Coq proof
assistant, which has a diverse library of Coq projects, each with its own
definitions, lemmas, and even custom tactic procedures used to prove those
lemmas. It is essential for agents to incorporate such new information into
their knowledge base on the fly. We work towards this goal by utilizing a new,
large-scale, graph-based dataset for machine learning in Coq. We leverage a
faithful graph-representation of Coq terms that induces a directed graph of
dependencies between definitions to create a novel graph neural network,
Graph2Tac (G2T), that takes into account not only the current goal, but also
the entire hierarchy of definitions that led to the current goal. G2T is an
online model that is deeply integrated into the users' workflow and can adapt
in real time to new Coq projects and their definitions. It complements well
with other online models that learn in real time from new proof scripts. Our
novel definition embedding task, which is trained to compute representations of
mathematical concepts not seen during training, boosts the performance of the
neural network to rival state-of-the-art k-nearest neighbor predictors.
]]></content:encoded>
<pubDate>2024-01-05T18:52:09Z</pubDate>
</item>
<item>
<title>ODIN: A Single Model for 2D and 3D Perception</title>
<link>http://arxiv.org/abs/2401.02416v1</link>
<guid>http://arxiv.org/abs/2401.02416v1</guid>
<content:encoded><![CDATA[
<div> 3D perception benchmarks, ScanNet, point clouds, transformer architecture, ODIN
<br />
ODIN2D RGB3D2D3Dtransformertoken2D3DODINScanNet200Matterport3DAI2THOR 3DScanNetS3DISCOCO3D3D3DTEACh <div>
State-of-the-art models on contemporary 3D perception benchmarks like ScanNet
consume and label dataset-provided 3D point clouds, obtained through post
processing of sensed multiview RGB-D images. They are typically trained
in-domain, forego large-scale 2D pre-training and outperform alternatives that
featurize the posed RGB-D multiview images instead. The gap in performance
between methods that consume posed images versus post-processed 3D point clouds
has fueled the belief that 2D and 3D perception require distinct model
architectures. In this paper, we challenge this view and propose ODIN
(Omni-Dimensional INstance segmentation), a model that can segment and label
both 2D RGB images and 3D point clouds, using a transformer architecture that
alternates between 2D within-view and 3D cross-view information fusion. Our
model differentiates 2D and 3D feature operations through the positional
encodings of the tokens involved, which capture pixel coordinates for 2D patch
tokens and 3D coordinates for 3D feature tokens. ODIN achieves state-of-the-art
performance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation
benchmarks, and competitive performance on ScanNet, S3DIS and COCO. It
outperforms all previous works by a wide margin when the sensed 3D point cloud
is used in place of the point cloud sampled from 3D mesh. When used as the 3D
perception engine in an instructable embodied agent architecture, it sets a new
state-of-the-art on the TEACh action-from-dialogue benchmark. Our code and
checkpoints can be found at the project website: https://odin-seg.github.io.
]]></content:encoded>
<pubDate>2024-01-04T18:59:25Z</pubDate>
</item>
<item>
<title>LLaMA Pro: Progressive LLaMA with Block Expansion</title>
<link>http://arxiv.org/abs/2401.02415v1</link>
<guid>http://arxiv.org/abs/2401.02415v1</guid>
<content:encoded><![CDATA[
<div> LLaMA, post-pretraining, Transformer blocks, programming, mathematics
<br />
Large Language ModelsLLMsTransformerLLaMA Pro-8.3B 
<br /><br />: 
<br />LLaMA Pro-8.3BTransformer <div>
Humans generally acquire new skills without compromising the old; however,
the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to
CodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with
an expansion of Transformer blocks. We tune the expanded blocks using only new
corpus, efficiently and effectively improving the model's knowledge without
catastrophic forgetting. In this paper, we experiment on the corpus of code and
math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from
LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro
and its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced
performance among various benchmarks, demonstrating superiority over existing
open models in the LLaMA family and the immense potential of reasoning and
addressing diverse tasks as an intelligent agent. Our findings provide valuable
insights into integrating natural and programming languages, laying a solid
foundation for developing advanced language agents that operate effectively in
various environments.
]]></content:encoded>
<pubDate>2024-01-04T18:59:12Z</pubDate>
</item>
<item>
<title>TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview</title>
<link>http://arxiv.org/abs/2401.01330v1</link>
<guid>http://arxiv.org/abs/2401.01330v1</guid>
<content:encoded><![CDATA[
<div> TREC iKAT, Conversational Search Agents, personalized context, decisional search tasks, information operators <br />
<br />
: TREC iKAT <div>
Conversational Information Seeking stands as a pivotal research area with
significant contributions from previous works. The TREC Interactive Knowledge
Assistance Track (iKAT) builds on the foundational work of the TREC
Conversational Assistance Track (CAsT). However, iKAT distinctively emphasizes
the creation and research of conversational search agents that adapt responses
based on user's prior interactions and present context. The challenge lies in
enabling Conversational Search Agents (CSA) to incorporate this personalized
context to efficiency and effectively guide users through the relevant
information to them. iKAT also emphasizes decisional search tasks, where users
sift through data and information to weigh up options in order to reach a
conclusion or perform an action. These tasks, prevalent in everyday
information-seeking decisions -- be it related to travel, health, or shopping
-- often revolve around a subset of high-level information operators where
queries or questions about the information space include: finding options,
comparing options, identifying the pros and cons of options, etc. Given the
different personas and their information need (expressed through the sequence
of questions), diverse conversation trajectories will arise -- because the
answers to these similar queries will be very different. In this paper, we
report on the first year of TREC iKAT, describing the task, topics, data
collection, and evaluation framework. We further review the submissions and
summarize the findings.
]]></content:encoded>
<pubDate>2024-01-02T18:40:03Z</pubDate>
</item>
<item>
<title>K-PERM: Personalized Response Generation Using Dynamic Knowledge
  Retrieval and Persona-Adaptive Queries</title>
<link>http://arxiv.org/abs/2312.17748v1</link>
<guid>http://arxiv.org/abs/2312.17748v1</guid>
<content:encoded><![CDATA[
<div> K-PERMFoCus
<br /><br />
K-PERMK-PERMFoCusK-PERMLLMsGPT 3.510.5K-PERM <div>
Personalizing conversational agents can enhance the quality of conversations
and increase user engagement. However, they often lack external knowledge to
appropriately tend to a user's persona. This is particularly crucial for
practical applications like mental health support, nutrition planning,
culturally sensitive conversations, or reducing toxic behavior in
conversational agents. To enhance the relevance and comprehensiveness of
personalized responses, we propose using a two-step approach that involves (1)
selectively integrating user personas and (2) contextualizing the response with
supplementing information from a background knowledge source. We develop K-PERM
(Knowledge-guided PErsonalization with Reward Modulation), a dynamic
conversational agent that combines these elements. K-PERM achieves
state-of-the-art performance on the popular FoCus dataset, containing
real-world personalized conversations concerning global landmarks. We show that
using responses from K-PERM can improve performance in state-of-the-art LLMs
(GPT 3.5) by 10.5%, highlighting the impact of K-PERM for personalizing
chatbots.
]]></content:encoded>
<pubDate>2023-12-29T18:59:58Z</pubDate>
</item>
<item>
<title>MURP: Multi-Agent Ultra-Wideband Relative Pose Estimation with
  Constrained Communications in 3D Environments</title>
<link>http://arxiv.org/abs/2312.17731v1</link>
<guid>http://arxiv.org/abs/2312.17731v1</guid>
<content:encoded><![CDATA[
<div> : , , 3D, , 
:
(UWB)3DUWB19%0.249.5 <br /><br />: UWB <div>
Inter-agent relative localization is critical for many multi-robot systems
operating in the absence of external positioning infrastructure or prior
environmental knowledge. We propose a novel inter-agent relative 3D pose
estimation system where each participating agent is equipped with several
ultra-wideband (UWB) ranging tags. Prior work typically supplements noisy UWB
range measurements with additional continuously transmitted data, such as
odometry, leading to potential scaling issues with increased team size and/or
decreased communication network capability. By equipping each agent with
multiple UWB antennas, our approach addresses these concerns by using only
locally collected UWB range measurements, a priori state constraints, and
detections of when said constraints are violated. Leveraging our learned mean
ranging bias correction, we gain a 19% positional error improvement giving us
experimental mean absolute position and heading errors of 0.24m and 9.5 degrees
respectively. When compared to other state-of-the-art approaches, our work
demonstrates improved performance over similar systems, while remaining
competitive with methods that have significantly higher communication costs.
Additionally, we make our datasets available.
]]></content:encoded>
<pubDate>2023-12-29T18:40:05Z</pubDate>
</item>
<item>
<title>EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards
  Embodied AI</title>
<link>http://arxiv.org/abs/2312.16170v1</link>
<guid>http://arxiv.org/abs/2312.16170v1</guid>
<content:encoded><![CDATA[
<div> : , , 3D, , Embodied Perceptron

:<br /><br />3DEmbodiedScanEmbodied Perceptron5k100RGB-D160k7603D80Embodied Perceptron3D3D3DGitHub <div>
In the realm of computer vision and robotics, embodied agents are expected to
explore their environment and carry out human instructions. This necessitates
the ability to fully understand 3D scenes given their first-person observations
and contextualize them into language for interaction. However, traditional
research focuses more on scene-level input and output setups from a global
view. To address the gap, we introduce EmbodiedScan, a multi-modal, ego-centric
3D perception dataset and benchmark for holistic 3D scene understanding. It
encompasses over 5k scans encapsulating 1M ego-centric RGB-D views, 1M language
prompts, 160k 3D-oriented boxes spanning over 760 categories, some of which
partially align with LVIS, and dense semantic occupancy with 80 common
categories. Building upon this database, we introduce a baseline framework
named Embodied Perceptron. It is capable of processing an arbitrary number of
multi-modal inputs and demonstrates remarkable 3D perception capabilities, both
within the two series of benchmarks we set up, i.e., fundamental 3D perception
tasks and language-grounded tasks, and in the wild. Codes, datasets, and
benchmarks will be available at https://github.com/OpenRobotLab/EmbodiedScan.
]]></content:encoded>
<pubDate>2023-12-26T18:59:11Z</pubDate>
</item>
<item>
<title>From Text to Multimodal: A Comprehensive Survey of Adversarial Example
  Generation in Question Answering Systems</title>
<link>http://arxiv.org/abs/2312.16156v1</link>
<guid>http://arxiv.org/abs/2312.16156v1</guid>
<content:encoded><![CDATA[
<div> : , , , , 

: 
seq2seq <div>
Integrating adversarial machine learning with Question Answering (QA) systems
has emerged as a critical area for understanding the vulnerabilities and
robustness of these systems. This article aims to comprehensively review
adversarial example-generation techniques in the QA field, including textual
and multimodal contexts. We examine the techniques employed through systematic
categorization, providing a comprehensive, structured review. Beginning with an
overview of traditional QA models, we traverse the adversarial example
generation by exploring rule-based perturbations and advanced generative
models. We then extend our research to include multimodal QA systems, analyze
them across various methods, and examine generative models, seq2seq
architectures, and hybrid methodologies. Our research grows to different
defense strategies, adversarial datasets, and evaluation metrics and
illustrates the comprehensive literature on adversarial QA. Finally, the paper
considers the future landscape of adversarial question generation, highlighting
potential research directions that can advance textual and multimodal QA
systems in the context of adversarial challenges.
]]></content:encoded>
<pubDate>2023-12-26T18:30:29Z</pubDate>
</item>
</channel>
</rss>