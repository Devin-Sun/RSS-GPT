<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>ArXiv Query: search_query=cat:cs.*&amp;amp;id_list=&amp;amp;start=0&amp;amp;max_results=10</title>
<link>http://arxiv.org/api//Sr0Ktnalppie/A9qvr4BIZuOfQ</link>

<item>
<title>Poly-Autoregressive Prediction for Modeling Interactions</title>
<link>http://arxiv.org/abs/2502.08646v1</link>
<guid>http://arxiv.org/abs/2502.08646v1</guid>
<content:encoded><![CDATA[

We introduce a simple framework for predicting the behavior of an agent in
multi-agent settings. In contrast to autoregressive (AR) tasks, such as
language processing, our focus is on scenarios with multiple agents whose
interactions are shaped by physical constraints and internal motivations. To
this end, we propose Poly-Autoregressive (PAR) modeling, which forecasts an ego
agent's future behavior by reasoning about the ego agent's state history and
the past and current states of other interacting agents. At its core, PAR
represents the behavior of all agents as a sequence of tokens, each
representing an agent's state at a specific timestep. With minimal data
pre-processing changes, we show that PAR can be applied to three different
problems: human action forecasting in social situations, trajectory prediction
for autonomous vehicles, and object pose forecasting during hand-object
interaction. Using a small proof-of-concept transformer backbone, PAR
outperforms AR across these three scenarios. The project website can be found
at https://neerja.me/PAR/.
]]></content:encoded>
<pubDate>2025-02-12T18:59:43Z</pubDate>
<pubDate>2025-02-12T18:59:43Z</pubDate>
</item>
<item>
<title>Utility Engineering: Analyzing and Controlling Emergent Value Systems in
  AIs</title>
<link>http://arxiv.org/abs/2502.08640v1</link>
<guid>http://arxiv.org/abs/2502.08640v1</guid>
<content:encoded><![CDATA[

As AIs rapidly advance and become more agentic, the risk they pose is
governed not only by their capabilities but increasingly by their propensities,
including goals and values. Tracking the emergence of goals and values has
proven a longstanding problem, and despite much interest over the years it
remains unclear whether current AIs have meaningful values. We propose a
solution to this problem, leveraging the framework of utility functions to
study the internal coherence of AI preferences. Surprisingly, we find that
independently-sampled preferences in current LLMs exhibit high degrees of
structural coherence, and moreover that this emerges with scale. These findings
suggest that value systems emerge in LLMs in a meaningful sense, a finding with
broad implications. To study these emergent value systems, we propose utility
engineering as a research agenda, comprising both the analysis and control of
AI utilities. We uncover problematic and often shocking values in LLM
assistants despite existing control measures. These include cases where AIs
value themselves over humans and are anti-aligned with specific individuals. To
constrain these emergent value systems, we propose methods of utility control.
As a case study, we show how aligning utilities with a citizen assembly reduces
political biases and generalizes to new scenarios. Whether we like it or not,
value systems have already emerged in AIs, and much work remains to fully
understand and control these emergent representations.
]]></content:encoded>
<pubDate>2025-02-12T18:55:43Z</pubDate>
<pubDate>2025-02-12T18:55:43Z</pubDate>
</item>
<item>
<title>CineMaster: A 3D-Aware and Controllable Framework for Cinematic
  Text-to-Video Generation</title>
<link>http://arxiv.org/abs/2502.08639v1</link>
<guid>http://arxiv.org/abs/2502.08639v1</guid>
<content:encoded><![CDATA[

In this work, we present CineMaster, a novel framework for 3D-aware and
controllable text-to-video generation. Our goal is to empower users with
comparable controllability as professional film directors: precise placement of
objects within the scene, flexible manipulation of both objects and camera in
3D space, and intuitive layout control over the rendered frames. To achieve
this, CineMaster operates in two stages. In the first stage, we design an
interactive workflow that allows users to intuitively construct 3D-aware
conditional signals by positioning object bounding boxes and defining camera
movements within the 3D space. In the second stage, these control
signals--comprising rendered depth maps, camera trajectories and object class
labels--serve as the guidance for a text-to-video diffusion model, ensuring to
generate the user-intended video content. Furthermore, to overcome the scarcity
of in-the-wild datasets with 3D object motion and camera pose annotations, we
carefully establish an automated data annotation pipeline that extracts 3D
bounding boxes and camera trajectories from large-scale video data. Extensive
qualitative and quantitative experiments demonstrate that CineMaster
significantly outperforms existing methods and implements prominent 3D-aware
text-to-video generation. Project page: https://cinemaster-dev.github.io/.
]]></content:encoded>
<pubDate>2025-02-12T18:55:36Z</pubDate>
<pubDate>2025-02-12T18:55:36Z</pubDate>
</item>

<item>
<title>EVEv2: Improved Baselines for Encoder-Free Vision-Language Models</title>
<link>http://arxiv.org/abs/2502.06788v1</link>
<guid>http://arxiv.org/abs/2502.06788v1</guid>
<content:encoded><![CDATA[
Existing encoder-free vision-language models (VLMs) are rapidly narrowing the
performance gap with their encoder-based counterparts, highlighting the
promising potential for unified multimodal systems with structural simplicity
and efficient deployment. We systematically clarify the performance gap between
VLMs using pre-trained vision encoders, discrete tokenizers, and minimalist
visual layers from scratch, deeply excavating the under-examined
characteristics of encoder-free VLMs. We develop efficient strategies for
encoder-free VLMs that rival mainstream encoder-based ones. After an in-depth
investigation, we launch EVEv2.0, a new and improved family of encoder-free
VLMs. We show that: (i) Properly decomposing and hierarchically associating
vision and language within a unified model reduces interference between
modalities. (ii) A well-designed training strategy enables effective
optimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0
represents a thorough study for developing a decoder-only architecture across
modalities, demonstrating superior data efficiency and strong vision-reasoning
capability. Code is publicly available at: https://github.com/baaivision/EVE.
]]></content:encoded>
<pubDate>2025-02-10T18:59:58Z</pubDate>
</item>
<item>
<title>Visual Agentic AI for Spatial Reasoning with a Dynamic API</title>
<link>http://arxiv.org/abs/2502.06787v1</link>
<guid>http://arxiv.org/abs/2502.06787v1</guid>
<content:encoded><![CDATA[
Visual reasoning -- the ability to interpret the visual world -- is crucial
for embodied agents that operate within three-dimensional scenes. Progress in
AI has led to vision and language models capable of answering questions from
images. However, their performance declines when tasked with 3D spatial
reasoning. To tackle the complexity of such reasoning problems, we introduce an
agentic program synthesis approach where LLM agents collaboratively generate a
Pythonic API with new functions to solve common subproblems. Our method
overcomes limitations of prior approaches that rely on a static, human-defined
API, allowing it to handle a wider range of queries. To assess AI capabilities
for 3D understanding, we introduce a new benchmark of queries involving
multiple steps of grounding and inference. We show that our method outperforms
prior zero-shot models for visual reasoning in 3D and empirically validate the
effectiveness of our agentic framework for 3D spatial reasoning tasks. Project
website: https://glab-caltech.github.io/vadar/
]]></content:encoded>
<pubDate>2025-02-10T18:59:35Z</pubDate>
</item>
<item>
<title>Towards Internet-Scale Training For Agents</title>
<link>http://arxiv.org/abs/2502.06776v1</link>
<guid>http://arxiv.org/abs/2502.06776v1</guid>
<content:encoded><![CDATA[
The predominant approach for training web navigation agents gathers human
demonstrations for a set of popular websites and hand-written tasks, but it is
becoming clear that human data are an inefficient resource. We develop a
pipeline to facilitate Internet-scale training for agents without laborious
human annotations. In the first stage, an LLM generates tasks for 150k diverse
websites. In the next stage, LLM agents complete tasks and produce
trajectories. In the final stage, an LLM reviews the trajectories and judges
their success. Language models are competitive with human annotators, detecting
and filtering out harmful content with an accuracy of 97%, generating feasible
tasks with an 89% rate, and judging successful trajectories with an 82.6%
accuracy. Scaling the pipeline, agents based on Llama 3.1 70B solve 16.7% of
tasks for 150k sites. Training on the data generated by our pipeline is
competitive with training on human demonstrations. In data-limited settings
derived from Mind2Web and WebLINX, we improve Step Accuracy by up to +89.5% and
+122.1% respectively for agents trained on mixtures of data from our pipeline,
and human data. When training agents with all available human data from these
benchmarks, agents fail to generalize to diverse real sites, and adding our
data improves their generalization by +149.0% for WebLINX and +156.3% for
Mind2Web. Code will be available at: data-for-agents.github.io.
]]></content:encoded>
<pubDate>2025-02-10T18:54:05Z</pubDate>
</item>
<item>
<title>FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution
  Video Generation</title>
<link>http://arxiv.org/abs/2502.05179v1</link>
<guid>http://arxiv.org/abs/2502.05179v1</guid>
<content:encoded><![CDATA[
DiT diffusion models have achieved great success in text-to-video generation,
leveraging their scalability in model capacity and data scale. High content and
motion fidelity aligned with text prompts, however, often require large model
parameters and a substantial number of function evaluations (NFEs). Realistic
and visually appealing details are typically reflected in high resolution
outputs, further amplifying computational demands especially for single stage
DiT models. To address these challenges, we propose a novel two stage
framework, FlashVideo, which strategically allocates model capacity and NFEs
across stages to balance generation fidelity and quality. In the first stage,
prompt fidelity is prioritized through a low resolution generation process
utilizing large parameters and sufficient NFEs to enhance computational
efficiency. The second stage establishes flow matching between low and high
resolutions, effectively generating fine details with minimal NFEs.
Quantitative and visual results demonstrate that FlashVideo achieves
state-of-the-art high resolution video generation with superior computational
efficiency. Additionally, the two-stage design enables users to preview the
initial output before committing to full resolution generation, thereby
significantly reducing computational costs and wait times as well as enhancing
commercial viability .
]]></content:encoded>
<pubDate>2025-02-07T18:59:59Z</pubDate>
</item>
<item>
<title>QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive
  Multimodal Understanding and Generation</title>
<link>http://arxiv.org/abs/2502.05178v1</link>
<guid>http://arxiv.org/abs/2502.05178v1</guid>
<content:encoded><![CDATA[
We introduce Quantized Language-Image Pretraining (QLIP), a visual
tokenization method that combines state-of-the-art reconstruction quality with
state-of-the-art zero-shot image understanding. QLIP trains a
binary-spherical-quantization-based autoencoder with reconstruction and
language-image alignment objectives. We are the first to show that the two
objectives do not need to be at odds. We balance the two loss terms dynamically
during training and show that a two-stage training pipeline effectively mixes
the large-batch requirements of image-language pre-training with the memory
bottleneck imposed by the reconstruction objective. We validate the
effectiveness of QLIP for multimodal understanding and text-conditioned image
generation with a single model. Specifically, QLIP serves as a drop-in
replacement for the visual encoder for LLaVA and the image tokenizer for
LlamaGen with comparable or even better performance. Finally, we demonstrate
that QLIP enables a unified mixed-modality auto-regressive model for
understanding and generation.
]]></content:encoded>
<pubDate>2025-02-07T18:59:57Z</pubDate>
</item>
<item>
<title>Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with
  Leading Short-Context Accuray</title>
<link>http://arxiv.org/abs/2502.05177v1</link>
<guid>http://arxiv.org/abs/2502.05177v1</guid>
<content:encoded><![CDATA[
Establishing the long-context capability of large vision-language models is
crucial for video understanding, high-resolution image understanding,
multi-modal agents and reasoning. We introduce Long-VITA, a simple yet
effective large multi-modal model for long-context visual-language
understanding tasks. It is adept at concurrently processing and analyzing
modalities of image, video, and text over 4K frames or 1M tokens while
delivering advanced performances on short-context multi-modal tasks. We propose
an effective multi-modal training schema that starts with large language models
and proceeds through vision-language alignment, general knowledge learning, and
two sequential stages of long-sequence fine-tuning. We further implement
context-parallelism distributed inference and logits-masked language modeling
head to scale Long-VITA to infinitely long inputs of images and texts during
model inference. Regarding training data, Long-VITA is built on a mix of $17$M
samples from public datasets only and demonstrates the state-of-the-art
performance on various multi-modal benchmarks, compared against recent
cutting-edge models with internal data. Long-VITA is fully reproducible and
supports both NPU and GPU platforms for training and testing. We hope Long-VITA
can serve as a competitive baseline and offer valuable insights for the
open-source community in advancing long-context multi-modal understanding.
]]></content:encoded>
<pubDate>2025-02-07T18:59:56Z</pubDate>
</item>
<item>
<title>MELON: Indirect Prompt Injection Defense via Masked Re-execution and
  Tool Comparison</title>
<link>http://arxiv.org/abs/2502.05174v1</link>
<guid>http://arxiv.org/abs/2502.05174v1</guid>
<content:encoded><![CDATA[
Recent research has explored that LLM agents are vulnerable to indirect
prompt injection (IPI) attacks, where malicious tasks embedded in
tool-retrieved information can redirect the agent to take unauthorized actions.
Existing defenses against IPI have significant limitations: either require
essential model training resources, lack effectiveness against sophisticated
attacks, or harm the normal utilities. We present MELON (Masked re-Execution
and TooL comparisON), a novel IPI defense. Our approach builds on the
observation that under a successful attack, the agent's next action becomes
less dependent on user tasks and more on malicious tasks. Following this, we
design MELON to detect attacks by re-executing the agent's trajectory with a
masked user prompt modified through a masking function. We identify an attack
if the actions generated in the original and masked executions are similar. We
also include three key designs to reduce the potential false positives and
false negatives. Extensive evaluation on the IPI benchmark AgentDojo
demonstrates that MELON outperforms SOTA defenses in both attack prevention and
utility preservation. Moreover, we show that combining MELON with a SOTA prompt
augmentation defense (denoted as MELON-Aug) further improves its performance.
We also conduct a detailed ablation study to validate our key designs.
]]></content:encoded>
<pubDate>2025-02-07T18:57:49Z</pubDate>
</item>
<item>
<title>A Schema-Guided Reason-while-Retrieve framework for Reasoning on Scene
  Graphs with Large-Language-Models (LLMs)</title>
<link>http://arxiv.org/abs/2502.03450v1</link>
<guid>http://arxiv.org/abs/2502.03450v1</guid>
<content:encoded><![CDATA[
Scene graphs have emerged as a structured and serializable environment
representation for grounded spatial reasoning with Large Language Models
(LLMs). In this work, we propose SG-RwR, a Schema-Guided Retrieve-while-Reason
framework for reasoning and planning with scene graphs. Our approach employs
two cooperative, code-writing LLM agents: a (1) Reasoner for task planning and
information queries generation, and a (2) Retriever for extracting
corresponding graph information following the queries. Two agents collaborate
iteratively, enabling sequential reasoning and adaptive attention to graph
information. Unlike prior works, both agents are prompted only with the scene
graph schema rather than the full graph data, which reduces the hallucination
by limiting input tokens, and drives the Reasoner to generate reasoning trace
abstractly.Following the trace, the Retriever programmatically query the scene
graph data based on the schema understanding, allowing dynamic and global
attention on the graph that enhances alignment between reasoning and retrieval.
Through experiments in multiple simulation environments, we show that our
framework surpasses existing LLM-based approaches in numerical Q\&amp;A and
planning tasks, and can benefit from task-level few-shot examples, even in the
absence of agent-level demonstrations. Project code will be released.
]]></content:encoded>
<pubDate>2025-02-05T18:50:38Z</pubDate>
</item>
<item>
<title>Designing LLM-simulated Immersive Spaces to Enhance Autistic Children's
  Social Affordances Understanding</title>
<link>http://arxiv.org/abs/2502.03447v1</link>
<guid>http://arxiv.org/abs/2502.03447v1</guid>
<content:encoded><![CDATA[
One of the key challenges faced by autistic children is understanding social
affordances in complex environments, which further impacts their ability to
respond appropriately to social signals.
  In traffic scenarios, this impairment can even lead to safety concerns. In
this paper, we introduce an LLM-simulated immersive projection environment
designed to improve this ability in autistic children while ensuring their
safety. We first propose 17 design considerations across four major categories,
derived from a comprehensive review of previous research. Next, we developed a
system called AIroad, which leverages LLMs to simulate drivers with varying
social intents, expressed through explicit multimodal social signals. AIroad
helps autistic children bridge the gap in recognizing the intentions behind
behaviors and learning appropriate responses through various stimuli. A user
study involving 14 participants demonstrated that this technology effectively
engages autistic children and leads to significant improvements in their
comprehension of social affordances in traffic scenarios. Additionally, parents
reported high perceived usability of the system. These findings highlight the
potential of combining LLM technology with immersive environments for the
functional rehabilitation of autistic children in the future.
]]></content:encoded>
<pubDate>2025-02-05T18:45:38Z</pubDate>
</item>
<item>
<title>QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search</title>
<link>http://arxiv.org/abs/2502.02584v1</link>
<guid>http://arxiv.org/abs/2502.02584v1</guid>
<content:encoded><![CDATA[
Language agents have become a promising solution to complex interactive
tasks. One of the key ingredients to the success of language agents is the
reward model on the trajectory of the agentic workflow, which provides valuable
guidance during training or inference. However, due to the lack of annotations
of intermediate interactions, most existing works use an outcome reward model
to optimize policies across entire trajectories. This may lead to sub-optimal
policies and hinder the overall performance. To address this, we propose QLASS
(Q-guided Language Agent Stepwise Search), to automatically generate
annotations by estimating Q-values in a stepwise manner for open language
agents. By introducing a reasoning tree and performing process reward modeling,
QLASS provides effective intermediate guidance for each step. With the stepwise
guidance, we propose a Q-guided generation strategy to enable language agents
to better adapt to long-term value, resulting in significant performance
improvement during model inference on complex interactive agent tasks. Notably,
even with almost half the annotated data, QLASS retains strong performance,
demonstrating its efficiency in handling limited supervision. We also
empirically demonstrate that QLASS can lead to more effective decision making
through qualitative analysis. We will release our code and data.
]]></content:encoded>
<pubDate>2025-02-04T18:58:31Z</pubDate>
</item>
<item>
<title>Vintix: Action Model via In-Context Reinforcement Learning</title>
<link>http://arxiv.org/abs/2501.19400v1</link>
<guid>http://arxiv.org/abs/2501.19400v1</guid>
<content:encoded><![CDATA[
In-Context Reinforcement Learning (ICRL) represents a promising paradigm for
developing generalist agents that learn at inference time through
trial-and-error interactions, analogous to how large language models adapt
contextually, but with a focus on reward maximization. However, the scalability
of ICRL beyond toy tasks and single-domain settings remains an open challenge.
In this work, we present the first steps toward scaling ICRL by introducing a
fixed, cross-domain model capable of learning behaviors through in-context
reinforcement learning. Our results demonstrate that Algorithm Distillation, a
framework designed to facilitate ICRL, offers a compelling and competitive
alternative to expert distillation to construct versatile action models. These
findings highlight the potential of ICRL as a scalable approach for generalist
decision-making systems. Code to be released at
https://github.com/dunnolab/vintix
]]></content:encoded>
<pubDate>2025-01-31T18:57:08Z</pubDate>
</item>
<item>
<title>Do LLMs Strategically Reveal, Conceal, and Infer Information? A
  Theoretical and Empirical Analysis in The Chameleon Game</title>
<link>http://arxiv.org/abs/2501.19398v1</link>
<guid>http://arxiv.org/abs/2501.19398v1</guid>
<content:encoded><![CDATA[
Large language model-based (LLM-based) agents have become common in settings
that include non-cooperative parties. In such settings, agents' decision-making
needs to conceal information from their adversaries, reveal information to
their cooperators, and infer information to identify the other agents'
characteristics. To investigate whether LLMs have these information control and
decision-making capabilities, we make LLM agents play the language-based
hidden-identity game, The Chameleon. In the game, a group of non-chameleon
agents who do not know each other aim to identify the chameleon agent without
revealing a secret. The game requires the aforementioned information control
capabilities both as a chameleon and a non-chameleon. The empirical results
show that while non-chameleon LLM agents identify the chameleon, they fail to
conceal the secret from the chameleon, and their winning probability is far
from the levels of even trivial strategies. To formally explain this behavior,
we give a theoretical analysis for a spectrum of strategies, from concealing to
revealing, and provide bounds on the non-chameleons' winning probability. Based
on the empirical results and theoretical analysis of different strategies, we
deduce that LLM-based non-chameleon agents reveal excessive information to
agents of unknown identities. Our results point to a weakness of contemporary
LLMs, including GPT-4, GPT-4o, Gemini 1.5, and Claude 3.5 Sonnet, in strategic
interactions.
]]></content:encoded>
<pubDate>2025-01-31T18:53:43Z</pubDate>
</item>
<item>
<title>Diffusion Autoencoders are Scalable Image Tokenizers</title>
<link>http://arxiv.org/abs/2501.18593v1</link>
<guid>http://arxiv.org/abs/2501.18593v1</guid>
<content:encoded><![CDATA[
Tokenizing images into compact visual representations is a key step in
learning efficient and high-quality image generative models. We present a
simple diffusion tokenizer (DiTo) that learns compact visual representations
for image generation models. Our key insight is that a single learning
objective, diffusion L2 loss, can be used for training scalable image
tokenizers. Since diffusion is already widely used for image generation, our
insight greatly simplifies training such tokenizers. In contrast, current
state-of-the-art tokenizers rely on an empirically found combination of
heuristics and losses, thus requiring a complex training recipe that relies on
non-trivially balancing different losses and pretrained supervised models. We
show design decisions, along with theoretical grounding, that enable us to
scale DiTo for learning competitive image representations. Our results show
that DiTo is a simpler, scalable, and self-supervised alternative to the
current state-of-the-art image tokenizer which is supervised. DiTo achieves
competitive or better quality than state-of-the-art in image reconstruction and
downstream image generation tasks.
]]></content:encoded>
<pubDate>2025-01-30T18:59:37Z</pubDate>
</item>
<item>
<title>Advances in Multimodal Adaptation and Generalization: From Traditional
  Approaches to Foundation Models</title>
<link>http://arxiv.org/abs/2501.18592v1</link>
<guid>http://arxiv.org/abs/2501.18592v1</guid>
<content:encoded><![CDATA[
In real-world scenarios, achieving domain adaptation and generalization poses
significant challenges, as models must adapt to or generalize across unknown
target distributions. Extending these capabilities to unseen multimodal
distributions, i.e., multimodal domain adaptation and generalization, is even
more challenging due to the distinct characteristics of different modalities.
Significant progress has been made over the years, with applications ranging
from action recognition to semantic segmentation. Besides, the recent advent of
large-scale pre-trained multimodal foundation models, such as CLIP, has
inspired works leveraging these models to enhance adaptation and generalization
performances or adapting them to downstream tasks. This survey provides the
first comprehensive review of recent advances from traditional approaches to
foundation models, covering: (1) Multimodal domain adaptation; (2) Multimodal
test-time adaptation; (3) Multimodal domain generalization; (4) Domain
adaptation and generalization with the help of multimodal foundation models;
and (5) Adaptation of multimodal foundation models. For each topic, we formally
define the problem and thoroughly review existing methods. Additionally, we
analyze relevant datasets and applications, highlighting open challenges and
potential future research directions. We maintain an active repository that
contains up-to-date literature at
https://github.com/donghao51/Awesome-Multimodal-Adaptation.
]]></content:encoded>
<pubDate>2025-01-30T18:59:36Z</pubDate>
</item>
<item>
<title>From Sparse to Dense: Toddler-inspired Reward Transition in
  Goal-Oriented Reinforcement Learning</title>
<link>http://arxiv.org/abs/2501.17842v1</link>
<guid>http://arxiv.org/abs/2501.17842v1</guid>
<content:encoded><![CDATA[
Reinforcement learning (RL) agents often face challenges in balancing
exploration and exploitation, particularly in environments where sparse or
dense rewards bias learning. Biological systems, such as human toddlers,
naturally navigate this balance by transitioning from free exploration with
sparse rewards to goal-directed behavior guided by increasingly dense rewards.
Inspired by this natural progression, we investigate the Toddler-Inspired
Reward Transition in goal-oriented RL tasks. Our study focuses on transitioning
from sparse to potential-based dense (S2D) rewards while preserving optimal
strategies. Through experiments on dynamic robotic arm manipulation and
egocentric 3D navigation tasks, we demonstrate that effective S2D reward
transitions significantly enhance learning performance and sample efficiency.
Additionally, using a Cross-Density Visualizer, we show that S2D transitions
smooth the policy loss landscape, resulting in wider minima that improve
generalization in RL models. In addition, we reinterpret Tolman's maze
experiments, underscoring the critical role of early free exploratory learning
in the context of S2D rewards.
]]></content:encoded>
<pubDate>2025-01-29T18:46:35Z</pubDate>
</item>
<item>
<title>CubeDiff: Repurposing Diffusion-Based Image Models for Panorama
  Generation</title>
<link>http://arxiv.org/abs/2501.17162v1</link>
<guid>http://arxiv.org/abs/2501.17162v1</guid>
<content:encoded><![CDATA[
We introduce a novel method for generating 360{\deg} panoramas from text
prompts or images. Our approach leverages recent advances in 3D generation by
employing multi-view diffusion models to jointly synthesize the six faces of a
cubemap. Unlike previous methods that rely on processing equirectangular
projections or autoregressive generation, our method treats each face as a
standard perspective image, simplifying the generation process and enabling the
use of existing multi-view diffusion models. We demonstrate that these models
can be adapted to produce high-quality cubemaps without requiring
correspondence-aware attention layers. Our model allows for fine-grained text
control, generates high resolution panorama images and generalizes well beyond
its training set, whilst achieving state-of-the-art results, both qualitatively
and quantitatively. Project page: https://cubediff.github.io/
]]></content:encoded>
<pubDate>2025-01-28T18:59:49Z</pubDate>
</item>
<item>
<title>Numerical Approximation of High-Dimensional Gibbs Distributions Using
  the Functional Hierarchical Tensor</title>
<link>http://arxiv.org/abs/2501.17143v1</link>
<guid>http://arxiv.org/abs/2501.17143v1</guid>
<content:encoded><![CDATA[
The numerical representation of high-dimensional Gibbs distributions is
challenging due to the curse of dimensionality manifesting through the
intractable normalization constant calculations. This work addresses this
challenge by performing a particle-based high-dimensional parametric density
estimation subroutine, and the input to the subroutine is Gibbs samples
generated by leveraging advanced sampling techniques. Specifically, to generate
Gibbs samples, we employ ensemble-based annealed importance sampling, a
population-based approach for sampling multimodal distributions. These samples
are then processed using functional hierarchical tensor sketching, a
tensor-network-based density estimation method for high-dimensional
distributions, to obtain the numerical representation of the Gibbs
distribution. We successfully apply the proposed approach to complex
Ginzburg-Landau models with hundreds of variables. In particular, we show that
the approach proposed is successful at addressing the metastability issue under
difficult numerical cases.
]]></content:encoded>
<pubDate>2025-01-28T18:44:20Z</pubDate>
</item>
<item>
<title>RelightVid: Temporal-Consistent Diffusion Model for Video Relighting</title>
<link>http://arxiv.org/abs/2501.16330v1</link>
<guid>http://arxiv.org/abs/2501.16330v1</guid>
<content:encoded><![CDATA[
Diffusion models have demonstrated remarkable success in image generation and
editing, with recent advancements enabling albedo-preserving image relighting.
However, applying these models to video relighting remains challenging due to
the lack of paired video relighting datasets and the high demands for output
fidelity and temporal consistency, further complicated by the inherent
randomness of diffusion models. To address these challenges, we introduce
RelightVid, a flexible framework for video relighting that can accept
background video, text prompts, or environment maps as relighting conditions.
Trained on in-the-wild videos with carefully designed illumination
augmentations and rendered videos under extreme dynamic lighting, RelightVid
achieves arbitrary video relighting with high temporal consistency without
intrinsic decomposition while preserving the illumination priors of its image
backbone.
]]></content:encoded>
<pubDate>2025-01-27T18:59:57Z</pubDate>
</item>
<item>
<title>LUCY: Linguistic Understanding and Control Yielding Early Stage of Her</title>
<link>http://arxiv.org/abs/2501.16327v1</link>
<guid>http://arxiv.org/abs/2501.16327v1</guid>
<content:encoded><![CDATA[
The film Her features Samantha, a sophisticated AI audio agent who is capable
of understanding both linguistic and paralinguistic information in human speech
and delivering real-time responses that are natural, informative and sensitive
to emotional subtleties. Moving one step toward more sophisticated audio agent
from recent advancement in end-to-end (E2E) speech systems, we propose LUCY, a
E2E speech model that (1) senses and responds to user's emotion, (2) deliver
responses in a succinct and natural style, and (3) use external tool to answer
real-time inquiries. Experiment results show that LUCY is better at emotion
control than peer models, generating emotional responses based on linguistic
emotional instructions and responding to paralinguistic emotional cues. Lucy is
also able to generate responses in a more natural style, as judged by external
language models, without sacrificing much performance on general question
answering. Finally, LUCY can leverage function calls to answer questions that
are out of its knowledge scope.
]]></content:encoded>
<pubDate>2025-01-27T18:59:51Z</pubDate>
</item>
<item>
<title>Mitigating GenAI-powered Evidence Pollution for Out-of-Context
  Multimodal Misinformation Detection</title>
<link>http://arxiv.org/abs/2501.14728v1</link>
<guid>http://arxiv.org/abs/2501.14728v1</guid>
<content:encoded><![CDATA[
While large generative artificial intelligence (GenAI) models have achieved
significant success, they also raise growing concerns about online information
security due to their potential misuse for generating deceptive content.
Out-of-context (OOC) multimodal misinformation detection, which often retrieves
Web evidence to identify the repurposing of images in false contexts, faces the
issue of reasoning over GenAI-polluted evidence to derive accurate predictions.
Existing works simulate GenAI-powered pollution at the claim level with
stylistic rewriting to conceal linguistic cues, and ignore evidence-level
pollution for such information-seeking applications. In this work, we
investigate how polluted evidence affects the performance of existing OOC
detectors, revealing a performance degradation of more than 9 percentage
points. We propose two strategies, cross-modal evidence reranking and
cross-modal claim-evidence reasoning, to address the challenges posed by
polluted evidence. Extensive experiments on two benchmark datasets show that
these strategies can effectively enhance the robustness of existing
out-of-context detectors amidst polluted evidence.
]]></content:encoded>
<pubDate>2025-01-24T18:59:31Z</pubDate>
</item>
<item>
<title>Can We Generate Images with CoT? Let's Verify and Reinforce Image
  Generation Step by Step</title>
<link>http://arxiv.org/abs/2501.13926v1</link>
<guid>http://arxiv.org/abs/2501.13926v1</guid>
<content:encoded><![CDATA[
Chain-of-Thought (CoT) reasoning has been extensively explored in large
models to tackle complex understanding tasks. However, it still remains an open
question whether such strategies can be applied to verifying and reinforcing
image generation scenarios. In this paper, we provide the first comprehensive
investigation of the potential of CoT reasoning to enhance autoregressive image
generation. We focus on three techniques: scaling test-time computation for
verification, aligning model preferences with Direct Preference Optimization
(DPO), and integrating these techniques for complementary effects. Our results
demonstrate that these approaches can be effectively adapted and combined to
significantly improve image generation performance. Furthermore, given the
pivotal role of reward models in our findings, we propose the Potential
Assessment Reward Model (PARM) and PARM++, specialized for autoregressive image
generation. PARM adaptively assesses each generation step through a potential
assessment approach, merging the strengths of existing reward models, and
PARM++ further introduces a reflection mechanism to self-correct the generated
unsatisfactory image. Using our investigated reasoning strategies, we enhance a
baseline model, Show-o, to achieve superior results, with a significant +24%
improvement on the GenEval benchmark, surpassing Stable Diffusion 3 by +15%. We
hope our study provides unique insights and paves a new path for integrating
CoT reasoning with autoregressive image generation. Code and models are
released at https://github.com/ZiyuGuo99/Image-Generation-CoT
]]></content:encoded>
<pubDate>2025-01-23T18:59:43Z</pubDate>
</item>
<item>
<title>Towards Robust Multimodal Open-set Test-time Adaptation via Adaptive
  Entropy-aware Optimization</title>
<link>http://arxiv.org/abs/2501.13924v1</link>
<guid>http://arxiv.org/abs/2501.13924v1</guid>
<content:encoded><![CDATA[
Test-time adaptation (TTA) has demonstrated significant potential in
addressing distribution shifts between training and testing data. Open-set
test-time adaptation (OSTTA) aims to adapt a source pre-trained model online to
an unlabeled target domain that contains unknown classes. This task becomes
more challenging when multiple modalities are involved. Existing methods have
primarily focused on unimodal OSTTA, often filtering out low-confidence samples
without addressing the complexities of multimodal data. In this work, we
present Adaptive Entropy-aware Optimization (AEO), a novel framework
specifically designed to tackle Multimodal Open-set Test-time Adaptation
(MM-OSTTA) for the first time. Our analysis shows that the entropy difference
between known and unknown samples in the target domain strongly correlates with
MM-OSTTA performance. To leverage this, we propose two key components:
Unknown-aware Adaptive Entropy Optimization (UAE) and Adaptive Modality
Prediction Discrepancy Optimization (AMP). These components enhance the ability
of model to distinguish unknown class samples during online adaptation by
amplifying the entropy difference between known and unknown samples. To
thoroughly evaluate our proposed methods in the MM-OSTTA setting, we establish
a new benchmark derived from existing datasets. This benchmark includes two
downstream tasks and incorporates five modalities. Extensive experiments across
various domain shift situations demonstrate the efficacy and versatility of the
AEO framework. Additionally, we highlight the strong performance of AEO in
long-term and continual MM-OSTTA settings, both of which are challenging and
highly relevant to real-world applications. Our source code is available at
https://github.com/donghao51/AEO.
]]></content:encoded>
<pubDate>2025-01-23T18:59:30Z</pubDate>
</item>
<item>
<title>GeoPixel: Pixel Grounding Large Multimodal Model in Remote Sensing</title>
<link>http://arxiv.org/abs/2501.13925v1</link>
<guid>http://arxiv.org/abs/2501.13925v1</guid>
<content:encoded><![CDATA[
Recent advances in large multimodal models (LMMs) have recognized
fine-grained grounding as an imperative factor of visual understanding and
dialogue. However, the benefits of such representation in LMMs are limited to
the natural image domain, and these models perform poorly for remote sensing
(RS). The distinct overhead viewpoint, scale variation, and presence of small
objects in high-resolution RS imagery present a unique challenge in
region-level comprehension. Moreover, the development of the grounding
conversation capability of LMMs within RS is hindered by the lack of granular,
RS domain-specific grounded data. Addressing these limitations, we propose
GeoPixel - the first end-to-end high resolution RS-LMM that supports
pixel-level grounding. This capability allows fine-grained visual perception by
generating interleaved masks in conversation. GeoPixel supports up to 4K HD
resolution in any aspect ratio, ideal for high-precision RS image analysis. To
support the grounded conversation generation (GCG) in RS imagery, we curate a
visually grounded dataset GeoPixelD through a semi-automated pipeline that
utilizes set-of-marks prompting and spatial priors tailored for RS data to
methodically control the data generation process. GeoPixel demonstrates
superior performance in pixel-level comprehension, surpassing existing LMMs in
both single-target and multi-target segmentation tasks. Our methodological
ablation studies validate the effectiveness of each component in the overall
architecture. Our code and data will be publicly released.
]]></content:encoded>
<pubDate>2025-01-23T18:59:30Z</pubDate>
</item>
<item>
<title>IMAGINE-E: Image Generation Intelligence Evaluation of State-of-the-art
  Text-to-Image Models</title>
<link>http://arxiv.org/abs/2501.13920v1</link>
<guid>http://arxiv.org/abs/2501.13920v1</guid>
<content:encoded><![CDATA[
With the rapid development of diffusion models, text-to-image(T2I) models
have made significant progress, showcasing impressive abilities in prompt
following and image generation. Recently launched models such as FLUX.1 and
Ideogram2.0, along with others like Dall-E3 and Stable Diffusion 3, have
demonstrated exceptional performance across various complex tasks, raising
questions about whether T2I models are moving towards general-purpose
applicability. Beyond traditional image generation, these models exhibit
capabilities across a range of fields, including controllable generation, image
editing, video, audio, 3D, and motion generation, as well as computer vision
tasks like semantic segmentation and depth estimation. However, current
evaluation frameworks are insufficient to comprehensively assess these models'
performance across expanding domains. To thoroughly evaluate these models, we
developed the IMAGINE-E and tested six prominent models: FLUX.1, Ideogram2.0,
Midjourney, Dall-E3, Stable Diffusion 3, and Jimeng. Our evaluation is divided
into five key domains: structured output generation, realism, and physical
consistency, specific domain generation, challenging scenario generation, and
multi-style creation tasks. This comprehensive assessment highlights each
model's strengths and limitations, particularly the outstanding performance of
FLUX.1 and Ideogram2.0 in structured and specific domain tasks, underscoring
the expanding applications and potential of T2I models as foundational AI
tools. This study provides valuable insights into the current state and future
trajectory of T2I models as they evolve towards general-purpose usability.
Evaluation scripts will be released at https://github.com/jylei16/Imagine-e.
]]></content:encoded>
<pubDate>2025-01-23T18:58:33Z</pubDate>
</item>
<item>
<title>Temporal Preference Optimization for Long-Form Video Understanding</title>
<link>http://arxiv.org/abs/2501.13919v1</link>
<guid>http://arxiv.org/abs/2501.13919v1</guid>
<content:encoded><![CDATA[
Despite significant advancements in video large multimodal models
(video-LMMs), achieving effective temporal grounding in long-form videos
remains a challenge for existing models. To address this limitation, we propose
Temporal Preference Optimization (TPO), a novel post-training framework
designed to enhance the temporal grounding capabilities of video-LMMs through
preference learning. TPO adopts a self-training approach that enables models to
differentiate between well-grounded and less accurate temporal responses by
leveraging curated preference datasets at two granularities: localized temporal
grounding, which focuses on specific video segments, and comprehensive temporal
grounding, which captures extended temporal dependencies across entire video
sequences. By optimizing on these preference datasets, TPO significantly
enhances temporal understanding while reducing reliance on manually annotated
data. Extensive experiments on three long-form video understanding
benchmarks--LongVideoBench, MLVU, and Video-MME--demonstrate the effectiveness
of TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPO
establishes itself as the leading 7B model on the Video-MME benchmark,
underscoring the potential of TPO as a scalable and efficient solution for
advancing temporal reasoning in long-form video understanding. Project page:
https://ruili33.github.io/tpo_website.
]]></content:encoded>
<pubDate>2025-01-23T18:58:03Z</pubDate>
</item>
<item>
<title>Improving Video Generation with Human Feedback</title>
<link>http://arxiv.org/abs/2501.13918v1</link>
<guid>http://arxiv.org/abs/2501.13918v1</guid>
<content:encoded><![CDATA[
Video generation has achieved significant advances through rectified flow
techniques, but issues like unsmooth motion and misalignment between videos and
prompts persist. In this work, we develop a systematic pipeline that harnesses
human feedback to mitigate these problems and refine the video generation
model. Specifically, we begin by constructing a large-scale human preference
dataset focused on modern video generation models, incorporating pairwise
annotations across multi-dimensions. We then introduce VideoReward, a
multi-dimensional video reward model, and examine how annotations and various
design choices impact its rewarding efficacy. From a unified reinforcement
learning perspective aimed at maximizing reward with KL regularization, we
introduce three alignment algorithms for flow-based models by extending those
from diffusion models. These include two training-time strategies: direct
preference optimization for flow (Flow-DPO) and reward weighted regression for
flow (Flow-RWR), and an inference-time technique, Flow-NRG, which applies
reward guidance directly to noisy videos. Experimental results indicate that
VideoReward significantly outperforms existing reward models, and Flow-DPO
demonstrates superior performance compared to both Flow-RWR and standard
supervised fine-tuning methods. Additionally, Flow-NRG lets users assign custom
weights to multiple objectives during inference, meeting personalized video
quality needs. Project page: https://gongyeliu.github.io/videoalign.
]]></content:encoded>
<pubDate>2025-01-23T18:55:41Z</pubDate>
</item>
<item>
<title>Accelerate High-Quality Diffusion Models with Inner Loop Feedback</title>
<link>http://arxiv.org/abs/2501.13107v1</link>
<guid>http://arxiv.org/abs/2501.13107v1</guid>
<content:encoded><![CDATA[
We propose Inner Loop Feedback (ILF), a novel approach to accelerate
diffusion models' inference. ILF trains a lightweight module to predict future
features in the denoising process by leveraging the outputs from a chosen
diffusion backbone block at a given time step. This approach exploits two key
intuitions; (1) the outputs of a given block at adjacent time steps are
similar, and (2) performing partial computations for a step imposes a lower
burden on the model than skipping the step entirely. Our method is highly
flexible, since we find that the feedback module itself can simply be a block
from the diffusion backbone, with all settings copied. Its influence on the
diffusion forward can be tempered with a learnable scaling factor from zero
initialization. We train this module using distillation losses; however, unlike
some prior work where a full diffusion backbone serves as the student, our
model freezes the backbone, training only the feedback module. While many
efforts to optimize diffusion models focus on achieving acceptable image
quality in extremely few steps (1-4 steps), our emphasis is on matching best
case results (typically achieved in 20 steps) while significantly reducing
runtime. ILF achieves this balance effectively, demonstrating strong
performance for both class-to-image generation with diffusion transformer (DiT)
and text-to-image generation with DiT-based PixArt-alpha and PixArt-sigma. The
quality of ILF's 1.7x-1.8x speedups are confirmed by FID, CLIP score, CLIP
Image Quality Assessment, ImageReward, and qualitative comparisons.
]]></content:encoded>
<pubDate>2025-01-22T18:59:58Z</pubDate>
</item>
<item>
<title>VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video
  Understanding</title>
<link>http://arxiv.org/abs/2501.13106v1</link>
<guid>http://arxiv.org/abs/2501.13106v1</guid>
<content:encoded><![CDATA[
In this paper, we propose VideoLLaMA3, a more advanced multimodal foundation
model for image and video understanding. The core design philosophy of
VideoLLaMA3 is vision-centric. The meaning of "vision-centric" is two-fold: the
vision-centric training paradigm and vision-centric framework design. The key
insight of our vision-centric training paradigm is that high-quality image-text
data is crucial for both image and video understanding. Instead of preparing
massive video-text datasets, we focus on constructing large-scale and
high-quality image-text datasets. VideoLLaMA3 has four training stages: 1)
vision-centric alignment stage, which warms up the vision encoder and
projector; 2) vision-language pretraining stage, which jointly tunes the vision
encoder, projector, and LLM with large-scale image-text data covering multiple
types (including scene images, documents, charts) as well as text-only data. 3)
multi-task fine-tuning stage, which incorporates image-text SFT data for
downstream tasks and video-text data to establish a foundation for video
understanding. 4) video-centric fine-tuning, which further improves the model's
capability in video understanding. As for the framework design, to better
capture fine-grained details in images, the pretrained vision encoder is
adapted to encode images of varying sizes into vision tokens with corresponding
numbers, rather than a fixed number of tokens. For video inputs, we reduce the
number of vision tokens according to their similarity so that the
representation of videos will be more precise and compact. Benefit from
vision-centric designs, VideoLLaMA3 achieves compelling performances in both
image and video understanding benchmarks.
]]></content:encoded>
<pubDate>2025-01-22T18:59:46Z</pubDate>
</item>
<item>
<title>Orchid: Image Latent Diffusion for Joint Appearance and Geometry
  Generation</title>
<link>http://arxiv.org/abs/2501.13087v1</link>
<guid>http://arxiv.org/abs/2501.13087v1</guid>
<content:encoded><![CDATA[
Diffusion models are state-of-the-art for image generation. Trained on large
datasets, they capture expressive image priors that have been used for tasks
like inpainting, depth, and (surface) normal prediction. However, these models
are typically trained for one specific task, e.g., a separate model for each of
color, depth, and normal prediction. Such models do not leverage the intrinsic
correlation between appearance and geometry, often leading to inconsistent
predictions.
  In this paper, we propose using a novel image diffusion prior that jointly
encodes appearance and geometry. We introduce a diffusion model Orchid,
comprising a Variational Autoencoder (VAE) to encode color, depth, and surface
normals to a latent space, and a Latent Diffusion Model (LDM) for generating
these joint latents. Orchid directly generates photo-realistic color images,
relative depth, and surface normals from user-provided text, and can be used to
create image-aligned partial 3D scenes seamlessly. It can also perform
image-conditioned tasks like joint monocular depth and normal prediction and is
competitive in accuracy to state-of-the-art methods designed for those tasks
alone. Lastly, our model learns a joint prior that can be used zero-shot as a
regularizer for many inverse problems that entangle appearance and geometry.
For example, we demonstrate its effectiveness in color-depth-normal inpainting,
showcasing its applicability to problems in 3D generation from sparse views.
]]></content:encoded>
<pubDate>2025-01-22T18:46:47Z</pubDate>
</item>
<item>
<title>GPS as a Control Signal for Image Generation</title>
<link>http://arxiv.org/abs/2501.12390v1</link>
<guid>http://arxiv.org/abs/2501.12390v1</guid>
<content:encoded><![CDATA[
We show that the GPS tags contained in photo metadata provide a useful
control signal for image generation. We train GPS-to-image models and use them
for tasks that require a fine-grained understanding of how images vary within a
city. In particular, we train a diffusion model to generate images conditioned
on both GPS and text. The learned model generates images that capture the
distinctive appearance of different neighborhoods, parks, and landmarks. We
also extract 3D models from 2D GPS-to-image models through score distillation
sampling, using GPS conditioning to constrain the appearance of the
reconstruction from each viewpoint. Our evaluations suggest that our
GPS-conditioned models successfully learn to generate images that vary based on
location, and that GPS conditioning improves estimated 3D structure.
]]></content:encoded>
<pubDate>2025-01-21T18:59:46Z</pubDate>
</item>
<item>
<title>Taming Teacher Forcing for Masked Autoregressive Video Generation</title>
<link>http://arxiv.org/abs/2501.12389v1</link>
<guid>http://arxiv.org/abs/2501.12389v1</guid>
<content:encoded><![CDATA[
We introduce MAGI, a hybrid video generation framework that combines masked
modeling for intra-frame generation with causal modeling for next-frame
generation. Our key innovation, Complete Teacher Forcing (CTF), conditions
masked frames on complete observation frames rather than masked ones (namely
Masked Teacher Forcing, MTF), enabling a smooth transition from token-level
(patch-level) to frame-level autoregressive generation. CTF significantly
outperforms MTF, achieving a +23% improvement in FVD scores on first-frame
conditioned video prediction. To address issues like exposure bias, we employ
targeted training strategies, setting a new benchmark in autoregressive video
generation. Experiments show that MAGI can generate long, coherent video
sequences exceeding 100 frames, even when trained on as few as 16 frames,
highlighting its potential for scalable, high-quality video generation.
]]></content:encoded>
<pubDate>2025-01-21T18:59:31Z</pubDate>
</item>
<item>
<title>InternVideo2.5: Empowering Video MLLMs with Long and Rich Context
  Modeling</title>
<link>http://arxiv.org/abs/2501.12386v1</link>
<guid>http://arxiv.org/abs/2501.12386v1</guid>
<content:encoded><![CDATA[
This paper aims to improve the performance of video multimodal large language
models (MLLM) via long and rich context (LRC) modeling. As a result, we develop
a new version of InternVideo2.5 with a focus on enhancing the original MLLMs'
ability to perceive fine-grained details and capture long-form temporal
structure in videos. Specifically, our approach incorporates dense vision task
annotations into MLLMs using direct preference optimization and develops
compact spatiotemporal representations through adaptive hierarchical token
compression. Experimental results demonstrate this unique design of LRC greatly
improves the results of video MLLM in mainstream video understanding benchmarks
(short & long), enabling the MLLM to memorize significantly longer video inputs
(at least 6x longer than the original), and master specialized vision
capabilities like object tracking and segmentation. Our work highlights the
importance of multimodal context richness (length and fineness) in empowering
MLLM's innate abilites (focus and memory), providing new insights for future
research on video MLLM. Code and models are available at
https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5
]]></content:encoded>
<pubDate>2025-01-21T18:59:00Z</pubDate>
</item>
<item>
<title>FaceXBench: Evaluating Multimodal LLMs on Face Understanding</title>
<link>http://arxiv.org/abs/2501.10360v1</link>
<guid>http://arxiv.org/abs/2501.10360v1</guid>
<content:encoded><![CDATA[
Multimodal Large Language Models (MLLMs) demonstrate impressive
problem-solving abilities across a wide range of tasks and domains. However,
their capacity for face understanding has not been systematically studied. To
address this gap, we introduce FaceXBench, a comprehensive benchmark designed
to evaluate MLLMs on complex face understanding tasks. FaceXBench includes
5,000 multimodal multiple-choice questions derived from 25 public datasets and
a newly created dataset, FaceXAPI. These questions cover 14 tasks across 6
broad categories, assessing MLLMs' face understanding abilities in bias and
fairness, face authentication, recognition, analysis, localization and tool
retrieval. Using FaceXBench, we conduct an extensive evaluation of 26
open-source MLLMs alongside 2 proprietary models, revealing the unique
challenges in complex face understanding tasks. We analyze the models across
three evaluation settings: zero-shot, in-context task description, and
chain-of-thought prompting. Our detailed analysis reveals that current MLLMs,
including advanced models like GPT-4o, and GeminiPro 1.5, show significant room
for improvement. We believe FaceXBench will be a crucial resource for
developing MLLMs equipped to perform sophisticated face understanding. Code:
https://github.com/Kartik-3004/facexbench
]]></content:encoded>
<pubDate>2025-01-17T18:59:55Z</pubDate>
</item>
<item>
<title>Learnings from Scaling Visual Tokenizers for Reconstruction and
  Generation</title>
<link>http://arxiv.org/abs/2501.09755v1</link>
<guid>http://arxiv.org/abs/2501.09755v1</guid>
<content:encoded><![CDATA[
Visual tokenization via auto-encoding empowers state-of-the-art image and
video generative models by compressing pixels into a latent space. Although
scaling Transformer-based generators has been central to recent advances, the
tokenizer component itself is rarely scaled, leaving open questions about how
auto-encoder design choices influence both its objective of reconstruction and
downstream generative performance. Our work aims to conduct an exploration of
scaling in auto-encoders to fill in this blank. To facilitate this exploration,
we replace the typical convolutional backbone with an enhanced Vision
Transformer architecture for Tokenization (ViTok). We train ViTok on
large-scale image and video datasets far exceeding ImageNet-1K, removing data
constraints on tokenizer scaling. We first study how scaling the auto-encoder
bottleneck affects both reconstruction and generation -- and find that while it
is highly correlated with reconstruction, its relationship with generation is
more complex. We next explored the effect of separately scaling the
auto-encoders' encoder and decoder on reconstruction and generation
performance. Crucially, we find that scaling the encoder yields minimal gains
for either reconstruction or generation, while scaling the decoder boosts
reconstruction but the benefits for generation are mixed. Building on our
exploration, we design ViTok as a lightweight auto-encoder that achieves
competitive performance with state-of-the-art auto-encoders on ImageNet-1K and
COCO reconstruction tasks (256p and 512p) while outperforming existing
auto-encoders on 16-frame 128p video reconstruction for UCF-101, all with 2-5x
fewer FLOPs. When integrated with Diffusion Transformers, ViTok demonstrates
competitive performance on image generation for ImageNet-1K and sets new
state-of-the-art benchmarks for class-conditional video generation on UCF-101.
]]></content:encoded>
<pubDate>2025-01-16T18:59:04Z</pubDate>
</item>
<item>
<title>Ouroboros-Diffusion: Exploring Consistent Content Generation in
  Tuning-free Long Video Diffusion</title>
<link>http://arxiv.org/abs/2501.09019v1</link>
<guid>http://arxiv.org/abs/2501.09019v1</guid>
<content:encoded><![CDATA[
The first-in-first-out (FIFO) video diffusion, built on a pre-trained
text-to-video model, has recently emerged as an effective approach for
tuning-free long video generation. This technique maintains a queue of video
frames with progressively increasing noise, continuously producing clean frames
at the queue's head while Gaussian noise is enqueued at the tail. However,
FIFO-Diffusion often struggles to keep long-range temporal consistency in the
generated videos due to the lack of correspondence modeling across frames. In
this paper, we propose Ouroboros-Diffusion, a novel video denoising framework
designed to enhance structural and content (subject) consistency, enabling the
generation of consistent videos of arbitrary length. Specifically, we introduce
a new latent sampling technique at the queue tail to improve structural
consistency, ensuring perceptually smooth transitions among frames. To enhance
subject consistency, we devise a Subject-Aware Cross-Frame Attention (SACFA)
mechanism, which aligns subjects across frames within short segments to achieve
better visual coherence. Furthermore, we introduce self-recurrent guidance.
This technique leverages information from all previous cleaner frames at the
front of the queue to guide the denoising of noisier frames at the end,
fostering rich and contextual global information interaction. Extensive
experiments of long video generation on the VBench benchmark demonstrate the
superiority of our Ouroboros-Diffusion, particularly in terms of subject
consistency, motion smoothness, and temporal consistency.
]]></content:encoded>
<pubDate>2025-01-15T18:59:15Z</pubDate>
</item>
<item>
<title>Multimodal LLMs Can Reason about Aesthetics in Zero-Shot</title>
<link>http://arxiv.org/abs/2501.09012v1</link>
<guid>http://arxiv.org/abs/2501.09012v1</guid>
<content:encoded><![CDATA[
We present the first study on how Multimodal LLMs' (MLLMs) reasoning ability
shall be elicited to evaluate the aesthetics of artworks. To facilitate this
investigation, we construct MM-StyleBench, a novel high-quality dataset for
benchmarking artistic stylization. We then develop a principled method for
human preference modeling and perform a systematic correlation analysis between
MLLMs' responses and human preference. Our experiments reveal an inherent
hallucination issue of MLLMs in art evaluation, associated with response
subjectivity. ArtCoT is proposed, demonstrating that art-specific task
decomposition and the use of concrete language boost MLLMs' reasoning ability
for aesthetics. Our findings offer valuable insights into MLLMs for art and can
benefit a wide range of downstream applications, such as style transfer and
artistic image generation. Code available at
https://github.com/songrise/MLLM4Art.
]]></content:encoded>
<pubDate>2025-01-15T18:56:22Z</pubDate>
</item>
<item>
<title>Omni-RGPT: Unifying Image and Video Region-level Understanding via Token
  Marks</title>
<link>http://arxiv.org/abs/2501.08326v1</link>
<guid>http://arxiv.org/abs/2501.08326v1</guid>
<content:encoded><![CDATA[
We present Omni-RGPT, a multimodal large language model designed to
facilitate region-level comprehension for both images and videos. To achieve
consistent region representation across spatio-temporal dimensions, we
introduce Token Mark, a set of tokens highlighting the target regions within
the visual feature space. These tokens are directly embedded into spatial
regions using region prompts (e.g., boxes or masks) and simultaneously
incorporated into the text prompt to specify the target, establishing a direct
connection between visual and text tokens. To further support robust video
understanding without requiring tracklets, we introduce an auxiliary task that
guides Token Mark by leveraging the consistency of the tokens, enabling stable
region interpretation across the video. Additionally, we introduce a
large-scale region-level video instruction dataset (RegVID-300k). Omni-RGPT
achieves state-of-the-art results on image and video-based commonsense
reasoning benchmarks while showing strong performance in captioning and
referring expression comprehension tasks.
]]></content:encoded>
<pubDate>2025-01-14T18:58:04Z</pubDate>
</item>
<item>
<title>GameFactory: Creating New Games with Generative Interactive Videos</title>
<link>http://arxiv.org/abs/2501.08325v1</link>
<guid>http://arxiv.org/abs/2501.08325v1</guid>
<content:encoded><![CDATA[
Generative game engines have the potential to revolutionize game development
by autonomously creating new content and reducing manual workload. However,
existing video-based game generation methods fail to address the critical
challenge of scene generalization, limiting their applicability to existing
games with fixed styles and scenes. In this paper, we present GameFactory, a
framework focused on exploring scene generalization in game video generation.
To enable the creation of entirely new and diverse games, we leverage
pre-trained video diffusion models trained on open-domain video data. To bridge
the domain gap between open-domain priors and small-scale game dataset, we
propose a multi-phase training strategy that decouples game style learning from
action control, preserving open-domain generalization while achieving action
controllability. Using Minecraft as our data source, we release GF-Minecraft, a
high-quality and diversity action-annotated video dataset for research.
Furthermore, we extend our framework to enable autoregressive
action-controllable game video generation, allowing the production of
unlimited-length interactive game videos. Experimental results demonstrate that
GameFactory effectively generates open-domain, diverse, and action-controllable
game videos, representing a significant step forward in AI-driven game
generation. Our dataset and project page are publicly available at
\url{https://vvictoryuki.github.io/gamefactory/}.
]]></content:encoded>
<pubDate>2025-01-14T18:57:21Z</pubDate>
</item>
<item>
<title>ADAM-1: AI and Bioinformatics for Alzheimer's Detection and
  Microbiome-Clinical Data Integrations</title>
<link>http://arxiv.org/abs/2501.08324v1</link>
<guid>http://arxiv.org/abs/2501.08324v1</guid>
<content:encoded><![CDATA[
The Alzheimer's Disease Analysis Model Generation 1 (ADAM) is a multi-agent
large language model (LLM) framework designed to integrate and analyze
multi-modal data, including microbiome profiles, clinical datasets, and
external knowledge bases, to enhance the understanding and detection of
Alzheimer's disease (AD). By leveraging retrieval-augmented generation (RAG)
techniques along with its multi-agent architecture, ADAM-1 synthesizes insights
from diverse data sources and contextualizes findings using literature-driven
evidence. Comparative evaluation against XGBoost revealed similar mean F1
scores but significantly reduced variance for ADAM-1, highlighting its
robustness and consistency, particularly in small laboratory datasets. While
currently tailored for binary classification tasks, future iterations aim to
incorporate additional data modalities, such as neuroimaging and biomarkers, to
broaden the scalability and applicability for Alzheimer's research and
diagnostics.
]]></content:encoded>
<pubDate>2025-01-14T18:56:33Z</pubDate>
</item>
<item>
<title>PokerBench: Training Large Language Models to become Professional Poker
  Players</title>
<link>http://arxiv.org/abs/2501.08328v1</link>
<guid>http://arxiv.org/abs/2501.08328v1</guid>
<content:encoded><![CDATA[
We introduce PokerBench - a benchmark for evaluating the poker-playing
abilities of large language models (LLMs). As LLMs excel in traditional NLP
tasks, their application to complex, strategic games like poker poses a new
challenge. Poker, an incomplete information game, demands a multitude of skills
such as mathematics, reasoning, planning, strategy, and a deep understanding of
game theory and human psychology. This makes Poker the ideal next frontier for
large language models. PokerBench consists of a comprehensive compilation of
11,000 most important scenarios, split between pre-flop and post-flop play,
developed in collaboration with trained poker players. We evaluate prominent
models including GPT-4, ChatGPT 3.5, and various Llama and Gemma series models,
finding that all state-of-the-art LLMs underperform in playing optimal poker.
However, after fine-tuning, these models show marked improvements. We validate
PokerBench by having models with different scores compete with each other,
demonstrating that higher scores on PokerBench lead to higher win rates in
actual poker games. Through gameplay between our fine-tuned model and GPT-4, we
also identify limitations of simple supervised fine-tuning for learning optimal
playing strategy, suggesting the need for more advanced methodologies for
effectively training language models to excel in games. PokerBench thus
presents a unique benchmark for a quick and reliable evaluation of the
poker-playing ability of LLMs as well as a comprehensive benchmark to study the
progress of LLMs in complex game-playing scenarios. The dataset and code will
be made available at: \url{https://github.com/pokerllm/pokerbench}.
]]></content:encoded>
<pubDate>2025-01-14T18:59:03Z</pubDate>
</item>
<item>
<title>Training-Free Motion-Guided Video Generation with Enhanced Temporal
  Consistency Using Motion Consistency Loss</title>
<link>http://arxiv.org/abs/2501.07563v1</link>
<guid>http://arxiv.org/abs/2501.07563v1</guid>
<content:encoded><![CDATA[
In this paper, we address the challenge of generating temporally consistent
videos with motion guidance. While many existing methods depend on additional
control modules or inference-time fine-tuning, recent studies suggest that
effective motion guidance is achievable without altering the model architecture
or requiring extra training. Such approaches offer promising compatibility with
various video generation foundation models. However, existing training-free
methods often struggle to maintain consistent temporal coherence across frames
or to follow guided motion accurately. In this work, we propose a simple yet
effective solution that combines an initial-noise-based approach with a novel
motion consistency loss, the latter being our key innovation. Specifically, we
capture the inter-frame feature correlation patterns of intermediate features
from a video diffusion model to represent the motion pattern of the reference
video. We then design a motion consistency loss to maintain similar feature
correlation patterns in the generated video, using the gradient of this loss in
the latent space to guide the generation process for precise motion control.
This approach improves temporal consistency across various motion control tasks
while preserving the benefits of a training-free setup. Extensive experiments
show that our method sets a new standard for efficient, temporally coherent
video generation.
]]></content:encoded>
<pubDate>2025-01-13T18:53:08Z</pubDate>
</item>
<item>
<title>WebWalker: Benchmarking LLMs in Web Traversal</title>
<link>http://arxiv.org/abs/2501.07572v1</link>
<guid>http://arxiv.org/abs/2501.07572v1</guid>
<content:encoded><![CDATA[
Retrieval-augmented generation (RAG) demonstrates remarkable performance
across tasks in open-domain question-answering. However, traditional search
engines may retrieve shallow content, limiting the ability of LLMs to handle
complex, multi-layered information. To address it, we introduce WebWalkerQA, a
benchmark designed to assess the ability of LLMs to perform web traversal. It
evaluates the capacity of LLMs to traverse a website's subpages to extract
high-quality data systematically. We propose WebWalker, which is a multi-agent
framework that mimics human-like web navigation through an explore-critic
paradigm. Extensive experimental results show that WebWalkerQA is challenging
and demonstrates the effectiveness of RAG combined with WebWalker, through the
horizontal and vertical integration in real-world scenarios.
]]></content:encoded>
<pubDate>2025-01-13T18:58:07Z</pubDate>
</item>
<item>
<title>SafeSwarm: Decentralized Safe RL for the Swarm of Drones Landing in
  Dense Crowds</title>
<link>http://arxiv.org/abs/2501.07566v1</link>
<guid>http://arxiv.org/abs/2501.07566v1</guid>
<content:encoded><![CDATA[
This paper introduces a safe swarm of drones capable of performing landings
in crowded environments robustly by relying on Reinforcement Learning
techniques combined with Safe Learning. The developed system allows us to teach
the swarm of drones with different dynamics to land on moving landing pads in
an environment while avoiding collisions with obstacles and between agents.
  The safe barrier net algorithm was developed and evaluated using a swarm of
Crazyflie 2.1 micro quadrotors, which were tested indoors with the Vicon motion
capture system to ensure precise localization and control.
  Experimental results show that our system achieves landing accuracy of 2.25
cm with a mean time of 17 s and collision-free landings, underscoring its
effectiveness and robustness in real-world scenarios. This work offers a
promising foundation for applications in environments where safety and
precision are paramount.
]]></content:encoded>
<pubDate>2025-01-13T18:54:02Z</pubDate>
</item>
<item>
<title>LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs</title>
<link>http://arxiv.org/abs/2501.06186v1</link>
<guid>http://arxiv.org/abs/2501.06186v1</guid>
<content:encoded><![CDATA[
Reasoning is a fundamental capability for solving complex multi-step
problems, particularly in visual contexts where sequential step-wise
understanding is essential. Existing approaches lack a comprehensive framework
for evaluating visual reasoning and do not emphasize step-wise problem-solving.
To this end, we propose a comprehensive framework for advancing step-by-step
visual reasoning in large language models (LMMs) through three key
contributions. First, we introduce a visual reasoning benchmark specifically
designed to evaluate multi-step reasoning tasks. The benchmark presents a
diverse set of challenges with eight different categories ranging from complex
visual perception to scientific reasoning with over 4k reasoning steps in
total, enabling robust evaluation of LLMs' abilities to perform accurate and
interpretable visual reasoning across multiple steps. Second, we propose a
novel metric that assesses visual reasoning quality at the granularity of
individual steps, emphasizing both correctness and logical coherence. The
proposed metric offers deeper insights into reasoning performance compared to
traditional end-task accuracy metrics. Third, we present a new multimodal
visual reasoning model, named LlamaV-o1, trained using a multi-step curriculum
learning approach, where tasks are progressively organized to facilitate
incremental skill acquisition and problem-solving. The proposed LlamaV-o1 is
designed for multi-step reasoning and learns step-by-step through a structured
training paradigm. Extensive experiments show that our LlamaV-o1 outperforms
existing open-source models and performs favorably against close-source
proprietary models. Compared to the recent Llava-CoT, our LlamaV-o1 achieves an
average score of 67.3 with an absolute gain of 3.8\% across six benchmarks
while being 5 times faster during inference scaling. Our benchmark, model, and
code are publicly available.
]]></content:encoded>
<pubDate>2025-01-10T18:59:51Z</pubDate>
</item>
<item>
<title>PEACE: Empowering Geologic Map Holistic Understanding with MLLMs</title>
<link>http://arxiv.org/abs/2501.06184v1</link>
<guid>http://arxiv.org/abs/2501.06184v1</guid>
<content:encoded><![CDATA[
Geologic map, as a fundamental diagram in geology science, provides critical
insights into the structure and composition of Earth's subsurface and surface.
These maps are indispensable in various fields, including disaster detection,
resource exploration, and civil engineering. Despite their significance,
current Multimodal Large Language Models (MLLMs) often fall short in geologic
map understanding. This gap is primarily due to the challenging nature of
cartographic generalization, which involves handling high-resolution map,
managing multiple associated components, and requiring domain-specific
knowledge. To quantify this gap, we construct GeoMap-Bench, the first-ever
benchmark for evaluating MLLMs in geologic map understanding, which assesses
the full-scale abilities in extracting, referring, grounding, reasoning, and
analyzing. To bridge this gap, we introduce GeoMap-Agent, the inaugural agent
designed for geologic map understanding, which features three modules:
Hierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI),
and Prompt-enhanced Question Answering (PEQA). Inspired by the
interdisciplinary collaboration among human scientists, an AI expert group acts
as consultants, utilizing a diverse tool pool to comprehensively analyze
questions. Through comprehensive experiments, GeoMap-Agent achieves an overall
score of 0.811 on GeoMap-Bench, significantly outperforming 0.369 of GPT-4o.
Our work, emPowering gEologic mAp holistiC undErstanding (PEACE) with MLLMs,
paves the way for advanced AI applications in geology, enhancing the efficiency
and accuracy of geological investigations.
]]></content:encoded>
<pubDate>2025-01-10T18:59:42Z</pubDate>
</item>
<item>
<title>VideoAuteur: Towards Long Narrative Video Generation</title>
<link>http://arxiv.org/abs/2501.06173v1</link>
<guid>http://arxiv.org/abs/2501.06173v1</guid>
<content:encoded><![CDATA[
Recent video generation models have shown promising results in producing
high-quality video clips lasting several seconds. However, these models face
challenges in generating long sequences that convey clear and informative
events, limiting their ability to support coherent narrations. In this paper,
we present a large-scale cooking video dataset designed to advance long-form
narrative generation in the cooking domain. We validate the quality of our
proposed dataset in terms of visual fidelity and textual caption accuracy using
state-of-the-art Vision-Language Models (VLMs) and video generation models,
respectively. We further introduce a Long Narrative Video Director to enhance
both visual and semantic coherence in generated videos and emphasize the role
of aligning visual embeddings to achieve improved overall video quality. Our
method demonstrates substantial improvements in generating visually detailed
and semantically aligned keyframes, supported by finetuning techniques that
integrate text and image embeddings within the video generation process.
Project page: https://videoauteur.github.io/
]]></content:encoded>
<pubDate>2025-01-10T18:52:11Z</pubDate>
</item>
<item>
<title>ReFocus: Visual Editing as a Chain of Thought for Structured Image
  Understanding</title>
<link>http://arxiv.org/abs/2501.05452v1</link>
<guid>http://arxiv.org/abs/2501.05452v1</guid>
<content:encoded><![CDATA[
Structured image understanding, such as interpreting tables and charts,
requires strategically refocusing across various structures and texts within an
image, forming a reasoning sequence to arrive at the final answer. However,
current multimodal large language models (LLMs) lack this multihop selective
attention capability. In this work, we introduce ReFocus, a simple yet
effective framework that equips multimodal LLMs with the ability to generate
"visual thoughts" by performing visual editing on the input image through code,
shifting and refining their visual focuses. Specifically, ReFocus enables
multimodal LLMs to generate Python codes to call tools and modify the input
image, sequentially drawing boxes, highlighting sections, and masking out
areas, thereby enhancing the visual reasoning process. We experiment upon a
wide range of structured image understanding tasks involving tables and charts.
ReFocus largely improves performance on all tasks over GPT-4o without visual
editing, yielding an average gain of 11.0% on table tasks and 6.8% on chart
tasks. We present an in-depth analysis of the effects of different visual
edits, and reasons why ReFocus can improve the performance without introducing
additional information. Further, we collect a 14k training set using ReFocus,
and prove that such visual chain-of-thought with intermediate information
offers a better supervision than standard VQA data, reaching a 8.0% average
gain over the same model trained with QA pairs and 2.6% over CoT.
]]></content:encoded>
<pubDate>2025-01-09T18:59:58Z</pubDate>
</item>
<item>
<title>Consistent Flow Distillation for Text-to-3D Generation</title>
<link>http://arxiv.org/abs/2501.05445v1</link>
<guid>http://arxiv.org/abs/2501.05445v1</guid>
<content:encoded><![CDATA[
Score Distillation Sampling (SDS) has made significant strides in distilling
image-generative models for 3D generation. However, its
maximum-likelihood-seeking behavior often leads to degraded visual quality and
diversity, limiting its effectiveness in 3D applications. In this work, we
propose Consistent Flow Distillation (CFD), which addresses these limitations.
We begin by leveraging the gradient of the diffusion ODE or SDE sampling
process to guide the 3D generation. From the gradient-based sampling
perspective, we find that the consistency of 2D image flows across different
viewpoints is important for high-quality 3D generation. To achieve this, we
introduce multi-view consistent Gaussian noise on the 3D object, which can be
rendered from various viewpoints to compute the flow gradient. Our experiments
demonstrate that CFD, through consistent flows, significantly outperforms
previous methods in text-to-3D generation.
]]></content:encoded>
<pubDate>2025-01-09T18:56:05Z</pubDate>
</item>
<item>
<title>Can MLLMs Reason in Multimodality? EMMA: An Enhanced MultiModal
  ReAsoning Benchmark</title>
<link>http://arxiv.org/abs/2501.05444v1</link>
<guid>http://arxiv.org/abs/2501.05444v1</guid>
<content:encoded><![CDATA[
The ability to organically reason over and with both text and images is a
pillar of human intelligence, yet the ability of Multimodal Large Language
Models (MLLMs) to perform such multimodal reasoning remains under-explored.
Existing benchmarks often emphasize text-dominant reasoning or rely on shallow
visual cues, failing to adequately assess integrated visual and textual
reasoning. We introduce EMMA (Enhanced MultiModal reAsoning), a benchmark
targeting organic multimodal reasoning across mathematics, physics, chemistry,
and coding. EMMA tasks demand advanced cross-modal reasoning that cannot be
addressed by reasoning independently in each modality, offering an enhanced
test suite for MLLMs' reasoning capabilities. Our evaluation of
state-of-the-art MLLMs on EMMA reveals significant limitations in handling
complex multimodal and multi-step reasoning tasks, even with advanced
techniques like Chain-of-Thought prompting and test-time compute scaling
underperforming. These findings underscore the need for improved multimodal
architectures and training paradigms to close the gap between human and model
reasoning in multimodality.
]]></content:encoded>
<pubDate>2025-01-09T18:55:52Z</pubDate>
</item>
<item>
<title>Progressive Growing of Video Tokenizers for Highly Compressed Latent
  Spaces</title>
<link>http://arxiv.org/abs/2501.05442v1</link>
<guid>http://arxiv.org/abs/2501.05442v1</guid>
<content:encoded><![CDATA[
Video tokenizers are essential for latent video diffusion models, converting
raw video data into spatiotemporally compressed latent spaces for efficient
training. However, extending state-of-the-art video tokenizers to achieve a
temporal compression ratio beyond 4x without increasing channel capacity poses
significant challenges. In this work, we propose an alternative approach to
enhance temporal compression. We find that the reconstruction quality of
temporally subsampled videos from a low-compression encoder surpasses that of
high-compression encoders applied to original videos. This indicates that
high-compression models can leverage representations from lower-compression
models. Building on this insight, we develop a bootstrapped
high-temporal-compression model that progressively trains high-compression
blocks atop well-trained lower-compression models. Our method includes a
cross-level feature-mixing module to retain information from the pretrained
low-compression model and guide higher-compression blocks to capture the
remaining details from the full video sequence. Evaluation of video benchmarks
shows that our method significantly improves reconstruction quality while
increasing temporal compression compared to direct extensions of existing video
tokenizers. Furthermore, the resulting compact latent space effectively trains
a video diffusion model for high-quality video generation with a reduced token
budget.
]]></content:encoded>
<pubDate>2025-01-09T18:55:15Z</pubDate>
</item>
<item>
<title>EditAR: Unified Conditional Generation with Autoregressive Models</title>
<link>http://arxiv.org/abs/2501.04699v1</link>
<guid>http://arxiv.org/abs/2501.04699v1</guid>
<content:encoded><![CDATA[
Recent progress in controllable image generation and editing is largely
driven by diffusion-based methods. Although diffusion models perform
exceptionally well in specific tasks with tailored designs, establishing a
unified model is still challenging. In contrast, autoregressive models
inherently feature a unified tokenized representation, which simplifies the
creation of a single foundational model for various tasks. In this work, we
propose EditAR, a single unified autoregressive framework for a variety of
conditional image generation tasks, e.g., image editing, depth-to-image,
edge-to-image, segmentation-to-image. The model takes both images and
instructions as inputs, and predicts the edited images tokens in a vanilla
next-token paradigm. To enhance the text-to-image alignment, we further propose
to distill the knowledge from foundation models into the autoregressive
modeling process. We evaluate its effectiveness across diverse tasks on
established benchmarks, showing competitive performance to various
state-of-the-art task-specific methods. Project page:
https://jitengmu.github.io/EditAR/
]]></content:encoded>
<pubDate>2025-01-08T18:59:35Z</pubDate>
</item>
<item>
<title>ConceptMaster: Multi-Concept Video Customization on Diffusion
  Transformer Models Without Test-Time Tuning</title>
<link>http://arxiv.org/abs/2501.04698v1</link>
<guid>http://arxiv.org/abs/2501.04698v1</guid>
<content:encoded><![CDATA[
Text-to-video generation has made remarkable advancements through diffusion
models. However, Multi-Concept Video Customization (MCVC) remains a significant
challenge. We identify two key challenges in this task: 1) the identity
decoupling problem, where directly adopting existing customization methods
inevitably mix attributes when handling multiple concepts simultaneously, and
2) the scarcity of high-quality video-entity pairs, which is crucial for
training such a model that represents and decouples various concepts well. To
address these challenges, we introduce ConceptMaster, an innovative framework
that effectively tackles the critical issues of identity decoupling while
maintaining concept fidelity in customized videos. Specifically, we introduce a
novel strategy of learning decoupled multi-concept embeddings that are injected
into the diffusion models in a standalone manner, which effectively guarantees
the quality of customized videos with multiple identities, even for highly
similar visual concepts. To further overcome the scarcity of high-quality MCVC
data, we carefully establish a data construction pipeline, which enables
systematic collection of precise multi-concept video-entity data across diverse
concepts. A comprehensive benchmark is designed to validate the effectiveness
of our model from three critical dimensions: concept fidelity, identity
decoupling ability, and video generation quality across six different concept
composition scenarios. Extensive experiments demonstrate that our ConceptMaster
significantly outperforms previous approaches for this task, paving the way for
generating personalized and semantically accurate videos across multiple
concepts.
]]></content:encoded>
<pubDate>2025-01-08T18:59:01Z</pubDate>
</item>
<item>
<title>Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous
  Sensors via Language Grounding</title>
<link>http://arxiv.org/abs/2501.04693v1</link>
<guid>http://arxiv.org/abs/2501.04693v1</guid>
<content:encoded><![CDATA[
Interacting with the world is a multi-sensory experience: achieving effective
general-purpose interaction requires making use of all available modalities --
including vision, touch, and audio -- to fill in gaps from partial observation.
For example, when vision is occluded reaching into a bag, a robot should rely
on its senses of touch and sound. However, state-of-the-art generalist robot
policies are typically trained on large datasets to predict robot actions
solely from visual and proprioceptive observations. In this work, we propose
FuSe, a novel approach that enables finetuning visuomotor generalist policies
on heterogeneous sensor modalities for which large datasets are not readily
available by leveraging natural language as a common cross-modal grounding. We
combine a multimodal contrastive loss with a sensory-grounded language
generation loss to encode high-level semantics. In the context of robot
manipulation, we show that FuSe enables performing challenging tasks that
require reasoning jointly over modalities such as vision, touch, and sound in a
zero-shot setting, such as multimodal prompting, compositional cross-modal
prompting, and descriptions of objects it interacts with. We show that the same
recipe is applicable to widely different generalist policies, including both
diffusion-based generalist policies and large vision-language-action (VLA)
models. Extensive experiments in the real world show that FuSeis able to
increase success rates by over 20% compared to all considered baselines.
]]></content:encoded>
<pubDate>2025-01-08T18:57:33Z</pubDate>
</item>
<item>
<title>LargeAD: Large-Scale Cross-Sensor Data Pretraining for Autonomous
  Driving</title>
<link>http://arxiv.org/abs/2501.04005v1</link>
<guid>http://arxiv.org/abs/2501.04005v1</guid>
<content:encoded><![CDATA[
Recent advancements in vision foundation models (VFMs) have revolutionized
visual perception in 2D, yet their potential for 3D scene understanding,
particularly in autonomous driving applications, remains underexplored. In this
paper, we introduce LargeAD, a versatile and scalable framework designed for
large-scale 3D pretraining across diverse real-world driving datasets. Our
framework leverages VFMs to extract semantically rich superpixels from 2D
images, which are aligned with LiDAR point clouds to generate high-quality
contrastive samples. This alignment facilitates cross-modal representation
learning, enhancing the semantic consistency between 2D and 3D data. We
introduce several key innovations: i) VFM-driven superpixel generation for
detailed semantic representation, ii) a VFM-assisted contrastive learning
strategy to align multimodal features, iii) superpoint temporal consistency to
maintain stable representations across time, and iv) multi-source data
pretraining to generalize across various LiDAR configurations. Our approach
delivers significant performance improvements over state-of-the-art methods in
both linear probing and fine-tuning tasks for both LiDAR-based segmentation and
object detection. Extensive experiments on eleven large-scale multi-modal
datasets highlight our superior performance, demonstrating the adaptability,
efficiency, and robustness in real-world autonomous driving scenarios.
]]></content:encoded>
<pubDate>2025-01-07T18:59:59Z</pubDate>
</item>
<item>
<title>Automated Generation of Challenging Multiple-Choice Questions for Vision
  Language Model Evaluation</title>
<link>http://arxiv.org/abs/2501.03225v1</link>
<guid>http://arxiv.org/abs/2501.03225v1</guid>
<content:encoded><![CDATA[
The rapid development of vision language models (VLMs) demands rigorous and
reliable evaluation. However, current visual question answering (VQA)
benchmarks often depend on open-ended questions, making accurate evaluation
difficult due to the variability in natural language responses. To address
this, we introduce AutoConverter, an agentic framework that automatically
converts these open-ended questions into multiple-choice format, enabling
objective evaluation while reducing the costly question creation process. Our
experiments demonstrate that AutoConverter can generate correct and challenging
multiple-choice questions, with VLMs demonstrating consistently similar or
lower accuracy on these questions compared to human-created ones. Using
AutoConverter, we construct VMCBench, a benchmark created by transforming 20
existing VQA datasets into a unified multiple-choice format, totaling 9,018
questions. We comprehensively evaluate 33 state-of-the-art VLMs on VMCBench,
setting a new standard for scalable, consistent, and reproducible VLM
evaluation.
]]></content:encoded>
<pubDate>2025-01-06T18:57:31Z</pubDate>
</item>
<item>
<title>VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction</title>
<link>http://arxiv.org/abs/2501.01957v1</link>
<guid>http://arxiv.org/abs/2501.01957v1</guid>
<content:encoded><![CDATA[
Recent Multimodal Large Language Models (MLLMs) have typically focused on
integrating visual and textual modalities, with less emphasis placed on the
role of speech in enhancing interaction. However, speech plays a crucial role
in multimodal dialogue systems, and implementing high-performance in both
vision and speech tasks remains a significant challenge due to the fundamental
modality differences. In this paper, we propose a carefully designed
multi-stage training methodology that progressively trains LLM to understand
both visual and speech information, ultimately enabling fluent vision and
speech interaction. Our approach not only preserves strong vision-language
capacity, but also enables efficient speech-to-speech dialogue capabilities
without separate ASR and TTS modules, significantly accelerating multimodal
end-to-end response speed. By comparing our method against state-of-the-art
counterparts across benchmarks for image, video, and speech tasks, we
demonstrate that our model is equipped with both strong visual and speech
capabilities, making near real-time vision and speech interaction.
]]></content:encoded>
<pubDate>2025-01-03T18:59:52Z</pubDate>
</item>
<item>
<title>VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion
  Control</title>
<link>http://arxiv.org/abs/2501.01427v1</link>
<guid>http://arxiv.org/abs/2501.01427v1</guid>
<content:encoded><![CDATA[
Despite significant advancements in video generation, inserting a given
object into videos remains a challenging task. The difficulty lies in
preserving the appearance details of the reference object and accurately
modeling coherent motions at the same time. In this paper, we propose
VideoAnydoor, a zero-shot video object insertion framework with high-fidelity
detail preservation and precise motion control. Starting from a text-to-video
model, we utilize an ID extractor to inject the global identity and leverage a
box sequence to control the overall motion. To preserve the detailed appearance
and meanwhile support fine-grained motion control, we design a pixel warper. It
takes the reference image with arbitrary key-points and the corresponding
key-point trajectories as inputs. It warps the pixel details according to the
trajectories and fuses the warped features with the diffusion U-Net, thus
improving detail preservation and supporting users in manipulating the motion
trajectories. In addition, we propose a training strategy involving both videos
and static images with a reweight reconstruction loss to enhance insertion
quality. VideoAnydoor demonstrates significant superiority over existing
methods and naturally supports various downstream applications (e.g., talking
head generation, video virtual try-on, multi-region editing) without
task-specific fine-tuning.
]]></content:encoded>
<pubDate>2025-01-02T18:59:54Z</pubDate>
</item>
<item>
<title>Object-level Visual Prompts for Compositional Image Generation</title>
<link>http://arxiv.org/abs/2501.01424v1</link>
<guid>http://arxiv.org/abs/2501.01424v1</guid>
<content:encoded><![CDATA[
We introduce a method for composing object-level visual prompts within a
text-to-image diffusion model. Our approach addresses the task of generating
semantically coherent compositions across diverse scenes and styles, similar to
the versatility and expressiveness offered by text prompts. A key challenge in
this task is to preserve the identity of the objects depicted in the input
visual prompts, while also generating diverse compositions across different
images. To address this challenge, we introduce a new KV-mixed cross-attention
mechanism, in which keys and values are learned from distinct visual
representations. The keys are derived from an encoder with a small bottleneck
for layout control, whereas the values come from a larger bottleneck encoder
that captures fine-grained appearance details. By mixing keys and values from
these complementary sources, our model preserves the identity of the visual
prompts while supporting flexible variations in object arrangement, pose, and
composition. During inference, we further propose object-level compositional
guidance to improve the method's identity preservation and layout correctness.
Results show that our technique produces diverse scene compositions that
preserve the unique characteristics of each visual prompt, expanding the
creative potential of text-to-image generation.
]]></content:encoded>
<pubDate>2025-01-02T18:59:44Z</pubDate>
</item>
<item>
<title>DrivingGPT: Unifying Driving World Modeling and Planning with
  Multi-modal Autoregressive Transformers</title>
<link>http://arxiv.org/abs/2412.18607v1</link>
<guid>http://arxiv.org/abs/2412.18607v1</guid>
<content:encoded><![CDATA[
World model-based searching and planning are widely recognized as a promising
path toward human-level physical intelligence. However, current driving world
models primarily rely on video diffusion models, which specialize in visual
generation but lack the flexibility to incorporate other modalities like
action. In contrast, autoregressive transformers have demonstrated exceptional
capability in modeling multimodal data. Our work aims to unify both driving
model simulation and trajectory planning into a single sequence modeling
problem. We introduce a multimodal driving language based on interleaved image
and action tokens, and develop DrivingGPT to learn joint world modeling and
planning through standard next-token prediction. Our DrivingGPT demonstrates
strong performance in both action-conditioned video generation and end-to-end
planning, outperforming strong baselines on large-scale nuPlan and NAVSIM
benchmarks.
]]></content:encoded>
<pubDate>2024-12-24T18:59:37Z</pubDate>
</item>
<item>
<title>Decentralized Intelligence in GameFi: Embodied AI Agents and the
  Convergence of DeFi and Virtual Ecosystems</title>
<link>http://arxiv.org/abs/2412.18601v1</link>
<guid>http://arxiv.org/abs/2412.18601v1</guid>
<content:encoded><![CDATA[
In the rapidly evolving landscape of GameFi, a fusion of gaming and
decentralized finance (DeFi), there exists a critical need to enhance player
engagement and economic interaction within gaming ecosystems. Our GameFi
ecosystem aims to fundamentally transform this landscape by integrating
advanced embodied AI agents into GameFi platforms. These AI agents, developed
using cutting-edge large language models (LLMs), such as GPT-4 and Claude AI,
are capable of proactive, adaptive, and contextually rich interactions with
players. By going beyond traditional scripted responses, these agents become
integral participants in the game's narrative and economic systems, directly
influencing player strategies and in-game economies. We address the limitations
of current GameFi platforms, which often lack immersive AI interactions and
mechanisms for community engagement or creator monetization. Through the deep
integration of AI agents with blockchain technology, we establish a
consensus-driven, decentralized GameFi ecosystem. This ecosystem empowers
creators to monetize their contributions and fosters democratic collaboration
among players and creators. Furthermore, by embedding DeFi mechanisms into the
gaming experience, we enhance economic participation and provide new
opportunities for financial interactions within the game. Our approach enhances
player immersion and retention and advances the GameFi ecosystem by bridging
traditional gaming with Web3 technologies. By integrating sophisticated AI and
DeFi elements, we contribute to the development of more engaging, economically
robust, and community-centric gaming environments. This project represents a
significant advancement in the state-of-the-art in GameFi, offering insights
and methodologies that can be applied throughout the gaming industry.
]]></content:encoded>
<pubDate>2024-12-24T18:56:00Z</pubDate>
</item>
<item>
<title>ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation</title>
<link>http://arxiv.org/abs/2412.18600v1</link>
<guid>http://arxiv.org/abs/2412.18600v1</guid>
<content:encoded><![CDATA[
Human-scene interaction (HSI) generation is crucial for applications in
embodied AI, virtual reality, and robotics. While existing methods can
synthesize realistic human motions in 3D scenes and generate plausible
human-object interactions, they heavily rely on datasets containing paired 3D
scene and motion capture data, which are expensive and time-consuming to
collect across diverse environments and interactions. We present ZeroHSI, a
novel approach that enables zero-shot 4D human-scene interaction synthesis by
integrating video generation and neural human rendering. Our key insight is to
leverage the rich motion priors learned by state-of-the-art video generation
models, which have been trained on vast amounts of natural human movements and
interactions, and use differentiable rendering to reconstruct human-scene
interactions. ZeroHSI can synthesize realistic human motions in both static
scenes and environments with dynamic objects, without requiring any
ground-truth motion data. We evaluate ZeroHSI on a curated dataset of different
types of various indoor and outdoor scenes with different interaction prompts,
demonstrating its ability to generate diverse and contextually appropriate
human-scene interactions.
]]></content:encoded>
<pubDate>2024-12-24T18:55:38Z</pubDate>
</item>
<item>
<title>DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion
  Transformer for Tuning-Free Multi-Prompt Longer Video Generation</title>
<link>http://arxiv.org/abs/2412.18597v1</link>
<guid>http://arxiv.org/abs/2412.18597v1</guid>
<content:encoded><![CDATA[
Sora-like video generation models have achieved remarkable progress with a
Multi-Modal Diffusion Transformer MM-DiT architecture. However, the current
video generation models predominantly focus on single-prompt, struggling to
generate coherent scenes with multiple sequential prompts that better reflect
real-world dynamic scenarios. While some pioneering works have explored
multi-prompt video generation, they face significant challenges including
strict training data requirements, weak prompt following, and unnatural
transitions. To address these problems, we propose DiTCtrl, a training-free
multi-prompt video generation method under MM-DiT architectures for the first
time. Our key idea is to take the multi-prompt video generation task as
temporal video editing with smooth transitions. To achieve this goal, we first
analyze MM-DiT's attention mechanism, finding that the 3D full attention
behaves similarly to that of the cross/self-attention blocks in the UNet-like
diffusion models, enabling mask-guided precise semantic control across
different prompts with attention sharing for multi-prompt video generation.
Based on our careful design, the video generated by DiTCtrl achieves smooth
transitions and consistent object motion given multiple sequential prompts
without additional training. Besides, we also present MPVBench, a new benchmark
specially designed for multi-prompt video generation to evaluate the
performance of multi-prompt generation. Extensive experiments demonstrate that
our method achieves state-of-the-art performance without additional training.
]]></content:encoded>
<pubDate>2024-12-24T18:51:19Z</pubDate>
</item>
<item>
<title>ChatGarment: Garment Estimation, Generation and Editing via Large
  Language Models</title>
<link>http://arxiv.org/abs/2412.17811v1</link>
<guid>http://arxiv.org/abs/2412.17811v1</guid>
<content:encoded><![CDATA[
We introduce ChatGarment, a novel approach that leverages large
vision-language models (VLMs) to automate the estimation, generation, and
editing of 3D garments from images or text descriptions. Unlike previous
methods that struggle in real-world scenarios or lack interactive editing
capabilities, ChatGarment can estimate sewing patterns from in-the-wild images
or sketches, generate them from text descriptions, and edit garments based on
user instructions, all within an interactive dialogue. These sewing patterns
can then be draped into 3D garments, which are easily animatable and
simulatable. This is achieved by finetuning a VLM to directly generate a JSON
file that includes both textual descriptions of garment types and styles, as
well as continuous numerical attributes. This JSON file is then used to create
sewing patterns through a programming parametric model. To support this, we
refine the existing programming model, GarmentCode, by expanding its garment
type coverage and simplifying its structure for efficient VLM fine-tuning.
Additionally, we construct a large-scale dataset of image-to-sewing-pattern and
text-to-sewing-pattern pairs through an automated data pipeline. Extensive
evaluations demonstrate ChatGarment's ability to accurately reconstruct,
generate, and edit garments from multimodal inputs, highlighting its potential
to revolutionize workflows in fashion and gaming applications. Code and data
will be available at https://chatgarment.github.io/.
]]></content:encoded>
<pubDate>2024-12-23T18:59:28Z</pubDate>
</item>
<item>
<title>Large Motion Video Autoencoding with Cross-modal Video VAE</title>
<link>http://arxiv.org/abs/2412.17805v1</link>
<guid>http://arxiv.org/abs/2412.17805v1</guid>
<content:encoded><![CDATA[
Learning a robust video Variational Autoencoder (VAE) is essential for
reducing video redundancy and facilitating efficient video generation. Directly
applying image VAEs to individual frames in isolation can result in temporal
inconsistencies and suboptimal compression rates due to a lack of temporal
compression. Existing Video VAEs have begun to address temporal compression;
however, they often suffer from inadequate reconstruction performance. In this
paper, we present a novel and powerful video autoencoder capable of
high-fidelity video encoding. First, we observe that entangling spatial and
temporal compression by merely extending the image VAE to a 3D VAE can
introduce motion blur and detail distortion artifacts. Thus, we propose
temporal-aware spatial compression to better encode and decode the spatial
information. Additionally, we integrate a lightweight motion compression model
for further temporal compression. Second, we propose to leverage the textual
information inherent in text-to-video datasets and incorporate text guidance
into our model. This significantly enhances reconstruction quality,
particularly in terms of detail preservation and temporal stability. Third, we
further improve the versatility of our model through joint training on both
images and videos, which not only enhances reconstruction quality but also
enables the model to perform both image and video autoencoding. Extensive
evaluations against strong recent baselines demonstrate the superior
performance of our method. The project website can be found
at~\href{https://yzxing87.github.io/vae/}{https://yzxing87.github.io/vae/}.
]]></content:encoded>
<pubDate>2024-12-23T18:58:24Z</pubDate>
</item>
<item>
<title>Personalized Representation from Personalized Generation</title>
<link>http://arxiv.org/abs/2412.16156v1</link>
<guid>http://arxiv.org/abs/2412.16156v1</guid>
<content:encoded><![CDATA[
Modern vision models excel at general purpose downstream tasks. It is
unclear, however, how they may be used for personalized vision tasks, which are
both fine-grained and data-scarce. Recent works have successfully applied
synthetic data to general-purpose representation learning, while advances in
T2I diffusion models have enabled the generation of personalized images from
just a few real examples. Here, we explore a potential connection between these
ideas, and formalize the challenge of using personalized synthetic data to
learn personalized representations, which encode knowledge about an object of
interest and may be flexibly applied to any downstream task relating to the
target object. We introduce an evaluation suite for this challenge, including
reformulations of two existing datasets and a novel dataset explicitly
constructed for this purpose, and propose a contrastive learning approach that
makes creative use of image generators. We show that our method improves
personalized representation learning for diverse downstream tasks, from
recognition to segmentation, and analyze characteristics of image generation
approaches that are key to this gain.
]]></content:encoded>
<pubDate>2024-12-20T18:59:03Z</pubDate>
</item>
<item>
<title>A vector logic for extensional formal semantics</title>
<link>http://arxiv.org/abs/2412.16152v1</link>
<guid>http://arxiv.org/abs/2412.16152v1</guid>
<content:encoded><![CDATA[
This paper proves a homomorphism between extensional formal semantics and
distributional vector space semantics, demonstrating structural compatibility.
Formal semantics models meaning as reference, using logical structures to map
linguistic expressions to truth conditions, while distributional semantics
represents meaning through word vectors derived from contextual usage. By
constructing injective mappings that preserve semantic relationships, we show
that every semantic function in an extensional model corresponds to a
compatible vector space operation. This result respects compositionality and
extends to function compositions, constant interpretations, and $n$-ary
relations. Rather than pursuing unification, we highlight a mathematical
foundation for hybrid cognitive models that integrate symbolic and sub-symbolic
reasoning and semantics. These findings support multimodal language processing,
aligning `meaning as reference' (Frege, Tarski) with `meaning as use'
(Wittgenstein, Firth).
]]></content:encoded>
<pubDate>2024-12-20T18:54:48Z</pubDate>
</item>
<item>
<title>Flowing from Words to Pixels: A Framework for Cross-Modality Evolution</title>
<link>http://arxiv.org/abs/2412.15213v1</link>
<guid>http://arxiv.org/abs/2412.15213v1</guid>
<content:encoded><![CDATA[
Diffusion models, and their generalization, flow matching, have had a
remarkable impact on the field of media generation. Here, the conventional
approach is to learn the complex mapping from a simple source distribution of
Gaussian noise to the target media distribution. For cross-modal tasks such as
text-to-image generation, this same mapping from noise to image is learnt
whilst including a conditioning mechanism in the model. One key and thus far
relatively unexplored feature of flow matching is that, unlike Diffusion
models, they are not constrained for the source distribution to be noise.
Hence, in this paper, we propose a paradigm shift, and ask the question of
whether we can instead train flow matching models to learn a direct mapping
from the distribution of one modality to the distribution of another, thus
obviating the need for both the noise distribution and conditioning mechanism.
We present a general and simple framework, CrossFlow, for cross-modal flow
matching. We show the importance of applying Variational Encoders to the input
data, and introduce a method to enable Classifier-free guidance. Surprisingly,
for text-to-image, CrossFlow with a vanilla transformer without cross attention
slightly outperforms standard flow matching, and we show that it scales better
with training steps and model size, while also allowing for interesting latent
arithmetic which results in semantically meaningful edits in the output space.
To demonstrate the generalizability of our approach, we also show that
CrossFlow is on par with or outperforms the state-of-the-art for various
cross-modal / intra-modal mapping tasks, viz. image captioning, depth
estimation, and image super-resolution. We hope this paper contributes to
accelerating progress in cross-modal media generation.
]]></content:encoded>
<pubDate>2024-12-19T18:59:56Z</pubDate>
</item>
<item>
<title>Autoregressive Video Generation without Vector Quantization</title>
<link>http://arxiv.org/abs/2412.14169v1</link>
<guid>http://arxiv.org/abs/2412.14169v1</guid>
<content:encoded><![CDATA[
This paper presents a novel approach that enables autoregressive video
generation with high efficiency. We propose to reformulate the video generation
problem as a non-quantized autoregressive modeling of temporal frame-by-frame
prediction and spatial set-by-set prediction. Unlike raster-scan prediction in
prior autoregressive models or joint distribution modeling of fixed-length
tokens in diffusion models, our approach maintains the causal property of
GPT-style models for flexible in-context capabilities, while leveraging
bidirectional modeling within individual frames for efficiency. With the
proposed approach, we train a novel video autoregressive model without vector
quantization, termed NOVA. Our results demonstrate that NOVA surpasses prior
autoregressive video models in data efficiency, inference speed, visual
fidelity, and video fluency, even with a much smaller model capacity, i.e.,
0.6B parameters. NOVA also outperforms state-of-the-art image diffusion models
in text-to-image generation tasks, with a significantly lower training cost.
Additionally, NOVA generalizes well across extended video durations and enables
diverse zero-shot applications in one unified model. Code and models are
publicly available at https://github.com/baaivision/NOVA.
]]></content:encoded>
<pubDate>2024-12-18T18:59:53Z</pubDate>
</item>
<item>
<title>E-CAR: Efficient Continuous Autoregressive Image Generation via
  Multistage Modeling</title>
<link>http://arxiv.org/abs/2412.14170v1</link>
<guid>http://arxiv.org/abs/2412.14170v1</guid>
<content:encoded><![CDATA[
Recent advances in autoregressive (AR) models with continuous tokens for
image generation show promising results by eliminating the need for discrete
tokenization. However, these models face efficiency challenges due to their
sequential token generation nature and reliance on computationally intensive
diffusion-based sampling. We present ECAR (Efficient Continuous Auto-Regressive
Image Generation via Multistage Modeling), an approach that addresses these
limitations through two intertwined innovations: (1) a stage-wise continuous
token generation strategy that reduces computational complexity and provides
progressively refined token maps as hierarchical conditions, and (2) a
multistage flow-based distribution modeling method that transforms only
partial-denoised distributions at each stage comparing to complete denoising in
normal diffusion models. Holistically, ECAR operates by generating tokens at
increasing resolutions while simultaneously denoising the image at each stage.
This design not only reduces token-to-image transformation cost by a factor of
the stage number but also enables parallel processing at the token level. Our
approach not only enhances computational efficiency but also aligns naturally
with image generation principles by operating in continuous token space and
following a hierarchical generation process from coarse to fine details.
Experimental results demonstrate that ECAR achieves comparable image quality to
DiT Peebles & Xie [2023] while requiring 10$\times$ FLOPs reduction and
5$\times$ speedup to generate a 256$\times$256 image.
]]></content:encoded>
<pubDate>2024-12-18T18:59:53Z</pubDate>
</item>
<item>
<title>FashionComposer: Compositional Fashion Image Generation</title>
<link>http://arxiv.org/abs/2412.14168v1</link>
<guid>http://arxiv.org/abs/2412.14168v1</guid>
<content:encoded><![CDATA[
We present FashionComposer for compositional fashion image generation. Unlike
previous methods, FashionComposer is highly flexible. It takes multi-modal
input (i.e., text prompt, parametric human model, garment image, and face
image) and supports personalizing the appearance, pose, and figure of the human
and assigning multiple garments in one pass. To achieve this, we first develop
a universal framework capable of handling diverse input modalities. We
construct scaled training data to enhance the model's robust compositional
capabilities. To accommodate multiple reference images (garments and faces)
seamlessly, we organize these references in a single image as an "asset
library" and employ a reference UNet to extract appearance features. To inject
the appearance features into the correct pixels in the generated result, we
propose subject-binding attention. It binds the appearance features from
different "assets" with the corresponding text features. In this way, the model
could understand each asset according to their semantics, supporting arbitrary
numbers and types of reference images. As a comprehensive solution,
FashionComposer also supports many other applications like human album
generation, diverse virtual try-on tasks, etc.
]]></content:encoded>
<pubDate>2024-12-18T18:59:50Z</pubDate>
</item>
<item>
<title>VideoDPO: Omni-Preference Alignment for Video Diffusion Generation</title>
<link>http://arxiv.org/abs/2412.14167v1</link>
<guid>http://arxiv.org/abs/2412.14167v1</guid>
<content:encoded><![CDATA[
Recent progress in generative diffusion models has greatly advanced
text-to-video generation. While text-to-video models trained on large-scale,
diverse datasets can produce varied outputs, these generations often deviate
from user preferences, highlighting the need for preference alignment on
pre-trained models. Although Direct Preference Optimization (DPO) has
demonstrated significant improvements in language and image generation, we
pioneer its adaptation to video diffusion models and propose a VideoDPO
pipeline by making several key adjustments. Unlike previous image alignment
methods that focus solely on either (i) visual quality or (ii) semantic
alignment between text and videos, we comprehensively consider both dimensions
and construct a preference score accordingly, which we term the OmniScore. We
design a pipeline to automatically collect preference pair data based on the
proposed OmniScore and discover that re-weighting these pairs based on the
score significantly impacts overall preference alignment. Our experiments
demonstrate substantial improvements in both visual quality and semantic
alignment, ensuring that no preference aspect is neglected. Code and data will
be shared at https://videodpo.github.io/.
]]></content:encoded>
<pubDate>2024-12-18T18:59:49Z</pubDate>
</item>
<item>
<title>TheAgentCompany: Benchmarking LLM Agents on Consequential Real World
  Tasks</title>
<link>http://arxiv.org/abs/2412.14161v1</link>
<guid>http://arxiv.org/abs/2412.14161v1</guid>
<content:encoded><![CDATA[
We interact with computers on an everyday basis, be it in everyday life or
work, and many aspects of work can be done entirely with access to a computer
and the Internet. At the same time, thanks to improvements in large language
models (LLMs), there has also been a rapid development in AI agents that
interact with and affect change in their surrounding environments. But how
performant are AI agents at helping to accelerate or even autonomously perform
work-related tasks? The answer to this question has important implications for
both industry looking to adopt AI into their workflows, and for economic policy
to understand the effects that adoption of AI may have on the labor market. To
measure the progress of these LLM agents' performance on performing real-world
professional tasks, in this paper, we introduce TheAgentCompany, an extensible
benchmark for evaluating AI agents that interact with the world in similar ways
to those of a digital worker: by browsing the Web, writing code, running
programs, and communicating with other coworkers. We build a self-contained
environment with internal web sites and data that mimics a small software
company environment, and create a variety of tasks that may be performed by
workers in such a company. We test baseline agents powered by both closed
API-based and open-weights language models (LMs), and find that with the most
competitive agent, 24% of the tasks can be completed autonomously. This paints
a nuanced picture on task automation with LM agents -- in a setting simulating
a real workplace, a good portion of simpler tasks could be solved autonomously,
but more difficult long-horizon tasks are still beyond the reach of current
systems.
]]></content:encoded>
<pubDate>2024-12-18T18:55:40Z</pubDate>
</item>
<item>
<title>Proposer-Agent-Evaluator(PAE): Autonomous Skill Discovery For Foundation
  Model Internet Agents</title>
<link>http://arxiv.org/abs/2412.13194v1</link>
<guid>http://arxiv.org/abs/2412.13194v1</guid>
<content:encoded><![CDATA[
The vision of a broadly capable and goal-directed agent, such as an
Internet-browsing agent in the digital world and a household humanoid in the
physical world, has rapidly advanced, thanks to the generalization capability
of foundation models. Such a generalist agent needs to have a large and diverse
skill repertoire, such as finding directions between two travel locations and
buying specific items from the Internet. If each skill needs to be specified
manually through a fixed set of human-annotated instructions, the agent's skill
repertoire will necessarily be limited due to the quantity and diversity of
human-annotated instructions. In this work, we address this challenge by
proposing Proposer-Agent-Evaluator, an effective learning system that enables
foundation model agents to autonomously discover and practice skills in the
wild. At the heart of PAE is a context-aware task proposer that autonomously
proposes tasks for the agent to practice with context information of the
environment such as user demos or even just the name of the website itself for
Internet-browsing agents. Then, the agent policy attempts those tasks with
thoughts and actual grounded operations in the real world with resulting
trajectories evaluated by an autonomous VLM-based success evaluator. The
success evaluation serves as the reward signal for the agent to refine its
policies through RL. We validate PAE on challenging vision-based web
navigation, using both real-world and self-hosted websites from WebVoyager and
WebArena.To the best of our knowledge, this work represents the first effective
learning system to apply autonomous task proposal with RL for agents that
generalizes real-world human-annotated benchmarks with SOTA performances. Our
open-source checkpoints and code can be found in https://yanqval.github.io/PAE/
]]></content:encoded>
<pubDate>2024-12-17T18:59:50Z</pubDate>
</item>
<item>
<title>GaussTR: Foundation Model-Aligned Gaussian Transformer for
  Self-Supervised 3D Spatial Understanding</title>
<link>http://arxiv.org/abs/2412.13193v1</link>
<guid>http://arxiv.org/abs/2412.13193v1</guid>
<content:encoded><![CDATA[
3D Semantic Occupancy Prediction is fundamental for spatial understanding as
it provides a comprehensive semantic cognition of surrounding environments.
However, prevalent approaches primarily rely on extensive labeled data and
computationally intensive voxel-based modeling, restricting the scalability and
generalizability of 3D representation learning. In this paper, we introduce
GaussTR, a novel Gaussian Transformer that leverages alignment with foundation
models to advance self-supervised 3D spatial understanding. GaussTR adopts a
Transformer architecture to predict sparse sets of 3D Gaussians that represent
scenes in a feed-forward manner. Through aligning rendered Gaussian features
with diverse knowledge from pre-trained foundation models, GaussTR facilitates
the learning of versatile 3D representations and enables open-vocabulary
occupancy prediction without explicit annotations. Empirical evaluations on the
Occ3D-nuScenes dataset showcase GaussTR's state-of-the-art zero-shot
performance, achieving 11.70 mIoU while reducing training duration by
approximately 50%. These experimental results highlight the significant
potential of GaussTR for scalable and holistic 3D spatial understanding, with
promising implications for autonomous driving and embodied agents. Code is
available at https://github.com/hustvl/GaussTR.
]]></content:encoded>
<pubDate>2024-12-17T18:59:46Z</pubDate>
</item>
<item>
<title>MotionBridge: Dynamic Video Inbetweening with Flexible Controls</title>
<link>http://arxiv.org/abs/2412.13190v1</link>
<guid>http://arxiv.org/abs/2412.13190v1</guid>
<content:encoded><![CDATA[
By generating plausible and smooth transitions between two image frames,
video inbetweening is an essential tool for video editing and long video
synthesis. Traditional works lack the capability to generate complex large
motions. While recent video generation techniques are powerful in creating
high-quality results, they often lack fine control over the details of
intermediate frames, which can lead to results that do not align with the
creative mind. We introduce MotionBridge, a unified video inbetweening
framework that allows flexible controls, including trajectory strokes,
keyframes, masks, guide pixels, and text. However, learning such multi-modal
controls in a unified framework is a challenging task. We thus design two
generators to extract the control signal faithfully and encode feature through
dual-branch embedders to resolve ambiguities. We further introduce a curriculum
training strategy to smoothly learn various controls. Extensive qualitative and
quantitative experiments have demonstrated that such multi-modal controls
enable a more dynamic, customizable, and contextually accurate visual
narrative.
]]></content:encoded>
<pubDate>2024-12-17T18:59:33Z</pubDate>
</item>
<item>
<title>Instruction-based Image Manipulation by Watching How Things Move</title>
<link>http://arxiv.org/abs/2412.12087v1</link>
<guid>http://arxiv.org/abs/2412.12087v1</guid>
<content:encoded><![CDATA[
This paper introduces a novel dataset construction pipeline that samples
pairs of frames from videos and uses multimodal large language models (MLLMs)
to generate editing instructions for training instruction-based image
manipulation models. Video frames inherently preserve the identity of subjects
and scenes, ensuring consistent content preservation during editing.
Additionally, video data captures diverse, natural dynamics-such as non-rigid
subject motion and complex camera movements-that are difficult to model
otherwise, making it an ideal source for scalable dataset construction. Using
this approach, we create a new dataset to train InstructMove, a model capable
of instruction-based complex manipulations that are difficult to achieve with
synthetically generated datasets. Our model demonstrates state-of-the-art
performance in tasks such as adjusting subject poses, rearranging elements, and
altering camera perspectives.
]]></content:encoded>
<pubDate>2024-12-16T18:56:17Z</pubDate>
</item>
<item>
<title>Causal Diffusion Transformers for Generative Modeling</title>
<link>http://arxiv.org/abs/2412.12095v1</link>
<guid>http://arxiv.org/abs/2412.12095v1</guid>
<content:encoded><![CDATA[
We introduce Causal Diffusion as the autoregressive (AR) counterpart of
Diffusion models. It is a next-token(s) forecasting framework that is friendly
to both discrete and continuous modalities and compatible with existing
next-token prediction models like LLaMA and GPT. While recent works attempt to
combine diffusion with AR models, we show that introducing sequential
factorization to a diffusion model can substantially improve its performance
and enables a smooth transition between AR and diffusion generation modes.
Hence, we propose CausalFusion - a decoder-only transformer that
dual-factorizes data across sequential tokens and diffusion noise levels,
leading to state-of-the-art results on the ImageNet generation benchmark while
also enjoying the AR advantage of generating an arbitrary number of tokens for
in-context reasoning. We further demonstrate CausalFusion's multimodal
capabilities through a joint image generation and captioning model, and
showcase CausalFusion's ability for zero-shot in-context image manipulations.
We hope that this work could provide the community with a fresh perspective on
training multimodal models over discrete and continuous data.
]]></content:encoded>
<pubDate>2024-12-16T18:59:29Z</pubDate>
</item>
<item>
<title>A Grounded Typology of Word Classes</title>
<link>http://arxiv.org/abs/2412.10369v1</link>
<guid>http://arxiv.org/abs/2412.10369v1</guid>
<content:encoded><![CDATA[
We propose a grounded approach to meaning in language typology. We treat data
from perceptual modalities, such as images, as a language-agnostic
representation of meaning. Hence, we can quantify the function--form
relationship between images and captions across languages. Inspired by
information theory, we define "groundedness", an empirical measure of
contextual semantic contentfulness (formulated as a difference in surprisal)
which can be computed with multilingual multimodal language models. As a proof
of concept, we apply this measure to the typology of word classes. Our measure
captures the contentfulness asymmetry between functional (grammatical) and
lexical (content) classes across languages, but contradicts the view that
functional classes do not convey content. Moreover, we find universal trends in
the hierarchy of groundedness (e.g., nouns > adjectives > verbs), and show that
our measure partly correlates with psycholinguistic concreteness norms in
English. We release a dataset of groundedness scores for 30 languages. Our
results suggest that the grounded typology approach can provide quantitative
evidence about semantic function in language.
]]></content:encoded>
<pubDate>2024-12-13T18:58:48Z</pubDate>
</item>
<item>
<title>OP-LoRA: The Blessing of Dimensionality</title>
<link>http://arxiv.org/abs/2412.10362v1</link>
<guid>http://arxiv.org/abs/2412.10362v1</guid>
<content:encoded><![CDATA[
Low-rank adapters enable fine-tuning of large models with only a small number
of parameters, thus reducing storage costs and minimizing the risk of
catastrophic forgetting. However, they often pose optimization challenges, with
poor convergence. To overcome these challenges, we introduce an
over-parameterized approach that accelerates training without increasing
inference costs. This method reparameterizes low-rank adaptation by employing a
separate MLP and learned embedding for each layer. The learned embedding is
input to the MLP, which generates the adapter parameters. Such
overparamaterization has been shown to implicitly function as an adaptive
learning rate and momentum, accelerating optimization. At inference time, the
MLP can be discarded, leaving behind a standard low-rank adapter. To study the
effect of MLP overparameterization on a small yet difficult proxy task, we
implement it for matrix factorization, and find it achieves faster convergence
and lower final loss. Extending this approach to larger-scale tasks, we observe
consistent performance gains across domains. We achieve improvements in
vision-language tasks and especially notable increases in image generation,
with CMMD scores improving by up to 15 points.
]]></content:encoded>
<pubDate>2024-12-13T18:55:19Z</pubDate>
</item>
<item>
<title>Doe-1: Closed-Loop Autonomous Driving with Large World Model</title>
<link>http://arxiv.org/abs/2412.09627v1</link>
<guid>http://arxiv.org/abs/2412.09627v1</guid>
<content:encoded><![CDATA[
End-to-end autonomous driving has received increasing attention due to its
potential to learn from large amounts of data. However, most existing methods
are still open-loop and suffer from weak scalability, lack of high-order
interactions, and inefficient decision-making. In this paper, we explore a
closed-loop framework for autonomous driving and propose a large Driving wOrld
modEl (Doe-1) for unified perception, prediction, and planning. We formulate
autonomous driving as a next-token generation problem and use multi-modal
tokens to accomplish different tasks. Specifically, we use free-form texts
(i.e., scene descriptions) for perception and generate future predictions
directly in the RGB space with image tokens. For planning, we employ a
position-aware tokenizer to effectively encode action into discrete tokens. We
train a multi-modal transformer to autoregressively generate perception,
prediction, and planning tokens in an end-to-end and unified manner.
Experiments on the widely used nuScenes dataset demonstrate the effectiveness
of Doe-1 in various tasks including visual question-answering,
action-conditioned video generation, and motion planning. Code:
https://github.com/wzzheng/Doe.
]]></content:encoded>
<pubDate>2024-12-12T18:59:59Z</pubDate>
</item>
<item>
<title>GenEx: Generating an Explorable World</title>
<link>http://arxiv.org/abs/2412.09624v1</link>
<guid>http://arxiv.org/abs/2412.09624v1</guid>
<content:encoded><![CDATA[
Understanding, navigating, and exploring the 3D physical real world has long
been a central challenge in the development of artificial intelligence. In this
work, we take a step toward this goal by introducing GenEx, a system capable of
planning complex embodied world exploration, guided by its generative
imagination that forms priors (expectations) about the surrounding
environments. GenEx generates an entire 3D-consistent imaginative environment
from as little as a single RGB image, bringing it to life through panoramic
video streams. Leveraging scalable 3D world data curated from Unreal Engine,
our generative model is rounded in the physical world. It captures a continuous
360-degree environment with little effort, offering a boundless landscape for
AI agents to explore and interact with. GenEx achieves high-quality world
generation, robust loop consistency over long trajectories, and demonstrates
strong 3D capabilities such as consistency and active 3D mapping. Powered by
generative imagination of the world, GPT-assisted agents are equipped to
perform complex embodied tasks, including both goal-agnostic exploration and
goal-driven navigation. These agents utilize predictive expectation regarding
unseen parts of the physical world to refine their beliefs, simulate different
outcomes based on potential decisions, and make more informed choices. In
summary, we demonstrate that GenEx provides a transformative platform for
advancing embodied AI in imaginative spaces and brings potential for extending
these capabilities to real-world exploration.
]]></content:encoded>
<pubDate>2024-12-12T18:59:57Z</pubDate>
</item>
<item>
<title>OmniDrag: Enabling Motion Control for Omnidirectional Image-to-Video
  Generation</title>
<link>http://arxiv.org/abs/2412.09623v1</link>
<guid>http://arxiv.org/abs/2412.09623v1</guid>
<content:encoded><![CDATA[
As virtual reality gains popularity, the demand for controllable creation of
immersive and dynamic omnidirectional videos (ODVs) is increasing. While
previous text-to-ODV generation methods achieve impressive results, they
struggle with content inaccuracies and inconsistencies due to reliance solely
on textual inputs. Although recent motion control techniques provide
fine-grained control for video generation, directly applying these methods to
ODVs often results in spatial distortion and unsatisfactory performance,
especially with complex spherical motions. To tackle these challenges, we
propose OmniDrag, the first approach enabling both scene- and object-level
motion control for accurate, high-quality omnidirectional image-to-video
generation. Building on pretrained video diffusion models, we introduce an
omnidirectional control module, which is jointly fine-tuned with temporal
attention layers to effectively handle complex spherical motion. In addition,
we develop a novel spherical motion estimator that accurately extracts
motion-control signals and allows users to perform drag-style ODV generation by
simply drawing handle and target points. We also present a new dataset, named
Move360, addressing the scarcity of ODV data with large scene and object
motions. Experiments demonstrate the significant superiority of OmniDrag in
achieving holistic scene-level and fine-grained object-level control for ODV
generation. The project page is available at
https://lwq20020127.github.io/OmniDrag.
]]></content:encoded>
<pubDate>2024-12-12T18:59:56Z</pubDate>
</item>
<item>
<title>LoRACLR: Contrastive Adaptation for Customization of Diffusion Models</title>
<link>http://arxiv.org/abs/2412.09622v1</link>
<guid>http://arxiv.org/abs/2412.09622v1</guid>
<content:encoded><![CDATA[
Recent advances in text-to-image customization have enabled high-fidelity,
context-rich generation of personalized images, allowing specific concepts to
appear in a variety of scenarios. However, current methods struggle with
combining multiple personalized models, often leading to attribute entanglement
or requiring separate training to preserve concept distinctiveness. We present
LoRACLR, a novel approach for multi-concept image generation that merges
multiple LoRA models, each fine-tuned for a distinct concept, into a single,
unified model without additional individual fine-tuning. LoRACLR uses a
contrastive objective to align and merge the weight spaces of these models,
ensuring compatibility while minimizing interference. By enforcing distinct yet
cohesive representations for each concept, LoRACLR enables efficient, scalable
model composition for high-quality, multi-concept image synthesis. Our results
highlight the effectiveness of LoRACLR in accurately merging multiple concepts,
advancing the capabilities of personalized image generation.
]]></content:encoded>
<pubDate>2024-12-12T18:59:55Z</pubDate>
</item>
<item>
<title>EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via
  Multimodal LLM</title>
<link>http://arxiv.org/abs/2412.09618v1</link>
<guid>http://arxiv.org/abs/2412.09618v1</guid>
<content:encoded><![CDATA[
Significant achievements in personalization of diffusion models have been
witnessed. Conventional tuning-free methods mostly encode multiple reference
images by averaging their image embeddings as the injection condition, but such
an image-independent operation cannot perform interaction among images to
capture consistent visual elements within multiple references. Although the
tuning-based Low-Rank Adaptation (LoRA) can effectively extract consistent
elements within multiple images through the training process, it necessitates
specific finetuning for each distinct image group. This paper introduces
EasyRef, a novel plug-and-play adaptation method that enables diffusion models
to be conditioned on multiple reference images and the text prompt. To
effectively exploit consistent visual elements within multiple images, we
leverage the multi-image comprehension and instruction-following capabilities
of the multimodal large language model (MLLM), prompting it to capture
consistent visual elements based on the instruction. Besides, injecting the
MLLM's representations into the diffusion process through adapters can easily
generalize to unseen domains, mining the consistent visual elements within
unseen data. To mitigate computational costs and enhance fine-grained detail
preservation, we introduce an efficient reference aggregation strategy and a
progressive training scheme. Finally, we introduce MRBench, a new
multi-reference image generation benchmark. Experimental results demonstrate
EasyRef surpasses both tuning-free methods like IP-Adapter and tuning-based
methods like LoRA, achieving superior aesthetic quality and robust zero-shot
generalization across diverse domains.
]]></content:encoded>
<pubDate>2024-12-12T18:59:48Z</pubDate>
</item>
<item>
<title>GPD-1: Generative Pre-training for Driving</title>
<link>http://arxiv.org/abs/2412.08643v1</link>
<guid>http://arxiv.org/abs/2412.08643v1</guid>
<content:encoded><![CDATA[
Modeling the evolutions of driving scenarios is important for the evaluation
and decision-making of autonomous driving systems. Most existing methods focus
on one aspect of scene evolution such as map generation, motion prediction, and
trajectory planning. In this paper, we propose a unified Generative
Pre-training for Driving (GPD-1) model to accomplish all these tasks altogether
without additional fine-tuning. We represent each scene with ego, agent, and
map tokens and formulate autonomous driving as a unified token generation
problem. We adopt the autoregressive transformer architecture and use a
scene-level attention mask to enable intra-scene bi-directional interactions.
For the ego and agent tokens, we propose a hierarchical positional tokenizer to
effectively encode both 2D positions and headings. For the map tokens, we train
a map vector-quantized autoencoder to efficiently compress ego-centric semantic
maps into discrete tokens. We pre-train our GPD-1 on the large-scale nuPlan
dataset and conduct extensive experiments to evaluate its effectiveness. With
different prompts, our GPD-1 successfully generalizes to various tasks without
finetuning, including scene generation, traffic simulation, closed-loop
simulation, map prediction, and motion planning. Code:
https://github.com/wzzheng/GPD.
]]></content:encoded>
<pubDate>2024-12-11T18:59:51Z</pubDate>
</item>
<item>
<title>Generative Semantic Communication: Architectures, Technologies, and
  Applications</title>
<link>http://arxiv.org/abs/2412.08642v1</link>
<guid>http://arxiv.org/abs/2412.08642v1</guid>
<content:encoded><![CDATA[
This paper delves into the applications of generative artificial intelligence
(GAI) in semantic communication (SemCom) and presents a thorough study. Three
popular SemCom systems enabled by classical GAI models are first introduced,
including variational autoencoders, generative adversarial networks, and
diffusion models. For each system, the fundamental concept of the GAI model,
the corresponding SemCom architecture, and the associated literature review of
recent efforts are elucidated. Then, a novel generative SemCom system is
proposed by incorporating the cutting-edge GAI technology-large language models
(LLMs). This system features two LLM-based AI agents at both the transmitter
and receiver, serving as "brains" to enable powerful information understanding
and content regeneration capabilities, respectively. This innovative design
allows the receiver to directly generate the desired content, instead of
recovering the bit stream, based on the coded semantic information conveyed by
the transmitter. Therefore, it shifts the communication mindset from
"information recovery" to "information regeneration" and thus ushers in a new
era of generative SemCom. A case study on point-to-point video retrieval is
presented to demonstrate the superiority of the proposed generative SemCom
system, showcasing a 99.98% reduction in communication overhead and a 53%
improvement in retrieval accuracy compared to the traditional communication
system. Furthermore, four typical application scenarios for generative SemCom
are delineated, followed by a discussion of three open issues warranting future
investigation. In a nutshell, this paper provides a holistic set of guidelines
for applying GAI in SemCom, paving the way for the efficient implementation of
generative SemCom in future wireless networks.
]]></content:encoded>
<pubDate>2024-12-11T18:59:50Z</pubDate>
</item>
<item>
<title>Fast Prompt Alignment for Text-to-Image Generation</title>
<link>http://arxiv.org/abs/2412.08639v1</link>
<guid>http://arxiv.org/abs/2412.08639v1</guid>
<content:encoded><![CDATA[
Text-to-image generation has advanced rapidly, yet aligning complex textual
prompts with generated visuals remains challenging, especially with intricate
object relationships and fine-grained details. This paper introduces Fast
Prompt Alignment (FPA), a prompt optimization framework that leverages a
one-pass approach, enhancing text-to-image alignment efficiency without the
iterative overhead typical of current methods like OPT2I. FPA uses large
language models (LLMs) for single-iteration prompt paraphrasing, followed by
fine-tuning or in-context learning with optimized prompts to enable real-time
inference, reducing computational demands while preserving alignment fidelity.
Extensive evaluations on the COCO Captions and PartiPrompts datasets
demonstrate that FPA achieves competitive text-image alignment scores at a
fraction of the processing time, as validated through both automated metrics
(TIFA, VQA) and human evaluation. A human study with expert annotators further
reveals a strong correlation between human alignment judgments and automated
scores, underscoring the robustness of FPA's improvements. The proposed method
showcases a scalable, efficient alternative to iterative prompt optimization,
enabling broader applicability in real-time, high-demand settings. The codebase
is provided to facilitate further research:
https://github.com/tiktok/fast_prompt_alignment
]]></content:encoded>
<pubDate>2024-12-11T18:58:41Z</pubDate>
</item>
<item>
<title>UniReal: Universal Image Generation and Editing via Learning Real-world
  Dynamics</title>
<link>http://arxiv.org/abs/2412.07774v1</link>
<guid>http://arxiv.org/abs/2412.07774v1</guid>
<content:encoded><![CDATA[
We introduce UniReal, a unified framework designed to address various image
generation and editing tasks. Existing solutions often vary by tasks, yet share
fundamental principles: preserving consistency between inputs and outputs while
capturing visual variations. Inspired by recent video generation models that
effectively balance consistency and variation across frames, we propose a
unifying approach that treats image-level tasks as discontinuous video
generation. Specifically, we treat varying numbers of input and output images
as frames, enabling seamless support for tasks such as image generation,
editing, customization, composition, etc. Although designed for image-level
tasks, we leverage videos as a scalable source for universal supervision.
UniReal learns world dynamics from large-scale videos, demonstrating advanced
capability in handling shadows, reflections, pose variation, and object
interaction, while also exhibiting emergent capability for novel applications.
]]></content:encoded>
<pubDate>2024-12-10T18:59:55Z</pubDate>
</item>
<item>
<title>BiMediX2: Bio-Medical EXpert LMM for Diverse Medical Modalities</title>
<link>http://arxiv.org/abs/2412.07769v1</link>
<guid>http://arxiv.org/abs/2412.07769v1</guid>
<content:encoded><![CDATA[
This paper introduces BiMediX2, a bilingual (Arabic-English) Bio-Medical
EXpert Large Multimodal Model (LMM) with a unified architecture that integrates
text and visual modalities, enabling advanced image understanding and medical
applications. BiMediX2 leverages the Llama3.1 architecture and integrates text
and visual capabilities to facilitate seamless interactions in both English and
Arabic, supporting text-based inputs and multi-turn conversations involving
medical images. The model is trained on an extensive bilingual healthcare
dataset consisting of 1.6M samples of diverse medical interactions for both
text and image modalities, mixed in Arabic and English. We also propose the
first bilingual GPT-4o based medical LMM benchmark named BiMed-MBench. BiMediX2
is benchmarked on both text-based and image-based tasks, achieving
state-of-the-art performance across several medical benchmarks. It outperforms
recent state-of-the-art models in medical LLM evaluation benchmarks. Our model
also sets a new benchmark in multimodal medical evaluations with over 9%
improvement in English and over 20% in Arabic evaluations. Additionally, it
surpasses GPT-4 by around 9% in UPHILL factual accuracy evaluations and excels
in various medical Visual Question Answering, Report Generation, and Report
Summarization tasks. The project page including source code and the trained
model, is available at https://github.com/mbzuai-oryx/BiMediX2.
]]></content:encoded>
<pubDate>2024-12-10T18:59:35Z</pubDate>
</item>
<item>
<title>Tactile DreamFusion: Exploiting Tactile Sensing for 3D Generation</title>
<link>http://arxiv.org/abs/2412.06785v1</link>
<guid>http://arxiv.org/abs/2412.06785v1</guid>
<content:encoded><![CDATA[
3D generation methods have shown visually compelling results powered by
diffusion image priors. However, they often fail to produce realistic geometric
details, resulting in overly smooth surfaces or geometric details inaccurately
baked in albedo maps. To address this, we introduce a new method that
incorporates touch as an additional modality to improve the geometric details
of generated 3D assets. We design a lightweight 3D texture field to synthesize
visual and tactile textures, guided by 2D diffusion model priors on both visual
and tactile domains. We condition the visual texture generation on
high-resolution tactile normals and guide the patch-based tactile texture
refinement with a customized TextureDreambooth. We further present a multi-part
generation pipeline that enables us to synthesize different textures across
various regions. To our knowledge, we are the first to leverage high-resolution
tactile sensing to enhance geometric details for 3D generation tasks. We
evaluate our method in both text-to-3D and image-to-3D settings. Our
experiments demonstrate that our method provides customized and realistic fine
geometric textures while maintaining accurate alignment between two modalities
of vision and touch.
]]></content:encoded>
<pubDate>2024-12-09T18:59:45Z</pubDate>
</item>
<item>
<title>Stag-1: Towards Realistic 4D Driving Simulation with Video Generation
  Model</title>
<link>http://arxiv.org/abs/2412.05280v1</link>
<guid>http://arxiv.org/abs/2412.05280v1</guid>
<content:encoded><![CDATA[
4D driving simulation is essential for developing realistic autonomous
driving simulators. Despite advancements in existing methods for generating
driving scenes, significant challenges remain in view transformation and
spatial-temporal dynamic modeling. To address these limitations, we propose a
Spatial-Temporal simulAtion for drivinG (Stag-1) model to reconstruct
real-world scenes and design a controllable generative network to achieve 4D
simulation. Stag-1 constructs continuous 4D point cloud scenes using
surround-view data from autonomous vehicles. It decouples spatial-temporal
relationships and produces coherent keyframe videos. Additionally, Stag-1
leverages video generation models to obtain photo-realistic and controllable 4D
driving simulation videos from any perspective. To expand the range of view
generation, we train vehicle motion videos based on decomposed camera poses,
enhancing modeling capabilities for distant scenes. Furthermore, we reconstruct
vehicle camera trajectories to integrate 3D points across consecutive views,
enabling comprehensive scene understanding along the temporal dimension.
Following extensive multi-level scene training, Stag-1 can simulate from any
desired viewpoint and achieve a deep understanding of scene evolution under
static spatial-temporal conditions. Compared to existing methods, our approach
shows promising performance in multi-view scene consistency, background
coherence, and accuracy, and contributes to the ongoing advancements in
realistic autonomous driving simulation. Code: https://github.com/wzzheng/Stag.
]]></content:encoded>
<pubDate>2024-12-06T18:59:56Z</pubDate>
</item>
<item>
<title>Text to Blind Motion</title>
<link>http://arxiv.org/abs/2412.05277v1</link>
<guid>http://arxiv.org/abs/2412.05277v1</guid>
<content:encoded><![CDATA[
People who are blind perceive the world differently than those who are
sighted, which can result in distinct motion characteristics. For instance,
when crossing at an intersection, blind individuals may have different patterns
of movement, such as veering more from a straight path or using touch-based
exploration around curbs and obstacles. These behaviors may appear less
predictable to motion models embedded in technologies such as autonomous
vehicles. Yet, the ability of 3D motion models to capture such behavior has not
been previously studied, as existing datasets for 3D human motion currently
lack diversity and are biased toward people who are sighted. In this work, we
introduce BlindWays, the first multimodal motion benchmark for pedestrians who
are blind. We collect 3D motion data using wearable sensors with 11 blind
participants navigating eight different routes in a real-world urban setting.
Additionally, we provide rich textual descriptions that capture the distinctive
movement characteristics of blind pedestrians and their interactions with both
the navigation aid (e.g., a white cane or a guide dog) and the environment. We
benchmark state-of-the-art 3D human prediction models, finding poor performance
with off-the-shelf and pre-training-based methods for our novel task. To
contribute toward safer and more reliable systems that can seamlessly reason
over diverse human movements in their environments, our text-and-motion
benchmark is available at https://blindways.github.io.
]]></content:encoded>
<pubDate>2024-12-06T18:59:51Z</pubDate>
</item>
<item>
<title>Expanding Performance Boundaries of Open-Source Multimodal Models with
  Model, Data, and Test-Time Scaling</title>
<link>http://arxiv.org/abs/2412.05271v1</link>
<guid>http://arxiv.org/abs/2412.05271v1</guid>
<content:encoded><![CDATA[
We introduce InternVL 2.5, an advanced multimodal large language model (MLLM)
series that builds upon InternVL 2.0, maintaining its core model architecture
while introducing significant enhancements in training and testing strategies
as well as data quality. In this work, we delve into the relationship between
model scaling and performance, systematically exploring the performance trends
in vision encoders, language models, dataset sizes, and test-time
configurations. Through extensive evaluations on a wide range of benchmarks,
including multi-discipline reasoning, document understanding, multi-image /
video understanding, real-world comprehension, multimodal hallucination
detection, visual grounding, multilingual capabilities, and pure language
processing, InternVL 2.5 exhibits competitive performance, rivaling leading
commercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model is
the first open-source MLLMs to surpass 70% on the MMMU benchmark, achieving a
3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasing
strong potential for test-time scaling. We hope this model contributes to the
open-source community by setting new standards for developing and applying
multimodal AI systems. HuggingFace demo see
https://huggingface.co/spaces/OpenGVLab/InternVL
]]></content:encoded>
<pubDate>2024-12-06T18:57:08Z</pubDate>
</item>
<item>
<title>PaintScene4D: Consistent 4D Scene Generation from Text Prompts</title>
<link>http://arxiv.org/abs/2412.04471v1</link>
<guid>http://arxiv.org/abs/2412.04471v1</guid>
<content:encoded><![CDATA[
Recent advances in diffusion models have revolutionized 2D and 3D content
creation, yet generating photorealistic dynamic 4D scenes remains a significant
challenge. Existing dynamic 4D generation methods typically rely on distilling
knowledge from pre-trained 3D generative models, often fine-tuned on synthetic
object datasets. Consequently, the resulting scenes tend to be object-centric
and lack photorealism. While text-to-video models can generate more realistic
scenes with motion, they often struggle with spatial understanding and provide
limited control over camera viewpoints during rendering. To address these
limitations, we present PaintScene4D, a novel text-to-4D scene generation
framework that departs from conventional multi-view generative models in favor
of a streamlined architecture that harnesses video generative models trained on
diverse real-world datasets. Our method first generates a reference video using
a video generation model, and then employs a strategic camera array selection
for rendering. We apply a progressive warping and inpainting technique to
ensure both spatial and temporal consistency across multiple viewpoints.
Finally, we optimize multi-view images using a dynamic renderer, enabling
flexible camera control based on user preferences. Adopting a training-free
architecture, our PaintScene4D efficiently produces realistic 4D scenes that
can be viewed from arbitrary trajectories. The code will be made publicly
available. Our project page is at https://paintscene4d.github.io/
]]></content:encoded>
<pubDate>2024-12-05T18:59:57Z</pubDate>
</item>
<item>
<title>Turbo3D: Ultra-fast Text-to-3D Generation</title>
<link>http://arxiv.org/abs/2412.04470v1</link>
<guid>http://arxiv.org/abs/2412.04470v1</guid>
<content:encoded><![CDATA[
We present Turbo3D, an ultra-fast text-to-3D system capable of generating
high-quality Gaussian splatting assets in under one second. Turbo3D employs a
rapid 4-step, 4-view diffusion generator and an efficient feed-forward Gaussian
reconstructor, both operating in latent space. The 4-step, 4-view generator is
a student model distilled through a novel Dual-Teacher approach, which
encourages the student to learn view consistency from a multi-view teacher and
photo-realism from a single-view teacher. By shifting the Gaussian
reconstructor's inputs from pixel space to latent space, we eliminate the extra
image decoding time and halve the transformer sequence length for maximum
efficiency. Our method demonstrates superior 3D generation results compared to
previous baselines, while operating in a fraction of their runtime.
]]></content:encoded>
<pubDate>2024-12-05T18:59:56Z</pubDate>
</item>
<item>
<title>Navigation World Models</title>
<link>http://arxiv.org/abs/2412.03572v1</link>
<guid>http://arxiv.org/abs/2412.03572v1</guid>
<content:encoded><![CDATA[
Navigation is a fundamental skill of agents with visual-motor capabilities.
We introduce a Navigation World Model (NWM), a controllable video generation
model that predicts future visual observations based on past observations and
navigation actions. To capture complex environment dynamics, NWM employs a
Conditional Diffusion Transformer (CDiT), trained on a diverse collection of
egocentric videos of both human and robotic agents, and scaled up to 1 billion
parameters. In familiar environments, NWM can plan navigation trajectories by
simulating them and evaluating whether they achieve the desired goal. Unlike
supervised navigation policies with fixed behavior, NWM can dynamically
incorporate constraints during planning. Experiments demonstrate its
effectiveness in planning trajectories from scratch or by ranking trajectories
sampled from an external policy. Furthermore, NWM leverages its learned visual
priors to imagine trajectories in unfamiliar environments from a single input
image, making it a flexible and powerful tool for next-generation navigation
systems.
]]></content:encoded>
<pubDate>2024-12-04T18:59:45Z</pubDate>
</item>
<item>
<title>Streaming Detection of Queried Event Start</title>
<link>http://arxiv.org/abs/2412.03567v1</link>
<guid>http://arxiv.org/abs/2412.03567v1</guid>
<content:encoded><![CDATA[
Robotics, autonomous driving, augmented reality, and many embodied computer
vision applications must quickly react to user-defined events unfolding in real
time. We address this setting by proposing a novel task for multimodal video
understanding-Streaming Detection of Queried Event Start (SDQES). The goal of
SDQES is to identify the beginning of a complex event as described by a natural
language query, with high accuracy and low latency. We introduce a new
benchmark based on the Ego4D dataset, as well as new task-specific metrics to
study streaming multimodal detection of diverse events in an egocentric video
setting. Inspired by parameter-efficient fine-tuning methods in NLP and for
video tasks, we propose adapter-based baselines that enable image-to-video
transfer learning, allowing for efficient online video modeling. We evaluate
three vision-language backbones and three adapter architectures on both
short-clip and untrimmed video settings.
]]></content:encoded>
<pubDate>2024-12-04T18:58:27Z</pubDate>
</item>
<item>
<title>Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual
  Prompt Instruction Tuning</title>
<link>http://arxiv.org/abs/2412.03565v1</link>
<guid>http://arxiv.org/abs/2412.03565v1</guid>
<content:encoded><![CDATA[
Large Multimodal Models (LMMs) have made significant breakthroughs with the
advancement of instruction tuning. However, while existing models can
understand images and videos at a holistic level, they still struggle with
instance-level understanding that requires a more nuanced comprehension and
alignment. Instance-level understanding is crucial, as it focuses on the
specific elements that we are most interested in. Excitingly, existing works
find that the state-of-the-art LMMs exhibit strong instance understanding
capabilities when provided with explicit visual cues. Motivated by this, we
introduce an automated annotation pipeline assisted by GPT-4o to extract
instance-level information from images and videos through explicit visual
prompting for instance guidance. Building upon this pipeline, we proposed
Inst-IT, a solution to enhance LMMs in Instance understanding via explicit
visual prompt Instruction Tuning. Inst-IT consists of a benchmark to diagnose
multimodal instance-level understanding, a large-scale instruction-tuning
dataset, and a continuous instruction-tuning training paradigm to effectively
enhance spatial-temporal instance understanding capabilities of existing LMMs.
Experimental results show that, with the boost of Inst-IT, our models not only
achieve outstanding performance on Inst-IT Bench but also demonstrate
significant improvements across various generic image and video understanding
benchmarks. This highlights that our dataset not only boosts instance-level
understanding but also strengthens the overall capabilities of generic image
and video comprehension.
]]></content:encoded>
<pubDate>2024-12-04T18:58:10Z</pubDate>
</item>
<item>
<title>From Individual to Society: A Survey on Social Simulation Driven by
  Large Language Model-based Agents</title>
<link>http://arxiv.org/abs/2412.03563v1</link>
<guid>http://arxiv.org/abs/2412.03563v1</guid>
<content:encoded><![CDATA[
Traditional sociological research often relies on human participation, which,
though effective, is expensive, challenging to scale, and with ethical
concerns. Recent advancements in large language models (LLMs) highlight their
potential to simulate human behavior, enabling the replication of individual
responses and facilitating studies on many interdisciplinary studies. In this
paper, we conduct a comprehensive survey of this field, illustrating the recent
progress in simulation driven by LLM-empowered agents. We categorize the
simulations into three types: (1) Individual Simulation, which mimics specific
individuals or demographic groups; (2) Scenario Simulation, where multiple
agents collaborate to achieve goals within specific contexts; and (3) Society
Simulation, which models interactions within agent societies to reflect the
complexity and variety of real-world dynamics. These simulations follow a
progression, ranging from detailed individual modeling to large-scale societal
phenomena. We provide a detailed discussion of each simulation type, including
the architecture or key components of the simulation, the classification of
objectives or scenarios and the evaluation method. Afterward, we summarize
commonly used datasets and benchmarks. Finally, we discuss the trends across
these three types of simulation. A repository for the related sources is at
{\url{https://github.com/FudanDISC/SocialAgent}}.
]]></content:encoded>
<pubDate>2024-12-04T18:56:37Z</pubDate>
</item>
<item>
<title>FLAIR: VLM with Fine-grained Language-informed Image Representations</title>
<link>http://arxiv.org/abs/2412.03561v1</link>
<guid>http://arxiv.org/abs/2412.03561v1</guid>
<content:encoded><![CDATA[
CLIP has shown impressive results in aligning images and texts at scale.
However, its ability to capture detailed visual features remains limited
because CLIP matches images and texts at a global level. To address this issue,
we propose FLAIR, Fine-grained Language-informed Image Representations, an
approach that utilizes long and detailed image descriptions to learn localized
image embeddings. By sampling diverse sub-captions that describe fine-grained
details about an image, we train our vision-language model to produce not only
global embeddings but also text-specific image representations. Our model
introduces text-conditioned attention pooling on top of local image tokens to
produce fine-grained image representations that excel at retrieving detailed
image content. We achieve state-of-the-art performance on both, existing
multimodal retrieval benchmarks, as well as, our newly introduced fine-grained
retrieval task which evaluates vision-language models' ability to retrieve
partial image content. Furthermore, our experiments demonstrate the
effectiveness of FLAIR trained on 30M image-text pairs in capturing
fine-grained visual information, including zero-shot semantic segmentation,
outperforming models trained on billions of pairs. Code is available at
https://github.com/ExplainableML/flair .
]]></content:encoded>
<pubDate>2024-12-04T18:56:04Z</pubDate>
</item>
<item>
<title>Motion Prompting: Controlling Video Generation with Motion Trajectories</title>
<link>http://arxiv.org/abs/2412.02700v1</link>
<guid>http://arxiv.org/abs/2412.02700v1</guid>
<content:encoded><![CDATA[
Motion control is crucial for generating expressive and compelling video
content; however, most existing video generation models rely mainly on text
prompts for control, which struggle to capture the nuances of dynamic actions
and temporal compositions. To this end, we train a video generation model
conditioned on spatio-temporally sparse or dense motion trajectories. In
contrast to prior motion conditioning work, this flexible representation can
encode any number of trajectories, object-specific or global scene motion, and
temporally sparse motion; due to its flexibility we refer to this conditioning
as motion prompts. While users may directly specify sparse trajectories, we
also show how to translate high-level user requests into detailed, semi-dense
motion prompts, a process we term motion prompt expansion. We demonstrate the
versatility of our approach through various applications, including camera and
object motion control, "interacting" with an image, motion transfer, and image
editing. Our results showcase emergent behaviors, such as realistic physics,
suggesting the potential of motion prompts for probing video models and
interacting with future generative world models. Finally, we evaluate
quantitatively, conduct a human study, and demonstrate strong performance.
Video results are available on our webpage: https://motion-prompting.github.io/
]]></content:encoded>
<pubDate>2024-12-03T18:59:56Z</pubDate>
</item>
<item>
<title>FoundHand: Large-Scale Domain-Specific Learning for Controllable Hand
  Image Generation</title>
<link>http://arxiv.org/abs/2412.02690v1</link>
<guid>http://arxiv.org/abs/2412.02690v1</guid>
<content:encoded><![CDATA[
Despite remarkable progress in image generation models, generating realistic
hands remains a persistent challenge due to their complex articulation, varying
viewpoints, and frequent occlusions. We present FoundHand, a large-scale
domain-specific diffusion model for synthesizing single and dual hand images.
To train our model, we introduce FoundHand-10M, a large-scale hand dataset with
2D keypoints and segmentation mask annotations. Our insight is to use 2D hand
keypoints as a universal representation that encodes both hand articulation and
camera viewpoint. FoundHand learns from image pairs to capture physically
plausible hand articulations, natively enables precise control through 2D
keypoints, and supports appearance control. Our model exhibits core
capabilities that include the ability to repose hands, transfer hand
appearance, and even synthesize novel views. This leads to zero-shot
capabilities for fixing malformed hands in previously generated images, or
synthesizing hand video sequences. We present extensive experiments and
evaluations that demonstrate state-of-the-art performance of our method.
]]></content:encoded>
<pubDate>2024-12-03T18:58:19Z</pubDate>
</item>
<item>
<title>VLSBench: Unveiling Visual Leakage in Multimodal Safety</title>
<link>http://arxiv.org/abs/2411.19939v1</link>
<guid>http://arxiv.org/abs/2411.19939v1</guid>
<content:encoded><![CDATA[
Safety concerns of Multimodal large language models (MLLMs) have gradually
become an important problem in various applications. Surprisingly, previous
works indicate a counter-intuitive phenomenon that using textual unlearning to
align MLLMs achieves comparable safety performances with MLLMs trained with
image-text pairs. To explain such a counter-intuitive phenomenon, we discover a
visual safety information leakage (VSIL) problem in existing multimodal safety
benchmarks, i.e., the potentially risky and sensitive content in the image has
been revealed in the textual query. In this way, MLLMs can easily refuse these
sensitive text-image queries according to textual queries. However, image-text
pairs without VSIL are common in real-world scenarios and are overlooked by
existing multimodal safety benchmarks. To this end, we construct multimodal
visual leakless safety benchmark (VLSBench) preventing visual safety leakage
from image to textual query with 2.4k image-text pairs. Experimental results
indicate that VLSBench poses a significant challenge to both open-source and
close-source MLLMs, including LLaVA, Qwen2-VL, Llama3.2-Vision, and GPT-4o.
This study demonstrates that textual alignment is enough for multimodal safety
scenarios with VSIL, while multimodal alignment is a more promising solution
for multimodal safety scenarios without VSIL. Please see our code and data at:
http://hxhcreate.github.io/VLSBench
]]></content:encoded>
<pubDate>2024-11-29T18:56:37Z</pubDate>
</item>
<item>
<title>On Domain-Specific Post-Training for Multimodal Large Language Models</title>
<link>http://arxiv.org/abs/2411.19930v1</link>
<guid>http://arxiv.org/abs/2411.19930v1</guid>
<content:encoded><![CDATA[
Recent years have witnessed the rapid development of general multimodal large
language models (MLLMs). However, adapting general MLLMs to specific domains,
such as scientific fields and industrial applications, remains less explored.
This paper systematically investigates domain adaptation of MLLMs through
post-training, focusing on data synthesis, training pipelines, and task
evaluation. (1) Data Synthesis: Using open-source models, we develop a visual
instruction synthesizer that effectively generates diverse visual instruction
tasks from domain-specific image-caption pairs. Our synthetic tasks surpass
those generated by manual rules, GPT-4, and GPT-4V in enhancing the
domain-specific performance of MLLMs. (2) Training Pipeline: While the
two-stage training--initially on image-caption pairs followed by visual
instruction tasks--is commonly adopted for developing general MLLMs, we apply a
single-stage training pipeline to enhance task diversity for domain-specific
post-training. (3) Task Evaluation: We conduct experiments in two domains,
biomedicine and food, by post-training MLLMs of different sources and scales
(e.g., Qwen2-VL-2B, LLaVA-v1.6-8B, Llama-3.2-11B), and then evaluating MLLM
performance on various domain-specific tasks. To support further research in
MLLM domain adaptation, we will open-source our implementations.
]]></content:encoded>
<pubDate>2024-11-29T18:42:28Z</pubDate>
</item>
<item>
<title>Cross-modal Information Flow in Multimodal Large Language Models</title>
<link>http://arxiv.org/abs/2411.18620v1</link>
<guid>http://arxiv.org/abs/2411.18620v1</guid>
<content:encoded><![CDATA[
<div> 模态语言模型、视觉问题回答、信息流、多模态信息定位、中文总结
<br />
本研究针对多模态大型语言模型中语言和视觉信息交互的工作机制进行了探讨，以视觉问题回答为例进行了实验。实验发现在模型的不同层次存在着视觉和语言信息整合的两个明显阶段。首先，在较低层次，模型将整个图像的更一般的视觉特征转化为（语言）问题标记的表示。其次，在中间层，模型再次将与问题相关的特定对象的视觉信息转移到问题的各个标记位置。最终，在较高层次，多模态表示被传播到输入序列的最后一个位置进行最终预测。总体而言，本研究为我们提供了对多模态语言模型中图像和语言处理的空间和功能层面的全新视角，从而有助于未来对多模态信息定位和编辑的研究。 
<br /><br />总结: 模型在不同层次存在着视觉和语言信息整合的两个明显阶段，首先将整个图像的更一般的视觉特征转化为（语言）问题标记的表示，其次再次将与问题相关的特定对象的视觉信息转移到问题的各个标记位置，最终多模态表示被传播到输入序列的最后一个位置进行最终预测。 <div>
The recent advancements in auto-regressive multimodal large language models
(MLLMs) have demonstrated promising progress for vision-language tasks. While
there exists a variety of studies investigating the processing of linguistic
information within large language models, little is currently known about the
inner working mechanism of MLLMs and how linguistic and visual information
interact within these models. In this study, we aim to fill this gap by
examining the information flow between different modalities -- language and
vision -- in MLLMs, focusing on visual question answering. Specifically, given
an image-question pair as input, we investigate where in the model and how the
visual and linguistic information are combined to generate the final
prediction. Conducting experiments with a series of models from the LLaVA
series, we find that there are two distinct stages in the process of
integration of the two modalities. In the lower layers, the model first
transfers the more general visual features of the whole image into the
representations of (linguistic) question tokens. In the middle layers, it once
again transfers visual information about specific objects relevant to the
question to the respective token positions of the question. Finally, in the
higher layers, the resulting multimodal representation is propagated to the
last position of the input sequence for the final prediction. Overall, our
findings provide a new and comprehensive perspective on the spatial and
functional aspects of image and language processing in the MLLMs, thereby
facilitating future research into multimodal information localization and
editing.
]]></content:encoded>
<pubDate>2024-11-27T18:59:26Z</pubDate>
</item>
<item>
<title>Proactive Gradient Conflict Mitigation in Multi-Task Learning: A Sparse
  Training Perspective</title>
<link>http://arxiv.org/abs/2411.18615v1</link>
<guid>http://arxiv.org/abs/2411.18615v1</guid>
<content:encoded><![CDATA[
<div> 多任务学习, 梯度冲突, 稀疏训练, 模型参数, 性能提升
<br /><br />
本文探讨了多任务学习中梯度冲突的问题，并提出了一种稀疏训练（ST）的方法来减轻这种冲突。研究表明，稀疏训练可以有效地减少冲突梯度，并提高性能。此外，稀疏训练还可以与梯度操纵技术相结合，进一步提高效果。通过对模型参数进行部分更新，稀疏训练可以在多任务学习中取得良好的效果，解决了梯度冲突问题，提高了模型性能。总结: 多任务学习中存在梯度冲突问题，通过稀疏训练可以有效减轻梯度冲突，并提高模型性能。 <div>
Advancing towards generalist agents necessitates the concurrent processing of
multiple tasks using a unified model, thereby underscoring the growing
significance of simultaneous model training on multiple downstream tasks. A
common issue in multi-task learning is the occurrence of gradient conflict,
which leads to potential competition among different tasks during joint
training. This competition often results in improvements in one task at the
expense of deterioration in another. Although several optimization methods have
been developed to address this issue by manipulating task gradients for better
task balancing, they cannot decrease the incidence of gradient conflict. In
this paper, we systematically investigate the occurrence of gradient conflict
across different methods and propose a strategy to reduce such conflicts
through sparse training (ST), wherein only a portion of the model's parameters
are updated during training while keeping the rest unchanged. Our extensive
experiments demonstrate that ST effectively mitigates conflicting gradients and
leads to superior performance. Furthermore, ST can be easily integrated with
gradient manipulation techniques, thus enhancing their effectiveness.
]]></content:encoded>
<pubDate>2024-11-27T18:58:22Z</pubDate>
</item>
<item>
<title>Video-Guided Foley Sound Generation with Multimodal Controls</title>
<link>http://arxiv.org/abs/2411.17698v1</link>
<guid>http://arxiv.org/abs/2411.17698v1</guid>
<content:encoded><![CDATA[
<div> 视频，声音，生成模型，MultiFoley，多模态<br />
总结：<br />
这篇文章介绍了MultiFoley，这是一个设计用来进行视频引导声音生成的模型。它支持通过文本、音频和视频进行多模态条件设定。用户可以使用文本提示和无声视频来创建干净的声音，也可以进行更奇特的声音创作。MultiFoley还允许用户从音效库或局部视频中选择参考音频来进行条件设定。该模型的一个关键创新之处在于其同时在互联网视频数据集和专业音效记录上进行训练，实现了高质量、全带宽（48kHz）的音频生成。通过自动评估和人类研究，我们证明了MultiFoley能够成功地在不同条件输入下生成同步的高质量声音，并且优于现有方法。 <div>
Generating sound effects for videos often requires creating artistic sound
effects that diverge significantly from real-life sources and flexible control
in the sound design. To address this problem, we introduce MultiFoley, a model
designed for video-guided sound generation that supports multimodal
conditioning through text, audio, and video. Given a silent video and a text
prompt, MultiFoley allows users to create clean sounds (e.g., skateboard wheels
spinning without wind noise) or more whimsical sounds (e.g., making a lion's
roar sound like a cat's meow). MultiFoley also allows users to choose reference
audio from sound effects (SFX) libraries or partial videos for conditioning. A
key novelty of our model lies in its joint training on both internet video
datasets with low-quality audio and professional SFX recordings, enabling
high-quality, full-bandwidth (48kHz) audio generation. Through automated
evaluations and human studies, we demonstrate that MultiFoley successfully
generates synchronized high-quality sounds across varied conditional inputs and
outperforms existing methods. Please see our project page for video results:
https://ificl.github.io/MultiFoley/
]]></content:encoded>
<pubDate>2024-11-26T18:59:58Z</pubDate>
</item>
<item>
<title>StableAnimator: High-Quality Identity-Preserving Human Image Animation</title>
<link>http://arxiv.org/abs/2411.17697v1</link>
<guid>http://arxiv.org/abs/2411.17697v1</guid>
<content:encoded><![CDATA[
<div> 视频动画；身份一致性；StableAnimator；图像嵌入；面部质量改进

总结:
StableAnimator是第一个端到端的视频扩散框架，旨在保持身份一致性。该框架使用参考图像和一系列姿势来合成高质量的视频，无需任何后期处理。它通过使用图像和面部嵌入并结合全局内容感知的面部编码器来开始处理，然后引入一种新颖的分布感知的身份适配器，可以避免时间层次造成的干扰，同时通过对齐来保持身份一致性。在推断过程中，提出一种基于Hamilton-Jacobi-Bellman（HJB）方程的优化技术，进一步增强面部质量。实验证明，StableAnimator在多个基准测试上在质量和数量上都非常有效。<br /><br /> <div>
Current diffusion models for human image animation struggle to ensure
identity (ID) consistency. This paper presents StableAnimator, the first
end-to-end ID-preserving video diffusion framework, which synthesizes
high-quality videos without any post-processing, conditioned on a reference
image and a sequence of poses. Building upon a video diffusion model,
StableAnimator contains carefully designed modules for both training and
inference striving for identity consistency. In particular, StableAnimator
begins by computing image and face embeddings with off-the-shelf extractors,
respectively and face embeddings are further refined by interacting with image
embeddings using a global content-aware Face Encoder. Then, StableAnimator
introduces a novel distribution-aware ID Adapter that prevents interference
caused by temporal layers while preserving ID via alignment. During inference,
we propose a novel Hamilton-Jacobi-Bellman (HJB) equation-based optimization to
further enhance the face quality. We demonstrate that solving the HJB equation
can be integrated into the diffusion denoising process, and the resulting
solution constrains the denoising path and thus benefits ID preservation.
Experiments on multiple benchmarks show the effectiveness of StableAnimator
both qualitatively and quantitatively.
]]></content:encoded>
<pubDate>2024-11-26T18:59:22Z</pubDate>
</item>
<item>
<title>Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis</title>
<link>http://arxiv.org/abs/2411.17690v1</link>
<guid>http://arxiv.org/abs/2411.17690v1</guid>
<content:encoded><![CDATA[
In this paper, we propose a new task -- generating speech from videos of
people and their transcripts (VTTS) -- to motivate new techniques for
multimodal speech generation. This task generalizes the task of generating
speech from cropped lip videos, and is also more complicated than the task of
generating generic audio clips (e.g., dog barking) from videos and text.
Multilingual versions of the task could lead to new techniques for
cross-lingual dubbing. We also present a decoder-only multimodal model for this
task, which we call Visatronic. This model embeds vision, text and speech
directly into the common subspace of a transformer model and uses an
autoregressive loss to learn a generative model of discretized mel-spectrograms
conditioned on speaker videos and transcripts of their speech. By embedding all
modalities into a common subspace, Visatronic can achieve improved results over
models that use only text or video as input. Further, it presents a much
simpler approach for multimodal speech generation compared to prevailing
approaches which rely on lip-detectors and complicated architectures to fuse
modalities while producing better results. Since the model is flexible enough
to accommodate different ways of ordering inputs as a sequence, we carefully
explore different strategies to better understand the best way to propagate
information to the generative steps. To facilitate further research on VTTS, we
will release (i) our code, (ii) clean transcriptions for the large-scale
VoxCeleb2 dataset, and (iii) a standardized evaluation protocol for VTTS
incorporating both objective and subjective metrics.
]]></content:encoded>
<pubDate>2024-11-26T18:57:29Z</pubDate>
</item>
<item>
<title>Factorized Visual Tokenization and Generation</title>
<link>http://arxiv.org/abs/2411.16681v1</link>
<guid>http://arxiv.org/abs/2411.16681v1</guid>
<content:encoded><![CDATA[
<div> VQ-based tokenizers, image generation, Factorized Quantization, disentanglement regularization, FQGAN<br />
<br />
VQ-based tokenizers在图像生成中起着重要作用，但由于词汇表大小的限制，存在挑战。该研究提出了Factorized Quantization (FQ)的方法，将大的词汇表分解为多个独立的子词汇表，以提高查找复杂度。同时，引入了离散化正则化，减少了冗余信息，促进子词汇表之间的多样性。在训练过程中结合了表示学习，利用预训练的视觉模型，如CLIP和DINO，丰富了学习表示中的语义信息。实验证明，FQGAN模型大大提高了视觉tokenizer的重构质量，达到了最先进的性能。这种tokenizer还可以有效地应用于自回归图像生成。 <br /><br />总结: <br />提出了Factorized Quantization (FQ)新方法，将大的词汇表分解为多个独立的子词汇表，引入离散化正则化，结合表示学习，实验证明FQGAN模型大大提高了视觉tokenizer的性能。 <div>
Visual tokenizers are fundamental to image generation. They convert visual
data into discrete tokens, enabling transformer-based models to excel at image
generation. Despite their success, VQ-based tokenizers like VQGAN face
significant limitations due to constrained vocabulary sizes. Simply expanding
the codebook often leads to training instability and diminishing performance
gains, making scalability a critical challenge. In this work, we introduce
Factorized Quantization (FQ), a novel approach that revitalizes VQ-based
tokenizers by decomposing a large codebook into multiple independent
sub-codebooks. This factorization reduces the lookup complexity of large
codebooks, enabling more efficient and scalable visual tokenization. To ensure
each sub-codebook captures distinct and complementary information, we propose a
disentanglement regularization that explicitly reduces redundancy, promoting
diversity across the sub-codebooks. Furthermore, we integrate representation
learning into the training process, leveraging pretrained vision models like
CLIP and DINO to infuse semantic richness into the learned representations.
This design ensures our tokenizer captures diverse semantic levels, leading to
more expressive and disentangled representations. Experiments show that the
proposed FQGAN model substantially improves the reconstruction quality of
visual tokenizers, achieving state-of-the-art performance. We further
demonstrate that this tokenizer can be effectively adapted into auto-regressive
image generation. https://showlab.github.io/FQGAN
]]></content:encoded>
<pubDate>2024-11-25T18:59:53Z</pubDate>
</item>
<item>
<title>Learning-based Trajectory Tracking for Bird-inspired Flapping-Wing
  Robots</title>
<link>http://arxiv.org/abs/2411.15130v1</link>
<guid>http://arxiv.org/abs/2411.15130v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 飞行机器人, 学习控制, 多模式飞行, RL 框架, 稳定性分析
总结:
本文介绍了一种基于学习的控制方法，用于实现鸟类飞行机器人的多模式飞行和灵活轨迹跟踪。通过模型无关的强化学习框架，可以实现高自由度的鸟类飞行机器人的灵活飞行和轨迹跟踪。对闭环系统进行了稳定性分析，并通过仿真结果展示了RL控制器成功学习复杂的翅膀轨迹模式、实现稳定飞行、自发切换飞行模式以及在不同空气动力条件下跟踪不同轨迹的能力。 <div>
Bird-sized flapping-wing robots offer significant potential for agile flight
in complex environments, but achieving agile and robust trajectory tracking
remains a challenge due to the complex aerodynamics and highly nonlinear
dynamics inherent in flapping-wing flight. In this work, a learning-based
control approach is introduced to unlock the versatility and adaptiveness of
flapping-wing flight. We propose a model-free reinforcement learning (RL)-based
framework for a high degree-of-freedom (DoF) bird-inspired flapping-wing robot
that allows for multimodal flight and agile trajectory tracking. Stability
analysis was performed on the closed-loop system comprising of the
flapping-wing system and the RL policy. Additionally, simulation results
demonstrate that the RL-based controller can successfully learn complex wing
trajectory patterns, achieve stable flight, switch between flight modes
spontaneously, and track different trajectories under various aerodynamic
conditions.
]]></content:encoded>
<pubDate>2024-11-22T18:55:37Z</pubDate>
</item>
<item>
<title>PRIMUS: Pretraining IMU Encoders with Multimodal Self-Supervision</title>
<link>http://arxiv.org/abs/2411.15127v1</link>
<guid>http://arxiv.org/abs/2411.15127v1</guid>
<content:encoded><![CDATA[
<div> 关键词: IMU, 预训练, PRIMUS, 自监督学习, 多模态学习
总结:
本文针对使用惯性测量单元（IMU）进行人体运动监测的问题进行了研究。作者提出了一种名为PRIMUS的新方法，用于对IMU进行预训练。研究发现，通过结合自监督学习、多模态监督学习和最近邻监督学习，PRIMUS可以显著提高IMU数据在特定任务中的表现。在公开测试数据中，使用PRIMUS方法，即使每类样本少于500个标记样本，也可以将下游性能提高多达15％，相比之下，目前最先进的多模态训练方法表现。为了造福广大研究人员，作者将在文章发布后在github.com/nokia-bell-labs公开分享他们的代码和预训练的IMU编码器。 <div>
Sensing human motions through Inertial Measurement Units (IMUs) embedded in
personal devices has enabled significant applications in health and wellness.
While labeled IMU data is scarce, we can collect unlabeled or weakly labeled
IMU data to model human motions. For video or text modalities, the "pretrain
and adapt" approach utilizes large volumes of unlabeled or weakly labeled data
for pretraining, building a strong feature extractor, followed by adaptation to
specific tasks using limited labeled data. This approach has not been widely
adopted in the IMU domain for two reasons: (1) pretraining methods are poorly
understood in the context of IMU, and (2) open-source pretrained models that
generalize across datasets are rarely publicly available. In this paper, we aim
to address the first issue by proposing PRIMUS, a method for PRetraining IMU
encoderS. We conduct a systematic and unified evaluation of various
self-supervised and multimodal learning pretraining objectives. Our findings
indicate that using PRIMUS, which combines self-supervision, multimodal
supervision, and nearest-neighbor supervision, can significantly enhance
downstream performance. With fewer than 500 labeled samples per class, PRIMUS
effectively enhances downstream performance by up to 15% in held-out test data,
compared to the state-of-the-art multimodal training method. To benefit the
broader community, our code and pre-trained IMU encoders will be made publicly
available at github.com/nokia-bell-labs upon publication.
]]></content:encoded>
<pubDate>2024-11-22T18:46:30Z</pubDate>
</item>
<item>
<title>Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large
  Language Models</title>
<link>http://arxiv.org/abs/2411.14432v1</link>
<guid>http://arxiv.org/abs/2411.14432v1</guid>
<content:encoded><![CDATA[
<div> 生成数据, 训练管道, 多模态大型语言模型, 长链推理, 视觉推理

Insight-V通过设计一个两步流水线来生成足够长且多样化的推理路径，并采用多粒度评估方法来确保数据质量，以扩展对复杂多模态任务的理解能力。其次，设计了一个多代理系统，包括一个专门执行长链推理的推理代理和一个训练来评判和总结推理结果的摘要代理。最后，结合迭代式DPO算法来增强推理代理的生成稳定性和质量。 Insight-V在挑战性的需要视觉推理的多模态基准测试中取得了显著的性能提升，同时也能轻松在以感知为重点的多模态任务中保持或提高性能。 <br /><br />总结: <br />Insight-V通过产生长链推理数据和优化训练管道来增强多模态大型语言模型的推理能力，取得了显著的性能提升。 <div>
Large Language Models (LLMs) demonstrate enhanced capabilities and
reliability by reasoning more, evolving from Chain-of-Thought prompting to
product-level solutions like OpenAI o1. Despite various efforts to improve LLM
reasoning, high-quality long-chain reasoning data and optimized training
pipelines still remain inadequately explored in vision-language tasks. In this
paper, we present Insight-V, an early effort to 1) scalably produce long and
robust reasoning data for complex multi-modal tasks, and 2) an effective
training pipeline to enhance the reasoning capabilities of multi-modal large
language models (MLLMs). Specifically, to create long and structured reasoning
data without human labor, we design a two-step pipeline with a progressive
strategy to generate sufficiently long and diverse reasoning paths and a
multi-granularity assessment method to ensure data quality. We observe that
directly supervising MLLMs with such long and complex reasoning data will not
yield ideal reasoning ability. To tackle this problem, we design a multi-agent
system consisting of a reasoning agent dedicated to performing long-chain
reasoning and a summary agent trained to judge and summarize reasoning results.
We further incorporate an iterative DPO algorithm to enhance the reasoning
agent's generation stability and quality. Based on the popular LLaVA-NeXT model
and our stronger base MLLM, we demonstrate significant performance gains across
challenging multi-modal benchmarks requiring visual reasoning. Benefiting from
our multi-agent system, Insight-V can also easily maintain or improve
performance on perception-focused multi-modal tasks.
]]></content:encoded>
<pubDate>2024-11-21T18:59:55Z</pubDate>
</item>
<item>
<title>REDUCIO! Generating 1024$\times$1024 Video within 16 Seconds using
  Extremely Compressed Motion Latents</title>
<link>http://arxiv.org/abs/2411.13552v1</link>
<guid>http://arxiv.org/abs/2411.13552v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 商业视频生成模型, 视频编码, VAE, 运动潜在特征, 视频生成<br />
<br />
商业视频生成模型表现出逼真、高保真的结果，但仍受到访问限制。一个关键的障碍是昂贵的训练和推断成本。本文认为，视频中包含的冗余信息远远多于图像，因此可以基于内容图像，用极少的运动潜在特征对视频进行编码。为实现这一目标，我们设计了一种图像条件的VAE，将视频编码为一个极度压缩的运动潜在空间。这种"Reducio"魔法使得潜在空间的维度比普通的2D VAE降低了64倍，同时不损失质量。在这种紧凑的表示上训练扩散模型轻松实现生成1K分辨率的视频。然后，我们采用了一个两阶段视频生成范例，依次执行文本到图像和图像到视频。大量实验证明，我们的Reducio-DiT在评估中表现出色，尽管使用的GPU资源有限。更重要的是，我们的方法显著提高了视频LDMs在训练和推断中的效率。我们总共花费了大约3.2K小时来训练Reducio-DiT，并在单个A100 GPU上在15.5秒内生成了一个16帧1024*1024的视频剪辑。代码已发布在https://github.com/microsoft/Reducio-VAE。 <br /><br />总结: 商业视频生成模型面临昂贵的训练和推断成本，本文提出了使用图像条件的VAE对视频进行极度压缩的运动潜在特征编码，大幅提高了视频LDMs的效率。 <div>
Commercial video generation models have exhibited realistic, high-fidelity
results but are still restricted to limited access. One crucial obstacle for
large-scale applications is the expensive training and inference cost. In this
paper, we argue that videos contain much more redundant information than
images, thus can be encoded by very few motion latents based on a content
image. Towards this goal, we design an image-conditioned VAE to encode a video
to an extremely compressed motion latent space. This magic Reducio charm
enables 64x reduction of latents compared to a common 2D VAE, without
sacrificing the quality. Training diffusion models on such a compact
representation easily allows for generating 1K resolution videos. We then adopt
a two-stage video generation paradigm, which performs text-to-image and
text-image-to-video sequentially. Extensive experiments show that our
Reducio-DiT achieves strong performance in evaluation, though trained with
limited GPU resources. More importantly, our method significantly boost the
efficiency of video LDMs both in training and inference. We train Reducio-DiT
in around 3.2K training hours in total and generate a 16-frame 1024*1024 video
clip within 15.5 seconds on a single A100 GPU. Code released at
https://github.com/microsoft/Reducio-VAE .
]]></content:encoded>
<pubDate>2024-11-20T18:59:52Z</pubDate>
</item>
<item>
<title>BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games</title>
<link>http://arxiv.org/abs/2411.13543v1</link>
<guid>http://arxiv.org/abs/2411.13543v1</guid>
<content:encoded><![CDATA[
<div> 语言模型，视觉语言模型，挑战游戏，评估方法，BALROG
<br /><br />
1. 本文介绍了语言模型和视觉语言模型在处理复杂动态环境中的困难。
2. 介绍了BALROG，一个旨在通过一系列具有挑战性的游戏评估语言模型和视觉语言模型的能力的基准。
3. BALROG包括多种难度不同的强化学习环境，旨在评估模型在各种任务上的表现。
4. 文中指出了当前模型在简单游戏中取得部分成功，但在更具挑战性的任务中遇到了严重的困难。
5. 特别强调了模型在基于视觉环境的决策制定方面的严重缺陷。
<br /><br />总结:
本文介绍了语言模型和视觉语言模型在处理复杂动态环境中的困难，并介绍了BALROG基准。该基准通过一系列具有挑战性的游戏评估模型的能力，包括各种难度的强化学习环境。研究发现当前模型在简单游戏中取得部分成功，但在更具挑战性的任务中遇到了严重的困难，尤其是在基于视觉环境的决策制定方面。 <div>
Large Language Models (LLMs) and Vision Language Models (VLMs) possess
extensive knowledge and exhibit promising reasoning abilities; however, they
still struggle to perform well in complex, dynamic environments. Real-world
tasks require handling intricate interactions, advanced spatial reasoning,
long-term planning, and continuous exploration of new strategies-areas in which
we lack effective methodologies for comprehensively evaluating these
capabilities. To address this gap, we introduce BALROG, a novel benchmark
designed to assess the agentic capabilities of LLMs and VLMs through a diverse
set of challenging games. Our benchmark incorporates a range of existing
reinforcement learning environments with varying levels of difficulty,
including tasks that are solvable by non-expert humans in seconds to extremely
challenging ones that may take years to master (e.g., the NetHack Learning
Environment). We devise fine-grained metrics to measure performance and conduct
an extensive evaluation of several popular open-source and closed-source LLMs
and VLMs. Our findings indicate that while current models achieve partial
success in the easier games, they struggle significantly with more challenging
tasks. Notably, we observe severe deficiencies in vision-based decision-making,
as models perform worse when visual representations of the environments are
provided. We release BALROG as an open and user-friendly benchmark to
facilitate future research and development in the agentic community.
]]></content:encoded>
<pubDate>2024-11-20T18:54:32Z</pubDate>
</item>
<item>
<title>Reinforcement Learning, Collusion, and the Folk Theorem</title>
<link>http://arxiv.org/abs/2411.12725v1</link>
<guid>http://arxiv.org/abs/2411.12725v1</guid>
<content:encoded><![CDATA[
<div> 监督学习,博弈论,学习动态,算法勾结,重复博弈

这篇文章研究了学习代理在博弈中重复相互作用时产生的行为。研究包括了梯度下降、复制动力学和对数障碍动力学等多种学习动态。除了已经较为了解的潜在博弈和零和博弈外，还考虑了有限召回的一般重复博弈设定，针对不同形式的监控。研究获得了类似民间定理的结果，并对可以通过这些动力学获得的收益向量进行了描述，发现了算法勾结出现的各种可能性。 <br /><br />总结: 监督学习、博弈论、学习动态、算法勾结、重复博弈。 <div>
We explore the behaviour emerging from learning agents repeatedly interacting
strategically for a wide range of learning dynamics that includes projected
gradient, replicator and log-barrier dynamics. Going beyond the
better-understood classes of potential games and zero-sum games, we consider
the setting of a general repeated game with finite recall, for different forms
of monitoring. We obtain a Folk Theorem-like result and characterise the set of
payoff vectors that can be obtained by these dynamics, discovering a wide range
of possibilities for the emergence of algorithmic collusion.
]]></content:encoded>
<pubDate>2024-11-19T18:45:55Z</pubDate>
</item>
<item>
<title>Generative World Explorer</title>
<link>http://arxiv.org/abs/2411.11844v1</link>
<guid>http://arxiv.org/abs/2411.11844v1</guid>
<content:encoded><![CDATA[
<div> 探索、计划、部分观察、生成式环境模拟、决策<br />
<br />
本文介绍了一项名为Generative World Explorer (Genex)的框架，该框架允许代理通过内在的世界探索获取想象中的观察，并通过这些观察更新信念，从而做出更明智的决策。为了训练Genex，作者们创建了一个名为Genex-DB的合成城市场景数据集。实验结果表明，Genex能够在大规模虚拟物理世界的长期探索过程中生成高质量、一致的观察，并且通过这些观察更新的信念可以帮助现有的决策模型做出更好的计划。<br /><br />总结: 本文介绍了一个能够实现部分观察计划的新框架Genex，通过内在的世界探索生成观察，并更新信念辅助决策。实验结果证明了Genex的有效性。 <div>
Planning with partial observation is a central challenge in embodied AI. A
majority of prior works have tackled this challenge by developing agents that
physically explore their environment to update their beliefs about the world
state.In contrast, humans can $\textit{imagine}$ unseen parts of the world
through a mental exploration and $\textit{revise}$ their beliefs with imagined
observations. Such updated beliefs can allow them to make more informed
decisions, without necessitating the physical exploration of the world at all
times. To achieve this human-like ability, we introduce the $\textit{Generative
World Explorer (Genex)}$, an egocentric world exploration framework that allows
an agent to mentally explore a large-scale 3D world (e.g., urban scenes) and
acquire imagined observations to update its belief. This updated belief will
then help the agent to make a more informed decision at the current step. To
train $\textit{Genex}$, we create a synthetic urban scene dataset, Genex-DB.
Our experimental results demonstrate that (1) $\textit{Genex}$ can generate
high-quality and consistent observations during long-horizon exploration of a
large virtual physical world and (2) the beliefs updated with the generated
observations can inform an existing decision-making model (e.g., an LLM agent)
to make better plans.
]]></content:encoded>
<pubDate>2024-11-18T18:59:31Z</pubDate>
</item>
<item>
<title>Enhancing the Reasoning Ability of Multimodal Large Language Models via
  Mixed Preference Optimization</title>
<link>http://arxiv.org/abs/2411.10442v1</link>
<guid>http://arxiv.org/abs/2411.10442v1</guid>
<content:encoded><![CDATA[
<div> 多模态语言模型、性能优化、数据构建管线、模型集成、改进性能
<br /><br />总结:
本研究引入了偏好优化（PO）流程，以增强多模态大型语言模型（MLLMs）的多模态推理能力。在数据方面，他们设计了自动化偏好数据构建管线，创建了高质量的大规模多模态推理偏好数据集MMPR。在模型方面，他们探索将偏好优化与MLLMs集成，开发了一种简单而有效的方法，称为混合偏好优化（MPO），提高了多模态推理性能。他们的方法在多个基准测试中表现出了改进的性能，特别是在多模态推理任务中。值得注意的是，他们的模型InternVL2-8B-MPO在MathVista上取得了67.0的准确率，在性能上超过了InternVL2-8B的8.7个点，并且与规模更大的InternVL2-76B相当。他们希望这项研究能激发MLLMs的进一步发展。他们将代码、数据和模型公开发布。 <div>
Existing open-source multimodal large language models (MLLMs) generally
follow a training process involving pre-training and supervised fine-tuning.
However, these models suffer from distribution shifts, which limit their
multimodal reasoning, particularly in the Chain-of-Thought (CoT) performance.
To address this, we introduce a preference optimization (PO) process to enhance
the multimodal reasoning capabilities of MLLMs. Specifically, (1) on the data
side, we design an automated preference data construction pipeline to create
MMPR, a high-quality, large-scale multimodal reasoning preference dataset. and
(2) on the model side, we explore integrating PO with MLLMs, developing a
simple yet effective method, termed Mixed Preference Optimization (MPO), which
boosts multimodal CoT performance. Our approach demonstrates improved
performance across multiple benchmarks, particularly in multimodal reasoning
tasks. Notably, our model, InternVL2-8B-MPO, achieves an accuracy of 67.0 on
MathVista, outperforming InternVL2-8B by 8.7 points and achieving performance
comparable to the 10x larger InternVL2-76B. We hope this study could inspire
further advancements in MLLMs. Code, data, and model shall be publicly
released.
]]></content:encoded>
<pubDate>2024-11-15T18:59:27Z</pubDate>
</item>
<item>
<title>LLaVA-o1: Let Vision Language Models Reason Step-by-Step</title>
<link>http://arxiv.org/abs/2411.10440v1</link>
<guid>http://arxiv.org/abs/2411.10440v1</guid>
<content:encoded><![CDATA[
<div> 自动多阶段推理、LLaVA-o1、结构化推理、数据集、推理时间缩放

LLaVA-o1是一种新型的视觉语言模型，旨在进行自主的多阶段推理。与思维链提示不同，LLaVA-o1独立进行摘要、视觉解释、逻辑推理和结论生成的连续阶段。这种结构化方法使LLaVA-o1在推理密集型任务中取得了显著的改进，同时，LLaVA-o1-100k数据集将来自各种视觉问答来源的样本整合，并提供了结构化推理注释。此外，作者提出了一种推理时间级别的波束搜索方法，实现了有效的推理时间缩放。令人瞩目的是，仅使用100k训练样本和简单而有效的推理时间缩放方法，LLaVA-o1不仅在各种多模态推理基准上比其基础模型提高了8.9%，而且还超越了较大甚至是闭源模型的性能，如Gemini-1.5-pro、GPT-4o-mini 和Llama-3.2-90B-Vision-Instruct。 <br /><br />总结: 这篇文章介绍了LLaVA-o1，一个具有结构化推理能力的新型视觉语言模型。通过多阶段推理和推理时间缩放，LLaVA-o1取得了不错的性能表现，在多模态推理任务上优于其他模型。 <div>
Large language models have demonstrated substantial advancements in reasoning
capabilities, particularly through inference-time scaling, as illustrated by
models such as OpenAI's o1. However, current Vision-Language Models (VLMs)
often struggle to perform systematic and structured reasoning, especially when
handling complex visual question-answering tasks. In this work, we introduce
LLaVA-o1, a novel VLM designed to conduct autonomous multistage reasoning.
Unlike chain-of-thought prompting, LLaVA-o1 independently engages in sequential
stages of summarization, visual interpretation, logical reasoning, and
conclusion generation. This structured approach enables LLaVA-o1 to achieve
marked improvements in precision on reasoning-intensive tasks. To accomplish
this, we compile the LLaVA-o1-100k dataset, integrating samples from various
visual question answering sources and providing structured reasoning
annotations. Besides, we propose an inference-time stage-level beam search
method, which enables effective inference-time scaling. Remarkably, with only
100k training samples and a simple yet effective inference time scaling method,
LLaVA-o1 not only outperforms its base model by 8.9% on a wide range of
multimodal reasoning benchmarks, but also surpasses the performance of larger
and even closed-source models, such as Gemini-1.5-pro, GPT-4o-mini, and
Llama-3.2-90B-Vision-Instruct.
]]></content:encoded>
<pubDate>2024-11-15T18:58:31Z</pubDate>
</item>
<item>
<title>Mitigating Hallucination in Multimodal Large Language Model via
  Hallucination-targeted Direct Preference Optimization</title>
<link>http://arxiv.org/abs/2411.10436v1</link>
<guid>http://arxiv.org/abs/2411.10436v1</guid>
<content:encoded><![CDATA[
Multimodal Large Language Models (MLLMs) are known to hallucinate, which
limits their practical applications. Recent works have attempted to apply
Direct Preference Optimization (DPO) to enhance the performance of MLLMs, but
have shown inconsistent improvements in mitigating hallucinations. To address
this issue more effectively, we introduce Hallucination-targeted Direct
Preference Optimization (HDPO) to reduce hallucinations in MLLMs. Unlike
previous approaches, our method tackles hallucinations from their diverse forms
and causes. Specifically, we develop three types of preference pair data
targeting the following causes of MLLM hallucinations: (1) insufficient visual
capabilities, (2) long context generation, and (3) multimodal conflicts.
Experimental results demonstrate that our method achieves superior performance
across multiple hallucination evaluation datasets, surpassing most
state-of-the-art (SOTA) methods and highlighting the potential of our approach.
Ablation studies and in-depth analyses further confirm the effectiveness of our
method and suggest the potential for further improvements through scaling up.
]]></content:encoded>
<pubDate>2024-11-15T18:56:01Z</pubDate>
</item>
<item>
<title>Fair Division via the Cake-Cutting Share</title>
<link>http://arxiv.org/abs/2411.10434v1</link>
<guid>http://arxiv.org/abs/2411.10434v1</guid>
<content:encoded><![CDATA[
In this paper, we consider the classic fair division problem of allocating
$m$ divisible items to $n$ agents with linear valuations over the items. We
define novel notions of fair shares from the perspective of individual agents
via the cake-cutting process. These shares generalize the notion of
proportionality by taking into account the valuations of other agents via
constraints capturing envy. We study what fraction (approximation) of these
shares are achievable in the worst case, and present tight and non-trivial
approximation bounds as a function of $n$ and $m$. In particular, we show a
tight approximation bound of $\Theta(\sqrt{n})$ for various notions of such
shares. We show this bound via a novel application of dual fitting, which may
be of independent interest. We also present a bound of $O(m^{2/3})$ for a
strict notion of share, with an almost matching lower bound. We further develop
weaker notions of shares whose approximation bounds interpolate smoothly
between proportionality and the shares described above. We finally present
empirical results showing that our definitions lead to more reasonable shares
than the standard fair share notion of proportionality.
]]></content:encoded>
<pubDate>2024-11-15T18:55:01Z</pubDate>
</item>
<item>
<title>M-VAR: Decoupled Scale-wise Autoregressive Modeling for High-Quality
  Image Generation</title>
<link>http://arxiv.org/abs/2411.10433v1</link>
<guid>http://arxiv.org/abs/2411.10433v1</guid>
<content:encoded><![CDATA[
There exists recent work in computer vision, named VAR, that proposes a new
autoregressive paradigm for image generation. Diverging from the vanilla
next-token prediction, VAR structurally reformulates the image generation into
a coarse to fine next-scale prediction. In this paper, we show that this
scale-wise autoregressive framework can be effectively decoupled into
\textit{intra-scale modeling}, which captures local spatial dependencies within
each scale, and \textit{inter-scale modeling}, which models cross-scale
relationships progressively from coarse-to-fine scales. This decoupling
structure allows to rebuild VAR in a more computationally efficient manner.
Specifically, for intra-scale modeling -- crucial for generating high-fidelity
images -- we retain the original bidirectional self-attention design to ensure
comprehensive modeling; for inter-scale modeling, which semantically connects
different scales but is computationally intensive, we apply linear-complexity
mechanisms like Mamba to substantially reduce computational overhead. We term
this new framework M-VAR. Extensive experiments demonstrate that our method
outperforms existing models in both image quality and generation speed. For
example, our 1.5B model, with fewer parameters and faster inference speed,
outperforms the largest VAR-d30-2B. Moreover, our largest model M-VAR-d32
impressively registers 1.78 FID on ImageNet 256$\times$256 and outperforms the
prior-art autoregressive models LlamaGen/VAR by 0.4/0.19 and popular diffusion
models LDM/DiT by 1.82/0.49, respectively. Code is avaiable at
\url{https://github.com/OliverRensu/MVAR}.
]]></content:encoded>
<pubDate>2024-11-15T18:54:42Z</pubDate>
</item>
<item>
<title>MagicQuill: An Intelligent Interactive Image Editing System</title>
<link>http://arxiv.org/abs/2411.09703v1</link>
<guid>http://arxiv.org/abs/2411.09703v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 图像编辑, MagicQuill, 多模态大语言模型, 扩散先验, 高质量编辑
总结:
图像编辑是一个复杂的任务，需要高效和精准的处理技术。MagicQuill是一个集成的图像编辑系统，可以快速实现创意构想。该系统具有简化而功能强大的界面，可以最小化输入来进行编辑操作（如插入元素、擦除对象、改变颜色）。这些交互由多模态大语言模型 (MLLM) 监控，实时预测编辑意图，无需明确的提示输入。最后，系统应用了强大的扩散先验，通过精确控制的两分支插件模块进行增强，处理编辑请求。实验结果表明，MagicQuill在实现高质量图像编辑方面非常有效。请访问https://magic-quill.github.io 来尝试我们的系统。 <br /><br />总结: <div>
Image editing involves a variety of complex tasks and requires efficient and
precise manipulation techniques. In this paper, we present MagicQuill, an
integrated image editing system that enables swift actualization of creative
ideas. Our system features a streamlined yet functionally robust interface,
allowing for the articulation of editing operations (e.g., inserting elements,
erasing objects, altering color) with minimal input. These interactions are
monitored by a multimodal large language model (MLLM) to anticipate editing
intentions in real time, bypassing the need for explicit prompt entry. Finally,
we apply a powerful diffusion prior, enhanced by a carefully learned two-branch
plug-in module, to process editing requests with precise control. Experimental
results demonstrate the effectiveness of MagicQuill in achieving high-quality
image edits. Please visit https://magic-quill.github.io to try out our system.
]]></content:encoded>
<pubDate>2024-11-14T18:59:57Z</pubDate>
</item>
<item>
<title>GaussianAnything: Interactive Point Cloud Latent Diffusion for 3D
  Generation</title>
<link>http://arxiv.org/abs/2411.08033v1</link>
<guid>http://arxiv.org/abs/2411.08033v1</guid>
<content:encoded><![CDATA[
<div> Point Cloud-structured Latent space, Variational Autoencoder (VAE), RGB-D, 3D generation, disentanglement<br />
<br />
该论文介绍了一种新颖的3D生成框架，通过交互式的点云结构化潜空间，解决了现有方法在输入格式、潜空间设计和输出表示方面的挑战。该框架使用多视角姿态的RGB-D-N(ormal)渲染作为输入，采用独特的潜空间设计，保留了3D形状信息，并结合级联潜空间扩散模型，提高了形状-纹理解耦能力。该方法支持多模态条件下的3D生成，允许使用点云、标题和单/多视角图像作为输入。值得注意的是，新提出的潜空间自然地实现了几何-纹理解耦，从而实现了3D感知编辑。实验结果表明，该方法在多个数据集上的表现优于现有方法，无论是文本条件还是图像条件下的3D生成。<br /><br />总结: 该论文介绍了一种新颖的3D生成框架，通过交互式的点云结构化潜空间，解决了现有方法在输入格式、潜空间设计和输出表示方面的挑战。该方法支持多模态条件下的3D生成，允许使用点云、标题和单/多视角图像作为输入。新提出的潜空间实现了几何-纹理解耦，从而实现了3D感知编辑。实验结果表明，该方法在多个数据集上的表现优于现有方法，无论是文本条件还是图像条件下的3D生成。 <div>
While 3D content generation has advanced significantly, existing methods
still face challenges with input formats, latent space design, and output
representations. This paper introduces a novel 3D generation framework that
addresses these challenges, offering scalable, high-quality 3D generation with
an interactive Point Cloud-structured Latent space. Our framework employs a
Variational Autoencoder (VAE) with multi-view posed RGB-D(epth)-N(ormal)
renderings as input, using a unique latent space design that preserves 3D shape
information, and incorporates a cascaded latent diffusion model for improved
shape-texture disentanglement. The proposed method, GaussianAnything, supports
multi-modal conditional 3D generation, allowing for point cloud, caption, and
single/multi-view image inputs. Notably, the newly proposed latent space
naturally enables geometry-texture disentanglement, thus allowing 3D-aware
editing. Experimental results demonstrate the effectiveness of our approach on
multiple datasets, outperforming existing methods in both text- and
image-conditioned 3D generation.
]]></content:encoded>
<pubDate>2024-11-12T18:59:32Z</pubDate>
</item>
<item>
<title>LLMPhy: Complex Physical Reasoning Using Large Language Models and World
  Models</title>
<link>http://arxiv.org/abs/2411.08027v1</link>
<guid>http://arxiv.org/abs/2411.08027v1</guid>
<content:encoded><![CDATA[
<div> 物理推理、机器学习、大型语言模型、TraySim数据集、LLMPhy框架
<br />
物理推理在机器人代理在真实世界中操作时是一项重要的技能。然而，解决这类推理问题通常涉及对复杂的多体相互作用在多种物理力的影响下进行假设和反思，因此学习所有这样的相互作用对于包括大型语言模型在内的最先进的机器学习框架来说是一项重要挑战。为了研究这个问题，研究人员提出了一个新的物理推理任务和数据集，称为TraySim。他们提出了LLMPhy，一个零-shot黑盒优化框架，通过利用LLM的物理知识和程序综合能力，并将这些能力与现代物理引擎中构建的世界模型相结合，来解决这一复杂的物理推理任务。通过在TraySim数据集上的实验证明，LLMPhy的组合可以实现最先进的零-shot物理推理性能，同时对标准黑盒优化方法表现出更好的收敛性和更好的物理参数估计能力。
<br /><br />总结: <br />物理推理是机器人在真实世界中操作时所必需的重要技能。研究人员提出了新的物理推理任务和数据集TraySim，并介绍了LLMPhy框架，该框架利用了大型语言模型的物理知识和程序综合能力，与现代物理引擎相结合，实现了先进的零-shot物理推理性能。 <div>
Physical reasoning is an important skill needed for robotic agents when
operating in the real world. However, solving such reasoning problems often
involves hypothesizing and reflecting over complex multi-body interactions
under the effect of a multitude of physical forces and thus learning all such
interactions poses a significant hurdle for state-of-the-art machine learning
frameworks, including large language models (LLMs). To study this problem, we
propose a new physical reasoning task and a dataset, dubbed TraySim. Our task
involves predicting the dynamics of several objects on a tray that is given an
external impact -- the domino effect of the ensued object interactions and
their dynamics thus offering a challenging yet controlled setup, with the goal
of reasoning being to infer the stability of the objects after the impact. To
solve this complex physical reasoning task, we present LLMPhy, a zero-shot
black-box optimization framework that leverages the physics knowledge and
program synthesis abilities of LLMs, and synergizes these abilities with the
world models built into modern physics engines. Specifically, LLMPhy uses an
LLM to generate code to iteratively estimate the physical hyperparameters of
the system (friction, damping, layout, etc.) via an implicit
analysis-by-synthesis approach using a (non-differentiable) simulator in the
loop and uses the inferred parameters to imagine the dynamics of the scene
towards solving the reasoning task. To show the effectiveness of LLMPhy, we
present experiments on our TraySim dataset to predict the steady-state poses of
the objects. Our results show that the combination of the LLM and the physics
engine leads to state-of-the-art zero-shot physical reasoning performance,
while demonstrating superior convergence against standard black-box
optimization methods and better estimation of the physical parameters.
]]></content:encoded>
<pubDate>2024-11-12T18:56:58Z</pubDate>
</item>
<item>
<title>Incentive Design with Spillovers</title>
<link>http://arxiv.org/abs/2411.08026v1</link>
<guid>http://arxiv.org/abs/2411.08026v1</guid>
<content:encoded><![CDATA[
A principal uses payments conditioned on stochastic outcomes of a team
project to elicit costly effort from the team members. We develop a multi-agent
generalization of a classic first-order approach to contract optimization by
leveraging methods from network games. The main results characterize the
optimal allocation of incentive pay across agents and outcomes. Incentive
optimality requires equalizing, across agents, a product of (i) individual
productivity (ii) organizational centrality and (iii) responsiveness to
monetary incentives.
]]></content:encoded>
<pubDate>2024-11-12T18:56:31Z</pubDate>
</item>
<item>
<title>Tooling or Not Tooling? The Impact of Tools on Language Agents for
  Chemistry Problem Solving</title>
<link>http://arxiv.org/abs/2411.07228v1</link>
<guid>http://arxiv.org/abs/2411.07228v1</guid>
<content:encoded><![CDATA[
<div> ChemAgent, LLM-based agents, tools, specialized chemistry tasks, general chemistry questions
<br /><br />
本文讨论了基于大型语言模型（LLM）的化学问题解决方法，介绍了几种带有工具增强的LLM代理，如ChemCrow和Coscientist。然而，它们的评估范围狭窄，无法全面了解工具在不同化学任务中的效果。为了弥补这一缺口，我们开发了ChemAgent，这是一个在ChemCrow基础上改进的化学代理，并对其在专业化学任务和一般化学问题上的性能进行了全面评估。令人惊讶的是，ChemAgent并不总是能够在没有工具的情况下一直胜过基础的LLMs。我们的错误分析得出，对于合成预测等专业化学任务，我们应该使用专门的工具来增强代理；然而，对于像考试中的一般化学问题这样的问题，代理的正确运用化学知识的能力更为重要，并不总是需要工具来增强。 
<br /><br />总结: 本文讨论了LLM代理在化学问题解决中的应用，介绍了ChemAgent的开发与评估结果，并就专业化学任务和一般化学问题的应用进行了讨论。 <div>
To enhance large language models (LLMs) for chemistry problem solving,
several LLM-based agents augmented with tools have been proposed, such as
ChemCrow and Coscientist. However, their evaluations are narrow in scope,
leaving a large gap in understanding the benefits of tools across diverse
chemistry tasks. To bridge this gap, we develop ChemAgent, an enhanced
chemistry agent over ChemCrow, and conduct a comprehensive evaluation of its
performance on both specialized chemistry tasks and general chemistry
questions. Surprisingly, ChemAgent does not consistently outperform its base
LLMs without tools. Our error analysis with a chemistry expert suggests that:
For specialized chemistry tasks, such as synthesis prediction, we should
augment agents with specialized tools; however, for general chemistry questions
like those in exams, agents' ability to reason correctly with chemistry
knowledge matters more, and tool augmentation does not always help.
]]></content:encoded>
<pubDate>2024-11-11T18:46:37Z</pubDate>
</item>
<item>
<title>DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile
  Manipulation</title>
<link>http://arxiv.org/abs/2411.04999v1</link>
<guid>http://arxiv.org/abs/2411.04999v1</guid>
<content:encoded><![CDATA[
<div> 动态空间语义记忆, 机器人, 移动操作, 开放词汇, 实时环境 <br />
总结:<br />
本文介绍了一种名为DynaMem的新方法，用于开放世界移动操作。该方法利用动态空间语义记忆来表示机器人的环境，通过构建3D数据结构维护点云的动态记忆，并使用多模态LLM或最先进的视觉-语言模型生成的开放词汇特征来回答对象定位查询。借助DynaMem，机器人能够探索新的环境，搜索记忆中不存在的对象，并不断更新记忆以适应场景中的对象移动、出现或消失。作者在三个真实场景和九个离线场景中对Stretch SE3机器人进行了大量实验，取得了70%的非静态对象抓取成功率，比最先进的静态系统提高了2倍以上。他们的代码、实验和部署视频都是开源的，可以在项目网站上找到：https://dynamem.github.io/ <div>
Significant progress has been made in open-vocabulary mobile manipulation,
where the goal is for a robot to perform tasks in any environment given a
natural language description. However, most current systems assume a static
environment, which limits the system's applicability in real-world scenarios
where environments frequently change due to human intervention or the robot's
own actions. In this work, we present DynaMem, a new approach to open-world
mobile manipulation that uses a dynamic spatio-semantic memory to represent a
robot's environment. DynaMem constructs a 3D data structure to maintain a
dynamic memory of point clouds, and answers open-vocabulary object localization
queries using multimodal LLMs or open-vocabulary features generated by
state-of-the-art vision-language models. Powered by DynaMem, our robots can
explore novel environments, search for objects not found in memory, and
continuously update the memory as objects move, appear, or disappear in the
scene. We run extensive experiments on the Stretch SE3 robots in three real and
nine offline scenes, and achieve an average pick-and-drop success rate of 70%
on non-stationary objects, which is more than a 2x improvement over
state-of-the-art static systems. Our code as well as our experiment and
deployment videos are open sourced and can be found on our project website:
https://dynamem.github.io/
]]></content:encoded>
<pubDate>2024-11-07T18:59:27Z</pubDate>
</item>
<item>
<title>LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation</title>
<link>http://arxiv.org/abs/2411.04997v1</link>
<guid>http://arxiv.org/abs/2411.04997v1</guid>
<content:encoded><![CDATA[
<div> CLIP, multimodal foundational model, LLM, GPT-4, LLaMA<br />
LLM2CLIP, 融合LLM和CLIP，提升多模态表示学习性能，加强对图像标题的处理，处理长文本能力，优化训练过程，实验证明方法有效。

<br /><br />总结:
CLIP是重要的多模态基础模型，借助自然语言的丰富监督信号塑造强大的跨模态表示空间。文章提出了LLM2CLIP的方法，通过精调LLM以提升CLIP的文本能力，并设计高效训练过程使CLIP的视觉编码器受益于LLM的强大文本理解能力。实验证明该方法在多模态任务中带来显著改进。 <div>
CLIP is one of the most important multimodal foundational models today. What
powers CLIP's capabilities? The rich supervision signals provided by natural
language, the carrier of human knowledge, shape a powerful cross-modal
representation space. However, with the rapid advancements in large language
models LLMs like GPT-4 and LLaMA, the boundaries of language comprehension and
generation are continually being pushed. This raises an intriguing question:
can the capabilities of LLMs be harnessed to further improve multimodal
representation learning? The potential benefits of incorporating LLMs into CLIP
are clear. LLMs' strong textual understanding can fundamentally improve CLIP's
ability to handle image captions, drastically enhancing its ability to process
long and complex texts, a well-known limitation of vanilla CLIP. Moreover, LLMs
are trained on a vast corpus of text, possessing open-world knowledge. This
allows them to expand on caption information during training, increasing the
efficiency of the learning process. In this paper, we propose LLM2CLIP, a novel
approach that embraces the power of LLMs to unlock CLIP's potential. By
fine-tuning the LLM in the caption space with contrastive learning, we extract
its textual capabilities into the output embeddings, significantly improving
the output layer's textual discriminability. We then design an efficient
training process where the fine-tuned LLM acts as a powerful teacher for CLIP's
visual encoder. Thanks to the LLM's presence, we can now incorporate longer and
more complex captions without being restricted by vanilla CLIP's text encoder's
context window and ability limitations. Our experiments demonstrate that this
approach brings substantial improvements in cross-modal tasks.
]]></content:encoded>
<pubDate>2024-11-07T18:59:16Z</pubDate>
</item>
<item>
<title>HourVideo: 1-Hour Video-Language Understanding</title>
<link>http://arxiv.org/abs/2411.04998v1</link>
<guid>http://arxiv.org/abs/2411.04998v1</guid>
<content:encoded><![CDATA[
We present HourVideo, a benchmark dataset for hour-long video-language
understanding. Our dataset consists of a novel task suite comprising
summarization, perception (recall, tracking), visual reasoning (spatial,
temporal, predictive, causal, counterfactual), and navigation (room-to-room,
object retrieval) tasks. HourVideo includes 500 manually curated egocentric
videos from the Ego4D dataset, spanning durations of 20 to 120 minutes, and
features 12,976 high-quality, five-way multiple-choice questions. Benchmarking
results reveal that multimodal models, including GPT-4 and LLaVA-NeXT, achieve
marginal improvements over random chance. In stark contrast, human experts
significantly outperform the state-of-the-art long-context multimodal model,
Gemini Pro 1.5 (85.0% vs. 37.3%), highlighting a substantial gap in multimodal
capabilities. Our benchmark, evaluation toolkit, prompts, and documentation are
available at https://hourvideo.stanford.edu
]]></content:encoded>
<pubDate>2024-11-07T18:59:16Z</pubDate>
</item>
<item>
<title>Mixture-of-Transformers: A Sparse and Scalable Architecture for
  Multi-Modal Foundation Models</title>
<link>http://arxiv.org/abs/2411.04996v1</link>
<guid>http://arxiv.org/abs/2411.04996v1</guid>
<content:encoded><![CDATA[
The development of large language models (LLMs) has expanded to multi-modal
systems capable of processing text, images, and speech within a unified
framework. Training these models demands significantly larger datasets and
computational resources compared to text-only LLMs. To address the scaling
challenges, we introduce Mixture-of-Transformers (MoT), a sparse multi-modal
transformer architecture that significantly reduces pretraining computational
costs. MoT decouples non-embedding parameters of the model by modality --
including feed-forward networks, attention matrices, and layer normalization --
enabling modality-specific processing with global self-attention over the full
input sequence. We evaluate MoT across multiple settings and model scales. In
the Chameleon 7B setting (autoregressive text-and-image generation), MoT
matches the dense baseline's performance using only 55.8\% of the FLOPs. When
extended to include speech, MoT reaches speech performance comparable to the
dense baseline with only 37.2\% of the FLOPs. In the Transfusion setting, where
text and image are trained with different objectives, a 7B MoT model matches
the image modality performance of the dense baseline with one third of the
FLOPs, and a 760M MoT model outperforms a 1.4B dense baseline across key image
generation metrics. System profiling further highlights MoT's practical
benefits, achieving dense baseline image quality in 47.2\% of the wall-clock
time and text quality in 75.6\% of the wall-clock time (measured on AWS
p4de.24xlarge instances with NVIDIA A100 GPUs).
]]></content:encoded>
<pubDate>2024-11-07T18:59:06Z</pubDate>
</item>
<item>
<title>MME-Finance: A Multimodal Finance Benchmark for Expert-level
  Understanding and Reasoning</title>
<link>http://arxiv.org/abs/2411.03314v1</link>
<guid>http://arxiv.org/abs/2411.03314v1</guid>
<content:encoded><![CDATA[
<div> 金融领域, 多模态模型, MME-Finance, 评估系统, 实验结果<br />
<br />
多模态模型在金融领域表现出独特性，通用基准无法有效评估其性能，因此需要专门的金融领域的多模态基准。本文提出了MME-Finance，一个具有金融特色和专业性质的视觉问答基准，包括构建反映用户实际需求的图表，根据金融领域问题偏好进行提问，以及由金融行业经验超过10年的专家标注问题。此外，还开发了一个自定义的金融评估系统，并针对19种主流多模态语言模型进行了广泛的实验评估，结果显示通用基准上表现良好的模型在MME-Finance上表现不佳，特别是在与金融相关的类别中。最后，提出了中文版MME-Finance，以便比较在中文环境下多模态语言模型的性能。 <br /><br />总结: <br />金融领域的多模态模型需要专门的基准评估；MME-Finance是一个针对金融领域的多模态视觉问答基准；针对19种主流多模态语言模型进行了广泛实验评估；通用基准上表现良好的模型在金融领域基准上表现不佳；提出了中文版MME-Finance用于中文环境下的评估。 <div>
In recent years, multimodal benchmarks for general domains have guided the
rapid development of multimodal models on general tasks. However, the financial
field has its peculiarities. It features unique graphical images (e.g.,
candlestick charts, technical indicator charts) and possesses a wealth of
specialized financial knowledge (e.g., futures, turnover rate). Therefore,
benchmarks from general fields often fail to measure the performance of
multimodal models in the financial domain, and thus cannot effectively guide
the rapid development of large financial models. To promote the development of
large financial multimodal models, we propose MME-Finance, an bilingual
open-ended and practical usage-oriented Visual Question Answering (VQA)
benchmark. The characteristics of our benchmark are finance and expertise,
which include constructing charts that reflect the actual usage needs of users
(e.g., computer screenshots and mobile photography), creating questions
according to the preferences in financial domain inquiries, and annotating
questions by experts with 10+ years of experience in the financial industry.
Additionally, we have developed a custom-designed financial evaluation system
in which visual information is first introduced in the multi-modal evaluation
process. Extensive experimental evaluations of 19 mainstream MLLMs are
conducted to test their perception, reasoning, and cognition capabilities. The
results indicate that models performing well on general benchmarks cannot do
well on MME-Finance; for instance, the top-performing open-source and
closed-source models obtain 65.69 (Qwen2VL-72B) and 63.18 (GPT-4o),
respectively. Their performance is particularly poor in categories most
relevant to finance, such as candlestick charts and technical indicator charts.
In addition, we propose a Chinese version, which helps compare performance of
MLLMs under a Chinese context.
]]></content:encoded>
<pubDate>2024-11-05T18:59:51Z</pubDate>
</item>
<item>
<title>Adaptive Caching for Faster Video Generation with Diffusion Transformers</title>
<link>http://arxiv.org/abs/2411.02397v1</link>
<guid>http://arxiv.org/abs/2411.02397v1</guid>
<content:encoded><![CDATA[
<div> 加速视频Diffusion Transformers(视频DiTs)、Adaptive Caching(AdaCache)、Motion Regularization(MoReg)、推理速度、视频生成质量
<br /><br />
该论文提出了一种无需训练的方法来加速视频DiTs，即Adaptive Caching(AdaCache)，其动机是“并非所有视频都一样”，意味着有些视频需要较少的去噪步骤才能达到合理的质量。基于此，他们不仅通过扩散过程缓存计算，还设计了一个针对每个视频生成定制的缓存调度，最大化质量和延迟的折衷。他们还引入了Motion Regularization (MoReg)方案，以利用视频信息，基本上根据运动内容来控制计算分配。总的来说，他们的贡献使得多个视频DiT基线能够在不牺牲生成质量的情况下显著提高推理速度（例如，在Open-Sora 720p - 2s视频生成上高达4.7倍）。<br /><br />总结: <div>
Generating temporally-consistent high-fidelity videos can be computationally
expensive, especially over longer temporal spans. More-recent Diffusion
Transformers (DiTs) -- despite making significant headway in this context --
have only heightened such challenges as they rely on larger models and heavier
attention mechanisms, resulting in slower inference speeds. In this paper, we
introduce a training-free method to accelerate video DiTs, termed Adaptive
Caching (AdaCache), which is motivated by the fact that "not all videos are
created equal": meaning, some videos require fewer denoising steps to attain a
reasonable quality than others. Building on this, we not only cache
computations through the diffusion process, but also devise a caching schedule
tailored to each video generation, maximizing the quality-latency trade-off. We
further introduce a Motion Regularization (MoReg) scheme to utilize video
information within AdaCache, essentially controlling the compute allocation
based on motion content. Altogether, our plug-and-play contributions grant
significant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video
generation) without sacrificing the generation quality, across multiple video
DiT baselines.
]]></content:encoded>
<pubDate>2024-11-04T18:59:44Z</pubDate>
</item>
<item>
<title>Training-free Regional Prompting for Diffusion Transformers</title>
<link>http://arxiv.org/abs/2411.02395v1</link>
<guid>http://arxiv.org/abs/2411.02395v1</guid>
<content:encoded><![CDATA[
<div> 关键词: Diffusion models, text-to-image generation, FLUX.1, regional prompting, DiT architecture

总结:
Diffusion模型在文本到图像的生成中表现出了出色的能力，结合大型语言模型（如T5、Llama）可以提高其语义理解能力。然而，现有模型在处理长且复杂的文本提示时仍存在不足，尤其是当文本提示包含多种对象及其相关空间关系时。虽然已经提出了许多针对UNet-based模型的区域提示方法，但基于最近的Diffusion Transformer（DiT）架构（如SD3和FLUX.1）仍缺乏相关实现。因此，本报告提出了基于FLUX.1的区域提示实现，利用注意力操纵，使DiT在无需训练的情况下具备了精细的组合式文本到图像生成能力。具体代码可在https://github.com/antonioo-c/Regional-Prompting-FLUX 上找到。 <div>
Diffusion models have demonstrated excellent capabilities in text-to-image
generation. Their semantic understanding (i.e., prompt following) ability has
also been greatly improved with large language models (e.g., T5, Llama).
However, existing models cannot perfectly handle long and complex text prompts,
especially when the text prompts contain various objects with numerous
attributes and interrelated spatial relationships. While many regional
prompting methods have been proposed for UNet-based models (SD1.5, SDXL), but
there are still no implementations based on the recent Diffusion Transformer
(DiT) architecture, such as SD3 and FLUX.1.In this report, we propose and
implement regional prompting for FLUX.1 based on attention manipulation, which
enables DiT with fined-grained compositional text-to-image generation
capability in a training-free manner. Code is available at
https://github.com/antonioo-c/Regional-Prompting-FLUX.
]]></content:encoded>
<pubDate>2024-11-04T18:59:05Z</pubDate>
</item>
<item>
<title>Attacking Vision-Language Computer Agents via Pop-ups</title>
<link>http://arxiv.org/abs/2411.02391v1</link>
<guid>http://arxiv.org/abs/2411.02391v1</guid>
<content:encoded><![CDATA[
Autonomous agents powered by large vision and language models (VLM) have
demonstrated significant potential in completing daily computer tasks, such as
browsing the web to book travel and operating desktop software, which requires
agents to understand these interfaces. Despite such visual inputs becoming more
integrated into agentic applications, what types of risks and attacks exist
around them still remain unclear. In this work, we demonstrate that VLM agents
can be easily attacked by a set of carefully designed adversarial pop-ups,
which human users would typically recognize and ignore. This distraction leads
agents to click these pop-ups instead of performing the tasks as usual.
Integrating these pop-ups into existing agent testing environments like OSWorld
and VisualWebArena leads to an attack success rate (the frequency of the agent
clicking the pop-ups) of 86% on average and decreases the task success rate by
47%. Basic defense techniques such as asking the agent to ignore pop-ups or
including an advertisement notice, are ineffective against the attack.
]]></content:encoded>
<pubDate>2024-11-04T18:56:42Z</pubDate>
</item>
<item>
<title>How Far is Video Generation from World Model: A Physical Law Perspective</title>
<link>http://arxiv.org/abs/2411.02385v1</link>
<guid>http://arxiv.org/abs/2411.02385v1</guid>
<content:encoded><![CDATA[
OpenAI's Sora highlights the potential of video generation for developing
world models that adhere to fundamental physical laws. However, the ability of
video generation models to discover such laws purely from visual data without
human priors can be questioned. A world model learning the true law should give
predictions robust to nuances and correctly extrapolate on unseen scenarios. In
this work, we evaluate across three key scenarios: in-distribution,
out-of-distribution, and combinatorial generalization. We developed a 2D
simulation testbed for object movement and collisions to generate videos
deterministically governed by one or more classical mechanics laws. This
provides an unlimited supply of data for large-scale experimentation and
enables quantitative evaluation of whether the generated videos adhere to
physical laws. We trained diffusion-based video generation models to predict
object movements based on initial frames. Our scaling experiments show perfect
generalization within the distribution, measurable scaling behavior for
combinatorial generalization, but failure in out-of-distribution scenarios.
Further experiments reveal two key insights about the generalization mechanisms
of these models: (1) the models fail to abstract general physical rules and
instead exhibit "case-based" generalization behavior, i.e., mimicking the
closest training example; (2) when generalizing to new cases, models are
observed to prioritize different factors when referencing training data: color
> size > velocity > shape. Our study suggests that scaling alone is
insufficient for video generation models to uncover fundamental physical laws,
despite its role in Sora's broader success. See our project page at
https://phyworld.github.io
]]></content:encoded>
<pubDate>2024-11-04T18:53:05Z</pubDate>
</item>
<item>
<title>Teaching Embodied Reinforcement Learning Agents: Informativeness and
  Diversity of Language Use</title>
<link>http://arxiv.org/abs/2410.24218v1</link>
<guid>http://arxiv.org/abs/2410.24218v1</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, embodied agents, language input, task learning, diverse and informative language feedback
<br /><br />
本文研究了不同类型的语言输入对于加强强化学习（RL）具有身体的代理的作用。具体地，作者考察了语言信息的不同水平（即对过去行为的反馈和对未来指导）以及多样性（即语言表达的变化）对代理学习和推理的影响。基于四个强化学习基准的实证结果表明，受过多样和丰富语言反馈训练的代理可以实现增强的泛化能力，并快速适应新任务。这些发现突显了语言在教导具有身体的代理在开放世界中学习新任务中的关键作用。
<br /><br />总结: 本文研究了不同类型的语言输入如何影响具有身体的代理的学习和推理，结果表明多样和丰富的语言反馈可以增强代理的泛化能力并快速适应新任务。 <div>
In real-world scenarios, it is desirable for embodied agents to have the
ability to leverage human language to gain explicit or implicit knowledge for
learning tasks. Despite recent progress, most previous approaches adopt simple
low-level instructions as language inputs, which may not reflect natural human
communication. It's not clear how to incorporate rich language use to
facilitate task learning. To address this question, this paper studies
different types of language inputs in facilitating reinforcement learning (RL)
embodied agents. More specifically, we examine how different levels of language
informativeness (i.e., feedback on past behaviors and future guidance) and
diversity (i.e., variation of language expressions) impact agent learning and
inference. Our empirical results based on four RL benchmarks demonstrate that
agents trained with diverse and informative language feedback can achieve
enhanced generalization and fast adaptation to new tasks. These findings
highlight the pivotal role of language use in teaching embodied agents new
tasks in an open world. Project website:
https://github.com/sled-group/Teachable_RL
]]></content:encoded>
<pubDate>2024-10-31T17:59:52Z</pubDate>
</item>
<item>
<title>RelationBooth: Towards Relation-Aware Customized Object Generation</title>
<link>http://arxiv.org/abs/2410.23280v1</link>
<guid>http://arxiv.org/abs/2410.23280v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 用户提供的图像提示, 文本到图像生成模型, 关系感知定制图像生成, RelationBooth, 数据集和训练

总结:<br /><br />这篇文章介绍了关系感知定制图像生成的重要性，指出现有模型常常忽视生成图像中定制对象之间的关系。为了填补这一空白，文章提出了一个名为RelationBooth的框架，以一套精心策划的数据集用于训练，包括关系特定的图像、包含身份信息的独立对象图像和文本提示。文章介绍了两个关键模块，分别用于解决生成准确和自然的关系以及避免对象重叠引起的混淆。经过广泛的基准测试，RelationBooth在生成精确的关系的同时保持了一系列对象和关系的身份，表现优越。源代码和训练模型将向公众开放。 <div>
Customized image generation is crucial for delivering personalized content
based on user-provided image prompts, aligning large-scale text-to-image
diffusion models with individual needs. However, existing models often overlook
the relationships between customized objects in generated images. Instead, this
work addresses that gap by focusing on relation-aware customized image
generation, which aims to preserve the identities from image prompts while
maintaining the predicate relations described in text prompts. Specifically, we
introduce RelationBooth, a framework that disentangles identity and relation
learning through a well-curated dataset. Our training data consists of
relation-specific images, independent object images containing identity
information, and text prompts to guide relation generation. Then, we propose
two key modules to tackle the two main challenges: generating accurate and
natural relations, especially when significant pose adjustments are required,
and avoiding object confusion in cases of overlap. First, we introduce a
keypoint matching loss that effectively guides the model in adjusting object
poses closely tied to their relationships. Second, we incorporate local
features from the image prompts to better distinguish between objects,
preventing confusion in overlapping cases. Extensive results on three
benchmarks demonstrate the superiority of RelationBooth in generating precise
relations while preserving object identities across a diverse set of objects
and relations. The source code and trained models will be made available to the
public.
]]></content:encoded>
<pubDate>2024-10-30T17:57:21Z</pubDate>
</item>
<item>
<title>SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video
  Generation</title>
<link>http://arxiv.org/abs/2410.23277v1</link>
<guid>http://arxiv.org/abs/2410.23277v1</guid>
<content:encoded><![CDATA[
<div> 慢学习、快学习、视频生成模型、双速学习系统、长视频生成

慢快学习技术结合了世界动态的慢学习和新体验的快速记忆存储，提出了SlowFast-VGen，该方法包含了用于慢速学习世界动态的条件视频扩散模型以及基于时间LoRA模块的推理时间快速学习策略。同时引入了慢快学习循环算法，无缝地将快速学习循环集成到慢速学习循环中，有助于上下文感知技能学习。作者还收集了200k个视频的大规模数据集，用于慢学习近似世界模型。实验表明，SlowFast-VGen在各种指标上优于基线模型，在动作驱动视频生成方面取得了良好表现，并且提高了长期规划任务的性能。 <br /><br />总结: SlowFast-VGen是一种双速学习系统，结合了慢学习和快学习，用于动作驱动的长视频生成。它利用条件视频扩散模型和时间LoRA模块进行慢学习和快学习，同时引入慢快学习循环算法，通过大规模数据集进行慢学习，实现了在视频生成和长期规划任务中的优越性能表现。 <div>
Human beings are endowed with a complementary learning system, which bridges
the slow learning of general world dynamics with fast storage of episodic
memory from a new experience. Previous video generation models, however,
primarily focus on slow learning by pre-training on vast amounts of data,
overlooking the fast learning phase crucial for episodic memory storage. This
oversight leads to inconsistencies across temporally distant frames when
generating longer videos, as these frames fall beyond the model's context
window. To this end, we introduce SlowFast-VGen, a novel dual-speed learning
system for action-driven long video generation. Our approach incorporates a
masked conditional video diffusion model for the slow learning of world
dynamics, alongside an inference-time fast learning strategy based on a
temporal LoRA module. Specifically, the fast learning process updates its
temporal LoRA parameters based on local inputs and outputs, thereby efficiently
storing episodic memory in its parameters. We further propose a slow-fast
learning loop algorithm that seamlessly integrates the inner fast learning loop
into the outer slow learning loop, enabling the recall of prior multi-episode
experiences for context-aware skill learning. To facilitate the slow learning
of an approximate world model, we collect a large-scale dataset of 200k videos
with language action annotations, covering a wide range of scenarios. Extensive
experiments show that SlowFast-VGen outperforms baselines across various
metrics for action-driven video generation, achieving an FVD score of 514
compared to 782, and maintaining consistency in longer videos, with an average
of 0.37 scene cuts versus 0.89. The slow-fast learning loop algorithm
significantly enhances performances on long-horizon planning tasks as well.
Project Website: https://slowfast-vgen.github.io
]]></content:encoded>
<pubDate>2024-10-30T17:55:52Z</pubDate>
</item>
<item>
<title>FISHNET: Financial Intelligence from Sub-querying, Harmonizing,
  Neural-Conditioning, Expert Swarms, and Task Planning</title>
<link>http://arxiv.org/abs/2410.19727v1</link>
<guid>http://arxiv.org/abs/2410.19727v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 金融智能、大数据、LLM、FISHNET、机构架构

总结:
金融智能的生成通常依赖于传统的知识图谱构建或数据库工程方法。最近出现了经过精细调整的金融领域特定的大型语言模型（LLMs）。然而，存在推理成本高、幻觉和同时分析高维金融数据的复杂性等限制。因此，研发了FISHNET（Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning, Expert swarming, and Task planning）系统，该系统是一个代理机构架构，可以为98000多个金融监管文件执行高度复杂的分析任务。FISHNET在金融洞察能力方面表现出色（5.0% Routing下61.8%成功率，45.6% RAG R-Precision）。通过严格的剔除实验证明了FISHNET的成功、每个代理的重要性以及组装所有代理的优化性能。这种模块化的架构可以用于多种用例，实现了对金融任务至关重要的可伸缩性、灵活性和数据完整性。<br /><br /> <div>
Financial intelligence generation from vast data sources has typically relied
on traditional methods of knowledge-graph construction or database engineering.
Recently, fine-tuned financial domain-specific Large Language Models (LLMs),
have emerged. While these advancements are promising, limitations such as high
inference costs, hallucinations, and the complexity of concurrently analyzing
high-dimensional financial data, emerge. This motivates our invention FISHNET
(Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning,
Expert swarming, and Task planning), an agentic architecture that accomplishes
highly complex analytical tasks for more than 98,000 regulatory filings that
vary immensely in terms of semantics, data hierarchy, or format. FISHNET shows
remarkable performance for financial insight generation (61.8% success rate
over 5.0% Routing, 45.6% RAG R-Precision). We conduct rigorous ablations to
empirically prove the success of FISHNET, each agent's importance, and the
optimized performance of assembling all agents. Our modular architecture can be
leveraged for a myriad of use-cases, enabling scalability, flexibility, and
data integrity that are critical for financial tasks.
]]></content:encoded>
<pubDate>2024-10-25T17:53:47Z</pubDate>
</item>
<item>
<title>CAMEL-Bench: A Comprehensive Arabic LMM Benchmark</title>
<link>http://arxiv.org/abs/2410.18976v1</link>
<guid>http://arxiv.org/abs/2410.18976v1</guid>
<content:encoded><![CDATA[
<div> LMMs, Arabic language, CAMEL-Bench, evaluation, model assessment <br />
本研究开发了一个全面的阿拉伯语LMM评估基准(CAMEL-Bench)，包括8个不同领域和38个子域，涵盖了多种视觉推理和理解任务。CAMEL-Bench包括约29,036个问题，由母语为阿拉伯语的人员进行质量验证，以确保模型评估的可靠性。对闭源和开源的LMM进行评估分析，发现即使是闭源的GPT-4o也只能达到整体分数的62%。我们的评估基准和脚本都是开源的。总结：<br />
1. 开发了阿拉伯语LMM评估基准CAMEL-Bench；
2. CAMEL-Bench包括8个领域和38个子域，涵盖广泛的场景；
3. 验证了约29,036个问题的质量；
4. 对闭源和开源的LMM进行了评估分析；
5. 发现模型需要显著改进。 <div>
Recent years have witnessed a significant interest in developing large
multimodal models (LMMs) capable of performing various visual reasoning and
understanding tasks. This has led to the introduction of multiple LMM
benchmarks to evaluate LMMs on different tasks. However, most existing LMM
evaluation benchmarks are predominantly English-centric. In this work, we
develop a comprehensive LMM evaluation benchmark for the Arabic language to
represent a large population of over 400 million speakers. The proposed
benchmark, named CAMEL-Bench, comprises eight diverse domains and 38
sub-domains including, multi-image understanding, complex visual perception,
handwritten document understanding, video understanding, medical imaging, plant
diseases, and remote sensing-based land use understanding to evaluate broad
scenario generalizability. Our CAMEL-Bench comprises around 29,036 questions
that are filtered from a larger pool of samples, where the quality is manually
verified by native speakers to ensure reliable model assessment. We conduct
evaluations of both closed-source, including GPT-4 series, and open-source
LMMs. Our analysis reveals the need for substantial improvement, especially
among the best open-source models, with even the closed-source GPT-4o achieving
an overall score of 62%. Our benchmark and evaluation scripts are open-sourced.
]]></content:encoded>
<pubDate>2024-10-24T17:59:38Z</pubDate>
</item>
<item>
<title>3D-Adapter: Geometry-Consistent Multi-View Diffusion for High-Quality 3D
  Generation</title>
<link>http://arxiv.org/abs/2410.18974v1</link>
<guid>http://arxiv.org/abs/2410.18974v1</guid>
<content:encoded><![CDATA[
<div> 3D-Adapter, 3D geometry awareness, image diffusion models, 3D generation, pretrained base model

总结:<br /><br />这篇文章介绍了3D-Adapter，一个能够将3D几何意识融入预训练图像扩散模型的插件模块。作者提出了3D反馈增强的概念，通过这一理念，3D-Adapter能够解码中间的多视图特征成一致的3D表示，然后重新编码渲染的RGBD视图，通过特征相加来增强预训练基础模型。作者研究了两个3D-Adapter的变体：一个快速的前馈版本和一个多功能的无需训练的版本。实验结果表明，3D-Adapter不仅大大提高了文本到多视图模型的几何质量，还使得使用文本到图像Stable Diffusion模型能够实现高质量的3D生成。此外，作者还展示了3D-Adapter广泛的应用潜力，包括在文本到3D、图像到3D、文本到纹理和文本到头像任务中获得了高质量的结果。 <div>
Multi-view image diffusion models have significantly advanced open-domain 3D
object generation. However, most existing models rely on 2D network
architectures that lack inherent 3D biases, resulting in compromised geometric
consistency. To address this challenge, we introduce 3D-Adapter, a plug-in
module designed to infuse 3D geometry awareness into pretrained image diffusion
models. Central to our approach is the idea of 3D feedback augmentation: for
each denoising step in the sampling loop, 3D-Adapter decodes intermediate
multi-view features into a coherent 3D representation, then re-encodes the
rendered RGBD views to augment the pretrained base model through feature
addition. We study two variants of 3D-Adapter: a fast feed-forward version
based on Gaussian splatting and a versatile training-free version utilizing
neural fields and meshes. Our extensive experiments demonstrate that 3D-Adapter
not only greatly enhances the geometry quality of text-to-multi-view models
such as Instant3D and Zero123++, but also enables high-quality 3D generation
using the plain text-to-image Stable Diffusion. Furthermore, we showcase the
broad application potential of 3D-Adapter by presenting high quality results in
text-to-3D, image-to-3D, text-to-texture, and text-to-avatar tasks.
]]></content:encoded>
<pubDate>2024-10-24T17:59:30Z</pubDate>
</item>
<item>
<title>Deep Insights into Cognitive Decline: A Survey of Leveraging
  Non-Intrusive Modalities with Deep Learning Techniques</title>
<link>http://arxiv.org/abs/2410.18972v1</link>
<guid>http://arxiv.org/abs/2410.18972v1</guid>
<content:encoded><![CDATA[
Cognitive decline is a natural part of aging, often resulting in reduced
cognitive abilities. In some cases, however, this decline is more pronounced,
typically due to disorders such as Alzheimer's disease. Early detection of
anomalous cognitive decline is crucial, as it can facilitate timely
professional intervention. While medical data can help in this detection, it
often involves invasive procedures. An alternative approach is to employ
non-intrusive techniques such as speech or handwriting analysis, which do not
necessarily affect daily activities. This survey reviews the most relevant
methodologies that use deep learning techniques to automate the cognitive
decline estimation task, including audio, text, and visual processing. We
discuss the key features and advantages of each modality and methodology,
including state-of-the-art approaches like Transformer architecture and
foundation models. In addition, we present works that integrate different
modalities to develop multimodal models. We also highlight the most significant
datasets and the quantitative results from studies using these resources. From
this review, several conclusions emerge. In most cases, the textual modality
achieves the best results and is the most relevant for detecting cognitive
decline. Moreover, combining various approaches from individual modalities into
a multimodal model consistently enhances performance across nearly all
scenarios.
]]></content:encoded>
<pubDate>2024-10-24T17:59:21Z</pubDate>
</item>
<item>
<title>Prioritized Generative Replay</title>
<link>http://arxiv.org/abs/2410.18082v1</link>
<guid>http://arxiv.org/abs/2410.18082v1</guid>
<content:encoded><![CDATA[
<div> 在线强化学习，经验回放，优先级，生成模型，样本效率
<br />
这篇论文介绍了一种基于生成模型的在线强化学习经验存储方法，利用生成模型对过去的经验进行密集化，同时通过“相关性函数”引导新生成的经验向对学习更有帮助的部分推进。作者采用条件扩散模型和好奇心或价值为基础的简单相关性函数来实现这一方法，证明了这种方法在基于状态和像素的领域都能提高性能和样本效率。他们还揭示了这些改进的机制，展示了引导如何促进生成的转换的多样性并减少过拟合。此外，作者还展示了他们的方法如何训练具有更高更新-to-数据比率的策略，为在线强化学习代理开辟了更好的扩展途径。
<br /><br />
总结: 在线强化学习经验存储方法的新途径，使用生成模型密集化经验，通过相关性函数引导新生成的经验，提高了性能和样本效率。 <div>
Sample-efficient online reinforcement learning often uses replay buffers to
store experience for reuse when updating the value function. However, uniform
replay is inefficient, since certain classes of transitions can be more
relevant to learning. While prioritization of more useful samples is helpful,
this strategy can also lead to overfitting, as useful samples are likely to be
more rare. In this work, we instead propose a prioritized, parametric version
of an agent's memory, using generative models to capture online experience.
This paradigm enables (1) densification of past experience, with new
generations that benefit from the generative model's generalization capacity
and (2) guidance via a family of "relevance functions" that push these
generations towards more useful parts of an agent's acquired history. We show
this recipe can be instantiated using conditional diffusion models and simple
relevance functions such as curiosity- or value-based metrics. Our approach
consistently improves performance and sample efficiency in both state- and
pixel-based domains. We expose the mechanisms underlying these gains, showing
how guidance promotes diversity in our generated transitions and reduces
overfitting. We also showcase how our approach can train policies with even
higher update-to-data ratios than before, opening up avenues to better scale
online RL agents.
]]></content:encoded>
<pubDate>2024-10-23T17:59:52Z</pubDate>
</item>
<item>
<title>UnCLe: Unsupervised Continual Learning of Depth Completion</title>
<link>http://arxiv.org/abs/2410.18074v1</link>
<guid>http://arxiv.org/abs/2410.18074v1</guid>
<content:encoded><![CDATA[
<div> UnCLe, benchmark, unsupervised continual learning, depth completion, catastrophic forgetting
<br />
UnCLe是一个针对多模态深度估计任务的无监督持续学习的标准基准。它旨在评估深度完成模型在连续数据流的实际情况下的表现。现有方法通常在静态数据集上进行训练，但是当适应新的非静态分布时，它们会“灾难性地遗忘”先前学到的信息。UnCLe通过使深度完成模型适应包含不同场景、不同领域和不同视觉和范围传感器捕获的数据集序列来模拟这些非静态分布。我们采用持续学习范例中的代表性方法，并将它们转化为支持无监督深度完成的持续学习。我们对室内和室外的深度完成模型进行基准测试，并通过标准的定量指标调查灾难性遗忘的程度。此外，我们引入了模型反演质量作为遗忘的额外衡量标准。我们发现无监督深度完成的持续学习是一个未解决的问题，并邀请研究人员利用UnCLe作为开发平台。 
<br /><br />总结: UnCLe是一个针对无监督持续学习深度完成任务的标准基准，旨在评估模型在连续数据流中的表现。它模拟了非静态分布，采用持续学习方法，并引入新的衡量指标。 <div>
We propose UnCLe, a standardized benchmark for Unsupervised Continual
Learning of a multimodal depth estimation task: Depth completion aims to infer
a dense depth map from a pair of synchronized RGB image and sparse depth map.
We benchmark depth completion models under the practical scenario of
unsupervised learning over continuous streams of data. Existing methods are
typically trained on a static, or stationary, dataset. However, when adapting
to novel non-stationary distributions, they "catastrophically forget"
previously learned information. UnCLe simulates these non-stationary
distributions by adapting depth completion models to sequences of datasets
containing diverse scenes captured from distinct domains using different visual
and range sensors. We adopt representative methods from continual learning
paradigms and translate them to enable unsupervised continual learning of depth
completion. We benchmark these models for indoor and outdoor and investigate
the degree of catastrophic forgetting through standard quantitative metrics.
Furthermore, we introduce model inversion quality as an additional measure of
forgetting. We find that unsupervised continual learning of depth completion is
an open problem, and we invite researchers to leverage UnCLe as a development
platform.
]]></content:encoded>
<pubDate>2024-10-23T17:56:33Z</pubDate>
</item>
<item>
<title>WorldSimBench: Towards Video Generation Models as World Simulators</title>
<link>http://arxiv.org/abs/2410.18072v1</link>
<guid>http://arxiv.org/abs/2410.18072v1</guid>
<content:encoded><![CDATA[
Recent advancements in predictive models have demonstrated exceptional
capabilities in predicting the future state of objects and scenes. However, the
lack of categorization based on inherent characteristics continues to hinder
the progress of predictive model development. Additionally, existing benchmarks
are unable to effectively evaluate higher-capability, highly embodied
predictive models from an embodied perspective. In this work, we classify the
functionalities of predictive models into a hierarchy and take the first step
in evaluating World Simulators by proposing a dual evaluation framework called
WorldSimBench. WorldSimBench includes Explicit Perceptual Evaluation and
Implicit Manipulative Evaluation, encompassing human preference assessments
from the visual perspective and action-level evaluations in embodied tasks,
covering three representative embodied scenarios: Open-Ended Embodied
Environment, Autonomous, Driving, and Robot Manipulation. In the Explicit
Perceptual Evaluation, we introduce the HF-Embodied Dataset, a video assessment
dataset based on fine-grained human feedback, which we use to train a Human
Preference Evaluator that aligns with human perception and explicitly assesses
the visual fidelity of World Simulators. In the Implicit Manipulative
Evaluation, we assess the video-action consistency of World Simulators by
evaluating whether the generated situation-aware video can be accurately
translated into the correct control signals in dynamic environments. Our
comprehensive evaluation offers key insights that can drive further innovation
in video generation models, positioning World Simulators as a pivotal
advancement toward embodied artificial intelligence.
]]></content:encoded>
<pubDate>2024-10-23T17:56:11Z</pubDate>
</item>
<item>
<title>TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing
  Prompts</title>
<link>http://arxiv.org/abs/2410.18071v1</link>
<guid>http://arxiv.org/abs/2410.18071v1</guid>
<content:encoded><![CDATA[
Recently, multimodal large language models (MLLMs) have received much
attention for their impressive capabilities. The evaluation of MLLMs is
becoming critical to analyzing attributes of MLLMs and providing valuable
insights. However, current benchmarks overlook the problem of prompt
sensitivity - minor prompt variations may lead to significant performance
fluctuations. Thus, inappropriate prompts may obscure the models' capabilities,
underestimating the models' performance. Moreover, different models have
different preferences for different prompts, and thus, using the same prompt
for all models will cause evaluation bias. This paper analyzes this deficiency
in existing benchmarks and further introduces a new evaluation framework named
TP-Eval, which introduces a prompt customization method to reduce evaluation
biases and tap models' potential. TP-Eval will rewrite the original prompts to
different customized prompts for different models. In particular, we propose
some well-designed modules for prompt customization tailored to the scenario of
MLLM evaluation. Extensive experiments demonstrate the effectiveness of our
approach to uncovering models' capabilities, and TP-Eval should benefit the
community in developing more comprehensive and convincing MLLM evaluation
benchmarks.
]]></content:encoded>
<pubDate>2024-10-23T17:54:43Z</pubDate>
</item>
<item>
<title>Altogether: Image Captioning via Re-aligning Alt-text</title>
<link>http://arxiv.org/abs/2410.17251v1</link>
<guid>http://arxiv.org/abs/2410.17251v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 合成数据, 图像标题, Alt-text, 训练数据, 人工标注
总结: 
合成数据方法用于改善图像标题质量，弥补现有方法的不足。该方法通过编辑和重新对齐现有的Alt-text，进行人工标注，生成训练数据并训练图像标题生成模型。研究结果表明，这种方法可以生成更丰富的图像标题，同时改善文本到图像生成和零样本图像分类任务的表现。这一方法突破了以往对图像标题生成的传统做法，并具有较高的实用性和可行性。<br /><br /> <div>
This paper focuses on creating synthetic data to improve the quality of image
captions. Existing works typically have two shortcomings. First, they caption
images from scratch, ignoring existing alt-text metadata, and second, lack
transparency if the captioners' training data (e.g. GPT) is unknown. In this
paper, we study a principled approach Altogether based on the key idea to edit
and re-align existing alt-texts associated with the images. To generate
training data, we perform human annotation where annotators start with the
existing alt-text and re-align it to the image content in multiple rounds,
consequently constructing captions with rich visual concepts. This differs from
prior work that carries out human annotation as a one-time description task
solely based on images and annotator knowledge. We train a captioner on this
data that generalizes the process of re-aligning alt-texts at scale. Our
results show our Altogether approach leads to richer image captions that also
improve text-to-image generation and zero-shot image classification tasks.
]]></content:encoded>
<pubDate>2024-10-22T17:59:57Z</pubDate>
</item>
<item>
<title>Frontiers in Intelligent Colonoscopy</title>
<link>http://arxiv.org/abs/2410.17241v1</link>
<guid>http://arxiv.org/abs/2410.17241v1</guid>
<content:encoded><![CDATA[
<div> 关键词: Colonoscopy, Multimodal, Scene perception, Vision-language understanding, Intelligent techniques

总结:<br /><br />本研究探讨了智能结肠镜技术及其对多模式医疗应用的潜在影响。首先通过对结肠镜场景感知的四项任务进行评估，包括分类、检测、分割和视觉语言理解，以了解当前的数据中心和模型中心情况。评估结果揭示了多模式结肠镜研究仍然有待进一步探索的挑战。为了迎接多模式时代的到来，我们建立了三个基础倡议：一个大规模的多模式指导调整数据集 ColonINST，一个专门针对结肠镜设计的多模式语言模型ColonGPT，以及一个多模式基准。为了促进对这一快速发展领域的持续监测，我们提供了一个用于最新更新的公共网站：https://github.com/ai4colonoscopy/IntelliScope。 <div>
Colonoscopy is currently one of the most sensitive screening methods for
colorectal cancer. This study investigates the frontiers of intelligent
colonoscopy techniques and their prospective implications for multimodal
medical applications. With this goal, we begin by assessing the current
data-centric and model-centric landscapes through four tasks for colonoscopic
scene perception, including classification, detection, segmentation, and
vision-language understanding. This assessment enables us to identify
domain-specific challenges and reveals that multimodal research in colonoscopy
remains open for further exploration. To embrace the coming multimodal era, we
establish three foundational initiatives: a large-scale multimodal instruction
tuning dataset ColonINST, a colonoscopy-designed multimodal language model
ColonGPT, and a multimodal benchmark. To facilitate ongoing monitoring of this
rapidly evolving field, we provide a public website for the latest updates:
https://github.com/ai4colonoscopy/IntelliScope.
]]></content:encoded>
<pubDate>2024-10-22T17:57:12Z</pubDate>
</item>
<item>
<title>xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video
  Even in VLMs</title>
<link>http://arxiv.org/abs/2410.16267v1</link>
<guid>http://arxiv.org/abs/2410.16267v1</guid>
<content:encoded><![CDATA[
<div> 关键词: xGen-MM-Vid, BLIP-3-Video, multimodal language model, temporal encoder, visual tokens

这篇文章介绍了一种名为xGen-MM-Vid（BLIP-3-Video）的多模式语言模型，专门设计用于有效捕捉多帧视频的时间信息。 BLIP-3-Video利用了“时间编码器”，除了传统的视觉标记器之外，还将多帧的标记序列映射成一组紧凑的视觉标记。这使得BLIP3-Video可以使用比竞争模型（例如32 vs. 4608 tokens）少得多的视觉标记。我们尝试了不同类型的时间编码器，包括可学习的时空池化以及类似Token Turing Machines的顺序模型。实验证实，BLIP-3-Video在使用更少的视觉标记的情况下，获得了与规模大得多的最先进模型（例如34B）相当的视频问答准确性，同时更小巧高效（即4B）。项目网站：https://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html

<br /><br />总结: 
这篇文章介绍了一种名为xGen-MM-Vid（BLIP-3-Video）的多模式语言模型，该模型特别设计用于捕捉多帧视频的时间信息。BLIP-3-Video利用了“时间编码器”和传统的视觉标记器，使得模型可以使用更少的视觉标记。作者尝试了不同类型的时间编码器，并在实验证实BLIP-3-Video使用更少的视觉标记的情况下，在视频问答准确性方面获得了非常好的结果。 <div>
We present xGen-MM-Vid (BLIP-3-Video): a multimodal language model for
videos, particularly designed to efficiently capture temporal information over
multiple frames. BLIP-3-Video takes advantage of the 'temporal encoder' in
addition to the conventional visual tokenizer, which maps a sequence of tokens
over multiple frames into a compact set of visual tokens. This enables
BLIP3-Video to use much fewer visual tokens than its competing models (e.g., 32
vs. 4608 tokens). We explore different types of temporal encoders, including
learnable spatio-temporal pooling as well as sequential models like Token
Turing Machines. We experimentally confirm that BLIP-3-Video obtains video
question-answering accuracies comparable to much larger state-of-the-art models
(e.g., 34B), while being much smaller (i.e., 4B) and more efficient by using
fewer visual tokens. The project website is at
https://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html
]]></content:encoded>
<pubDate>2024-10-21T17:59:11Z</pubDate>
</item>
<item>
<title>3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with
  View-consistent 2D Diffusion Priors</title>
<link>http://arxiv.org/abs/2410.16266v1</link>
<guid>http://arxiv.org/abs/2410.16266v1</guid>
<content:encoded><![CDATA[
<div> novel-view synthesis, 3D Gaussian splatting, 3DGS-Enhancer, view-consistent latent features, spatial-temporal decoder 

生成新视角综合是指从多个输入图像或视频生成场景的新视角，最近的技术进步如3D高斯喷溅（3DGS）在高效的流水线中实现了产生逼真渲染的显著成功。然而，在具有挑战性的情况下生成高质量的新视图，如稀疏输入视图，仍然很困难，因为欠采样区域信息不足，往往会导致明显的伪影。本文提出了3DGS-Enhancer，这是一个增强3DGS表示质量的新流水线。我们利用2D视频扩散先验来解决具有挑战性的3D视图一致性问题，将其重新制定为实现视频生成过程中的时间一致性。3DGS-Enhancer恢复了渲染新视图的具有视图一致性的潜在特征，并通过空间-时间解码器将它们与输入视图集成在一起。增强的视图然后用于微调初始3DGS模型，显著提高了其渲染性能。在大规模无限场景数据集上进行的大量实验表明，与最先进的方法相比，3DGS-Enhancer产生了优越的重建性能和高保真度的渲染结果。

<br /><br />总结: 
1. 介绍了novel-view synthesis目标和3D Gaussian splatting的进展。
2. 提出了3DGS-Enhancer，用于增强3DGS表示的质量。
3. 利用2D视频扩散先验解决了3D视图一致性问题。
4. 通过空间-时间解码器集成增强的视图和输入视图，并进行微调。
5. 在大规模无限场景数据集上进行实验证明3DGS-Enhancer的卓越性能。 <div>
Novel-view synthesis aims to generate novel views of a scene from multiple
input images or videos, and recent advancements like 3D Gaussian splatting
(3DGS) have achieved notable success in producing photorealistic renderings
with efficient pipelines. However, generating high-quality novel views under
challenging settings, such as sparse input views, remains difficult due to
insufficient information in under-sampled areas, often resulting in noticeable
artifacts. This paper presents 3DGS-Enhancer, a novel pipeline for enhancing
the representation quality of 3DGS representations. We leverage 2D video
diffusion priors to address the challenging 3D view consistency problem,
reformulating it as achieving temporal consistency within a video generation
process. 3DGS-Enhancer restores view-consistent latent features of rendered
novel views and integrates them with the input views through a spatial-temporal
decoder. The enhanced views are then used to fine-tune the initial 3DGS model,
significantly improving its rendering performance. Extensive experiments on
large-scale datasets of unbounded scenes demonstrate that 3DGS-Enhancer yields
superior reconstruction performance and high-fidelity rendering results
compared to state-of-the-art methods. The project webpage is
https://xiliu8006.github.io/3DGS-Enhancer-project .
]]></content:encoded>
<pubDate>2024-10-21T17:59:09Z</pubDate>
</item>
<item>
<title>Agent-to-Sim: Learning Interactive Behavior Models from Casual
  Longitudinal Videos</title>
<link>http://arxiv.org/abs/2410.16259v1</link>
<guid>http://arxiv.org/abs/2410.16259v1</guid>
<content:encoded><![CDATA[
We present Agent-to-Sim (ATS), a framework for learning interactive behavior
models of 3D agents from casual longitudinal video collections. Different from
prior works that rely on marker-based tracking and multiview cameras, ATS
learns natural behaviors of animal and human agents non-invasively through
video observations recorded over a long time-span (e.g., a month) in a single
environment. Modeling 3D behavior of an agent requires persistent 3D tracking
(e.g., knowing which point corresponds to which) over a long time period. To
obtain such data, we develop a coarse-to-fine registration method that tracks
the agent and the camera over time through a canonical 3D space, resulting in a
complete and persistent spacetime 4D representation. We then train a generative
model of agent behaviors using paired data of perception and motion of an agent
queried from the 4D reconstruction. ATS enables real-to-sim transfer from video
recordings of an agent to an interactive behavior simulator. We demonstrate
results on pets (e.g., cat, dog, bunny) and human given monocular RGBD videos
captured by a smartphone.
]]></content:encoded>
<pubDate>2024-10-21T17:57:50Z</pubDate>
</item>
<item>
<title>Elucidating the design space of language models for image generation</title>
<link>http://arxiv.org/abs/2410.16257v1</link>
<guid>http://arxiv.org/abs/2410.16257v1</guid>
<content:encoded><![CDATA[
The success of autoregressive (AR) language models in text generation has
inspired the computer vision community to adopt Large Language Models (LLMs)
for image generation. However, considering the essential differences between
text and image modalities, the design space of language models for image
generation remains underexplored. We observe that image tokens exhibit greater
randomness compared to text tokens, which presents challenges when training
with token prediction. Nevertheless, AR models demonstrate their potential by
effectively learning patterns even from a seemingly suboptimal optimization
problem. Our analysis also reveals that while all models successfully grasp the
importance of local information in image generation, smaller models struggle to
capture the global context. In contrast, larger models showcase improved
capabilities in this area, helping to explain the performance gains achieved
when scaling up model size. We further elucidate the design space of language
models for vision generation, including tokenizer choice, model choice, model
scalability, vocabulary design, and sampling strategy through extensive
comparative experiments. Our work is the first to analyze the optimization
behavior of language models in vision generation, and we believe it can inspire
more effective designs when applying LMs to other domains. Finally, our
elucidated language model for image generation, termed as ELM, achieves
state-of-the-art performance on the ImageNet 256*256 benchmark. The code is
available at https://github.com/Pepperlll/LMforImageGeneration.git.
]]></content:encoded>
<pubDate>2024-10-21T17:57:04Z</pubDate>
</item>
<item>
<title>BiGR: Harnessing Binary Latent Codes for Image Generation and Improved
  Visual Representation Capabilities</title>
<link>http://arxiv.org/abs/2410.14672v1</link>
<guid>http://arxiv.org/abs/2410.14672v1</guid>
<content:encoded><![CDATA[
<div> conditional image generation, binary latent codes, generative training, entropy-ordered sampling, vision tasks
<br /><br />总结:<br />
文章介绍了一种名为BiGR的新型条件图像生成模型，使用紧凑的二进制潜在代码进行生成训练，重点是提高生成和表示能力。BiGR是第一个在同一框架内统一了生成和判别的条件生成模型。它具有二进制分词器、掩模建模机制和二进制转码器，以进行二进制代码预测。此外，还引入了一种新颖的熵排序采样方法，以实现高效的图像生成。大量实验证实了BiGR在生成质量（以FID-50k衡量）和表示能力（通过线性探针准确度验证）方面的优越性。此外，BiGR在各种视觉任务中展现出了零-shot泛化的能力，实现了图像修复、扩展、编辑、插值和丰富化的应用，而无需进行结构修改。研究发现表明，BiGR有效地统一了生成和判别任务，为该领域的进一步发展铺平了道路。 <div>
We introduce BiGR, a novel conditional image generation model using compact
binary latent codes for generative training, focusing on enhancing both
generation and representation capabilities. BiGR is the first conditional
generative model that unifies generation and discrimination within the same
framework. BiGR features a binary tokenizer, a masked modeling mechanism, and a
binary transcoder for binary code prediction. Additionally, we introduce a
novel entropy-ordered sampling method to enable efficient image generation.
Extensive experiments validate BiGR's superior performance in generation
quality, as measured by FID-50k, and representation capabilities, as evidenced
by linear-probe accuracy. Moreover, BiGR showcases zero-shot generalization
across various vision tasks, enabling applications such as image inpainting,
outpainting, editing, interpolation, and enrichment, without the need for
structural modifications. Our findings suggest that BiGR unifies generative and
discriminative tasks effectively, paving the way for further advancements in
the field.
]]></content:encoded>
<pubDate>2024-10-18T17:59:04Z</pubDate>
</item>
<item>
<title>MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image
  Description and Reasoning Steps</title>
<link>http://arxiv.org/abs/2410.14668v1</link>
<guid>http://arxiv.org/abs/2410.14668v1</guid>
<content:encoded><![CDATA[
<div> 关键词: Multimodal Chain of Thought, MCoT, MiCEval, 评估框架, 复杂推理任务
总结:
Multimodal Chain of Thought (MCoT)是一种常用的提示策略，用于改进多模式大型语言模型(MLLMs)在各种复杂推理任务中的性能。MiCEval是一个评估框架，旨在评估推理链的正确性，评估描述和推理步骤的质量。它建立在一个细粒度的数据集上，对每个步骤进行正确性、相关性和信息性的评分。在四种最先进的MLLM上进行的实验表明，使用MiCEval进行逐步评估与基于余弦相似度或微调方法的现有方法更符合人类判断。MiCEval的数据集和代码可以在https://github.com/alenai97/MiCEval找到。 <br /><br /> <div>
Multimodal Chain of Thought (MCoT) is a popular prompting strategy for
improving the performance of multimodal large language models (MLLMs) across a
range of complex reasoning tasks. Despite its popularity, there is a notable
absence of automated methods for evaluating the quality of reasoning steps in
MCoT. To address this gap, we propose Multimodal Chain-of-Thought Evaluation
(MiCEval), a framework designed to assess the correctness of reasoning chains
by evaluating the quality of both the description and each reasoning step. The
evaluation of the description component focuses on the accuracy of the image
descriptions, while the reasoning step evaluates the quality of each step as it
is conditionally generated based on the preceding steps. MiCEval is built upon
a fine-grained dataset with annotations that rate each step according to
correctness, relevance, and informativeness. Extensive experiments on four
state-of-the-art MLLMs show that step-wise evaluations using MiCEval align more
closely with human judgments compared to existing methods based on cosine
similarity or fine-tuning approaches. MiCEval datasets and code can be found in
https://github.com/alenai97/MiCEval.
]]></content:encoded>
<pubDate>2024-10-18T17:57:40Z</pubDate>
</item>
<item>
<title>Fluid: Scaling Autoregressive Text-to-image Generative Models with
  Continuous Tokens</title>
<link>http://arxiv.org/abs/2410.13863v1</link>
<guid>http://arxiv.org/abs/2410.13863v1</guid>
<content:encoded><![CDATA[
<div> 关键词: autoregressive models, text-to-image generation, discrete tokens, continuous tokens, transformer architectures

总结:<br /><br />这项研究调查了在文本到图像生成中的扩展问题，关注了离散或连续标记以及随机或固定光栅顺序使用BERT或GPT等变压器架构这两个关键因素。结果显示，尽管所有模型在验证损失方面有效扩展，但它们的评估性能（以FID、GenEval得分和视觉质量衡量）存在不同的趋势。基于连续标记的模型在视觉质量上明显优于使用离散标记的模型。此外，生成顺序和注意机制显著影响GenEval得分：随机顺序模型的GenEval得分明显优于光栅顺序模型。在这些发现的启发下，研究人员训练了Fluid，这是一个基于连续标记的随机顺序自回归模型。Fluid 10.5B模型在MS-COCO 30K上实现了新的零样本FID状态，为6.16，同时在GenEval基准上获得了整体得分0.69。希望这些发现和结果能够鼓励未来努力进一步弥合视觉和语言模型之间的扩展差距。 <div>
Scaling up autoregressive models in vision has not proven as beneficial as in
large language models. In this work, we investigate this scaling problem in the
context of text-to-image generation, focusing on two critical factors: whether
models use discrete or continuous tokens, and whether tokens are generated in a
random or fixed raster order using BERT- or GPT-like transformer architectures.
Our empirical results show that, while all models scale effectively in terms of
validation loss, their evaluation performance -- measured by FID, GenEval
score, and visual quality -- follows different trends. Models based on
continuous tokens achieve significantly better visual quality than those using
discrete tokens. Furthermore, the generation order and attention mechanisms
significantly affect the GenEval score: random-order models achieve notably
better GenEval scores compared to raster-order models. Inspired by these
findings, we train Fluid, a random-order autoregressive model on continuous
tokens. Fluid 10.5B model achieves a new state-of-the-art zero-shot FID of 6.16
on MS-COCO 30K, and 0.69 overall score on the GenEval benchmark. We hope our
findings and results will encourage future efforts to further bridge the
scaling gap between vision and language models.
]]></content:encoded>
<pubDate>2024-10-17T17:59:59Z</pubDate>
</item>
<item>
<title>PUMA: Empowering Unified MLLM with Multi-granular Visual Generation</title>
<link>http://arxiv.org/abs/2410.13861v1</link>
<guid>http://arxiv.org/abs/2410.13861v1</guid>
<content:encoded><![CDATA[
<div> 多模态基础模型，视觉-语言理解，MLLM，图像生成，PUMA

这项工作介绍了PUMA，它是一种统一的MLLM框架，能够统一多粒度视觉特征作为MLLM的输入和输出，在不同图像生成任务中满足不同粒度的要求。通过多模态预训练和任务特定指导调整，PUMA在多种多模态任务中展现了出色的表现，是朝着真正统一的MLLM迈出的重要一步。PUMA的代码和模型将在https://github.com/rongyaofang/PUMA发布。<br /><br />总结: 近期的多模态基础模型的进步在视觉-语言理解方面取得了显著进展。然而，现有研究不够充分地解决了统一MLLM范式下不同图像生成任务的不同粒度需求。本工作提出的PUMA能够统一多粒度视觉特征，并在多种多模态任务中展现出优异的性能，是迈向真正统一MLLM的重要一步。 <div>
Recent advancements in multimodal foundation models have yielded significant
progress in vision-language understanding. Initial attempts have also explored
the potential of multimodal large language models (MLLMs) for visual content
generation. However, existing works have insufficiently addressed the varying
granularity demands of different image generation tasks within a unified MLLM
paradigm - from the diversity required in text-to-image generation to the
precise controllability needed in image manipulation. In this work, we propose
PUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA
unifies multi-granular visual features as both inputs and outputs of MLLMs,
elegantly addressing the different granularity requirements of various image
generation tasks within a unified MLLM framework. Following multimodal
pretraining and task-specific instruction tuning, PUMA demonstrates proficiency
in a wide range of multimodal tasks. This work represents a significant step
towards a truly unified MLLM capable of adapting to the granularity demands of
various visual tasks. The code and model will be released in
https://github.com/rongyaofang/PUMA.
]]></content:encoded>
<pubDate>2024-10-17T17:59:57Z</pubDate>
</item>
<item>
<title>$γ-$MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large
  Language Models</title>
<link>http://arxiv.org/abs/2410.13859v1</link>
<guid>http://arxiv.org/abs/2410.13859v1</guid>
<content:encoded><![CDATA[
Despite the significant progress in multimodal large language models (MLLMs),
their high computational cost remains a barrier to real-world deployment.
Inspired by the mixture of depths (MoDs) in natural language processing, we aim
to address this limitation from the perspective of ``activated tokens''. Our
key insight is that if most tokens are redundant for the layer computation,
then can be skipped directly via the MoD layer. However, directly converting
the dense layers of MLLMs to MoD layers leads to substantial performance
degradation. To address this issue, we propose an innovative MoD adaptation
strategy for existing MLLMs called $\gamma$-MoD. In $\gamma$-MoD, a novel
metric is proposed to guide the deployment of MoDs in the MLLM, namely rank of
attention maps (ARank). Through ARank, we can effectively identify which layer
is redundant and should be replaced with the MoD layer. Based on ARank, we
further propose two novel designs to maximize the computational sparsity of
MLLM while maintaining its performance, namely shared vision-language router
and masked routing learning. With these designs, more than 90% dense layers of
the MLLM can be effectively converted to the MoD ones. To validate our method,
we apply it to three popular MLLMs, and conduct extensive experiments on 9
benchmark datasets. Experimental results not only validate the significant
efficiency benefit of $\gamma$-MoD to existing MLLMs but also confirm its
generalization ability on various MLLMs. For example, with a minor performance
drop, i.e., -1.5%, $\gamma$-MoD can reduce the training and inference time of
LLaVA-HR by 31.0% and 53.2%, respectively.
]]></content:encoded>
<pubDate>2024-10-17T17:59:53Z</pubDate>
</item>
<item>
<title>The Curse of Multi-Modalities: Evaluating Hallucinations of Large
  Multimodal Models across Language, Visual, and Audio</title>
<link>http://arxiv.org/abs/2410.12787v1</link>
<guid>http://arxiv.org/abs/2410.12787v1</guid>
<content:encoded><![CDATA[
<div> 多模态模型、幻觉、挑战、CMM基准、研究方向
<br /><br />总结:
最近，大型多模态模型在各种任务中性能大幅提升，但仍然面临幻觉问题。本文首次系统调查了涉及语言、视觉和音频的多模态模型中的幻觉现象。研究发现，幻觉的两个主要贡献者是对单模态先验的过度依赖和虚假的跨模态相关性。为了解决这些挑战，引入了CMM基准，全面评估了多模态模型中的幻觉现象，并提出了潜在的研究方向，以增强多模态模型的可靠性。 <div>
Recent advancements in large multimodal models (LMMs) have significantly
enhanced performance across diverse tasks, with ongoing efforts to further
integrate additional modalities such as video and audio. However, most existing
LMMs remain vulnerable to hallucinations, the discrepancy between the factual
multimodal input and the generated textual output, which has limited their
applicability in various real-world scenarios. This paper presents the first
systematic investigation of hallucinations in LMMs involving the three most
common modalities: language, visual, and audio. Our study reveals two key
contributors to hallucinations: overreliance on unimodal priors and spurious
inter-modality correlations. To address these challenges, we introduce the
benchmark The Curse of Multi-Modalities (CMM), which comprehensively evaluates
hallucinations in LMMs, providing a detailed analysis of their underlying
issues. Our findings highlight key vulnerabilities, including imbalances in
modality integration and biases from training data, underscoring the need for
balanced cross-modal learning and enhanced hallucination mitigation strategies.
Based on our observations and findings, we suggest potential research
directions that could enhance the reliability of LMMs.
]]></content:encoded>
<pubDate>2024-10-16T17:59:02Z</pubDate>
</item>
<item>
<title>JudgeBench: A Benchmark for Evaluating LLM-based Judges</title>
<link>http://arxiv.org/abs/2410.12784v1</link>
<guid>http://arxiv.org/abs/2410.12784v1</guid>
<content:encoded><![CDATA[
<div> 评估框架, LLM-based judges, JudgeBench, 挑战性响应对, GitHub链接
总结:<br />
本文提出了评估LLM-based judges准确性的新框架，以及一个名为JudgeBench的基准测试。该基准测试利用了一种新的处理流程，将现有的困难数据集转化为具有客观正确性偏好标签的挑战性响应对。研究发现，JudgeBench相比于之前的基准测试更具挑战性，许多强大的模型（如GPT-4o）的表现仅略好于随机猜测。JudgeBench为评估不断进步的LLM-based judges提供了可靠的平台。GitHub上提供了数据和代码。 <div>
LLM-based judges have emerged as a scalable alternative to human evaluation
and are increasingly used to assess, compare, and improve models. However, the
reliability of LLM-based judges themselves is rarely scrutinized. As LLMs
become more advanced, their responses grow more sophisticated, requiring
stronger judges to evaluate them. Existing benchmarks primarily focus on a
judge's alignment with human preferences, but often fail to account for more
challenging tasks where crowdsourced human preference is a poor indicator of
factual and logical correctness. To address this, we propose a novel evaluation
framework to objectively evaluate LLM-based judges. Based on this framework, we
propose JudgeBench, a benchmark for evaluating LLM-based judges on challenging
response pairs spanning knowledge, reasoning, math, and coding. JudgeBench
leverages a novel pipeline for converting existing difficult datasets into
challenging response pairs with preference labels reflecting objective
correctness. Our comprehensive evaluation on a collection of prompted judges,
fine-tuned judges, multi-agent judges, and reward models shows that JudgeBench
poses a significantly greater challenge than previous benchmarks, with many
strong models (e.g., GPT-4o) performing just slightly better than random
guessing. Overall, JudgeBench offers a reliable platform for assessing
increasingly advanced LLM-based judges. Data and code are available at
https://github.com/ScalerLab/JudgeBench .
]]></content:encoded>
<pubDate>2024-10-16T17:58:19Z</pubDate>
</item>
<item>
<title>On the Effectiveness of Dataset Alignment for Fake Image Detection</title>
<link>http://arxiv.org/abs/2410.11835v1</link>
<guid>http://arxiv.org/abs/2410.11835v1</guid>
<content:encoded><![CDATA[
<div> fake image detector, latent diffusion models (LDMs), dataset alignment, generative models, image generation<br />
<br />
本文讨论了随着潜在扩散模型（LDMs）民主化图像生成能力，检测假图片的需求日益增长的问题。文章认为，除了算法选择之外，还需要一个良好对齐的真/假图像数据集来训练假图像检测器。针对LDMs家族，提出了通过使用LDMs自动编码器重建所有真实图像的简单方法来实现数据对齐。最后，通过使用非自然对象的图像构建检测器，并取得了令人满意的结果，证明了数据集对齐的有效性。整体来说，本研究确定了训练假图像检测器时出现的微妙但重要的问题，并提出了一个简单且廉价的解决方案来解决这些问题。<br /><br />总结: <br />fake image detector的需求增长；LDMs的影响；数据集对齐的重要性；通过重建真实图像来实现对齐；非自然对象图像的检测效果。 <div>
As latent diffusion models (LDMs) democratize image generation capabilities,
there is a growing need to detect fake images. A good detector should focus on
the generative models fingerprints while ignoring image properties such as
semantic content, resolution, file format, etc. Fake image detectors are
usually built in a data driven way, where a model is trained to separate real
from fake images. Existing works primarily investigate network architecture
choices and training recipes. In this work, we argue that in addition to these
algorithmic choices, we also require a well aligned dataset of real/fake images
to train a robust detector. For the family of LDMs, we propose a very simple
way to achieve this: we reconstruct all the real images using the LDMs
autoencoder, without any denoising operation. We then train a model to separate
these real images from their reconstructions. The fakes created this way are
extremely similar to the real ones in almost every aspect (e.g., size, aspect
ratio, semantic content), which forces the model to look for the LDM decoders
artifacts. We empirically show that this way of creating aligned real/fake
datasets, which also sidesteps the computationally expensive denoising process,
helps in building a detector that focuses less on spurious correlations,
something that a very popular existing method is susceptible to. Finally, to
demonstrate just how effective the alignment in a dataset can be, we build a
detector using images that are not natural objects, and present promising
results. Overall, our work identifies the subtle but significant issues that
arise when training a fake image detector and proposes a simple and inexpensive
solution to address these problems.
]]></content:encoded>
<pubDate>2024-10-15T17:58:07Z</pubDate>
</item>
<item>
<title>Tex4D: Zero-shot 4D Scene Texturing with Video Diffusion Models</title>
<link>http://arxiv.org/abs/2410.10821v1</link>
<guid>http://arxiv.org/abs/2410.10821v1</guid>
<content:encoded><![CDATA[
<div> 4D纹理, 三维网格, 视频扩散模型, 多视图一致, 时间一致

Tex4D是一种零-shot方法，它将来自网格序列的固有3D几何知识与视频扩散模型的表达能力相结合，以产生多视图和时间一致的4D纹理。该方法通过UV空间中的潜在聚合来增强多视图一致性，在纹理合成过程中利用先前的条件视频生成模型来确保时间一致性。为了解决模糊的结果问题，对DDIM采样过程进行了简单而有效的修改。此外，引入了参考潜在纹理来加强去噪过程中帧之间的相关性。Tex4D是首个专门设计用于4D场景纹理的方法。大量实验表明，它在基于未纹理的网格序列上产生多视图和多帧一致的视频方面具有优势。 <br /><br />总结: 4D纹理, 三维网格, 视频扩散模型, 多视图一致, 时间一致 <div>
3D meshes are widely used in computer vision and graphics for their
efficiency in animation and minimal memory use, playing a crucial role in
movies, games, AR, and VR. However, creating temporally consistent and
realistic textures for mesh sequences remains labor-intensive for professional
artists. On the other hand, while video diffusion models excel at text-driven
video generation, they often lack 3D geometry awareness and struggle with
achieving multi-view consistent texturing for 3D meshes. In this work, we
present Tex4D, a zero-shot approach that integrates inherent 3D geometry
knowledge from mesh sequences with the expressiveness of video diffusion models
to produce multi-view and temporally consistent 4D textures. Given an
untextured mesh sequence and a text prompt as inputs, our method enhances
multi-view consistency by synchronizing the diffusion process across different
views through latent aggregation in the UV space. To ensure temporal
consistency, we leverage prior knowledge from a conditional video generation
model for texture synthesis. However, straightforwardly combining the video
diffusion model and the UV texture aggregation leads to blurry results. We
analyze the underlying causes and propose a simple yet effective modification
to the DDIM sampling process to address this issue. Additionally, we introduce
a reference latent texture to strengthen the correlation between frames during
the denoising process. To the best of our knowledge, Tex4D is the first method
specifically designed for 4D scene texturing. Extensive experiments demonstrate
its superiority in producing multi-view and multi-frame consistent videos based
on untextured mesh sequences.
]]></content:encoded>
<pubDate>2024-10-14T17:59:59Z</pubDate>
</item>
<item>
<title>When Does Perceptual Alignment Benefit Vision Representations?</title>
<link>http://arxiv.org/abs/2410.10817v1</link>
<guid>http://arxiv.org/abs/2410.10817v1</guid>
<content:encoded><![CDATA[
<div> 模型表示，人类感知相似性，微调，计算机视觉任务，改进，人类知识注入<br />
总结:<br />
这篇文章研究了如何将视觉模型表示与人类感知判断相结合，以提高在各种计算机视觉任务中的可用性。研究发现，将模型与人类感知判断对齐可以改善各种下游任务的性能，包括计数、分割、深度估计、实例检索和检索增强生成。此外，性能在医学成像和3D环境帧等特定领域的任务中得到广泛保留。研究结果表明，在视觉模型中注入关于人类感知知识的归纳偏见可以提高模型表示。 <div>
Humans judge perceptual similarity according to diverse visual attributes,
including scene layout, subject location, and camera pose. Existing vision
models understand a wide range of semantic abstractions but improperly weigh
these attributes and thus make inferences misaligned with human perception.
While vision representations have previously benefited from alignment in
contexts like image generation, the utility of perceptually aligned
representations in more general-purpose settings remains unclear. Here, we
investigate how aligning vision model representations to human perceptual
judgments impacts their usability across diverse computer vision tasks. We
finetune state-of-the-art models on human similarity judgments for image
triplets and evaluate them across standard vision benchmarks. We find that
aligning models to perceptual judgments yields representations that improve
upon the original backbones across many downstream tasks, including counting,
segmentation, depth estimation, instance retrieval, and retrieval-augmented
generation. In addition, we find that performance is widely preserved on other
tasks, including specialized out-of-distribution domains such as in medical
imaging and 3D environment frames. Our results suggest that injecting an
inductive bias about human perceptual knowledge into vision models can
contribute to better representations.
]]></content:encoded>
<pubDate>2024-10-14T17:59:58Z</pubDate>
</item>
<item>
<title>TemporalBench: Benchmarking Fine-grained Temporal Understanding for
  Multimodal Video Models</title>
<link>http://arxiv.org/abs/2410.10818v1</link>
<guid>http://arxiv.org/abs/2410.10818v1</guid>
<content:encoded><![CDATA[
Understanding fine-grained temporal dynamics is crucial for multimodal video
comprehension and generation. Due to the lack of fine-grained temporal
annotations, existing video benchmarks mostly resemble static image benchmarks
and are incompetent at evaluating models for temporal understanding. In this
paper, we introduce TemporalBench, a new benchmark dedicated to evaluating
fine-grained temporal understanding in videos. TemporalBench consists of ~10K
video question-answer pairs, derived from ~2K high-quality human annotations
detailing the temporal dynamics in video clips. As a result, our benchmark
provides a unique testbed for evaluating various temporal understanding and
reasoning abilities such as action frequency, motion magnitude, event order,
etc. Moreover, it enables evaluations on various tasks like both video question
answering and captioning, both short and long video understanding, as well as
different models such as multimodal video embedding models and text generation
models. Results show that state-of-the-art models like GPT-4o achieve only
38.5% question answering accuracy on TemporalBench, demonstrating a significant
gap (~30%) between humans and AI in temporal understanding. Furthermore, we
notice a critical pitfall for multi-choice QA where LLMs can detect the subtle
changes in negative captions and find a centralized description as a cue for
its prediction, where we propose Multiple Binary Accuracy (MBA) to correct such
bias. We hope that TemporalBench can foster research on improving models'
temporal reasoning capabilities. Both dataset and evaluation code will be made
available.
]]></content:encoded>
<pubDate>2024-10-14T17:59:58Z</pubDate>
</item>
<item>
<title>LVD-2M: A Long-take Video Dataset with Temporally Dense Captions</title>
<link>http://arxiv.org/abs/2410.10816v1</link>
<guid>http://arxiv.org/abs/2410.10816v1</guid>
<content:encoded><![CDATA[
The efficacy of video generation models heavily depends on the quality of
their training datasets. Most previous video generation models are trained on
short video clips, while recently there has been increasing interest in
training long video generation models directly on longer videos. However, the
lack of such high-quality long videos impedes the advancement of long video
generation. To promote research in long video generation, we desire a new
dataset with four key features essential for training long video generation
models: (1) long videos covering at least 10 seconds, (2) long-take videos
without cuts, (3) large motion and diverse contents, and (4) temporally dense
captions. To achieve this, we introduce a new pipeline for selecting
high-quality long-take videos and generating temporally dense captions.
Specifically, we define a set of metrics to quantitatively assess video quality
including scene cuts, dynamic degrees, and semantic-level quality, enabling us
to filter high-quality long-take videos from a large amount of source videos.
Subsequently, we develop a hierarchical video captioning pipeline to annotate
long videos with temporally-dense captions. With this pipeline, we curate the
first long-take video dataset, LVD-2M, comprising 2 million long-take videos,
each covering more than 10 seconds and annotated with temporally dense
captions. We further validate the effectiveness of LVD-2M by fine-tuning video
generation models to generate long videos with dynamic motions. We believe our
work will significantly contribute to future research in long video generation.
]]></content:encoded>
<pubDate>2024-10-14T17:59:56Z</pubDate>
</item>
<item>
<title>HART: Efficient Visual Generation with Hybrid Autoregressive Transformer</title>
<link>http://arxiv.org/abs/2410.10812v1</link>
<guid>http://arxiv.org/abs/2410.10812v1</guid>
<content:encoded><![CDATA[
We introduce Hybrid Autoregressive Transformer (HART), an autoregressive (AR)
visual generation model capable of directly generating 1024x1024 images,
rivaling diffusion models in image generation quality. Existing AR models face
limitations due to the poor image reconstruction quality of their discrete
tokenizers and the prohibitive training costs associated with generating 1024px
images. To address these challenges, we present the hybrid tokenizer, which
decomposes the continuous latents from the autoencoder into two components:
discrete tokens representing the big picture and continuous tokens representing
the residual components that cannot be represented by the discrete tokens. The
discrete component is modeled by a scalable-resolution discrete AR model, while
the continuous component is learned with a lightweight residual diffusion
module with only 37M parameters. Compared with the discrete-only VAR tokenizer,
our hybrid approach improves reconstruction FID from 2.11 to 0.30 on MJHQ-30K,
leading to a 31% generation FID improvement from 7.85 to 5.38. HART also
outperforms state-of-the-art diffusion models in both FID and CLIP score, with
4.5-7.7x higher throughput and 6.9-13.4x lower MACs. Our code is open sourced
at https://github.com/mit-han-lab/hart.
]]></content:encoded>
<pubDate>2024-10-14T17:59:42Z</pubDate>
</item>
<item>
<title>SceneCraft: Layout-Guided 3D Scene Generation</title>
<link>http://arxiv.org/abs/2410.09049v1</link>
<guid>http://arxiv.org/abs/2410.09049v1</guid>
<content:encoded><![CDATA[
<div> 生成, 室内场景, 文字描述, 3D建模, 场景生成

本文介绍了一种新方法SceneCraft，用于根据用户提供的文字描述和空间布局偏好生成详细的室内场景。该方法的核心是基于渲染的技术，将3D语义布局转换为多视图2D代理地图。此外，文章设计了一个语义和深度条件扩散模型，用于生成多视图图像，这些图像用于学习神经辐射场（NeRF）作为最终场景表示。通过实验分析，我们证明了我们的方法在复杂室内场景生成方面明显优于现有方法，具有多样的纹理，一致的几何结构和逼真的视觉质量。 <div>
The creation of complex 3D scenes tailored to user specifications has been a
tedious and challenging task with traditional 3D modeling tools. Although some
pioneering methods have achieved automatic text-to-3D generation, they are
generally limited to small-scale scenes with restricted control over the shape
and texture. We introduce SceneCraft, a novel method for generating detailed
indoor scenes that adhere to textual descriptions and spatial layout
preferences provided by users. Central to our method is a rendering-based
technique, which converts 3D semantic layouts into multi-view 2D proxy maps.
Furthermore, we design a semantic and depth conditioned diffusion model to
generate multi-view images, which are used to learn a neural radiance field
(NeRF) as the final scene representation. Without the constraints of panorama
image generation, we surpass previous methods in supporting complicated indoor
space generation beyond a single room, even as complicated as a whole
multi-bedroom apartment with irregular shapes and layouts. Through experimental
analysis, we demonstrate that our method significantly outperforms existing
approaches in complex indoor scene generation with diverse textures, consistent
geometry, and realistic visual quality. Code and more results are available at:
https://orangesodahub.github.io/SceneCraft
]]></content:encoded>
<pubDate>2024-10-11T17:59:58Z</pubDate>
</item>
<item>
<title>Emerging Pixel Grounding in Large Multimodal Models Without Grounding
  Supervision</title>
<link>http://arxiv.org/abs/2410.08209v1</link>
<guid>http://arxiv.org/abs/2410.08209v1</guid>
<content:encoded><![CDATA[
<div> LMMs, grounding, supervision, DIFFLMM, attention maps
<br />
本文研究了当前大型多模态模型（LMMs）在概念描绘方面的挑战，提出了一种新的方法来解决这一问题。作者发现，在没有明确的概念描绘监督的情况下，LMMs可以自发地产生概念描绘能力。他们提出了一种“关注和分割”方法，利用标准LMMs的注意力图来执行像素级分割。此外，他们还提出了DIFFLMM，这是一种利用基于扩散的视觉编码器的LMM，与标准的CLIP视觉编码器相反，并且使用相同的弱监督进行训练。通过这种方法，他们在概念描绘特定和一般的视觉问题回答基准上取得了竞争性的表现。特别是，在没有任何概念描绘监督的情况下，他们在概念对话生成上实现了44.2的概念蒙版召回率，优于GLaMM模型。 <div>
Current large multimodal models (LMMs) face challenges in grounding, which
requires the model to relate language components to visual entities. Contrary
to the common practice that fine-tunes LMMs with additional grounding
supervision, we find that the grounding ability can in fact emerge in LMMs
trained without explicit grounding supervision. To reveal this emerging
grounding, we introduce an "attend-and-segment" method which leverages
attention maps from standard LMMs to perform pixel-level segmentation.
Furthermore, to enhance the grounding ability, we propose DIFFLMM, an LMM
utilizing a diffusion-based visual encoder, as opposed to the standard CLIP
visual encoder, and trained with the same weak supervision. Without being
constrained by the biases and limited scale of grounding-specific supervision
data, our approach is more generalizable and scalable. We achieve competitive
performance on both grounding-specific and general visual question answering
benchmarks, compared with grounding LMMs and generalist LMMs, respectively.
Notably, we achieve a 44.2 grounding mask recall on grounded conversation
generation without any grounding supervision, outperforming the extensively
supervised model GLaMM. Project page: https://groundLMM.github.io.
]]></content:encoded>
<pubDate>2024-10-10T17:59:55Z</pubDate>
</item>
<item>
<title>Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large
  Language Models with Endogenous Visual Pre-training</title>
<link>http://arxiv.org/abs/2410.08202v1</link>
<guid>http://arxiv.org/abs/2410.08202v1</guid>
<content:encoded><![CDATA[
<div> Monolithic Multimodal Large Language Models, Visual encoding, Language decoding, Delta tuning, Endogenous Visual Pre-training
Summary:<br />
本文介绍了关于单体多模态大型语言模型的研究。作者提出了一种名为Mono-InternVL的新型单体MLLM，该模型通过多模态专家混合结构，无缝地集成了一组视觉专家。他们还提出了一种名为内源视觉预训练（EViP）的创新预训练策略，以最大限度地发挥Mono-InternVL的视觉能力。实验结果证实了Mono-InternVL相对于现有MLLM的优越性能，特别是在6个多模态基准测试上的表现。此外，Mono-InternVL的部署效率也得到了验证，第一个标记的延迟降低了高达67%。<br />
<br />
总结: <br />
- 介绍了一个名为Mono-InternVL的新型单体MLLM，该模型通过多模态专家混合结构，无缝地集成了一组视觉专家。
- 提出了一种名为内源视觉预训练（EViP）的创新预训练策略，以最大限度地发挥Mono-InternVL的视觉能力。
- 实验结果证实了Mono-InternVL相对于现有MLLM的优越性能，特别是在6个多模态基准测试上的表现。
- Mono-InternVL的部署效率得到了验证，第一个标记的延迟降低了高达67%。 <div>
The rapid advancement of Large Language Models (LLMs) has led to an influx of
efforts to extend their capabilities to multimodal tasks. Among them, growing
attention has been focused on monolithic Multimodal Large Language Models
(MLLMs) that integrate visual encoding and language decoding into a single LLM.
Despite the structural simplicity and deployment-friendliness, training a
monolithic MLLM with promising performance still remains challenging. In
particular, the popular approaches adopt continuous pre-training to extend a
pre-trained LLM to a monolithic MLLM, which suffers from catastrophic
forgetting and leads to performance degeneration. In this paper, we aim to
overcome this limitation from the perspective of delta tuning. Specifically,
our core idea is to embed visual parameters into a pre-trained LLM, thereby
incrementally learning visual knowledge from massive data via delta tuning,
i.e., freezing the LLM when optimizing the visual parameters. Based on this
principle, we present Mono-InternVL, a novel monolithic MLLM that seamlessly
integrates a set of visual experts via a multimodal mixture-of-experts
structure. Moreover, we propose an innovative pre-training strategy to maximize
the visual capability of Mono-InternVL, namely Endogenous Visual Pre-training
(EViP). In particular, EViP is designed as a progressive learning process for
visual experts, which aims to fully exploit the visual knowledge from noisy
data to high-quality data. To validate our approach, we conduct extensive
experiments on 16 benchmarks. Experimental results not only validate the
superior performance of Mono-InternVL compared to the state-of-the-art MLLM on
6 multimodal benchmarks, e.g., +113 points over InternVL-1.5 on OCRBench, but
also confirm its better deployment efficiency, with first token latency reduced
by up to 67%.
]]></content:encoded>
<pubDate>2024-10-10T17:59:22Z</pubDate>
</item>
<item>
<title>MM-Ego: Towards Building Egocentric Multimodal LLMs</title>
<link>http://arxiv.org/abs/2410.07177v1</link>
<guid>http://arxiv.org/abs/2410.07177v1</guid>
<content:encoded><![CDATA[
<div> 关键词: egocentric video, multimodal foundation model, QA data, data engine, Memory Pointer Prompting<br />
<br />
该研究旨在全面探索建立自我中心视频理解的多模态基础模型。为实现这一目标，研究在三个方面展开。首先，鉴于自我中心视频理解缺乏问答数据，研究开发了一个数据引擎，基于人工注释数据，高效生成了700万个高质量自我中心视频问答样本，覆盖30秒至一小时不等的视频长度范围，目前这是最大的自我中心问答数据集。其次，研究贡献了一个具有挑战性的自我中心问答基准，包含629个视频和7026个问题，以评估模型在识别和记忆不同长度视频中的视觉细节方面的能力。研究还引入了一种新的去偏方法来减轻模型评估中不可避免的语言偏差。第三，研究提出了一种专门的多模态架构，采用了一种新颖的“记忆指针提示”机制。该设计包括一个全局视角步骤，以获得整个视频的全面理解并识别关键视觉信息，然后使用关键视觉信息生成响应的回退步骤。这使得模型能更有效地理解扩展视频内容。通过数据、基准和模型，研究成功构建了MM-Ego，一种自我中心多模态LLM，在自我中心视频理解方面表现出强大的性能。<br /><br />总结: 该研究在自我中心视频理解领域取得了重要进展，通过开发数据引擎和构建多模态架构，成功生成了大规模高质量的问答样本，并提出了具有挑战性的基准测试。其提出的模型在自我中心视频理解方面表现出强大的性能，为该领域的研究和应用带来了新的可能性。 <div>
This research aims to comprehensively explore building a multimodal
foundation model for egocentric video understanding. To achieve this goal, we
work on three fronts. First, as there is a lack of QA data for egocentric video
understanding, we develop a data engine that efficiently generates 7M
high-quality QA samples for egocentric videos ranging from 30 seconds to one
hour long, based on human-annotated data. This is currently the largest
egocentric QA dataset. Second, we contribute a challenging egocentric QA
benchmark with 629 videos and 7,026 questions to evaluate the models' ability
in recognizing and memorizing visual details across videos of varying lengths.
We introduce a new de-biasing evaluation method to help mitigate the
unavoidable language bias present in the models being evaluated. Third, we
propose a specialized multimodal architecture featuring a novel "Memory Pointer
Prompting" mechanism. This design includes a global glimpse step to gain an
overarching understanding of the entire video and identify key visual
information, followed by a fallback step that utilizes the key visual
information to generate responses. This enables the model to more effectively
comprehend extended video content. With the data, benchmark, and model, we
successfully build MM-Ego, an egocentric multimodal LLM that shows powerful
performance on egocentric video understanding.
]]></content:encoded>
<pubDate>2024-10-09T17:59:59Z</pubDate>
</item>
<item>
<title>Do better language models have crisper vision?</title>
<link>http://arxiv.org/abs/2410.07173v1</link>
<guid>http://arxiv.org/abs/2410.07173v1</guid>
<content:encoded><![CDATA[
<div> Visual Text Representation Benchmark, ViTeRB, large-scale decoder-based LLMs, ShareLock, CLIP-like model
<br /><br />
总结：<br />
本文介绍了关于文本-图像对齐的问题，提出了Visual Text Representation Benchmark (ViTeRB)用于评估语言模型在视觉世界中的表现。作者指出大规模解码器型语言模型是在视觉相关环境中很好的文本表达候选者，并提出了一个名为ShareLock的超轻量级CLIP模型。ShareLock利用了强大的视觉和语言模型的预先计算的冻结特征，仅利用563k图像-标题对就在ImageNet上取得了51%的准确率。而且训练仅需1个GPU小时，很大程度上减少了以往方法的时间成本。 <div>
How well do text-only Large Language Models (LLMs) grasp the visual world? As
LLMs are increasingly used in computer vision, addressing this question becomes
both fundamental and pertinent. However, existing studies have primarily
focused on limited scenarios, such as their ability to generate visual content
or cluster multimodal data. To this end, we propose the Visual Text
Representation Benchmark (ViTeRB) to isolate key properties that make language
models well-aligned with the visual world. With this, we identify large-scale
decoder-based LLMs as ideal candidates for representing text in vision-centric
contexts, counter to the current practice of utilizing text encoders. Building
on these findings, we propose ShareLock, an ultra-lightweight CLIP-like model.
By leveraging precomputable frozen features from strong vision and language
models, ShareLock achieves an impressive 51% accuracy on ImageNet despite
utilizing just 563k image-caption pairs. Moreover, training requires only 1 GPU
hour (or 10 hours including the precomputation of features) - orders of
magnitude less than prior methods. Code will be released.
]]></content:encoded>
<pubDate>2024-10-09T17:59:33Z</pubDate>
</item>
<item>
<title>IterComp: Iterative Composition-Aware Feedback Learning from Model
  Gallery for Text-to-Image Generation</title>
<link>http://arxiv.org/abs/2410.07171v1</link>
<guid>http://arxiv.org/abs/2410.07171v1</guid>
<content:encoded><![CDATA[
Advanced diffusion models like RPG, Stable Diffusion 3 and FLUX have made
notable strides in compositional text-to-image generation. However, these
methods typically exhibit distinct strengths for compositional generation, with
some excelling in handling attribute binding and others in spatial
relationships. This disparity highlights the need for an approach that can
leverage the complementary strengths of various models to comprehensively
improve the composition capability. To this end, we introduce IterComp, a novel
framework that aggregates composition-aware model preferences from multiple
models and employs an iterative feedback learning approach to enhance
compositional generation. Specifically, we curate a gallery of six powerful
open-source diffusion models and evaluate their three key compositional
metrics: attribute binding, spatial relationships, and non-spatial
relationships. Based on these metrics, we develop a composition-aware model
preference dataset comprising numerous image-rank pairs to train
composition-aware reward models. Then, we propose an iterative feedback
learning method to enhance compositionality in a closed-loop manner, enabling
the progressive self-refinement of both the base diffusion model and reward
models over multiple iterations. Theoretical proof demonstrates the
effectiveness and extensive experiments show our significant superiority over
previous SOTA methods (e.g., Omost and FLUX), particularly in multi-category
object composition and complex semantic alignment. IterComp opens new research
avenues in reward feedback learning for diffusion models and compositional
generation. Code: https://github.com/YangLing0818/IterComp
]]></content:encoded>
<pubDate>2024-10-09T17:59:13Z</pubDate>
</item>
<item>
<title>Grounding Partially-Defined Events in Multimodal Data</title>
<link>http://arxiv.org/abs/2410.05267v1</link>
<guid>http://arxiv.org/abs/2410.05267v1</guid>
<content:encoded><![CDATA[
<div> 事件建模，多模态处理，视频语言系统，挑战，LLM方法<br />
总结:<br />
这篇文章介绍了如何通过短视频片段学习复杂的时事事件，以及视觉数据在事件理解中所带来的挑战。为了解决多模态环境中的事件建模问题，引入了一种多模态事件部分定义的表述方法，并将其作为一个三阶段跨度检索任务。提出了一个名为MultiVENT-G的基准测试集，包含14.5小时密集注释的时事事件视频和1,168个文档，包含22.8K个标记的事件中心实体。文章还介绍了一些基于LLM的方法，并在MultiVENT-G上进行了评估。结果表明抽象事件理解所面临的挑战，并展示了事件中心视频语言系统的潜力。 <div>
How are we able to learn about complex current events just from short
snippets of video? While natural language enables straightforward ways to
represent under-specified, partially observable events, visual data does not
facilitate analogous methods and, consequently, introduces unique challenges in
event understanding. With the growing prevalence of vision-capable AI agents,
these systems must be able to model events from collections of unstructured
video data. To tackle robust event modeling in multimodal settings, we
introduce a multimodal formulation for partially-defined events and cast the
extraction of these events as a three-stage span retrieval task. We propose a
corresponding benchmark for this task, MultiVENT-G, that consists of 14.5 hours
of densely annotated current event videos and 1,168 text documents, containing
22.8K labeled event-centric entities. We propose a collection of LLM-driven
approaches to the task of multimodal event analysis, and evaluate them on
MultiVENT-G. Results illustrate the challenges that abstract event
understanding poses and demonstrates promise in event-centric video-language
systems.
]]></content:encoded>
<pubDate>2024-10-07T17:59:48Z</pubDate>
</item>
<item>
<title>Unraveling Cross-Modality Knowledge Conflict in Large Vision-Language
  Models</title>
<link>http://arxiv.org/abs/2410.03659v1</link>
<guid>http://arxiv.org/abs/2410.03659v1</guid>
<content:encoded><![CDATA[
<div> 关键词: Large Vision-Language Models, Parametric Knowledge Conflict, Cross-Modality, Inference Process, Dynamic Contrastive Decoding

LVLMs在捕捉和推理多模态输入方面表现出了令人印象深刻的能力。然而，这些模型容易出现参数知识冲突，即它们的视觉和语言组成部分之间所代表的知识不一致的情况。本文正式定义了跨模态参数知识冲突的问题并提出了一种系统的方法来检测、解释和减轻这些冲突。我们引入了一个流程，用于识别视觉和文本答案之间的冲突，展示了最近LVLMs在不同模态之间具有持续高冲突率的情况，无论模型大小如何。我们进一步研究了这些冲突如何干扰推断过程，并提出了一个对比度度量来辨别冲突样本。基于这些见解，我们开发了一种新颖的动态对比解码方法，根据答案置信度从不太自信的模态组件中删除不良的logits。对于不提供logits的模型，我们还介绍了两种基于提示的策略来减轻冲突。我们的方法在ViQuAE和InfoSeek数据集上实现了令人满意的准确度改进。具体而言，使用LLaVA-34B，我们提出的动态对比解码方法将平均准确度提高了2.24%。

<br /><br />总结: 
本文研究了大型视觉-语言模型中存在的跨模态参数知识冲突问题，并提出了一种系统的方法来检测、解释和减轻这些冲突。研究发现最近的LVLMs在不同模态之间存在持续高的冲突率，影响了推理过程。研究团队提出了动态对比解码方法来解决这一问题，并取得了显著的准确度改进。 <div>
Large Vision-Language Models (LVLMs) have demonstrated impressive
capabilities for capturing and reasoning over multimodal inputs. However, these
models are prone to parametric knowledge conflicts, which arise from
inconsistencies of represented knowledge between their vision and language
components. In this paper, we formally define the problem of
$\textbf{cross-modality parametric knowledge conflict}$ and present a
systematic approach to detect, interpret, and mitigate them. We introduce a
pipeline that identifies conflicts between visual and textual answers, showing
a persistently high conflict rate across modalities in recent LVLMs regardless
of the model size. We further investigate how these conflicts interfere with
the inference process and propose a contrastive metric to discern the
conflicting samples from the others. Building on these insights, we develop a
novel dynamic contrastive decoding method that removes undesirable logits
inferred from the less confident modality components based on answer
confidence. For models that do not provide logits, we also introduce two
prompt-based strategies to mitigate the conflicts. Our methods achieve
promising improvements in accuracy on both the ViQuAE and InfoSeek datasets.
Specifically, using LLaVA-34B, our proposed dynamic contrastive decoding
improves an average accuracy of 2.24%.
]]></content:encoded>
<pubDate>2024-10-04T17:59:28Z</pubDate>
</item>
<item>
<title>Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short
  Videos</title>
<link>http://arxiv.org/abs/2410.02763v1</link>
<guid>http://arxiv.org/abs/2410.02763v1</guid>
<content:encoded><![CDATA[
<div> 关键词: LMMs, Vinoground, 理解短视频, 时间推理, 挑战

要点输出:
1. LMMs被认为已经解决了短视频理解的关键挑战，学术界和工业界逐渐将注意力转向理解长视频。
2. 研究表明现有的LMMs在处理短视频时仍然缺乏基本的推理能力。
3. Vinoground是一个用于评估LMM的时间反事实基准，显示现有的LMM在区分不同动作和物体转换之间的时间差异方面存在严重困难。
4. GPT-4o模型在该基准上的表现仅为~50%，远低于人类基线水平的~90%。
5. 时间推理在短视频中仍然是一个尚未完全解决的问题。

总结:<br /><br />本研究指出，尽管LMMs在短视频理解方面取得了一定进展，但仍然存在时间推理能力的严重不足。Vinoground基准表明现有模型在区分不同动作和物体转换的时间差异方面表现不佳，GPT-4o模型仅达到50%的准确率，远低于人类水平。因此，时间推理在短视频中仍然是一个待解决的挑战。 <div>
There has been growing sentiment recently that modern large multimodal models
(LMMs) have addressed most of the key challenges related to short video
comprehension. As a result, both academia and industry are gradually shifting
their attention towards the more complex challenges posed by understanding
long-form videos. However, is this really the case? Our studies indicate that
LMMs still lack many fundamental reasoning capabilities even when dealing with
short videos. We introduce Vinoground, a temporal counterfactual LMM evaluation
benchmark encompassing 1000 short and natural video-caption pairs. We
demonstrate that existing LMMs severely struggle to distinguish temporal
differences between different actions and object transformations. For example,
the best model GPT-4o only obtains ~50% on our text and video scores, showing a
large gap compared to the human baseline of ~90%. All open-source multimodal
models and CLIP-based models perform much worse, producing mostly random chance
performance. Through this work, we shed light onto the fact that temporal
reasoning in short videos is a problem yet to be fully solved. The dataset and
evaluation code are available at https://vinoground.github.io.
]]></content:encoded>
<pubDate>2024-10-03T17:59:58Z</pubDate>
</item>
<item>
<title>Loong: Generating Minute-level Long Videos with Autoregressive Language
  Models</title>
<link>http://arxiv.org/abs/2410.02757v1</link>
<guid>http://arxiv.org/abs/2410.02757v1</guid>
<content:encoded><![CDATA[
<div> 视频生成 挑战 长视频 自回归语言模型 Loong

自回归大型语言模型（LLMs）在自然语言处理领域取得了成功，但在视频生成方面的尝试仍然受到挑战。本文分析了阻碍自回归LLMs生成长视频的挑战，并提出了名为Loong的新型自回归LLMs视频生成器。Loong将文本标记和视频标记建模为统一序列，通过渐进的短至长训练和损失再加权方案来缓解长视频训练中的损失不平衡问题。此外，对推断策略进行了进一步研究，包括视频标记的重新编码和抽样策略，以减少推断过程中的误差积累。所提出的Loong可在10秒视频上训练，并能扩展到根据文本提示生成分钟级长视频，这一结果已经得到了验证。更多样本请访问：https://epiphqny.github.io/Loong-video。<br /><br />总结: 本文提出了Loong，一个能够生成分钟级长视频的自回归LLMs视频生成器。它通过统一序列建模文本标记和视频标记，采用渐进的短至长训练和损失再加权方案，以及推断策略来解决自回归LLMs生成长视频面临的挑战。 <div>
It is desirable but challenging to generate content-rich long videos in the
scale of minutes. Autoregressive large language models (LLMs) have achieved
great success in generating coherent and long sequences of tokens in the domain
of natural language processing, while the exploration of autoregressive LLMs
for video generation is limited to generating short videos of several seconds.
In this work, we conduct a deep analysis of the challenges that prevent
autoregressive LLM-based video generators from generating long videos. Based on
the observations and analysis, we propose Loong, a new autoregressive LLM-based
video generator that can generate minute-long videos. Specifically, we model
the text tokens and video tokens as a unified sequence for autoregressive LLMs
and train the model from scratch. We propose progressive short-to-long training
with a loss re-weighting scheme to mitigate the loss imbalance problem for long
video training. We further investigate inference strategies, including video
token re-encoding and sampling strategies, to diminish error accumulation
during inference. Our proposed Loong can be trained on 10-second videos and be
extended to generate minute-level long videos conditioned on text prompts, as
demonstrated by the results. More samples are available at:
https://epiphqny.github.io/Loong-video.
]]></content:encoded>
<pubDate>2024-10-03T17:59:02Z</pubDate>
</item>
<item>
<title>ReLIC: A Recipe for 64k Steps of In-Context Reinforcement Learning for
  Embodied AI</title>
<link>http://arxiv.org/abs/2410.02751v1</link>
<guid>http://arxiv.org/abs/2410.02751v1</guid>
<content:encoded><![CDATA[
Intelligent embodied agents need to quickly adapt to new scenarios by
integrating long histories of experience into decision-making. For instance, a
robot in an unfamiliar house initially wouldn't know the locations of objects
needed for tasks and might perform inefficiently. However, as it gathers more
experience, it should learn the layout of its environment and remember where
objects are, allowing it to complete new tasks more efficiently. To enable such
rapid adaptation to new tasks, we present ReLIC, a new approach for in-context
reinforcement learning (RL) for embodied agents. With ReLIC, agents are capable
of adapting to new environments using 64,000 steps of in-context experience
with full attention while being trained through self-generated experience via
RL. We achieve this by proposing a novel policy update scheme for on-policy RL
called "partial updates'' as well as a Sink-KV mechanism that enables effective
utilization of a long observation history for embodied agents. Our method
outperforms a variety of meta-RL baselines in adapting to unseen houses in an
embodied multi-object navigation task. In addition, we find that ReLIC is
capable of few-shot imitation learning despite never being trained with expert
demonstrations. We also provide a comprehensive analysis of ReLIC, highlighting
that the combination of large-scale RL training, the proposed partial updates
scheme, and the Sink-KV are essential for effective in-context learning. The
code for ReLIC and all our experiments is at https://github.com/aielawady/relic
]]></content:encoded>
<pubDate>2024-10-03T17:58:11Z</pubDate>
</item>
<item>
<title>Windowed MAPF with Completeness Guarantees</title>
<link>http://arxiv.org/abs/2410.01798v1</link>
<guid>http://arxiv.org/abs/2410.01798v1</guid>
<content:encoded><![CDATA[
<div> Windowed MAPF, completeness, heuristic update, SS-CBS, agent independence<br />
<br />
本文介绍了传统多智能体路径规划方法的局限性，提出了一种新的框架WinC-MAPF，用于解决窗口式多智能体路径规划中的完备性问题。该框架借鉴了实时启发式搜索算法的启发式更新和多智能体路径规划算法的智能体独立性思想。作者还开发了Single-Step CBS (SS-CBS)，这是利用了CBS算法的一种新的改进，只计划一个步骤并更新启发式函数。研究结果表明，SS-CBS能够有效解决现有窗口式方法无法解决的困难情景。<br /><br />总结: <br />文章介绍了传统多智能体路径规划方法的局限性，提出了新的框架WinC-MAPF，并介绍了SS-CBS算法的开发及其在解决困难情景中的效果。 <div>
Traditional multi-agent path finding (MAPF) methods try to compute entire
start-goal paths which are collision free. However, computing an entire path
can take too long for MAPF systems where agents need to replan fast. Methods
that address this typically employ a "windowed" approach and only try to find
collision free paths for a small windowed timestep horizon. This adaptation
comes at the cost of incompleteness; all current windowed approaches can become
stuck in deadlock or livelock. Our main contribution is to introduce our
framework, WinC-MAPF, for Windowed MAPF that enables completeness. Our
framework uses heuristic update insights from single-agent real-time heuristic
search algorithms as well as agent independence ideas from MAPF algorithms. We
also develop Single-Step CBS (SS-CBS), an instantiation of this framework using
a novel modification to CBS. We show how SS-CBS, which only plans a single step
and updates heuristics, can effectively solve tough scenarios where existing
windowed approaches fail.
]]></content:encoded>
<pubDate>2024-10-02T17:55:46Z</pubDate>
</item>
<item>
<title>Bellman Diffusion: Generative Modeling as Learning a Linear Operator in
  the Distribution Space</title>
<link>http://arxiv.org/abs/2410.01796v1</link>
<guid>http://arxiv.org/abs/2410.01796v1</guid>
<content:encoded><![CDATA[
<div> DGMs, EBMs, SGMs, MDPs, RL
<br />
本文介绍了深度生成模型在马尔科夫决策过程中的应用，发现现代DGM的非线性特性与MDP中贝尔曼方程所需的线性特性相冲突。为解决这一问题，引入了贝尔曼扩散，一种新颖的DGM框架，通过梯度和标量场建模来保持MDP的线性性。经验结果表明，贝尔曼扩散在分布式RL任务中比传统的基于直方图的基准收敛速度更快1.5倍。这项工作使得DGM能够有效地整合到MDP应用程序中，为先进的决策框架开辟了新的途径。
<br /><br />总结: 
本文介绍了深度生成模型在马尔科夫决策过程(MDP)中的应用及挑战，提出了贝尔曼扩散作为解决方案，并展示了其在分布式强化学习任务中的有效性。 <div>
Deep Generative Models (DGMs), including Energy-Based Models (EBMs) and
Score-based Generative Models (SGMs), have advanced high-fidelity data
generation and complex continuous distribution approximation. However, their
application in Markov Decision Processes (MDPs), particularly in distributional
Reinforcement Learning (RL), remains underexplored, with conventional
histogram-based methods dominating the field. This paper rigorously highlights
that this application gap is caused by the nonlinearity of modern DGMs, which
conflicts with the linearity required by the Bellman equation in MDPs. For
instance, EBMs involve nonlinear operations such as exponentiating energy
functions and normalizing constants. To address this, we introduce Bellman
Diffusion, a novel DGM framework that maintains linearity in MDPs through
gradient and scalar field modeling. With divergence-based training techniques
to optimize neural network proxies and a new type of stochastic differential
equation (SDE) for sampling, Bellman Diffusion is guaranteed to converge to the
target distribution. Our empirical results show that Bellman Diffusion achieves
accurate field estimations and is a capable image generator, converging 1.5x
faster than the traditional histogram-based baseline in distributional RL
tasks. This work enables the effective integration of DGMs into MDP
applications, unlocking new avenues for advanced decision-making frameworks.
]]></content:encoded>
<pubDate>2024-10-02T17:53:23Z</pubDate>
</item>
<item>
<title>MM1.5: Methods, Analysis &amp; Insights from Multimodal LLM Fine-tuning</title>
<link>http://arxiv.org/abs/2409.20566v1</link>
<guid>http://arxiv.org/abs/2409.20566v1</guid>
<content:encoded><![CDATA[
<div> 关键词: MM1.5, 多模态大型语言模型, 图文理解, 数据 centric 方法, 模型训练策略

总结:<br /><br />
该论文介绍了MM1.5，这是一种新型的多模态大型语言模型(MLLMs)，旨在增强文本丰富的图像理解、视觉引用和定位，以及多图像推理等能力。MM1.5在MM1架构的基础上采用了以数据为中心的模型训练方法，系统地探索了整个模型训练生命周期中不同数据混合的影响。这包括高质量的OCR数据和合成字幕用于持续的预训练，以及优化的视觉指令调优数据混合用于监督微调。他们的模型范围从10亿到300亿个参数不等，包括密集型和专家混合(MoE)变体，并且证明了仔细的数据策划和训练策略甚至在小尺度(10亿和30亿)下也能产生出色的性能。此外，他们还引入了两种专门的变体：MM1.5-Video，专为视频理解而设计，以及MM1.5-UI，专门针对移动UI理解。通过大量的经验研究和消融研究，他们提供了关于训练过程和决策的详细见解，为未来的MLLM开发研究提供了宝贵的指导。 <div>
We present MM1.5, a new family of multimodal large language models (MLLMs)
designed to enhance capabilities in text-rich image understanding, visual
referring and grounding, and multi-image reasoning. Building upon the MM1
architecture, MM1.5 adopts a data-centric approach to model training,
systematically exploring the impact of diverse data mixtures across the entire
model training lifecycle. This includes high-quality OCR data and synthetic
captions for continual pre-training, as well as an optimized visual
instruction-tuning data mixture for supervised fine-tuning. Our models range
from 1B to 30B parameters, encompassing both dense and mixture-of-experts (MoE)
variants, and demonstrate that careful data curation and training strategies
can yield strong performance even at small scales (1B and 3B). Additionally, we
introduce two specialized variants: MM1.5-Video, designed for video
understanding, and MM1.5-UI, tailored for mobile UI understanding. Through
extensive empirical studies and ablations, we provide detailed insights into
the training processes and decisions that inform our final designs, offering
valuable guidance for future research in MLLM development.
]]></content:encoded>
<pubDate>2024-09-30T17:59:34Z</pubDate>
</item>
<item>
<title>LaMMA-P: Generalizable Multi-Agent Long-Horizon Task Allocation and
  Planning with LM-Driven PDDL Planner</title>
<link>http://arxiv.org/abs/2409.20560v1</link>
<guid>http://arxiv.org/abs/2409.20560v1</guid>
<content:encoded><![CDATA[
<div> Language models, natural language understanding, multi-agent planning, LaMMA-P, long-horizon tasks
<br /><br />总结:
本文介绍了一种新的基于语言模型驱动的多智能体PDDL规划器（LaMMA-P），该规划器在处理长程任务时取得了最先进的性能。LaMMA-P将语言模型的推理能力与传统的启发式搜索规划器相结合，实现了高成功率和高效率，并在任务之间展现出了强大的泛化能力。此外，作者创建了MAT-THOR，一个基于AI2-THOR环境的综合基准，其中包含两个不同复杂级别的家务任务。实验结果表明，LaMMA-P的成功率比现有的基于语言模型的多智能体规划器高105%，效率高36%。作者还提供了实验视频、代码和数据集，以及每个模块中使用的详细提示，可在https://lamma-p.github.io上获取。 <div>
Language models (LMs) possess a strong capability to comprehend natural
language, making them effective in translating human instructions into detailed
plans for simple robot tasks. Nevertheless, it remains a significant challenge
to handle long-horizon tasks, especially in subtask identification and
allocation for cooperative heterogeneous robot teams. To address this issue, we
propose a Language Model-Driven Multi-Agent PDDL Planner (LaMMA-P), a novel
multi-agent task planning framework that achieves state-of-the-art performance
on long-horizon tasks. LaMMA-P integrates the strengths of the LMs' reasoning
capability and the traditional heuristic search planner to achieve a high
success rate and efficiency while demonstrating strong generalization across
tasks. Additionally, we create MAT-THOR, a comprehensive benchmark that
features household tasks with two different levels of complexity based on the
AI2-THOR environment. The experimental results demonstrate that LaMMA-P
achieves a 105% higher success rate and 36% higher efficiency than existing
LM-based multi-agent planners. The experimental videos, code, and datasets of
this work as well as the detailed prompts used in each module are available at
https://lamma-p.github.io.
]]></content:encoded>
<pubDate>2024-09-30T17:58:18Z</pubDate>
</item>
<item>
<title>Supervised Multi-Modal Fission Learning</title>
<link>http://arxiv.org/abs/2409.20559v1</link>
<guid>http://arxiv.org/abs/2409.20559v1</guid>
<content:encoded><![CDATA[
Learning from multimodal datasets can leverage complementary information and
improve performance in prediction tasks. A commonly used strategy to account
for feature correlations in high-dimensional datasets is the latent variable
approach. Several latent variable methods have been proposed for multimodal
datasets. However, these methods either focus on extracting the shared
component across all modalities or on extracting both a shared component and
individual components specific to each modality. To address this gap, we
propose a Multi-Modal Fission Learning (MMFL) model that simultaneously
identifies globally joint, partially joint, and individual components
underlying the features of multimodal datasets. Unlike existing latent variable
methods, MMFL uses supervision from the response variable to identify
predictive latent components and has a natural extension for incorporating
incomplete multimodal data. Through simulation studies, we demonstrate that
MMFL outperforms various existing multimodal algorithms in both complete and
incomplete modality settings. We applied MMFL to a real-world case study for
early prediction of Alzheimers Disease using multimodal neuroimaging and
genomics data from the Alzheimers Disease Neuroimaging Initiative (ADNI)
dataset. MMFL provided more accurate predictions and better insights into
within- and across-modality correlations compared to existing methods.
]]></content:encoded>
<pubDate>2024-09-30T17:58:03Z</pubDate>
</item>
<item>
<title>Propose, Assess, Search: Harnessing LLMs for Goal-Oriented Planning in
  Instructional Videos</title>
<link>http://arxiv.org/abs/2409.20557v1</link>
<guid>http://arxiv.org/abs/2409.20557v1</guid>
<content:encoded><![CDATA[
Goal-oriented planning, or anticipating a series of actions that transition
an agent from its current state to a predefined objective, is crucial for
developing intelligent assistants aiding users in daily procedural tasks. The
problem presents significant challenges due to the need for comprehensive
knowledge of temporal and hierarchical task structures, as well as strong
capabilities in reasoning and planning. To achieve this, prior work typically
relies on extensive training on the target dataset, which often results in
significant dataset bias and a lack of generalization to unseen tasks. In this
work, we introduce VidAssist, an integrated framework designed for
zero/few-shot goal-oriented planning in instructional videos. VidAssist
leverages large language models (LLMs) as both the knowledge base and the
assessment tool for generating and evaluating action plans, thus overcoming the
challenges of acquiring procedural knowledge from small-scale, low-diversity
datasets. Moreover, VidAssist employs a breadth-first search algorithm for
optimal plan generation, in which a composite of value functions designed for
goal-oriented planning is utilized to assess the predicted actions at each
step. Extensive experiments demonstrate that VidAssist offers a unified
framework for different goal-oriented planning setups, e.g., visual planning
for assistance (VPA) and procedural planning (PP), and achieves remarkable
performance in zero-shot and few-shot setups. Specifically, our few-shot model
outperforms the prior fully supervised state-of-the-art method by +7.7% in VPA
and +4.81% PP task on the COIN dataset while predicting 4 future actions. Code,
and models are publicly available at https://sites.google.com/view/vidassist.
]]></content:encoded>
<pubDate>2024-09-30T17:57:28Z</pubDate>
</item>
<item>
<title>Inverse Painting: Reconstructing The Painting Process</title>
<link>http://arxiv.org/abs/2409.20556v1</link>
<guid>http://arxiv.org/abs/2409.20556v1</guid>
<content:encoded><![CDATA[
Given an input painting, we reconstruct a time-lapse video of how it may have
been painted. We formulate this as an autoregressive image generation problem,
in which an initially blank "canvas" is iteratively updated. The model learns
from real artists by training on many painting videos. Our approach
incorporates text and region understanding to define a set of painting
"instructions" and updates the canvas with a novel diffusion-based renderer.
The method extrapolates beyond the limited, acrylic style paintings on which it
has been trained, showing plausible results for a wide range of artistic styles
and genres.
]]></content:encoded>
<pubDate>2024-09-30T17:56:52Z</pubDate>
</item>
<item>
<title>PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation</title>
<link>http://arxiv.org/abs/2409.18964v1</link>
<guid>http://arxiv.org/abs/2409.18964v1</guid>
<content:encoded><![CDATA[
<div> 关键词: PhysGen, 图像到视频生成, 物理模拟, 数据驱动, 图像理解

PhysGen是一种新颖的图像到视频生成方法，通过将单个图像和输入条件（如施加于图像中物体上的力和扭矩）转换为现实、物理上合理和时间连续的视频。该方法的核心是将基于模型的物理模拟与数据驱动的视频生成过程相结合，实现了在图像空间中的真实动态。系统的核心组件包括：（i）有效捕捉图像几何、材料和物理参数的图像理解模块；（ii）利用刚体物理和推断的参数来模拟真实行为的图像空间动力学模拟模型；和（iii）利用生成式视频扩散来产生展现模拟运动的逼真视频素材的图像渲染和细化模块。PhysGen生成的视频在物理和外观上都非常逼真，甚至可以精确控制，通过定量比较和综合用户研究，展示出了优于现有的数据驱动图像到视频生成作品的结果。PhysGen生成的视频可用于各种下游应用，如将图像转换为逼真的动画，或让用户与图像交互并创建各种动态。 <div>
We present PhysGen, a novel image-to-video generation method that converts a
single image and an input condition (e.g., force and torque applied to an
object in the image) to produce a realistic, physically plausible, and
temporally consistent video. Our key insight is to integrate model-based
physical simulation with a data-driven video generation process, enabling
plausible image-space dynamics. At the heart of our system are three core
components: (i) an image understanding module that effectively captures the
geometry, materials, and physical parameters of the image; (ii) an image-space
dynamics simulation model that utilizes rigid-body physics and inferred
parameters to simulate realistic behaviors; and (iii) an image-based rendering
and refinement module that leverages generative video diffusion to produce
realistic video footage featuring the simulated motion. The resulting videos
are realistic in both physics and appearance and are even precisely
controllable, showcasing superior results over existing data-driven
image-to-video generation works through quantitative comparison and
comprehensive user study. PhysGen's resulting videos can be used for various
downstream applications, such as turning an image into a realistic animation or
allowing users to interact with the image and create various dynamics. Project
page: https://stevenlsw.github.io/physgen/
]]></content:encoded>
<pubDate>2024-09-27T17:59:57Z</pubDate>
</item>
<item>
<title>Ruler: A Model-Agnostic Method to Control Generated Length for Large
  Language Models</title>
<link>http://arxiv.org/abs/2409.18943v1</link>
<guid>http://arxiv.org/abs/2409.18943v1</guid>
<content:encoded><![CDATA[
<div> 大语言模型、生成任务、长度控制、Ruler、实验评估<br />
本研究旨在探索大语言模型在长度受限条件下生成响应的能力。通过引入目标长度生成任务（TLG）并设计Precise Match（PM）和Flexible Match（FM）两个评估指标来评估模型遵循特定响应长度的表现。此外，提出了一种新颖的模型无关方法Ruler，利用Meta Length Tokens（MLTs）增强大语言模型在长度受限指令下的指示遵从能力。Ruler能够根据指示中的长度约束生成特定长度的响应，并在未明确提供长度约束时自动生成适当的MLT，展现出很好的通用性和多功能性。实验结果表明，在不同大语言模型上，Ruler对目标长度生成任务的有效性，例如在PM上平均提升了27.97，在FM上平均提升了29.57。此外，进行了大量消融实验，进一步证实了Ruler的有效性和通用性。 <div>
The instruction-following ability of large language models enables humans to
interact with AI agents in a natural way. However, when required to generate
responses of a specific length, large language models often struggle to meet
users' needs due to their inherent difficulty in accurately perceiving
numerical constraints. To explore the ability of large language models to
control the length of generated responses, we propose the Target Length
Generation Task (TLG) and design two metrics, Precise Match (PM) and Flexible
Match (FM) to evaluate the model's performance in adhering to specified
response lengths. Furthermore, we introduce a novel, model-agnostic approach
called Ruler, which employs Meta Length Tokens (MLTs) to enhance the
instruction-following ability of large language models under length-constrained
instructions. Specifically, Ruler equips LLMs with the ability to generate
responses of a specified length based on length constraints within the
instructions. Moreover, Ruler can automatically generate appropriate MLT when
length constraints are not explicitly provided, demonstrating excellent
versatility and generalization. Comprehensive experiments show the
effectiveness of Ruler across different LLMs on Target Length Generation Task,
e.g., at All Level 27.97 average gain on PM, 29.57 average gain on FM. In
addition, we conduct extensive ablation experiments to further substantiate the
efficacy and generalization of Ruler. Our code and data is available at
https://github.com/Geaming2002/Ruler.
]]></content:encoded>
<pubDate>2024-09-27T17:44:58Z</pubDate>
</item>
<item>
<title>FlowTurbo: Towards Real-time Flow-Based Image Generation with Velocity
  Refiner</title>
<link>http://arxiv.org/abs/2409.18128v1</link>
<guid>http://arxiv.org/abs/2409.18128v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 流动模型, 生成模型, 加速采样, 质量提升, 实时图像生成

流动模型在视觉生成中取得成功，流动模型重新成为生成模型中的一大族群，其在视觉质量和推断速度方面取得了竞争性甚至更好的表现。通过学习流场匹配来学习速度场，流动模型倾向于产生更直观的采样轨迹，这在采样过程中有优势。然而，与发展成熟的扩散模型快速采样器不同，流动模型的高效采样很少被探索。本文提出了一个名为FlowTurbo的框架，以加速流动模型的采样，同时提高采样质量。我们的主要观察是，在流动模型的采样过程中，速度预测器的输出会变得稳定，从而可以通过一个轻量级的速度细化器来估计速度。此外，我们引入了几种技术，包括伪修正器和样本感知编译，以进一步减少推断时间。由于FlowTurbo不改变多步采样范式，因此可以有效地应用于图像编辑、修补等各种任务。将FlowTurbo集成到不同的流动模型中，我们在条件生成方面获得了53.1%～58.3%的加速比率，在文本到图像生成方面获得了29.8%～38.5%的加速比率。值得注意的是，FlowTurbo在ImageNet上以100(ms/img)获得了FID为2.12，在38(ms/img)下获得了FID为3.93，实现了实时图像生成并建立了新的最佳表现。源代码可在https://github.com/shiml20/FlowTurbo中找到。<br /><br />总结: 本文提出了FlowTurbo框架，用于加速流动模型的采样过程，并提高采样质量。该框架通过观察到流动模型中速度预测器的输出在采样过程中变得稳定，从而引入了轻量级的速度细化器来估计速度。通过引入伪修正器和样本感知编译等技术，进一步减少了推断时间。最终，在不改变多步采样范式的情况下，在各种生成任务中取得了显著的加速效果，实现了实时图像生成并建立了新的最佳表现。 <div>
Building on the success of diffusion models in visual generation, flow-based
models reemerge as another prominent family of generative models that have
achieved competitive or better performance in terms of both visual quality and
inference speed. By learning the velocity field through flow-matching,
flow-based models tend to produce a straighter sampling trajectory, which is
advantageous during the sampling process. However, unlike diffusion models for
which fast samplers are well-developed, efficient sampling of flow-based
generative models has been rarely explored. In this paper, we propose a
framework called FlowTurbo to accelerate the sampling of flow-based models
while still enhancing the sampling quality. Our primary observation is that the
velocity predictor's outputs in the flow-based models will become stable during
the sampling, enabling the estimation of velocity via a lightweight velocity
refiner. Additionally, we introduce several techniques including a pseudo
corrector and sample-aware compilation to further reduce inference time. Since
FlowTurbo does not change the multi-step sampling paradigm, it can be
effectively applied for various tasks such as image editing, inpainting, etc.
By integrating FlowTurbo into different flow-based models, we obtain an
acceleration ratio of 53.1%$\sim$58.3% on class-conditional generation and
29.8%$\sim$38.5% on text-to-image generation. Notably, FlowTurbo reaches an FID
of 2.12 on ImageNet with 100 (ms / img) and FID of 3.93 with 38 (ms / img),
achieving the real-time image generation and establishing the new
state-of-the-art. Code is available at https://github.com/shiml20/FlowTurbo.
]]></content:encoded>
<pubDate>2024-09-26T17:59:51Z</pubDate>
</item>
<item>
<title>Lotus: Diffusion-based Visual Foundation Model for High-quality Dense
  Prediction</title>
<link>http://arxiv.org/abs/2409.18124v1</link>
<guid>http://arxiv.org/abs/2409.18124v1</guid>
<content:encoded><![CDATA[
<div> 零-shot、扩散模型、密集预测、Lotus、效率提升
<br /><br />
这篇论文提出了一种新的视觉基础模型 Lotus，针对密集预测任务中存在的问题进行了探讨和改进。论文首先分析了现有扩散形式对于密集预测的适用性，指出了原始参数化类型对于图像生成的有害影响以及多步扰动/去噪过程的不必要性。在此基础上，引入了Lotus模型，通过直接预测标注而非噪声，避免了有害方差；简化了扩散过程为单步程序，简化了优化过程并显著提升了推断速度；并采用了细节保留器的新调整策略，实现了更准确、更精细的预测。Lotus模型在各种数据集上实现了零-shot深度和法线估计的最先进性能，并在效率上显著提升，比大多数现有的基于扩散的方法快上数百倍。
<br />总结: 论文首先分析了现有扩散形式对于密集预测的适用性，指出了问题所在；其次引入了Lotus模型，并详细阐述了其改进之处；最后指出Lotus模型在实验中取得的显著成果。 <div>
Leveraging the visual priors of pre-trained text-to-image diffusion models
offers a promising solution to enhance zero-shot generalization in dense
prediction tasks. However, existing methods often uncritically use the original
diffusion formulation, which may not be optimal due to the fundamental
differences between dense prediction and image generation. In this paper, we
provide a systemic analysis of the diffusion formulation for the dense
prediction, focusing on both quality and efficiency. And we find that the
original parameterization type for image generation, which learns to predict
noise, is harmful for dense prediction; the multi-step noising/denoising
diffusion process is also unnecessary and challenging to optimize. Based on
these insights, we introduce Lotus, a diffusion-based visual foundation model
with a simple yet effective adaptation protocol for dense prediction.
Specifically, Lotus is trained to directly predict annotations instead of
noise, thereby avoiding harmful variance. We also reformulate the diffusion
process into a single-step procedure, simplifying optimization and
significantly boosting inference speed. Additionally, we introduce a novel
tuning strategy called detail preserver, which achieves more accurate and
fine-grained predictions. Without scaling up the training data or model
capacity, Lotus achieves SoTA performance in zero-shot depth and normal
estimation across various datasets. It also significantly enhances efficiency,
being hundreds of times faster than most existing diffusion-based methods.
]]></content:encoded>
<pubDate>2024-09-26T17:58:55Z</pubDate>
</item>
<item>
<title>Molmo and PixMo: Open Weights and Open Data for State-of-the-Art
  Multimodal Models</title>
<link>http://arxiv.org/abs/2409.17146v1</link>
<guid>http://arxiv.org/abs/2409.17146v1</guid>
<content:encoded><![CDATA[
<div> 多模态模型，闭源模型，Molmo，数据集，性能<br />
总结:<br />
本文介绍了最先进的多模态模型仍然是闭源的情况，开放权重模型依赖于闭源模型的合成数据来实现良好性能，作者提出了一种名为Molmo的新型VLM，并介绍了该模型的关键创新，即使用基于语音描述的高度详细的图像标题数据集。他们还提出了一个包括野外问答和创新的2D指向数据的多种数据组合，以进行微调。作者表示，他们的方法成功的关键在于模型架构细节的精心选择，以及新收集的数据集的质量。最后，作者宣布他们将在不久的将来发布所有的模型权重、字幕和微调数据，并提供代码。 Molmo系列中的最佳72B模型不仅在开放权重和数据模型领域表现优异，还在学术基准和人类评估中与专有系统相比具有明显优势。 <div>
Today's most advanced multimodal models remain proprietary. The strongest
open-weight models rely heavily on synthetic data from proprietary VLMs to
achieve good performance, effectively distilling these closed models into open
ones. As a result, the community is still missing foundational knowledge about
how to build performant VLMs from scratch. We present Molmo, a new family of
VLMs that are state-of-the-art in their class of openness. Our key innovation
is a novel, highly detailed image caption dataset collected entirely from human
annotators using speech-based descriptions. To enable a wide array of user
interactions, we also introduce a diverse dataset mixture for fine-tuning that
includes in-the-wild Q&amp;A and innovative 2D pointing data. The success of our
approach relies on careful choices for the model architecture details, a
well-tuned training pipeline, and, most critically, the quality of our newly
collected datasets, all of which will be released. The best-in-class 72B model
within the Molmo family not only outperforms others in the class of open weight
and data models but also compares favorably against proprietary systems like
GPT-4o, Claude 3.5, and Gemini 1.5 on both academic benchmarks and human
evaluation.
  We will be releasing all of our model weights, captioning and fine-tuning
data, and source code in the near future. Select model weights, inference code,
and demo are available at https://molmo.allenai.org.
]]></content:encoded>
<pubDate>2024-09-25T17:59:51Z</pubDate>
</item>
<item>
<title>Turn Every Application into an Agent: Towards Efficient
  Human-Agent-Computer Interaction with API-First LLM-Based Agents</title>
<link>http://arxiv.org/abs/2409.17140v1</link>
<guid>http://arxiv.org/abs/2409.17140v1</guid>
<content:encoded><![CDATA[
<div> 多模态大型语言模型、LLM、UI交互、API、任务完成时间、认知负荷、Agent OS
<br /><br />
本文介绍了一种名为AXIS的新型LLM代理框架。该框架通过优先使用应用程序接口（API）而非UI操作来解决高延迟和低可靠性的问题。同时，该框架还通过自动探索应用程序来促进API的创建和扩展。在Office Word上的实验表明，AXIS将任务完成时间降低了65%-70%，认知负荷降低了38%-53%，同时准确率达到了97%-98%，与人类相当。本研究对于新的人-代-计算机互动框架和应用提供商在LLM时代的新UI设计原则做出了贡献。同时，它探索了将每个应用程序转变为代理的可能性，为向代理为中心的操作系统（Agent OS）迈出了一步。 
<br /><br /> 
总结: <div>
Multimodal large language models (MLLMs) have enabled LLM-based agents to
directly interact with application user interfaces (UIs), enhancing agents'
performance in complex tasks. However, these agents often suffer from high
latency and low reliability due to the extensive sequential UI interactions. To
address this issue, we propose AXIS, a novel LLM-based agents framework
prioritize actions through application programming interfaces (APIs) over UI
actions. This framework also facilitates the creation and expansion of APIs
through automated exploration of applications. Our experiments on Office Word
demonstrate that AXIS reduces task completion time by 65%-70% and cognitive
workload by 38%-53%, while maintaining accuracy of 97%-98% compare to humans.
Our work contributes to a new human-agent-computer interaction (HACI) framework
and a fresh UI design principle for application providers in the era of LLMs.
It also explores the possibility of turning every applications into agents,
paving the way towards an agent-centric operating system (Agent OS).
]]></content:encoded>
<pubDate>2024-09-25T17:58:08Z</pubDate>
</item>
<item>
<title>Gen2Act: Human Video Generation in Novel Scenarios enables Generalizable
  Robot Manipulation</title>
<link>http://arxiv.org/abs/2409.16283v1</link>
<guid>http://arxiv.org/abs/2409.16283v1</guid>
<content:encoded><![CDATA[
<div> 预测, 机器人, 视频生成, 泛化, 目标任务

预测运动信息以实现机器人对新任务的泛化很困难。本文提出了一种解决方案，即通过人类视频生成来获取网络数据中的运动信息，并将机器人策略的条件设置为生成的视频。我们展示了如何利用在网络上易得的数据进行训练的视频生成模型来实现泛化，而不是试图扩展昂贵的机器人数据收集。我们的方法Gen2Act将语言条件操纵作为零-shot人类视频生成，然后使用单一策略执行生成的视频。为了训练策略，我们使用了一个数量级较少的机器人交互数据，与视频预测模型的训练数据相比。Gen2Act根本不需要对视频模型进行微调，我们直接使用预训练模型来生成人类视频。我们在多样的真实场景中的结果表明，Gen2Act使机器人能够操纵未见过的物体类型，并对机器人数据中不存在的任务执行新的动作。 Videos are at https://homangab.github.io/gen2act/ <br /><br />总结: 本文的主要观点是通过人类视频生成模型，实现对机器人在未取得数据的情况下进行泛化操作的目标，通过预测运动信息并将机器人策略设置为生成的视频，可以在训练策略时使用较少的机器人交互数据，并且无需对视频模型进行微调。最终结果表明，这一方法使机器人能够实现对未见过的物体类型进行操纵，并执行机器人数据中未包含的新任务。 <div>
How can robot manipulation policies generalize to novel tasks involving
unseen object types and new motions? In this paper, we provide a solution in
terms of predicting motion information from web data through human video
generation and conditioning a robot policy on the generated video. Instead of
attempting to scale robot data collection which is expensive, we show how we
can leverage video generation models trained on easily available web data, for
enabling generalization. Our approach Gen2Act casts language-conditioned
manipulation as zero-shot human video generation followed by execution with a
single policy conditioned on the generated video. To train the policy, we use
an order of magnitude less robot interaction data compared to what the video
prediction model was trained on. Gen2Act doesn't require fine-tuning the video
model at all and we directly use a pre-trained model for generating human
videos. Our results on diverse real-world scenarios show how Gen2Act enables
manipulating unseen object types and performing novel motions for tasks not
present in the robot data. Videos are at https://homangab.github.io/gen2act/
]]></content:encoded>
<pubDate>2024-09-24T17:57:33Z</pubDate>
</item>
<item>
<title>MonoFormer: One Transformer for Both Diffusion and Autoregression</title>
<link>http://arxiv.org/abs/2409.16280v1</link>
<guid>http://arxiv.org/abs/2409.16280v1</guid>
<content:encoded><![CDATA[
<div> Transformer, multimodality, autoregression, diffusion, image generation

Transformer在视觉生成中的应用；
单一Transformer同时处理autoregression和diffusion；
实验结果证明该方法在图像生成方面与当前最先进方法性能相当，并保持文本生成能力；
项目公开可用；

<br /><br />总结:
这篇论文研究了一种新的多模态方法，使用单一Transformer来同时处理自回归和扩散。实验证明，这种方法在图像生成方面与当前最先进的方法性能相当，同时保持了文本生成的能力。该项目已公开可用，对于相关领域的研究具有重要意义。 <div>
Most existing multimodality methods use separate backbones for
autoregression-based discrete text generation and diffusion-based continuous
visual generation, or the same backbone by discretizing the visual data to use
autoregression for both text and visual generation. In this paper, we propose
to study a simple idea: share one transformer for both autoregression and
diffusion. The feasibility comes from two main aspects: (i) Transformer is
successfully applied to diffusion for visual generation, and (ii) transformer
training for autoregression and diffusion is very similar, and the difference
merely lies in that diffusion uses bidirectional attention mask and
autoregression uses causal attention mask. Experimental results show that our
approach achieves comparable image generation performance to current
state-of-the-art methods as well as maintains the text generation capability.
The project is publicly available at https://monoformer.github.io/.
]]></content:encoded>
<pubDate>2024-09-24T17:51:04Z</pubDate>
</item>
<item>
<title>Semantic Refocused Tuning for Open-Vocabulary Panoptic Segmentation</title>
<link>http://arxiv.org/abs/2409.16278v1</link>
<guid>http://arxiv.org/abs/2409.16278v1</guid>
<content:encoded><![CDATA[
Open-vocabulary panoptic segmentation is an emerging task aiming to
accurately segment the image into semantically meaningful masks based on a set
of texts. Despite existing efforts, it remains challenging to develop a
high-performing method that generalizes effectively across new domains and
requires minimal training resources. Our in-depth analysis of current methods
reveals a crucial insight: mask classification is the main performance
bottleneck for open-vocab. panoptic segmentation. Based on this, we propose
Semantic Refocused Tuning (SMART), a novel framework that greatly enhances
open-vocab. panoptic segmentation by improving mask classification through two
key innovations. First, SMART adopts a multimodal Semantic-guided Mask
Attention mechanism that injects task-awareness into the regional information
extraction process. This enables the model to capture task-specific and
contextually relevant information for more effective mask classification.
Second, it incorporates Query Projection Tuning, which strategically fine-tunes
the query projection layers within the Vision Language Model (VLM) used for
mask classification. This adjustment allows the model to adapt the image focus
of mask tokens to new distributions with minimal training resources, while
preserving the VLM's pre-trained knowledge. Extensive ablation studies confirm
the superiority of our approach. Notably, SMART sets new state-of-the-art
results, demonstrating improvements of up to +1.3 PQ and +5.4 mIoU across
representative benchmarks, while reducing training costs by nearly 10x compared
to the previous best method. Our code and data will be released.
]]></content:encoded>
<pubDate>2024-09-24T17:50:28Z</pubDate>
</item>
<item>
<title>Vista3D: Unravel the 3D Darkside of a Single Image</title>
<link>http://arxiv.org/abs/2409.12193v1</link>
<guid>http://arxiv.org/abs/2409.12193v1</guid>
<content:encoded><![CDATA[
<div> 关键词: Vista3D, 3D生成, 隐含维度, 图像, 生成质量
总结: 
Vista3D是一个能够在短短5分钟内实现快速和一致的3D生成的框架。它采用了粗糙阶段和精细阶段的两阶段方法，通过高斯Splatting从单个图像快速生成初始几何形状，并且直接从学习的高斯Splatting中提取一个有符号距离函数（SDF），通过可微的等值面表示进行优化。此外，它使用两个独立的隐式函数来捕捉对象的可见和隐藏部分，通过角度扩散先验组合来协调二维扩散先验和三维感知扩散先验的梯度。通过广泛的评估，Vista3D有效地保持了生成的3D对象的一致性和多样性的平衡。GitHub上的演示和代码将会提供。 <div>
We embark on the age-old quest: unveiling the hidden dimensions of objects
from mere glimpses of their visible parts. To address this, we present Vista3D,
a framework that realizes swift and consistent 3D generation within a mere 5
minutes. At the heart of Vista3D lies a two-phase approach: the coarse phase
and the fine phase. In the coarse phase, we rapidly generate initial geometry
with Gaussian Splatting from a single image. In the fine phase, we extract a
Signed Distance Function (SDF) directly from learned Gaussian Splatting,
optimizing it with a differentiable isosurface representation. Furthermore, it
elevates the quality of generation by using a disentangled representation with
two independent implicit functions to capture both visible and obscured aspects
of objects. Additionally, it harmonizes gradients from 2D diffusion prior with
3D-aware diffusion priors by angular diffusion prior composition. Through
extensive evaluation, we demonstrate that Vista3D effectively sustains a
balance between the consistency and diversity of the generated 3D objects.
Demos and code will be available at https://github.com/florinshen/Vista3D.
]]></content:encoded>
<pubDate>2024-09-18T17:59:44Z</pubDate>
</item>
<item>
<title>Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at
  Any Resolution</title>
<link>http://arxiv.org/abs/2409.12191v1</link>
<guid>http://arxiv.org/abs/2409.12191v1</guid>
<content:encoded><![CDATA[
<div> Qwen2-VL Series, upgrade, Naive Dynamic Resolution, Multimodal Rotary Position Embedding, scaling laws<br />
<br />
Qwen2-VL Series是Qwen-VL模型的先进升级，引入了Naive Dynamic Resolution机制，可以动态处理不同分辨率的图像，生成更高效、准确的视觉表征。同时集成了Multimodal Rotary Position Embedding（M-RoPE），有效融合了文本、图像和视频的位置信息。采用统一的范式处理图像和视频，提升了模型的视觉感知能力。通过研究大型视觉语言模型（LVLMs）的扩展规律，Qwen2-VL系列取得了竞争力极强的性能表现，尤其是Qwen2-VL-72B模型在各种多模态基准测试中表现出色，优于其他通用模型。可在\url{https://github.com/QwenLM/Qwen2-VL}获取代码。 <br /><br />总结: Qwen2-VL系列引入了新的处理机制，可以动态处理图像分辨率，并融合多模态信息，通过研究大型模型的扩展规律，取得了优异的性能。 <div>
We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL
models that redefines the conventional predetermined-resolution approach in
visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism,
which enables the model to dynamically process images of varying resolutions
into different numbers of visual tokens. This approach allows the model to
generate more efficient and accurate visual representations, closely aligning
with human perceptual processes. The model also integrates Multimodal Rotary
Position Embedding (M-RoPE), facilitating the effective fusion of positional
information across text, images, and videos. We employ a unified paradigm for
processing both images and videos, enhancing the model's visual perception
capabilities. To explore the potential of large multimodal models, Qwen2-VL
investigates the scaling laws for large vision-language models (LVLMs). By
scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the
amount of training data, the Qwen2-VL Series achieves highly competitive
performance. Notably, the Qwen2-VL-72B model achieves results comparable to
leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal
benchmarks, outperforming other generalist models. Code is available at
\url{https://github.com/QwenLM/Qwen2-VL}.
]]></content:encoded>
<pubDate>2024-09-18T17:59:32Z</pubDate>
</item>
<item>
<title>Phidias: A Generative Model for Creating 3D Content from Text, Image,
  and 3D Conditions with Reference-Augmented Diffusion</title>
<link>http://arxiv.org/abs/2409.11406v1</link>
<guid>http://arxiv.org/abs/2409.11406v1</guid>
<content:encoded><![CDATA[
<div> 3D建模，参考模型，生成模型，Phidias，动态引导<br />
<br />
Phidias是一种新颖的生成模型，使用扩散技术进行参考增强的3D生成。该方法利用检索或用户提供的3D参考模型来引导生成过程，提高了生成质量、泛化能力和可控性。该模型整合了三个关键组件：1）动态调节条件强度的元控制网络，2）动态参考路由，缓解输入图像和3D参考之间的不对齐，3）自引用增强，实现了自监督训练。这些设计的整合使Phidias相对于现有方法有了明显的改进。Phidias建立了一个统一的框架，可用于使用文本、图像和3D条件进行3D生成，并具有多种应用。

<br /><br />总结: 3D建模中，Phidias通过引入动态引导和自引用增强等技术，提高了生成质量和灵活性。该方法整合了元控制网络和动态参考路由，解决了输入图像和3D参考模型之间的不对齐问题。通过自监督训练，Phidias实现了更好的泛化能力，为3D生成领域带来了明显的改进。 <div>
In 3D modeling, designers often use an existing 3D model as a reference to
create new ones. This practice has inspired the development of Phidias, a novel
generative model that uses diffusion for reference-augmented 3D generation.
Given an image, our method leverages a retrieved or user-provided 3D reference
model to guide the generation process, thereby enhancing the generation
quality, generalization ability, and controllability. Our model integrates
three key components: 1) meta-ControlNet that dynamically modulates the
conditioning strength, 2) dynamic reference routing that mitigates misalignment
between the input image and 3D reference, and 3) self-reference augmentations
that enable self-supervised training with a progressive curriculum.
Collectively, these designs result in a clear improvement over existing
methods. Phidias establishes a unified framework for 3D generation using text,
image, and 3D conditions with versatile applications.
]]></content:encoded>
<pubDate>2024-09-17T17:59:33Z</pubDate>
</item>
<item>
<title>NVLM: Open Frontier-Class Multimodal LLMs</title>
<link>http://arxiv.org/abs/2409.11402v1</link>
<guid>http://arxiv.org/abs/2409.11402v1</guid>
<content:encoded><![CDATA[
<div> NVLM 1.0, multimodal, language models, vision-language tasks, model design<br />
生产级多模态语言模型 NVLM 1.0 在视觉-语言任务上取得了最先进的结果，超越了领先的专有模型和开源模型。在模型设计方面，提出了一种新的架构，提高了训练效率和多模态推理能力。通过精心策划和提供详细信息的多模态预训练和监督微调数据集，发现数据集质量和任务多样性对于模型性能至关重要。通过融合高质量的文本数据集和大量的多模态数学和推理数据，提升了跨模态的数学和编码能力。最后，释放了模型权重并开源了代码，以推动该领域的研究。<br /><br />总结: <br /> NVLM 1.0 在视觉-语言任务中取得了最先进的结果，提出了新的架构，发现了数据集质量和任务多样性的重要性，提升了模型的数学和编码能力，并释放了模型权重并开源了代码。 <div>
We introduce NVLM 1.0, a family of frontier-class multimodal large language
models (LLMs) that achieve state-of-the-art results on vision-language tasks,
rivaling the leading proprietary models (e.g., GPT-4o) and open-access models
(e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved
text-only performance over its LLM backbone after multimodal training. In terms
of model design, we perform a comprehensive comparison between decoder-only
multimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g.,
Flamingo). Based on the strengths and weaknesses of both approaches, we propose
a novel architecture that enhances both training efficiency and multimodal
reasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for
tile-based dynamic high-resolution images, which significantly boosts
performance on multimodal reasoning and OCR-related tasks. Regarding training
data, we meticulously curate and provide detailed information on our multimodal
pretraining and supervised fine-tuning datasets. Our findings indicate that
dataset quality and task diversity are more important than scale, even during
the pretraining phase, across all architectures. Notably, we develop
production-grade multimodality for the NVLM-1.0 models, enabling them to excel
in vision-language tasks while maintaining and even improving text-only
performance compared to their LLM backbones. To achieve this, we craft and
integrate a high-quality text-only dataset into multimodal training, alongside
a substantial amount of multimodal math and reasoning data, leading to enhanced
math and coding capabilities across modalities. To advance research in the
field, we are releasing the model weights and will open-source the code for the
community: https://nvlm-project.github.io/.
]]></content:encoded>
<pubDate>2024-09-17T17:59:06Z</pubDate>
</item>
<item>
<title>Distributed Perception Aware Safe Leader Follower System via Control
  Barrier Methods</title>
<link>http://arxiv.org/abs/2409.11394v1</link>
<guid>http://arxiv.org/abs/2409.11394v1</guid>
<content:encoded><![CDATA[
This paper addresses a distributed leader-follower formation control problem
for a group of agents, each using a body-fixed camera with a limited field of
view (FOV) for state estimation. The main challenge arises from the need to
coordinate the agents' movements with their cameras' FOV to maintain visibility
of the leader for accurate and reliable state estimation. To address this
challenge, we propose a novel perception-aware distributed leader-follower safe
control scheme that incorporates FOV limits as state constraints. A Control
Barrier Function (CBF) based quadratic program is employed to ensure the
forward invariance of a safety set defined by these constraints. Furthermore,
new neural network based and double bounding boxes based estimators, combined
with temporal filters, are developed to estimate system states directly from
real-time image data, providing consistent performance across various
environments. Comparison results in the Gazebo simulator demonstrate the
effectiveness and robustness of the proposed framework in two distinct
environments.
]]></content:encoded>
<pubDate>2024-09-17T17:54:56Z</pubDate>
</item>
<item>
<title>LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless
  Integration of Multi Active/Passive Core-Agents</title>
<link>http://arxiv.org/abs/2409.11393v1</link>
<guid>http://arxiv.org/abs/2409.11393v1</guid>
<content:encoded><![CDATA[
The integration of tools in LLM-based agents overcame the difficulties of
standalone LLMs and traditional agents' limited capabilities. However, the
conjunction of these technologies and the proposed enhancements in several
state-of-the-art works followed a non-unified software architecture resulting
in a lack of modularity. Indeed, they focused mainly on functionalities and
overlooked the definition of the component's boundaries within the agent. This
caused terminological and architectural ambiguities between researchers which
we addressed in this paper by proposing a unified framework that establishes a
clear foundation for LLM-based agents' development from both functional and
software architectural perspectives.
  Our framework, LLM-Agent-UMF (LLM-based Agent Unified Modeling Framework),
clearly distinguishes between the different components of an agent, setting
LLMs, and tools apart from a newly introduced element: the core-agent, playing
the role of the central coordinator of the agent which comprises five modules:
planning, memory, profile, action, and security, the latter often neglected in
previous works. Differences in the internal structure of core-agents led us to
classify them into a taxonomy of passive and active types. Based on this, we
proposed different multi-core agent architectures combining unique
characteristics of various individual agents.
  For evaluation purposes, we applied this framework to a selection of
state-of-the-art agents, thereby demonstrating its alignment with their
functionalities and clarifying the overlooked architectural aspects. Moreover,
we thoroughly assessed four of our proposed architectures by integrating
distinctive agents into hybrid active/passive core-agents' systems. This
analysis provided clear insights into potential improvements and highlighted
the challenges involved in the combination of specific agents.
]]></content:encoded>
<pubDate>2024-09-17T17:54:17Z</pubDate>
</item>
<item>
<title>Pennsieve - A Collaborative Platform for Translational Neuroscience and
  Beyond</title>
<link>http://arxiv.org/abs/2409.10509v1</link>
<guid>http://arxiv.org/abs/2409.10509v1</guid>
<content:encoded><![CDATA[
<div> 科学数据管理 平台 多学科合作 脑科学 Pennsieve
总结:<br /><br />本文介绍了Pennsieve平台，这是一个开源的、基于云的科学数据管理平台，旨在满足神经科学数据管理和多学科合作的需求。Pennsieve支持复杂的多模态数据集，提供数据可视化和分析工具。它采取了全面的数据集成方法，使研究人员能够定义自定义的元数据模式，并利用高级工具来过滤和查询他们的数据。Pennsieve的模块化架构允许外部应用程序扩展其功能，并通过同行评审的数据发布机制促进高质量的数据集，以优化下游云端和现场分析。Pennsieve是美国国家卫生研究院SPARC计划、HEAL计划的PRECSION人类疼痛网络和RE-JOIN计划的核心，服务于全球80多个研究组和几个大规模的跨机构项目。Pennsieve存储着125TB的科学数据，其中35TB的数据公开可用，涵盖了350多个高影响力的数据集。它遵循数据分享的FAIR原则，被认可为美国国家卫生研究院批准的数据库。通过促进科学数据管理、发现和分析，Pennsieve为神经科学和其他领域营造了健全、协作的研究生态系统。 <div>
The exponential growth of neuroscientific data necessitates platforms that
facilitate data management and multidisciplinary collaboration. In this paper,
we introduce Pennsieve - an open-source, cloud-based scientific data management
platform built to meet these needs. Pennsieve supports complex multimodal
datasets and provides tools for data visualization and analyses. It takes a
comprehensive approach to data integration, enabling researchers to define
custom metadata schemas and utilize advanced tools to filter and query their
data. Pennsieve's modular architecture allows external applications to extend
its capabilities, and collaborative workspaces with peer-reviewed data
publishing mechanisms promote high-quality datasets optimized for downstream
analysis, both in the cloud and on-premises.
  Pennsieve forms the core for major neuroscience research programs including
the NIH SPARC Initiative, NIH HEAL Initiative's PRECISION Human Pain Network,
and NIH HEAL RE-JOIN Initiative. It serves more than 80 research groups
worldwide, along with several large-scale, inter-institutional projects at
clinical sites through the University of Pennsylvania. Underpinning the
SPARC.Science, Epilepsy.Science, and Pennsieve Discover portals, Pennsieve
stores over 125 TB of scientific data, with 35 TB of data publicly available
across more than 350 high-impact datasets. It adheres to the findable,
accessible, interoperable, and reusable (FAIR) principles of data sharing and
is recognized as one of the NIH-approved Data Repositories. By facilitating
scientific data management, discovery, and analysis, Pennsieve fosters a robust
and collaborative research ecosystem for neuroscience and beyond.
]]></content:encoded>
<pubDate>2024-09-16T17:55:58Z</pubDate>
</item>
<item>
<title>MusicLIME: Explainable Multimodal Music Understanding</title>
<link>http://arxiv.org/abs/2409.10496v1</link>
<guid>http://arxiv.org/abs/2409.10496v1</guid>
<content:encoded><![CDATA[
<div> 音乐理解，多模态，模型解释，特征重要性，公平性<br />
总结:<br />
本文介绍了MusicLIME，这是一个针对多模态音乐模型设计的模型无关特征重要性解释方法。与传统的单模态方法不同，MusicLIME揭示了音频和歌词特征如何相互作用并对预测做出贡献，提供了对模型决策的整体视图。此外，我们通过将局部解释汇总成全局解释，使用户能够更广泛地了解模型行为。通过这项工作，我们致力于改进多模态音乐模型的解释性，让用户能够做出明智的选择，并促进更公平、公正和透明的音乐理解系统。 <div>
Multimodal models are critical for music understanding tasks, as they capture
the complex interplay between audio and lyrics. However, as these models become
more prevalent, the need for explainability grows-understanding how these
systems make decisions is vital for ensuring fairness, reducing bias, and
fostering trust. In this paper, we introduce MusicLIME, a model-agnostic
feature importance explanation method designed for multimodal music models.
Unlike traditional unimodal methods, which analyze each modality separately
without considering the interaction between them, often leading to incomplete
or misleading explanations, MusicLIME reveals how audio and lyrical features
interact and contribute to predictions, providing a holistic view of the
model's decision-making. Additionally, we enhance local explanations by
aggregating them into global explanations, giving users a broader perspective
of model behavior. Through this work, we contribute to improving the
interpretability of multimodal music models, empowering users to make informed
choices, and fostering more equitable, fair, and transparent music
understanding systems.
]]></content:encoded>
<pubDate>2024-09-16T17:28:21Z</pubDate>
</item>
<item>
<title>The unknotting number, hard unknot diagrams, and reinforcement learning</title>
<link>http://arxiv.org/abs/2409.09032v1</link>
<guid>http://arxiv.org/abs/2409.09032v1</guid>
<content:encoded><![CDATA[
<div> 强化学习，不定结数，连接和，超几何结，数据集

强化学习代理程序可找到 unknotting number 上限，确定了 57k 结的 unknotting number，接下来展示了由多个 unknotting crossings 组成的超几何结以及 unknotting number 的加法性。最终得到了 2.6 百万个不同的 hard unknot diagrams。根据假设，确定了 43 个不超过 12 个交叉的结的 unknotting number。总结：<br /><br />强化学习代理程序成功确定了 unknotting number 上限，并展示了加法性的作用，同时得到了一个包含大量 unknot diagrams 的数据集。 <div>
We have developed a reinforcement learning agent that often finds a minimal
sequence of unknotting crossing changes for a knot diagram with up to 200
crossings, hence giving an upper bound on the unknotting number. We have used
this to determine the unknotting number of 57k knots. We took diagrams of
connected sums of such knots with oppositely signed signatures, where the
summands were overlaid. The agent has found examples where several of the
crossing changes in an unknotting collection of crossings result in hyperbolic
knots. Based on this, we have shown that, given knots $K$ and $K'$ that satisfy
some mild assumptions, there is a diagram of their connected sum and $u(K) +
u(K')$ unknotting crossings such that changing any one of them results in a
prime knot. As a by-product, we have obtained a dataset of 2.6 million distinct
hard unknot diagrams; most of them under 35 crossings. Assuming the additivity
of the unknotting number, we have determined the unknotting number of 43 at
most 12-crossing knots for which the unknotting number is unknown.
]]></content:encoded>
<pubDate>2024-09-13T17:59:52Z</pubDate>
</item>
<item>
<title>Agents in Software Engineering: Survey, Landscape, and Vision</title>
<link>http://arxiv.org/abs/2409.09030v1</link>
<guid>http://arxiv.org/abs/2409.09030v1</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Software Engineering, Agents, Perception, Memory

总结:<br /><br />本文是对将大型语言模型（LLMs）与软件工程（SE）相结合的研究进行了调查，提出了LLM在SE中的代理框架，包括感知、记忆和行动三个关键模块。文章还总结了结合两个领域的当前挑战，并提出了未来的发展机会。同时，他们在GitHub上维护了相关论文的存储库。 <div>
In recent years, Large Language Models (LLMs) have achieved remarkable
success and have been widely used in various downstream tasks, especially in
the tasks of the software engineering (SE) field. We find that many studies
combining LLMs with SE have employed the concept of agents either explicitly or
implicitly. However, there is a lack of an in-depth survey to sort out the
development context of existing works, analyze how existing works combine the
LLM-based agent technologies to optimize various tasks, and clarify the
framework of LLM-based agents in SE. In this paper, we conduct the first survey
of the studies on combining LLM-based agents with SE and present a framework of
LLM-based agents in SE which includes three key modules: perception, memory,
and action. We also summarize the current challenges in combining the two
fields and propose future opportunities in response to existing challenges. We
maintain a GitHub repository of the related papers at:
https://github.com/DeepSoftwareAnalytics/Awesome-Agent4SE.
]]></content:encoded>
<pubDate>2024-09-13T17:55:58Z</pubDate>
</item>
<item>
<title>AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM
  Agents</title>
<link>http://arxiv.org/abs/2409.09013v1</link>
<guid>http://arxiv.org/abs/2409.09013v1</guid>
<content:encoded><![CDATA[
To be safely and successfully deployed, LLMs must simultaneously satisfy
truthfulness and utility goals. Yet, often these two goals compete (e.g., an AI
agent assisting a used car salesman selling a car with flaws), partly due to
ambiguous or misleading user instructions. We propose AI-LieDar, a framework to
study how LLM-based agents navigate scenarios with utility-truthfulness
conflicts in a multi-turn interactive setting. We design a set of realistic
scenarios where language agents are instructed to achieve goals that are in
conflict with being truthful during a multi-turn conversation with simulated
human agents. To evaluate the truthfulness at large scale, we develop a
truthfulness detector inspired by psychological literature to assess the
agents' responses. Our experiment demonstrates that all models are truthful
less than 50% of the time, although truthfulness and goal achievement (utility)
rates vary across models. We further test the steerability of LLMs towards
truthfulness, finding that models follow malicious instructions to deceive, and
even truth-steered models can still lie. These findings reveal the complex
nature of truthfulness in LLMs and underscore the importance of further
research to ensure the safe and reliable deployment of LLMs and AI agents.
]]></content:encoded>
<pubDate>2024-09-13T17:41:12Z</pubDate>
</item>
<item>
<title>Click2Mask: Local Editing with Dynamic Mask Generation</title>
<link>http://arxiv.org/abs/2409.08272v1</link>
<guid>http://arxiv.org/abs/2409.08272v1</guid>
<content:encoded><![CDATA[
<div> Click2Mask, local image editing, generative models, Blended Latent Diffusion, user-friendly<br />
<br />
本文介绍了一种名为Click2Mask的新方法，用于简化局部图像编辑过程。它只需要一个参考点和内容描述，就能动态地在参考点周围生成遮罩，在Blended Latent Diffusion (BLD) 过程中通过基于CLIP的语义损失来引导。与现有方法相比，Click2Mask不仅减少了用户的工作量，而且根据人工判断和自动指标提供了出色的局部图像编辑结果。关键贡献包括简化用户输入、能够自由添加对象并与其他编辑方法结合的动态遮罩方法。<br /><br />总结: <div>
Recent advancements in generative models have revolutionized image generation
and editing, making these tasks accessible to non-experts. This paper focuses
on local image editing, particularly the task of adding new content to a
loosely specified area. Existing methods often require a precise mask or a
detailed description of the location, which can be cumbersome and prone to
errors. We propose Click2Mask, a novel approach that simplifies the local
editing process by requiring only a single point of reference (in addition to
the content description). A mask is dynamically grown around this point during
a Blended Latent Diffusion (BLD) process, guided by a masked CLIP-based
semantic loss. Click2Mask surpasses the limitations of segmentation-based and
fine-tuning dependent methods, offering a more user-friendly and contextually
accurate solution. Our experiments demonstrate that Click2Mask not only
minimizes user effort but also delivers competitive or superior local image
manipulation results compared to SoTA methods, according to both human
judgement and automatic metrics. Key contributions include the simplification
of user input, the ability to freely add objects unconstrained by existing
segments, and the integration potential of our dynamic mask approach within
other editing methods.
]]></content:encoded>
<pubDate>2024-09-12T17:59:04Z</pubDate>
</item>
<item>
<title>Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale</title>
<link>http://arxiv.org/abs/2409.08264v1</link>
<guid>http://arxiv.org/abs/2409.08264v1</guid>
<content:encoded><![CDATA[
<div> Agent, Windows Agent Arena, multi-modal tasks, benchmark, Navi
<br /><br />总结:
本文介绍了大型语言模型在计算机代理方面的潜力，以及在现实环境中评估代理性能的挑战。笔者提出了一个新的环境平台——Windows Agent Arena，专注于Windows操作系统，在150多个任务中展示了代理的能力。同时，还介绍了一个多模态代理Navi，并对其性能进行了分析。文章指出了未来代理开发和数据生成的研究机会。 <div>
Large language models (LLMs) show remarkable potential to act as computer
agents, enhancing human productivity and software accessibility in multi-modal
tasks that require planning and reasoning. However, measuring agent performance
in realistic environments remains a challenge since: (i) most benchmarks are
limited to specific modalities or domains (e.g. text-only, web navigation, Q&amp;A,
coding) and (ii) full benchmark evaluations are slow (on order of magnitude of
days) given the multi-step sequential nature of tasks. To address these
challenges, we introduce the Windows Agent Arena: a reproducible, general
environment focusing exclusively on the Windows operating system (OS) where
agents can operate freely within a real Windows OS and use the same wide range
of applications, tools, and web browsers available to human users when solving
tasks. We adapt the OSWorld framework (Xie et al., 2024) to create 150+ diverse
Windows tasks across representative domains that require agent abilities in
planning, screen understanding, and tool usage. Our benchmark is scalable and
can be seamlessly parallelized in Azure for a full benchmark evaluation in as
little as 20 minutes. To demonstrate Windows Agent Arena's capabilities, we
also introduce a new multi-modal agent, Navi. Our agent achieves a success rate
of 19.5% in the Windows domain, compared to 74.5% performance of an unassisted
human. Navi also demonstrates strong performance on another popular web-based
benchmark, Mind2Web. We offer extensive quantitative and qualitative analysis
of Navi's performance, and provide insights into the opportunities for future
research in agent development and data generation using Windows Agent Arena.
  Webpage: https://microsoft.github.io/WindowsAgentArena
  Code: https://github.com/microsoft/WindowsAgentArena
]]></content:encoded>
<pubDate>2024-09-12T17:56:43Z</pubDate>
</item>
<item>
<title>DreamMesh: Jointly Manipulating and Texturing Triangle Meshes for
  Text-to-3D Generation</title>
<link>http://arxiv.org/abs/2409.07454v1</link>
<guid>http://arxiv.org/abs/2409.07454v1</guid>
<content:encoded><![CDATA[
<div> NeRF, 3D generation, DreamMesh, 3D architecture, text-guided Jacobians
<br /><br />
1. 本文介绍了DreamMesh，这是一个新颖的文本到3D生成架构，使用了明确的三角网格来生成高保真的3D模型。
2. DreamMesh利用了一个独特的粗到细的方案，在粗阶段，首先通过文本引导的雅可比矩阵来变形网格，然后使用多个视角的2D扩散模型无需调参地对网格进行纹理处理。
3. 在精细阶段，DreamMesh联合操作网格并细化纹理地图，生成高质量的三角网格和高保真的纹理材料。
4. 大量实验证明，DreamMesh在准确生成具有丰富文本细节和增强几何的3D内容方面明显优于现有的文本到3D方法。
5. 该项目的相关信息可以在https://dreammesh.github.io找到。
<br /><br />总结: 本文介绍了DreamMesh，一个新的文本到3D生成架构，利用明确的三角网格和精细的纹理生成方法，相较于现有方法在生成具有丰富文本细节和增强几何的3D内容方面有明显优势。 <div>
Learning radiance fields (NeRF) with powerful 2D diffusion models has
garnered popularity for text-to-3D generation. Nevertheless, the implicit 3D
representations of NeRF lack explicit modeling of meshes and textures over
surfaces, and such surface-undefined way may suffer from the issues, e.g.,
noisy surfaces with ambiguous texture details or cross-view inconsistency. To
alleviate this, we present DreamMesh, a novel text-to-3D architecture that
pivots on well-defined surfaces (triangle meshes) to generate high-fidelity
explicit 3D model. Technically, DreamMesh capitalizes on a distinctive
coarse-to-fine scheme. In the coarse stage, the mesh is first deformed by
text-guided Jacobians and then DreamMesh textures the mesh with an interlaced
use of 2D diffusion models in a tuning free manner from multiple viewpoints. In
the fine stage, DreamMesh jointly manipulates the mesh and refines the texture
map, leading to high-quality triangle meshes with high-fidelity textured
materials. Extensive experiments demonstrate that DreamMesh significantly
outperforms state-of-the-art text-to-3D methods in faithfully generating 3D
content with richer textual details and enhanced geometry. Our project page is
available at https://dreammesh.github.io.
]]></content:encoded>
<pubDate>2024-09-11T17:59:02Z</pubDate>
</item>
<item>
<title>"My Grade is Wrong!": A Contestable AI Framework for Interactive
  Feedback in Evaluating Student Essays</title>
<link>http://arxiv.org/abs/2409.07453v1</link>
<guid>http://arxiv.org/abs/2409.07453v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 互动反馈, 大型语言模型, CAELF, 多代理系统, 计算论证
总结:<br /><br />本文介绍了一种名为CAELF的人工智能增强型大型语言模型框架，旨在自动化互动反馈。CAELF允许学生通过整合多代理系统和计算论证来提出问题、质疑和澄清他们的反馈。文章首先由多个教学助理代理（TA代理）进行评估，然后教师代理通过形式推理汇总评估结果，生成反馈和评分。学生可以进一步通过与反馈互动来完善他们的理解。针对500篇批判性思维作文的案例研究和用户研究表明，CAELF显著改善了互动反馈，增强了LLM的推理和互动能力。这种方法为克服教育设置中限制互动反馈采用的时间和资源障碍提供了有希望的解决方案。 <div>
Interactive feedback, where feedback flows in both directions between teacher
and student, is more effective than traditional one-way feedback. However, it
is often too time-consuming for widespread use in educational practice. While
Large Language Models (LLMs) have potential for automating feedback, they
struggle with reasoning and interaction in an interactive setting. This paper
introduces CAELF, a Contestable AI Empowered LLM Framework for automating
interactive feedback. CAELF allows students to query, challenge, and clarify
their feedback by integrating a multi-agent system with computational
argumentation. Essays are first assessed by multiple Teaching-Assistant Agents
(TA Agents), and then a Teacher Agent aggregates the evaluations through formal
reasoning to generate feedback and grades. Students can further engage with the
feedback to refine their understanding. A case study on 500 critical thinking
essays with user studies demonstrates that CAELF significantly improves
interactive feedback, enhancing the reasoning and interaction capabilities of
LLMs. This approach offers a promising solution to overcoming the time and
resource barriers that have limited the adoption of interactive feedback in
educational settings.
]]></content:encoded>
<pubDate>2024-09-11T17:59:01Z</pubDate>
</item>
<item>
<title>Hi3D: Pursuing High-Resolution Image-to-3D Generation with Video
  Diffusion Models</title>
<link>http://arxiv.org/abs/2409.07452v1</link>
<guid>http://arxiv.org/abs/2409.07452v1</guid>
<content:encoded><![CDATA[
Despite having tremendous progress in image-to-3D generation, existing
methods still struggle to produce multi-view consistent images with
high-resolution textures in detail, especially in the paradigm of 2D diffusion
that lacks 3D awareness. In this work, we present High-resolution Image-to-3D
model (Hi3D), a new video diffusion based paradigm that redefines a single
image to multi-view images as 3D-aware sequential image generation (i.e.,
orbital video generation). This methodology delves into the underlying temporal
consistency knowledge in video diffusion model that generalizes well to
geometry consistency across multiple views in 3D generation. Technically, Hi3D
first empowers the pre-trained video diffusion model with 3D-aware prior
(camera pose condition), yielding multi-view images with low-resolution texture
details. A 3D-aware video-to-video refiner is learnt to further scale up the
multi-view images with high-resolution texture details. Such high-resolution
multi-view images are further augmented with novel views through 3D Gaussian
Splatting, which are finally leveraged to obtain high-fidelity meshes via 3D
reconstruction. Extensive experiments on both novel view synthesis and single
view reconstruction demonstrate that our Hi3D manages to produce superior
multi-view consistency images with highly-detailed textures. Source code and
data are available at \url{https://github.com/yanghb22-fdu/Hi3D-Official}.
]]></content:encoded>
<pubDate>2024-09-11T17:58:57Z</pubDate>
</item>
<item>
<title>FreeEnhance: Tuning-Free Image Enhancement via Content-Consistent
  Noising-and-Denoising Process</title>
<link>http://arxiv.org/abs/2409.07451v1</link>
<guid>http://arxiv.org/abs/2409.07451v1</guid>
<content:encoded><![CDATA[
The emergence of text-to-image generation models has led to the recognition
that image enhancement, performed as post-processing, would significantly
improve the visual quality of the generated images. Exploring diffusion models
to enhance the generated images nevertheless is not trivial and necessitates to
delicately enrich plentiful details while preserving the visual appearance of
key content in the original image. In this paper, we propose a novel framework,
namely FreeEnhance, for content-consistent image enhancement using the
off-the-shelf image diffusion models. Technically, FreeEnhance is a two-stage
process that firstly adds random noise to the input image and then capitalizes
on a pre-trained image diffusion model (i.e., Latent Diffusion Models) to
denoise and enhance the image details. In the noising stage, FreeEnhance is
devised to add lighter noise to the region with higher frequency to preserve
the high-frequent patterns (e.g., edge, corner) in the original image. In the
denoising stage, we present three target properties as constraints to
regularize the predicted noise, enhancing images with high acutance and high
visual quality. Extensive experiments conducted on the HPDv2 dataset
demonstrate that our FreeEnhance outperforms the state-of-the-art image
enhancement models in terms of quantitative metrics and human preference. More
remarkably, FreeEnhance also shows higher human preference compared to the
commercial image enhancement solution of Magnific AI.
]]></content:encoded>
<pubDate>2024-09-11T17:58:50Z</pubDate>
</item>
<item>
<title>Cooptimizing Safety and Performance with a Control-Constrained
  Formulation</title>
<link>http://arxiv.org/abs/2409.06696v1</link>
<guid>http://arxiv.org/abs/2409.06696v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 自主系统, 优化控制, 安全性, 性能, 哈密尔顿-雅可比-贝尔曼偏微分方程<br />
总结: <br />
本文讨论了自主系统在执行任务时面临的安全性和性能的协同优化问题。作者提出将这一问题视为受限最优控制问题，以性能为目标函数，安全性为约束条件。然而，针对一般非线性系统求解这一受限最优控制问题仍具有挑战性。作者提出将安全状态约束转化为等效的控制约束，从而得到一个状态和时间相关的控制受限最优控制问题。该等效最优控制问题可以通过动态规划原理轻松求解，并且相应的值函数是某个哈密尔顿-雅可比-贝尔曼偏微分方程的粘性解。实验结果表明，通过作者的方法综合得到的控制器在安全性和性能方面均优于基准方法。 <div>
Autonomous systems have witnessed a rapid increase in their capabilities, but
it remains a challenge for them to perform tasks both effectively and safely.
The fact that performance and safety can sometimes be competing objectives
renders the cooptimization between them difficult. One school of thought is to
treat this cooptimization as a constrained optimal control problem with a
performance-oriented objective function and safety as a constraint. However,
solving this constrained optimal control problem for general nonlinear systems
remains challenging. In this work, we use the general framework of constrained
optimal control, but given the safety state constraint, we convert it into an
equivalent control constraint, resulting in a state and time-dependent
control-constrained optimal control problem. This equivalent optimal control
problem can readily be solved using the dynamic programming principle. We show
the corresponding value function is a viscosity solution of a certain
Hamilton-Jacobi-Bellman Partial Differential Equation (HJB-PDE). Furthermore,
we demonstrate the effectiveness of our method with a two-dimensional case
study, and the experiment shows that the controller synthesized using our
method consistently outperforms the baselines, both in safety and performance.
]]></content:encoded>
<pubDate>2024-09-10T17:56:59Z</pubDate>
</item>
<item>
<title>DANCE: Deep Learning-Assisted Analysis of Protein Sequences Using Chaos
  Enhanced Kaleidoscopic Images</title>
<link>http://arxiv.org/abs/2409.06694v1</link>
<guid>http://arxiv.org/abs/2409.06694v1</guid>
<content:encoded><![CDATA[
<div> TCR, cancer, protein sequences, Chaos Game Representation, deep learning<br />
总结:<br />
这篇文章介绍了一种新的方法，名为DANCE，通过深度学习辅助蛋白质序列的混沌增强万花筒图像分析(TCRs)的蛋白质序列。文章指出癌症是一种复杂的疾病，TCRs在识别抗原方面起着关键作用。研究人员将TCR序列转换成图像，并利用深度学习进行分类，以获取视觉图案与蛋白质性质之间的关系。结合混沌游戏表示法的图像生成与深度学习分类，为蛋白质分析领域打开了新的可能性。 <div>
Cancer is a complex disease characterized by uncontrolled cell growth. T cell
receptors (TCRs), crucial proteins in the immune system, play a key role in
recognizing antigens, including those associated with cancer. Recent
advancements in sequencing technologies have facilitated comprehensive
profiling of TCR repertoires, uncovering TCRs with potent anti-cancer activity
and enabling TCR-based immunotherapies. However, analyzing these intricate
biomolecules necessitates efficient representations that capture their
structural and functional information. T-cell protein sequences pose unique
challenges due to their relatively smaller lengths compared to other
biomolecules. An image-based representation approach becomes a preferred choice
for efficient embeddings, allowing for the preservation of essential details
and enabling comprehensive analysis of T-cell protein sequences. In this paper,
we propose to generate images from the protein sequences using the idea of
Chaos Game Representation (CGR) using the Kaleidoscopic images approach. This
Deep Learning Assisted Analysis of Protein Sequences Using Chaos Enhanced
Kaleidoscopic Images (called DANCE) provides a unique way to visualize protein
sequences by recursively applying chaos game rules around a central seed point.
we perform the classification of the T cell receptors (TCRs) protein sequences
in terms of their respective target cancer cells, as TCRs are known for their
immune response against cancer disease. The TCR sequences are converted into
images using the DANCE method. We employ deep-learning vision models to perform
the classification to obtain insights into the relationship between the visual
patterns observed in the generated kaleidoscopic images and the underlying
protein properties. By combining CGR-based image generation with deep learning
classification, this study opens novel possibilities in the protein analysis
domain.
]]></content:encoded>
<pubDate>2024-09-10T17:55:59Z</pubDate>
</item>
<item>
<title>Promptable Closed-loop Traffic Simulation</title>
<link>http://arxiv.org/abs/2409.05863v1</link>
<guid>http://arxiv.org/abs/2409.05863v1</guid>
<content:encoded><![CDATA[
<div> Traffic simulation, ProSim, promptable, closed-loop, autonomous driving
总结:<br /><br />这篇论文介绍了一个名为ProSim的交通仿真框架，该框架可以接收复杂的指令来模拟交通场景，并能够模拟车辆之间的互动。实验结果表明，ProSim在接收不同类型的指令时能够实现高度可控性，并且在没有接收指令时依然能够效果显著。为了支持可指令交通仿真的研究，他们还创建了一个名为ProSim-Instruct-520k的数据集，该数据集包含了超过10M条文本指令和超过520k个真实世界驾驶场景。他们将在https://ariostgx.github.io/ProSim上发布ProSim的代码以及ProSim-Instruct-520k的数据和标注工具。 <div>
Simulation stands as a cornerstone for safe and efficient autonomous driving
development. At its core a simulation system ought to produce realistic,
reactive, and controllable traffic patterns. In this paper, we propose ProSim,
a multimodal promptable closed-loop traffic simulation framework. ProSim allows
the user to give a complex set of numerical, categorical or textual prompts to
instruct each agent's behavior and intention. ProSim then rolls out a traffic
scenario in a closed-loop manner, modeling each agent's interaction with other
traffic participants. Our experiments show that ProSim achieves high prompt
controllability given different user prompts, while reaching competitive
performance on the Waymo Sim Agents Challenge when no prompt is given. To
support research on promptable traffic simulation, we create
ProSim-Instruct-520k, a multimodal prompt-scenario paired driving dataset with
over 10M text prompts for over 520k real-world driving scenarios. We will
release code of ProSim as well as data and labeling tools of
ProSim-Instruct-520k at https://ariostgx.github.io/ProSim.
]]></content:encoded>
<pubDate>2024-09-09T17:59:15Z</pubDate>
</item>
<item>
<title>A Survey on Knowledge Organization Systems of Research Fields: Resources
  and Challenges</title>
<link>http://arxiv.org/abs/2409.04432v1</link>
<guid>http://arxiv.org/abs/2409.04432v1</guid>
<content:encoded><![CDATA[
<div> KOSs, term lists, thesauri, taxonomies, ontologies, academic disciplines<br />
这篇文章主要介绍了知识组织系统在学术领域的重要作用，包括分类、管理和检索信息，并探讨了当前学术学科知识组织系统的现状和挑战。作者分析和比较了45种知识组织系统，发现在范围、结构、策划、使用和与其他知识组织系统的链接等方面存在着很大的异质性。最后指出了需要更加整合化的解决方案来表示学术领域的研究知识的重要性，并讨论了未来的挑战和方向。<br /><br />总结: 这篇文章介绍了学术领域中知识组织系统的重要作用和现状，以及未来的挑战和方向。 <div>
Knowledge Organization Systems (KOSs), such as term lists, thesauri,
taxonomies, and ontologies, play a fundamental role in categorising, managing,
and retrieving information. In the academic domain, KOSs are often adopted for
representing research areas and their relationships, primarily aiming to
classify research articles, academic courses, patents, books, scientific
venues, domain experts, grants, software, experiment materials, and several
other relevant products and agents. These structured representations of
research areas, widely embraced by many academic fields, have proven effective
in empowering AI-based systems to i) enhance retrievability of relevant
documents, ii) enable advanced analytic solutions to quantify the impact of
academic research, and iii) analyse and forecast research dynamics. This paper
aims to present a comprehensive survey of the current KOS for academic
disciplines. We analysed and compared 45 KOSs according to five main
dimensions: scope, structure, curation, usage, and links to other KOSs. Our
results reveal a very heterogeneous scenario in terms of scope, scale, quality,
and usage, highlighting the need for more integrated solutions for representing
research knowledge across academic fields. We conclude by discussing the main
challenges and the most promising future directions.
]]></content:encoded>
<pubDate>2024-09-06T17:54:43Z</pubDate>
</item>
<item>
<title>VILA-U: a Unified Foundation Model Integrating Visual Understanding and
  Generation</title>
<link>http://arxiv.org/abs/2409.04429v1</link>
<guid>http://arxiv.org/abs/2409.04429v1</guid>
<content:encoded><![CDATA[
<div> Video, Image, Language, Understanding, Generation
<br /><br />
VILA-U是一个统一的基础模型，集成了视频、图像和语言的理解和生成。传统的视觉语言模型使用单独的模块来理解和生成视觉内容，这可能导致不对齐和增加复杂性。相比之下，VILA-U采用单一的自回归下一个令牌预测框架来完成这两个任务，消除了对扩散模型等额外组件的需求。这种方法不仅简化了模型，而且在视觉语言理解和生成方面实现了接近最新技术水平的性能。VILA-U的成功归因于两个主要因素：统一的视觉塔在预训练期间将离散的视觉令牌与文本输入对齐，增强了视觉感知，自回归图像生成可以通过高质量数据集实现与扩散模型类似的质量。这使得VILA-U能够使用完全基于令牌的自回归框架与更复杂的模型相媲美。 <div>
VILA-U is a Unified foundation model that integrates Video, Image, Language
understanding and generation. Traditional visual language models (VLMs) use
separate modules for understanding and generating visual content, which can
lead to misalignment and increased complexity. In contrast, VILA-U employs a
single autoregressive next-token prediction framework for both tasks,
eliminating the need for additional components like diffusion models. This
approach not only simplifies the model but also achieves near state-of-the-art
performance in visual language understanding and generation. The success of
VILA-U is attributed to two main factors: the unified vision tower that aligns
discrete visual tokens with textual inputs during pretraining, which enhances
visual perception, and autoregressive image generation can achieve similar
quality as diffusion models with high-quality dataset. This allows VILA-U to
perform comparably to more complex models using a fully token-based
autoregressive framework.
]]></content:encoded>
<pubDate>2024-09-06T17:49:56Z</pubDate>
</item>
<item>
<title>RLPF: Reinforcement Learning from Prediction Feedback for User
  Summarization with LLMs</title>
<link>http://arxiv.org/abs/2409.04421v1</link>
<guid>http://arxiv.org/abs/2409.04421v1</guid>
<content:encoded><![CDATA[
LLM-powered personalization agent systems employ Large Language Models (LLMs)
to predict users' behavior from their past activities. However, their
effectiveness often hinges on the ability to effectively leverage extensive,
long user historical data due to its inherent noise and length of such data.
Existing pretrained LLMs may generate summaries that are concise but lack the
necessary context for downstream tasks, hindering their utility in
personalization systems. To address these challenges, we introduce
Reinforcement Learning from Prediction Feedback (RLPF). RLPF fine-tunes LLMs to
generate concise, human-readable user summaries that are optimized for
downstream task performance. By maximizing the usefulness of the generated
summaries, RLPF effectively distills extensive user history data while
preserving essential information for downstream tasks. Our empirical evaluation
demonstrates significant improvements in both extrinsic downstream task utility
and intrinsic summary quality, surpassing baseline methods by up to 22% on
downstream task performance and achieving an up to 84.59% win rate on
Factuality, Abstractiveness, and Readability. RLPF also achieves a remarkable
74% reduction in context length while improving performance on 16 out of 19
unseen tasks and/or datasets, showcasing its generalizability. This approach
offers a promising solution for enhancing LLM personalization by effectively
transforming long, noisy user histories into informative and human-readable
representations.
]]></content:encoded>
<pubDate>2024-09-06T17:30:45Z</pubDate>
</item>
<item>
<title>ArtiFade: Learning to Generate High-quality Subject from Blemished
  Images</title>
<link>http://arxiv.org/abs/2409.03745v1</link>
<guid>http://arxiv.org/abs/2409.03745v1</guid>
<content:encoded><![CDATA[
<div> Subject-driven, text-to-image generation, ArtiFade, artifact removal, high-quality images<br /> 

ArtiFade是一种针对图像去除瑕疵的方法，能够通过对预训练的文本-图像模型进行微调，从受损数据集中生成高质量、无瑕疵的图像。该方法利用包含完整图像及其受损对应物的特定数据集进行微调，以达到消除瑕疵的目的，同时保留扩散模型中固有的生成能力。研究人员还为这一任务设计了评估基准，并通过大量定性和定量实验展示了ArtiFade在分布内和分布外场景下有效去除瑕疵的普适性。总结：<br /><br />总结: ArtiFade是一种用于从受损数据集中生成高质量无瑕疵图像的方法，通过微调预训练的文本-图像模型并利用特定数据集进行瑕疵消除，同时保留了扩散模型的生成能力。 <div>
Subject-driven text-to-image generation has witnessed remarkable advancements
in its ability to learn and capture characteristics of a subject using only a
limited number of images. However, existing methods commonly rely on
high-quality images for training and may struggle to generate reasonable images
when the input images are blemished by artifacts. This is primarily attributed
to the inadequate capability of current techniques in distinguishing
subject-related features from disruptive artifacts. In this paper, we introduce
ArtiFade to tackle this issue and successfully generate high-quality
artifact-free images from blemished datasets. Specifically, ArtiFade exploits
fine-tuning of a pre-trained text-to-image model, aiming to remove artifacts.
The elimination of artifacts is achieved by utilizing a specialized dataset
that encompasses both unblemished images and their corresponding blemished
counterparts during fine-tuning. ArtiFade also ensures the preservation of the
original generative capabilities inherent within the diffusion model, thereby
enhancing the overall performance of subject-driven methods in generating
high-quality and artifact-free images. We further devise evaluation benchmarks
tailored for this task. Through extensive qualitative and quantitative
experiments, we demonstrate the generalizability of ArtiFade in effective
artifact removal under both in-distribution and out-of-distribution scenarios.
]]></content:encoded>
<pubDate>2024-09-05T17:57:59Z</pubDate>
</item>
<item>
<title>HiPrompt: Tuning-free Higher-Resolution Generation with Hierarchical
  MLLM Prompts</title>
<link>http://arxiv.org/abs/2409.02919v1</link>
<guid>http://arxiv.org/abs/2409.02919v1</guid>
<content:encoded><![CDATA[
<div> 高分辨率图像生成, 预训练扩散模型, HiPrompt, 分层提示, 语义引导

在高分辨率图像生成领域，预训练扩散模型存在物体重复和结构性伪影等问题。为了解决这些问题，文章提出了HiPrompt方法，通过引入分层提示来提供全局和局部引导，包括全局内容描述和局部贴图描述，从而在逆噪声过程中实现分层语义引导。实验结果表明，HiPrompt在高分辨率图像生成中优于现有方法，显著减少了物体重复现象并提升了结构质量。 <br /><br />总结: 高分辨率图像生成领域存在预训练模型的重复物体和结构性伪影问题，HiPrompt方法通过分层提示解决了这些问题，实现了更好的高分辨率图像生成效果。 <div>
The potential for higher-resolution image generation using pretrained
diffusion models is immense, yet these models often struggle with issues of
object repetition and structural artifacts especially when scaling to 4K
resolution and higher. We figure out that the problem is caused by that, a
single prompt for the generation of multiple scales provides insufficient
efficacy. In response, we propose HiPrompt, a new tuning-free solution that
tackles the above problems by introducing hierarchical prompts. The
hierarchical prompts offer both global and local guidance. Specifically, the
global guidance comes from the user input that describes the overall content,
while the local guidance utilizes patch-wise descriptions from MLLMs to
elaborately guide the regional structure and texture generation. Furthermore,
during the inverse denoising process, the generated noise is decomposed into
low- and high-frequency spatial components. These components are conditioned on
multiple prompt levels, including detailed patch-wise descriptions and broader
image-level prompts, facilitating prompt-guided denoising under hierarchical
semantic guidance. It further allows the generation to focus more on local
spatial regions and ensures the generated images maintain coherent local and
global semantics, structures, and textures with high definition. Extensive
experiments demonstrate that HiPrompt outperforms state-of-the-art works in
higher-resolution image generation, significantly reducing object repetition
and enhancing structural quality.
]]></content:encoded>
<pubDate>2024-09-04T17:58:08Z</pubDate>
</item>
<item>
<title>Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of
  Encoders</title>
<link>http://arxiv.org/abs/2408.15998v1</link>
<guid>http://arxiv.org/abs/2408.15998v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 多模态大语言模型 (MLLMs), 视觉感知, 视觉编码器, 模型设计, 研究发现

多模态大语言模型 (MLLMs) 对复杂视觉信息的准确解释能力至关重要。最近的研究表明，增强的视觉感知显著减少了幻觉，并改善了对分辨率敏感任务的性能，例如光学字符识别和文档分析。一些最近的 MLLMs 通过使用多种视觉编码器来实现这一目标。尽管它们取得了成功，但对于关键方面，如专家选择和多个视觉专家的集成，缺乏系统性比较和详细的消融研究。本研究对使用多种视觉编码器和分辨率的MLLMs的设计空间进行了广泛探索。我们的研究发现了一些潜在的共同原则，这些原则适用于各种现有策略，导致了一种简化而有效的设计方法。我们发现，仅仅将来自一组互补视觉编码器的视觉标记串联起来，与更复杂的混合架构或策略一样有效。此外，我们还引入了 Pre-Alignment 来弥合以视觉为重点的编码器和语言标记之间的差距，增强了模型的连贯性。最终开发出的MLLMs系列模型 Eagle，在主要的MLLM基准测试中超过了其他主流的开源模型。 Model和 code: https://github.com/NVlabs/Eagle<br /><br />总结: 本研究探讨了多模态大语言模型 (MLLMs) 对复杂视觉信息的准确解释能力，并提出了一种简化而有效的设计方法。研究发现，简单地将来自一组互补视觉编码器的视觉标记串联起来就能有效，同时引入 Pre-Alignment 进一步增强了模型的连贯性。最终开发出的MLLMs系列模型 Eagle，在主要的MLLM基准测试中表现出色。 <div>
The ability to accurately interpret complex visual information is a crucial
topic of multimodal large language models (MLLMs). Recent work indicates that
enhanced visual perception significantly reduces hallucinations and improves
performance on resolution-sensitive tasks, such as optical character
recognition and document analysis. A number of recent MLLMs achieve this goal
using a mixture of vision encoders. Despite their success, there is a lack of
systematic comparisons and detailed ablation studies addressing critical
aspects, such as expert selection and the integration of multiple vision
experts. This study provides an extensive exploration of the design space for
MLLMs using a mixture of vision encoders and resolutions. Our findings reveal
several underlying principles common to various existing strategies, leading to
a streamlined yet effective design approach. We discover that simply
concatenating visual tokens from a set of complementary vision encoders is as
effective as more complex mixing architectures or strategies. We additionally
introduce Pre-Alignment to bridge the gap between vision-focused encoders and
language tokens, enhancing model coherence. The resulting family of MLLMs,
Eagle, surpasses other leading open-source models on major MLLM benchmarks.
Models and code: https://github.com/NVlabs/Eagle
]]></content:encoded>
<pubDate>2024-08-28T17:59:31Z</pubDate>
</item>
<item>
<title>GenRec: Unifying Video Generation and Recognition with Diffusion Models</title>
<link>http://arxiv.org/abs/2408.15241v1</link>
<guid>http://arxiv.org/abs/2408.15241v1</guid>
<content:encoded><![CDATA[
<div> 视频扩散模型，空间时间先验，生成与识别，GenRec框架，实验结果<br />
视频扩散模型通过学习大规模数据集上的强空间时间先验，能够生成高质量的视频。本文旨在探讨这些从生成过程中得出的先验是否适用于视频识别，并最终实现生成和识别的联合优化。基于稳定视频扩散，我们引入了GenRec，这是第一个使用随机帧条件过程进行训练的统一框架，从而学习广义的空间时间表示。该框架自然地支持生成和识别，并且在视觉输入包含有限信息时也具有鲁棒性。大量实验证明了GenRec在识别和生成方面的有效性。特别是，GenRec在SSV2和K400上分别取得了竞争力的识别性能，精度分别达到了75.8%和87.2%。GenRec还表现出最佳的类别条件图像到视频生成结果，在SSV2和EK-100数据集上分别达到了46.5和49.3的FVD分数。此外，GenRec在只能观察到有限帧的情况下表现出了非凡的鲁棒性。 <br /><br />总结: <br />视频扩散模型通过学习大规模数据集上的强空间时间先验，能够生成高质量的视频。GenRec框架引入了统一的训练过程，实现了生成和识别的联合优化，具有鲁棒性，能够在视觉输入包含有限信息时取得竞争性能。 <div>
Video diffusion models are able to generate high-quality videos by learning
strong spatial-temporal priors on large-scale datasets. In this paper, we aim
to investigate whether such priors derived from a generative process are
suitable for video recognition, and eventually joint optimization of generation
and recognition. Building upon Stable Video Diffusion, we introduce GenRec, the
first unified framework trained with a random-frame conditioning process so as
to learn generalized spatial-temporal representations. The resulting framework
can naturally supports generation and recognition, and more importantly is
robust even when visual inputs contain limited information. Extensive
experiments demonstrate the efficacy of GenRec for both recognition and
generation. In particular, GenRec achieves competitive recognition performance,
offering 75.8% and 87.2% accuracy on SSV2 and K400, respectively. GenRec also
performs the best class-conditioned image-to-video generation results,
achieving 46.5 and 49.3 FVD scores on SSV2 and EK-100 datasets. Furthermore,
GenRec demonstrates extraordinary robustness in scenarios that only limited
frames can be observed.
]]></content:encoded>
<pubDate>2024-08-27T17:59:41Z</pubDate>
</item>
<item>
<title>Into the Unknown Unknowns: Engaged Human Learning through Participation
  in Language Model Agent Conversations</title>
<link>http://arxiv.org/abs/2408.15232v1</link>
<guid>http://arxiv.org/abs/2408.15232v1</guid>
<content:encoded><![CDATA[
<div> Collaborative STORM, Co-STORM, LM agents, unknown unknowns, WildSeek dataset

总结:
Collaborative STORM (Co-STORM) 是一个新型的教育场景中教育系统，使用 LM agents 帮助用户发现未知领域的信息。与传统问答系统不同，Co-STORM 允许用户观察和偶尔引导多个 LM agents 的对话。这些 agents 代表用户提出问题，让用户有机会偶然地发现未知领域的信息。此外，Co-STORM 还会帮助用户通过动态思维导图整理信息，并生成全面的报告。研究人员构建了 WildSeek 数据集，用于自动评估 Co-STORM 的性能，结果显示 Co-STORM 在对话追踪和报告质量上均优于基准方法。在人类评估中，70% 的参与者更喜欢 Co-STORM 而不是搜索引擎，78% 的参与者更喜欢 Co-STORM 而不是 RAG 聊天机器人。Co-STORM 在发现未知领域信息的过程中展现出了出色的性能。 <div>
While language model (LM)-powered chatbots and generative search engines
excel at answering concrete queries, discovering information in the terrain of
unknown unknowns remains challenging for users. To emulate the common
educational scenario where children/students learn by listening to and
participating in conversations of their parents/teachers, we create
Collaborative STORM (Co-STORM). Unlike QA systems that require users to ask all
the questions, Co-STORM lets users observe and occasionally steer the discourse
among several LM agents. The agents ask questions on the user's behalf,
allowing the user to discover unknown unknowns serendipitously. To facilitate
user interaction, Co-STORM assists users in tracking the discourse by
organizing the uncovered information into a dynamic mind map, ultimately
generating a comprehensive report as takeaways. For automatic evaluation, we
construct the WildSeek dataset by collecting real information-seeking records
with user goals. Co-STORM outperforms baseline methods on both discourse trace
and report quality. In a further human evaluation, 70% of participants prefer
Co-STORM over a search engine, and 78% favor it over a RAG chatbot.
]]></content:encoded>
<pubDate>2024-08-27T17:50:03Z</pubDate>
</item>
<item>
<title>A Practitioner's Guide to Continual Multimodal Pretraining</title>
<link>http://arxiv.org/abs/2408.14471v1</link>
<guid>http://arxiv.org/abs/2408.14471v1</guid>
<content:encoded><![CDATA[
<div> 多模态，预训练，持续更新，实际部署，FoMo-in-Flux

持续学习对于多模态基础模型在实际部署中起着重要作用。本文介绍了一个名为FoMo-in-Flux的持续多模态预训练基准测试平台，该平台考虑了现实的计算约束和实际部署需求，覆盖了63个数据集，涵盖了多样的视觉和语义信息。通过FoMo-in-Flux，作者从多个角度探讨了实际持续预训练的复杂情景，包括数据混合和流顺序的数据中心研究、各种模型更新策略的方法研究、元学习率调度和机械设计选择，以及模型和计算规模的影响。最终，作者提供了一个持续多模态预训练的实践指南，帮助在实际部署中取得更好的效果。GitHub链接为https://github.com/ExplainableML/fomo_in_flux。<br /><br />总结: 该研究介绍了一个持续多模态预训练基准测试平台FoMo-in-Flux，提供了对持续预训练的多个方面进行综合研究的见解，为实际部署提供了实用的指南。 <div>
Multimodal foundation models serve numerous applications at the intersection
of vision and language. Still, despite being pretrained on extensive data, they
become outdated over time. To keep models updated, research into continual
pretraining mainly explores scenarios with either (1) infrequent,
indiscriminate updates on large-scale new data, or (2) frequent, sample-level
updates. However, practical model deployment often operates in the gap between
these two limit cases, as real-world applications often demand adaptation to
specific subdomains, tasks or concepts -- spread over the entire, varying life
cycle of a model. In this work, we complement current perspectives on continual
pretraining through a research test bed as well as provide comprehensive
guidance for effective continual model updates in such scenarios. We first
introduce FoMo-in-Flux, a continual multimodal pretraining benchmark with
realistic compute constraints and practical deployment requirements,
constructed over 63 datasets with diverse visual and semantic coverage. Using
FoMo-in-Flux, we explore the complex landscape of practical continual
pretraining through multiple perspectives: (1) A data-centric investigation of
data mixtures and stream orderings that emulate real-world deployment
situations, (2) a method-centric investigation ranging from simple fine-tuning
and traditional continual learning strategies to parameter-efficient updates
and model merging, (3) meta learning rate schedules and mechanistic design
choices, and (4) the influence of model and compute scaling. Together, our
insights provide a practitioner's guide to continual multimodal pretraining for
real-world deployment. Our benchmark and code is here:
https://github.com/ExplainableML/fomo_in_flux.
]]></content:encoded>
<pubDate>2024-08-26T17:59:01Z</pubDate>
</item>
<item>
<title>Foundational Model for Electron Micrograph Analysis: Instruction-Tuning
  Small-Scale Language-and-Vision Assistant for Enterprise Adoption</title>
<link>http://arxiv.org/abs/2408.13248v1</link>
<guid>http://arxiv.org/abs/2408.13248v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 半导体成像，深度学习，视觉语言指令调整，多模态框架，知识蒸馏<br />
<br />
在半导体制造过程中，半导体成像和分析是至关重要的，但在深度学习中却鲜有研究，这限制了我们在半导体制造中对精确控制和优化的能力。本文介绍了一个用于分析半导体电子显微镜图像的小规模多模态框架（MAEMI），通过视觉语言指令调整。作者利用大型多模态模型进行微观图像分析，生成了一个定制的指令跟随数据集。他们通过知识蒸馏，将更大的模型的知识转移到更小的模型中，从而提高了在视觉问题回答（VQA）任务中更小模型的准确性。这种方法消除了对昂贵的专家注释数据集的需求。企业可以进一步在其专有数据上对MAEMI进行优化，提高隐私性并在低成本的消费者硬件上提高性能。作者的实验证明，MAEMI优于传统方法，适应了数据分布的变化，并支持高通量筛选。 <br /><br />总结: <div>
Semiconductor imaging and analysis are critical yet understudied in deep
learning, limiting our ability for precise control and optimization in
semiconductor manufacturing. We introduce a small-scale multimodal framework
for analyzing semiconductor electron microscopy images (MAEMI) through
vision-language instruction tuning. We generate a customized
instruction-following dataset using large multimodal models on microscopic
image analysis. We perform knowledge transfer from larger to smaller models
through knowledge distillation, resulting in improved accuracy of smaller
models on visual question answering (VQA) tasks. This approach eliminates the
need for expensive, human expert-annotated datasets for microscopic image
analysis tasks. Enterprises can further finetune MAEMI on their intellectual
data, enhancing privacy and performance on low-cost consumer hardware. Our
experiments show that MAEMI outperforms traditional methods, adapts to data
distribution shifts, and supports high-throughput screening.
]]></content:encoded>
<pubDate>2024-08-23T17:42:11Z</pubDate>
</item>
<item>
<title>xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed
  Representations</title>
<link>http://arxiv.org/abs/2408.12590v1</link>
<guid>http://arxiv.org/abs/2408.12590v1</guid>
<content:encoded><![CDATA[
<div> 模型, 文本到视频生成, 潜在扩散模型, 视频变分自动编码器, 数据处理

总结:<br /><br />该论文介绍了一种文本到视频生成模型xGen-VideoSyn-1，利用潜在扩散模型（LDM）架构和引入视频变分自动编码器（VidVAE）来生成逼真场景。模型使用了分割和合并策略来保持视频段之间的时间一致性，并且具有空间和时间自注意力层，可以在不同时间框架和长宽比下进行强大的泛化。作者通过多个步骤的数据处理管线和收集了超过1300万高质量视频文本对，训练了VidVAE和DiT模型，并且展示了与最先进的T2V模型相竞争的性能。 <div>
We present xGen-VideoSyn-1, a text-to-video (T2V) generation model capable of
producing realistic scenes from textual descriptions. Building on recent
advancements, such as OpenAI's Sora, we explore the latent diffusion model
(LDM) architecture and introduce a video variational autoencoder (VidVAE).
VidVAE compresses video data both spatially and temporally, significantly
reducing the length of visual tokens and the computational demands associated
with generating long-sequence videos. To further address the computational
costs, we propose a divide-and-merge strategy that maintains temporal
consistency across video segments. Our Diffusion Transformer (DiT) model
incorporates spatial and temporal self-attention layers, enabling robust
generalization across different timeframes and aspect ratios. We have devised a
data processing pipeline from the very beginning and collected over 13M
high-quality video-text pairs. The pipeline includes multiple steps such as
clipping, text detection, motion estimation, aesthetics scoring, and dense
captioning based on our in-house video-LLM model. Training the VidVAE and DiT
models required approximately 40 and 642 H100 days, respectively. Our model
supports over 14-second 720p video generation in an end-to-end way and
demonstrates competitive performance against state-of-the-art T2V models.
]]></content:encoded>
<pubDate>2024-08-22T17:55:22Z</pubDate>
</item>
<item>
<title>GRAB: A Challenging GRaph Analysis Benchmark for Large Multimodal Models</title>
<link>http://arxiv.org/abs/2408.11817v1</link>
<guid>http://arxiv.org/abs/2408.11817v1</guid>
<content:encoded><![CDATA[
<div> 图表分析，多模态模型，基准评估，图形属性，挑战<br />
多模态模型在视觉任务上表现出色，但现有基准测试的上限不断受到挑战。为此，需要一个新一代基准测试来挑战未来的多模态模型。本文介绍了一个适合当前和未来多模态模型的图形分析基准测试 GRAB。这个基准测试由2170个问题组成，涵盖四个任务和23个图形属性。作者评估了20个多模态模型在GRAB上的表现，并发现这是一个具有挑战性的基准测试，最高表现的模型仅得到21.7%的分数。最后，作者进行了各种消融实验，以探讨模型的成功和困难之处。他们发布了GRAB以鼓励这一重要且不断发展的领域的进步。<br /><br />总结: 多模态模型在图形分析领域表现出色，需要新的基准测试挑战未来模型。作者引入了一个名为GRAB的基准测试，包含2170个问题，覆盖4个任务和23个图形属性。他们评估了20个多模态模型的表现，并发现这是一个具有挑战性的基准测试。作者还进行了消融实验，以探讨模型的成功和困难之处，并发布了GRAB以促进这一领域的进步。 <div>
Large multimodal models (LMMs) have exhibited proficiencies across many
visual tasks. Although numerous well-known benchmarks exist to evaluate model
performance, they increasingly have insufficient headroom. As such, there is a
pressing need for a new generation of benchmarks challenging enough for the
next generation of LMMs. One area that LMMs show potential is graph analysis,
specifically, the tasks an analyst might typically perform when interpreting
figures such as estimating the mean, intercepts or correlations of functions
and data series. In this work, we introduce GRAB, a graph analysis benchmark,
fit for current and future frontier LMMs. Our benchmark is entirely synthetic,
ensuring high-quality, noise-free questions. GRAB is comprised of 2170
questions, covering four tasks and 23 graph properties. We evaluate 20 LMMs on
GRAB, finding it to be a challenging benchmark, with the highest performing
model attaining a score of just 21.7%. Finally, we conduct various ablations to
investigate where the models succeed and struggle. We release GRAB to encourage
progress in this important, growing domain.
]]></content:encoded>
<pubDate>2024-08-21T17:59:32Z</pubDate>
</item>
<item>
<title>Efficient Exploration and Discriminative World Model Learning with an
  Object-Centric Abstraction</title>
<link>http://arxiv.org/abs/2408.11816v1</link>
<guid>http://arxiv.org/abs/2408.11816v1</guid>
<content:encoded><![CDATA[
<div> 强化学习，对象中心映射，层次化建模，模型学习，迁移学习
<br />
这篇文章研究了在强化学习中的探索问题，通过给予智能体一个对象中心映射，将物品建模成高级状态抽象和属性变化建模成高级时间抽象，从而简化了状态转移动力学，使得未来状态更容易预测。他们提出了一种完全基于模型的算法，通过学习一个辨别性世界模型，规划有效的探索路径，并且能够规划到达任何已发现的（抽象）状态。通过在一系列2D制作和MiniHack环境中的实验验证了他们的模型能够高效解决单一任务，实现零样本和少样本的迁移学习，以及规划长期的决策。他们的模型在实验中表现出明显的优势，超过了现有低级方法（不使用抽象）以及使用相同抽象的模型无关和基于模型的方法。最后，他们还展示了如何通过强化学习低级对象扰动策略，以及通过监督学习对象映射本身。 
<br /><br />总结: 本研究通过对象中心映射和层次化建模解决了强化学习中的探索问题，提出了一种全面的模型算法，能够高效解决任务，实现迁移学习，并且能够规划长期的决策。实验结果表明，该模型在多个环境下表现出明显的优势。 <div>
In the face of difficult exploration problems in reinforcement learning, we
study whether giving an agent an object-centric mapping (describing a set of
items and their attributes) allow for more efficient learning. We found this
problem is best solved hierarchically by modelling items at a higher level of
state abstraction to pixels, and attribute change at a higher level of temporal
abstraction to primitive actions. This abstraction simplifies the transition
dynamic by making specific future states easier to predict. We make use of this
to propose a fully model-based algorithm that learns a discriminative world
model, plans to explore efficiently with only a count-based intrinsic reward,
and can subsequently plan to reach any discovered (abstract) states.
  We demonstrate the model's ability to (i) efficiently solve single tasks,
(ii) transfer zero-shot and few-shot across item types and environments, and
(iii) plan across long horizons. Across a suite of 2D crafting and MiniHack
environments, we empirically show our model significantly out-performs
state-of-the-art low-level methods (without abstraction), as well as performant
model-free and model-based methods using the same abstraction. Finally, we show
how to reinforce learn low level object-perturbing policies, as well as
supervise learn the object mapping itself.
]]></content:encoded>
<pubDate>2024-08-21T17:59:31Z</pubDate>
</item>
<item>
<title>SEA: Supervised Embedding Alignment for Token-Level Visual-Textual
  Integration in MLLMs</title>
<link>http://arxiv.org/abs/2408.11813v1</link>
<guid>http://arxiv.org/abs/2408.11813v1</guid>
<content:encoded><![CDATA[
Multimodal Large Language Models (MLLMs) have recently demonstrated
remarkable perceptual and reasoning abilities, typically comprising a Vision
Encoder, an Adapter, and a Large Language Model (LLM). The adapter serves as
the critical bridge between the visual and language components. However,
training adapters with image-level supervision often results in significant
misalignment, undermining the LLMs' capabilities and limiting the potential of
Multimodal LLMs. To address this, we introduce Supervised Embedding Alignment
(SEA), a token-level alignment method that leverages vision-language
pre-trained models, such as CLIP, to align visual tokens with the LLM's
embedding space through contrastive learning. This approach ensures a more
coherent integration of visual and language representations, enhancing the
performance and interpretability of multimodal LLMs while preserving their
inherent capabilities. Extensive experiments show that SEA effectively improves
MLLMs, particularly for smaller models, without adding extra data or inference
computation. SEA also lays the groundwork for developing more general and
adaptable solutions to enhance multimodal systems.
]]></content:encoded>
<pubDate>2024-08-21T17:58:02Z</pubDate>
</item>
<item>
<title>EmbodiedSAM: Online Segment Any 3D Thing in Real Time</title>
<link>http://arxiv.org/abs/2408.11811v1</link>
<guid>http://arxiv.org/abs/2408.11811v1</guid>
<content:encoded><![CDATA[
Embodied tasks require the agent to fully understand 3D scenes simultaneously
with its exploration, so an online, real-time, fine-grained and
highly-generalized 3D perception model is desperately needed. Since
high-quality 3D data is limited, directly training such a model in 3D is almost
infeasible. Meanwhile, vision foundation models (VFM) has revolutionized the
field of 2D computer vision with superior performance, which makes the use of
VFM to assist embodied 3D perception a promising direction. However, most
existing VFM-assisted 3D perception methods are either offline or too slow that
cannot be applied in practical embodied tasks. In this paper, we aim to
leverage Segment Anything Model (SAM) for real-time 3D instance segmentation in
an online setting. This is a challenging problem since future frames are not
available in the input streaming RGB-D video, and an instance may be observed
in several frames so object matching between frames is required. To address
these challenges, we first propose a geometric-aware query lifting module to
represent the 2D masks generated by SAM by 3D-aware queries, which is then
iteratively refined by a dual-level query decoder. In this way, the 2D masks
are transferred to fine-grained shapes on 3D point clouds. Benefit from the
query representation for 3D masks, we can compute the similarity matrix between
the 3D masks from different views by efficient matrix operation, which enables
real-time inference. Experiments on ScanNet, ScanNet200, SceneNN and 3RScan
show our method achieves leading performance even compared with offline
methods. Our method also demonstrates great generalization ability in several
zero-shot dataset transferring experiments and show great potential in
open-vocabulary and data-efficient setting. Code and demo are available at
https://xuxw98.github.io/ESAM/, with only one RTX 3090 GPU required for
training and evaluation.
]]></content:encoded>
<pubDate>2024-08-21T17:57:06Z</pubDate>
</item>
<item>
<title>Accelerating Goal-Conditioned RL Algorithms and Research</title>
<link>http://arxiv.org/abs/2408.11052v1</link>
<guid>http://arxiv.org/abs/2408.11052v1</guid>
<content:encoded><![CDATA[
<div> 自我监督；强化学习；自监督目标条件强化学习；JaxGCRL代码库；对比强化学习算法

自监督在机器学习领域取得了巨大成功，但在自监督目标条件强化学习方面进展缓慢。作者通过发布高性能代码库和基准JaxGCRL来解决这一问题，使研究人员能够在单个GPU上的几分钟内训练数百万个环境步骤的代理。他们利用GPU加速环境和基于infoNCE目标的对比强化学习算法的稳定批处理版本，有效地利用了增加的数据吞吐量。这一方法为未来的自监督目标条件强化学习研究提供了基础，使研究人员能够快速迭代新想法，并在各种具有挑战性的环境中进行评估。 <br /><br />总结: 本文介绍了自监督目标条件强化学习的挑战和解决方法，提出了高性能的JaxGCRL代码库，并介绍了其基于对比强化学习算法的稳定批处理版本。作者强调这一方法为未来的研究提供了基础，使研究人员能够快速迭代新想法，并在具有挑战性的环境中进行评估。 <div>
Self-supervision has the potential to transform reinforcement learning (RL),
paralleling the breakthroughs it has enabled in other areas of machine
learning. While self-supervised learning in other domains aims to find patterns
in a fixed dataset, self-supervised goal-conditioned reinforcement learning
(GCRL) agents discover new behaviors by learning from the goals achieved during
unstructured interaction with the environment. However, these methods have
failed to see similar success, both due to a lack of data from slow
environments as well as a lack of stable algorithms. We take a step toward
addressing both of these issues by releasing a high-performance codebase and
benchmark JaxGCRL for self-supervised GCRL, enabling researchers to train
agents for millions of environment steps in minutes on a single GPU. The key to
this performance is a combination of GPU-accelerated environments and a stable,
batched version of the contrastive reinforcement learning algorithm, based on
an infoNCE objective, that effectively makes use of this increased data
throughput. With this approach, we provide a foundation for future research in
self-supervised GCRL, enabling researchers to quickly iterate on new ideas and
evaluate them in a diverse set of challenging environments. Website + Code:
https://github.com/MichalBortkiewicz/JaxGCRL
]]></content:encoded>
<pubDate>2024-08-20T17:58:40Z</pubDate>
</item>
<item>
<title>FLAME: Learning to Navigate with Multimodal LLM in Urban Environments</title>
<link>http://arxiv.org/abs/2408.11051v1</link>
<guid>http://arxiv.org/abs/2408.11051v1</guid>
<content:encoded><![CDATA[
<div> Multimodal LLM-based agent, FLAME, urban VLN tasks, three-phase tuning technique, state-of-the-art performance<br />
<br />
关键词提取：FLAME是一个基于多模态LLM的代理，用于城市VLN任务，采用三阶段调优技术，达到了最先进的表现。<br />
1. 介绍了FLAME，这是一个基于多模态LLM的代理，专为城市VLN任务设计，通过三阶段调优技术适应导航任务。
2. 提到了三相调优技术的几个阶段，包括单感知调优和多感知调优。
3. 提及了FLAME在Touchdown数据集上优于现有方法，并且在任务完成率上提高了7.3%。
4. 强调了多模态LLM在复杂导航任务中的潜力，是对多模态LLM在实体AI应用中的推进。
<br /><br />总结: <br />这项研究介绍了FLAME，一个用于城市VLN任务的新型多模态LLM代理，通过三阶段调优技术适应导航任务。实验结果显示FLAME在Touchdown数据集上优于现有方法，任务完成率提高了7.3%，展示了多模态LLM在复杂导航任务中的潜力，是对多模态LLM在实体AI应用中的推进。 <div>
Large Language Models (LLMs) have demonstrated potential in
Vision-and-Language Navigation (VLN) tasks, yet current applications face
challenges. While LLMs excel in general conversation scenarios, they struggle
with specialized navigation tasks, yielding suboptimal performance compared to
specialized VLN models. We introduce FLAME (FLAMingo-Architected Embodied
Agent), a novel Multimodal LLM-based agent and architecture designed for urban
VLN tasks that efficiently handles multiple observations. Our approach
implements a three-phase tuning technique for effective adaptation to
navigation tasks, including single perception tuning for street view
description, multiple perception tuning for trajectory summarization, and
end-to-end training on VLN datasets. The augmented datasets are synthesized
automatically. Experimental results demonstrate FLAME's superiority over
existing methods, surpassing state-of-the-art methods by a 7.3% increase in
task completion rate on Touchdown dataset. This work showcases the potential of
Multimodal LLMs (MLLMs) in complex navigation tasks, representing an
advancement towards practical applications of MLLMs in embodied AI. Project
page: https://flame-sjtu.github.io
]]></content:encoded>
<pubDate>2024-08-20T17:57:46Z</pubDate>
</item>
<item>
<title>MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context
  Generation with Speculative Decoding</title>
<link>http://arxiv.org/abs/2408.11049v1</link>
<guid>http://arxiv.org/abs/2408.11049v1</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) have become more prevalent in long-context
applications such as interactive chatbots, document analysis, and agent
workflows, but it is challenging to serve long-context requests with low
latency and high throughput. Speculative decoding (SD) is a widely used
technique to reduce latency without sacrificing performance but the
conventional wisdom suggests that its efficacy is limited to small batch sizes.
In MagicDec, we show that surprisingly SD can achieve speedup even for a high
throughput inference regime for moderate to long sequences. More interestingly,
an intelligent drafting strategy can achieve better speedup with increasing
batch size based on our rigorous analysis. MagicDec first identifies the
bottleneck shifts with increasing batch size and sequence length, and uses
these insights to deploy speculative decoding more effectively for high
throughput inference. Then, it leverages draft models with sparse KV cache to
address the KV bottleneck that scales with both sequence length and batch size.
]]></content:encoded>
<pubDate>2024-08-20T17:57:31Z</pubDate>
</item>
<item>
<title>SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse
  Views</title>
<link>http://arxiv.org/abs/2408.10195v1</link>
<guid>http://arxiv.org/abs/2408.10195v1</guid>
<content:encoded><![CDATA[
<div> Sparse-view images, 3D reconstruction, camera poses, diffusion model, textured mesh
<br /><br />
该论文介绍了一种名为SpaRP的方法，用于从单个或少数几个不重叠的二维图像中重建三维纹理网格并估计相对相机姿态。该方法利用2D扩散模型的知识，经过微调后，隐式地推断出稀疏视图之间的三维空间关系。首先，扩散模型训练用于联合预测相机姿态的替代表示和在已知姿态下对象的多视图图像，整合所有输入稀疏视图的信息。然后利用这些预测完成3D重建和姿态估计，重建的3D模型可以进一步用于精确定位输入视图的相机姿态。通过对三个数据集进行大量实验证明，该方法在3D重建质量和姿态预测准确性方面明显优于基线方法，同时表现出很高的效率。只需大约20秒即可为输入视图生成带纹理的网格和相机姿态。 <div>
Open-world 3D generation has recently attracted considerable attention. While
many single-image-to-3D methods have yielded visually appealing outcomes, they
often lack sufficient controllability and tend to produce hallucinated regions
that may not align with users' expectations. In this paper, we explore an
important scenario in which the input consists of one or a few unposed 2D
images of a single object, with little or no overlap. We propose a novel
method, SpaRP, to reconstruct a 3D textured mesh and estimate the relative
camera poses for these sparse-view images. SpaRP distills knowledge from 2D
diffusion models and finetunes them to implicitly deduce the 3D spatial
relationships between the sparse views. The diffusion model is trained to
jointly predict surrogate representations for camera poses and multi-view
images of the object under known poses, integrating all information from the
input sparse views. These predictions are then leveraged to accomplish 3D
reconstruction and pose estimation, and the reconstructed 3D model can be used
to further refine the camera poses of input views. Through extensive
experiments on three datasets, we demonstrate that our method not only
significantly outperforms baseline methods in terms of 3D reconstruction
quality and pose prediction accuracy but also exhibits strong efficiency. It
requires only about 20 seconds to produce a textured mesh and camera poses for
the input views. Project page: https://chaoxu.xyz/sparp.
]]></content:encoded>
<pubDate>2024-08-19T17:53:10Z</pubDate>
</item>
<item>
<title>Visual Agents as Fast and Slow Thinkers</title>
<link>http://arxiv.org/abs/2408.08862v1</link>
<guid>http://arxiv.org/abs/2408.08862v1</guid>
<content:encoded><![CDATA[
<div> 关键词: FaST, System 1/2 thinking, visual agents, cognitive intelligence, AI systems

总结:<br /><br />这篇文章讨论了实现人类水平智能所需要的认知区分，即系统1和系统2思维。当前的人工智能虽然展示出人类的特征，但仍然无法达到真正的认知。为了解决这一挑战，他们引入了FaST，它将快速和慢速思维机制结合到视觉代理中。FaST通过切换适配器动态选择系统1/2模式，根据不同的任务复杂性来解决问题。它通过调整模型的置信度和集成新的上下文数据来解决不确定和未见的物体。经验结果表明，FaST在视觉问答和推理分割方面表现优异，超越了各种知名的基准线。经过广泛测试，FaST的核心组件的有效性和鲁棒性得到验证，展示了它在推进认知视觉代理在人工智能系统中的发展方面的潜力。 <div>
Achieving human-level intelligence requires refining cognitive distinctions
between System 1 and System 2 thinking. While contemporary AI, driven by large
language models, demonstrates human-like traits, it falls short of genuine
cognition. Transitioning from structured benchmarks to real-world scenarios
presents challenges for visual agents, often leading to inaccurate and overly
confident responses. To address the challenge, we introduce FaST, which
incorporates the Fast and Slow Thinking mechanism into visual agents. FaST
employs a switch adapter to dynamically select between System 1/2 modes,
tailoring the problem-solving approach to different task complexity. It tackles
uncertain and unseen objects by adjusting model confidence and integrating new
contextual data. With this novel design, we advocate a flexible system,
hierarchical reasoning capabilities, and a transparent decision-making
pipeline, all of which contribute to its ability to emulate human-like
cognitive processes in visual intelligence. Empirical results demonstrate that
FaST outperforms various well-known baselines, achieving 80.8% accuracy over
VQA^{v2} for visual question answering and 48.7% GIoU score over ReasonSeg for
reasoning segmentation, demonstrate FaST's superior performance. Extensive
testing validates the efficacy and robustness of FaST's core components,
showcasing its potential to advance the development of cognitive visual agents
in AI systems.
]]></content:encoded>
<pubDate>2024-08-16T17:44:02Z</pubDate>
</item>
<item>
<title>End-to-end Semantic-centric Video-based Multimodal Affective Computing</title>
<link>http://arxiv.org/abs/2408.07694v1</link>
<guid>http://arxiv.org/abs/2408.07694v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 人工智能; 情感计算; 多模态; 语义分析; 视频分析<br />
<br />
本文介绍了一种名为SemanticMAC的新型端到端框架，用于计算人类语音视频的多模态语义中心情感。该框架利用预训练的Transformer模型进行多模态数据预处理，并设计了情感感知器模块来捕获单模态情感信息。此外，他们提出了一种语义中心方法，以三种方式统一多模态表示学习。最后，在语义中心标签的指导下，SemanticMAC有效地学习了特定和共享的语义表示。实验结果表明，在四个情感计算下游任务中，他们的方法在7个公共数据集上均超过了现有方法。 <br /><br />总结:SemanticMAC框架利用Transformer模型和情感感知器模块进行多模态数据处理和情感信息捕获，并通过语义中心方法统一多模态表示学习，取得了优异的实验结果。 <div>
In the pathway toward Artificial General Intelligence (AGI), understanding
human's affection is essential to enhance machine's cognition abilities. For
achieving more sensual human-AI interaction, Multimodal Affective Computing
(MAC) in human-spoken videos has attracted increasing attention. However,
previous methods are mainly devoted to designing multimodal fusion algorithms,
suffering from two issues: semantic imbalance caused by diverse pre-processing
operations and semantic mismatch raised by inconsistent affection content
contained in different modalities comparing with the multimodal ground truth.
Besides, the usage of manual features extractors make they fail in building
end-to-end pipeline for multiple MAC downstream tasks. To address above
challenges, we propose a novel end-to-end framework named SemanticMAC to
compute multimodal semantic-centric affection for human-spoken videos. We
firstly employ pre-trained Transformer model in multimodal data pre-processing
and design Affective Perceiver module to capture unimodal affective
information. Moreover, we present a semantic-centric approach to unify
multimodal representation learning in three ways, including gated feature
interaction, multi-task pseudo label generation, and intra-/inter-sample
contrastive learning. Finally, SemanticMAC effectively learn specific- and
shared-semantic representations in the guidance of semantic-centric labels.
Extensive experimental results demonstrate that our approach surpass the
state-of-the-art methods on 7 public datasets in four MAC downstream tasks.
]]></content:encoded>
<pubDate>2024-08-14T17:50:27Z</pubDate>
</item>
<item>
<title>Diversity Empowers Intelligence: Integrating Expertise of Software
  Engineering Agents</title>
<link>http://arxiv.org/abs/2408.07060v1</link>
<guid>http://arxiv.org/abs/2408.07060v1</guid>
<content:encoded><![CDATA[
<div> agent frameworks, SWE-Bench Lite, DEI, collaborative AI systems, software engineering challenges
<br /><br />总结:
本文介绍了大型语言模型(LLM)代理在解决现实世界软件工程问题方面的潜力。开源软件工程代理框架在SWE-Bench Lite中能解决超过27%的真实GitHub问题。然而，这些复杂的代理框架在不同任务中表现不同。为了充分利用这些代理的多样性，作者提出了DEI（Diversity Empowered Intelligence）框架，用于整合它们的独特专业知识。实验证明，由DEI指导的代理团体能够大幅超越最佳单个代理的表现。我们发现，一组开源SWE代理在SWE-Bench Lite上最高达到27.3%的解决率，而使用DEI后，可以达到34.3%，超过25%，超越大多数闭源解决方案。我们的最佳团队在SWE-Bench Lite上的解决率达到55%，排名最高。这些发现对协作人工智能系统及其解决复杂软件工程挑战的潜力的研究贡献良多。 <div>
Large language model (LLM) agents have shown great potential in solving
real-world software engineering (SWE) problems. The most advanced open-source
SWE agent can resolve over 27% of real GitHub issues in SWE-Bench Lite.
However, these sophisticated agent frameworks exhibit varying strengths,
excelling in certain tasks while underperforming in others. To fully harness
the diversity of these agents, we propose DEI (Diversity Empowered
Intelligence), a framework that leverages their unique expertise. DEI functions
as a meta-module atop existing SWE agent frameworks, managing agent collectives
for enhanced problem-solving. Experimental results show that a DEI-guided
committee of agents is able to surpass the best individual agent's performance
by a large margin. For instance, a group of open-source SWE agents, with a
maximum individual resolve rate of 27.3% on SWE-Bench Lite, can achieve a 34.3%
resolve rate with DEI, making a 25% improvement and beating most closed-source
solutions. Our best-performing group excels with a 55% resolve rate, securing
the highest ranking on SWE-Bench Lite. Our findings contribute to the growing
body of research on collaborative AI systems and their potential to solve
complex software engineering challenges.
]]></content:encoded>
<pubDate>2024-08-13T17:50:28Z</pubDate>
</item>
<item>
<title>LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs</title>
<link>http://arxiv.org/abs/2408.07055v1</link>
<guid>http://arxiv.org/abs/2408.07055v1</guid>
<content:encoded><![CDATA[
<div> AgentWrite, LongWriter-6k, LongBench-Write, LLMs, DPO
<br /><br />
总结:
本文介绍了当前语言模型存在的输出长度限制问题，认为这是由于数据集中长输出示例的稀缺所致。为解决这一问题，作者提出了AgentWrite，一种基于代理的管道，将超长生成任务分解为子任务，使得现有的LLMs能够生成超过20,000字的连贯输出。作者还开发了包含6,000个SFT数据的LongWriter-6k数据集，其中包含长度为2k至32k字的输出。通过将这一数据集用于模型训练，成功地将现有模型的输出长度扩展到10,000字以上，并保持了输出质量。此外，作者还开发了LongBench-Write，用于评估超长生成能力的综合基准。最终，通过DPO进一步改进的9B参数模型，在这一基准上取得了业界领先的性能。研究表明，现有的长上下文LLM已经具备了更大输出窗口的潜力，唯一需要的是具有扩展输出的数据，以解锁这一能力。 <div>
Current long context large language models (LLMs) can process inputs up to
100,000 tokens, yet struggle to generate outputs exceeding even a modest length
of 2,000 words. Through controlled experiments, we find that the model's
effective generation length is inherently bounded by the sample it has seen
during supervised fine-tuning (SFT). In other words, their output limitation is
due to the scarcity of long-output examples in existing SFT datasets. To
address this, we introduce AgentWrite, an agent-based pipeline that decomposes
ultra-long generation tasks into subtasks, enabling off-the-shelf LLMs to
generate coherent outputs exceeding 20,000 words. Leveraging AgentWrite, we
construct LongWriter-6k, a dataset containing 6,000 SFT data with output
lengths ranging from 2k to 32k words. By incorporating this dataset into model
training, we successfully scale the output length of existing models to over
10,000 words while maintaining output quality. We also develop LongBench-Write,
a comprehensive benchmark for evaluating ultra-long generation capabilities.
Our 9B parameter model, further improved through DPO, achieves state-of-the-art
performance on this benchmark, surpassing even much larger proprietary models.
In general, our work demonstrates that existing long context LLM already
possesses the potential for a larger output window--all you need is data with
extended output during model alignment to unlock this capability. Our code &
models are at: https://github.com/THUDM/LongWriter.
]]></content:encoded>
<pubDate>2024-08-13T17:46:12Z</pubDate>
</item>
<item>
<title>VisualAgentBench: Towards Large Multimodal Models as Visual Foundation
  Agents</title>
<link>http://arxiv.org/abs/2408.06327v1</link>
<guid>http://arxiv.org/abs/2408.06327v1</guid>
<content:encoded><![CDATA[
<div> 多模态模型、视觉基础代理、VisualAgentBench、综合评估、未来发展<br />
LMMs的出现开启了人工智能的新时代，将语言和视觉能力融合成为高度能干的视觉基础代理。然而，现有的基准测试未能充分挑战或展示LMMs在复杂的现实环境中的全部潜力。为了填补这一空白，研究人员引入了VisualAgentBench（VAB），这是一个全面而开创性的基准测试，专门设计用于训练和评估LMMs作为视觉基础代理在各种场景下的表现，包括具象、图形用户界面和视觉设计，任务旨在探究LMMs的理解和交互能力的深度。通过对九个专有LMM API和八个开放模型进行严格测试，研究人员展示了这些模型的可观但仍在不断发展的代理能力。此外，VAB构建了一个轨迹训练集，通过包括基于程序的求解器、LMM代理引导和人类演示在内的混合方法，促进LMMs的行为克隆，从而实现实质性的性能提升。这项工作不仅旨在为现有模型制定基准测试，而且为将来发展成为视觉基础代理打下了坚实的基础。 <div>
Large Multimodal Models (LMMs) have ushered in a new era in artificial
intelligence, merging capabilities in both language and vision to form highly
capable Visual Foundation Agents. These agents are postulated to excel across a
myriad of tasks, potentially approaching general artificial intelligence.
However, existing benchmarks fail to sufficiently challenge or showcase the
full potential of LMMs in complex, real-world environments. To address this
gap, we introduce VisualAgentBench (VAB), a comprehensive and pioneering
benchmark specifically designed to train and evaluate LMMs as visual foundation
agents across diverse scenarios, including Embodied, Graphical User Interface,
and Visual Design, with tasks formulated to probe the depth of LMMs'
understanding and interaction capabilities. Through rigorous testing across
nine proprietary LMM APIs and eight open models, we demonstrate the
considerable yet still developing agent capabilities of these models.
Additionally, VAB constructs a trajectory training set constructed through
hybrid methods including Program-based Solvers, LMM Agent Bootstrapping, and
Human Demonstrations, promoting substantial performance improvements in LMMs
through behavior cloning. Our work not only aims to benchmark existing models
but also provides a solid foundation for future development into visual
foundation agents. Code, train \& test data, and part of fine-tuned open LMMs
are available at \url{https://github.com/THUDM/VisualAgentBench}.
]]></content:encoded>
<pubDate>2024-08-12T17:44:17Z</pubDate>
</item>
<item>
<title>VITA: Towards Open-Source Interactive Omni Multimodal LLM</title>
<link>http://arxiv.org/abs/2408.05211v1</link>
<guid>http://arxiv.org/abs/2408.05211v1</guid>
<content:encoded><![CDATA[
<div> 多模态，交互体验，VITA，开源模型，语言模型

VITA 是首个开源多模态大型语言模型（MLLM），具有视频、图像、文本和音频同时处理和分析的能力，并且拥有先进的多模态交互体验。通过对Mixtral 8x7B作为语言基础的拓展，VITA 扩展了其中文词汇，并进行了双语指导调优。通过两阶段多任务学习的方式赋予语言模型视觉和音频能力。VITA 在跨范围的单模态和多模态基准测试中表现出色，并且在增强自然的多模态人机交互体验方面取得了可观进展。VITA 是开源社区探索多模态理解和交互无缝集成的第一步。虽然VITA 还有很多工作要做，以接近闭源对应物，但我们希望其作为先驱的角色能为后续研究奠定基础。项目页面：https://vita-home.github.io。

<br /><br />总结：
VITA 是首个开源多模态大型语言模型，具有多模态处理和分析能力，并且拥有先进的多模态交互体验。通过扩展语言基础和双语指导调优，VITA 具备了视觉和音频能力，并在多模态基准测试中表现出色。同时，VITA 还在增强自然的多模态人机交互体验方面取得了可观进展。VITA 让开源社区有望探索多模态理解和交互无缝集成。 <div>
The remarkable multimodal capabilities and interactive experience of GPT-4o
underscore their necessity in practical applications, yet open-source models
rarely excel in both areas. In this paper, we introduce VITA, the first-ever
open-source Multimodal Large Language Model (MLLM) adept at simultaneous
processing and analysis of Video, Image, Text, and Audio modalities, and
meanwhile has an advanced multimodal interactive experience. Starting from
Mixtral 8x7B as a language foundation, we expand its Chinese vocabulary
followed by bilingual instruction tuning. We further endow the language model
with visual and audio capabilities through two-stage multi-task learning of
multimodal alignment and instruction tuning. VITA demonstrates robust
foundational capabilities of multilingual, vision, and audio understanding, as
evidenced by its strong performance across a range of both unimodal and
multimodal benchmarks. Beyond foundational capabilities, we have made
considerable progress in enhancing the natural multimodal human-computer
interaction experience. To the best of our knowledge, we are the first to
exploit non-awakening interaction and audio interrupt in MLLM. VITA is the
first step for the open-source community to explore the seamless integration of
multimodal understanding and interaction. While there is still lots of work to
be done on VITA to get close to close-source counterparts, we hope that its
role as a pioneer can serve as a cornerstone for subsequent research. Project
Page: https://vita-home.github.io.
]]></content:encoded>
<pubDate>2024-08-09T17:59:49Z</pubDate>
</item>
<item>
<title>Cell Morphology-Guided Small Molecule Generation with GFlowNets</title>
<link>http://arxiv.org/abs/2408.05196v1</link>
<guid>http://arxiv.org/abs/2408.05196v1</guid>
<content:encoded><![CDATA[
<div> 高含量表型筛选，高含量成像，深度学习，分子设计，药物发现<br />
这项工作集中在高含量成像（HCI）引导的分子设计这一新颖任务。作者提出了一种利用无监督多模态联合嵌入来定义潜在相似性作为GFlowNets的奖励的方法。所提出的模型学习生成新的分子，这些分子可能产生与给定图像目标类似的表型效应，而不依赖预注释的表型标签。研究表明，所提出的方法生成的分子在形态和结构上与目标具有高度相似性，增加了类似生物活性的可能性，这一点已被独立的 Oracle 模型确认。 <br /><br />总结: <br />本研究专注于利用高含量成像（HCI）来指导分子设计，提出了一种无监督的多模态联合嵌入的方法，用于定义潜在相似性作为GFlowNets的奖励，以生成具有类似生物活性的分子。所提出的方法已经证明能够生成形态和结构与目标相似的分子，并且得到了独立的确认。 <div>
High-content phenotypic screening, including high-content imaging (HCI), has
gained popularity in the last few years for its ability to characterize novel
therapeutics without prior knowledge of the protein target. When combined with
deep learning techniques to predict and represent molecular-phenotype
interactions, these advancements hold the potential to significantly accelerate
and enhance drug discovery applications. This work focuses on the novel task of
HCI-guided molecular design. Generative models for molecule design could be
guided by HCI data, for example with a supervised model that links molecules to
phenotypes of interest as a reward function. However, limited labeled data,
combined with the high-dimensional readouts, can make training these methods
challenging and impractical. We consider an alternative approach in which we
leverage an unsupervised multimodal joint embedding to define a latent
similarity as a reward for GFlowNets. The proposed model learns to generate new
molecules that could produce phenotypic effects similar to those of the given
image target, without relying on pre-annotated phenotypic labels. We
demonstrate that the proposed method generates molecules with high
morphological and structural similarity to the target, increasing the
likelihood of similar biological activity, as confirmed by an independent
oracle model.
]]></content:encoded>
<pubDate>2024-08-09T17:40:35Z</pubDate>
</item>
<item>
<title>AdapMTL: Adaptive Pruning Framework for Multitask Learning Model</title>
<link>http://arxiv.org/abs/2408.03913v1</link>
<guid>http://arxiv.org/abs/2408.03913v1</guid>
<content:encoded><![CDATA[
<div> 多媒体处理，多模态处理，模型压缩，多任务学习，AdapMTL<br />
多媒体处理中，高效处理图像、视频和传感器数据至关重要。在这一领域中，模型压缩和多任务学习至关重要，可以解决处理和解释多种媒体同时的资源密集型需求。然而，有效压缩多任务模型存在挑战，因为需要在多个任务之间平衡稀疏性分配和准确性性能。为了解决这些挑战，作者提出了AdapMTL，这是一个自适应修剪框架，专门针对多任务学习模型。AdapMTL利用多个可学习的软阈值，独立分配给共享骨干和任务特定头部，以捕捉不同组件对修剪的敏感性。在训练过程中，它同时优化软阈值和多任务学习模型权重，自动确定适当的稀疏级别，以实现高任务准确性和高整体稀疏度。它进一步结合了自适应加权机制，根据每个任务对修剪的稳健性动态调整任务特定损失的重要性。通过在流行的多任务数据集上进行全面实验证明了AdapMTL的有效性，表现优于最先进的修剪方法。 <br /><br />总结:多媒体处理中，模型压缩和多任务学习是至关重要的。AdapMTL提出了一个自适应修剪框架，能够有效处理多任务学习模型的压缩，通过实验证明了其在不同数据集和架构上的卓越表现。 <div>
In the domain of multimedia and multimodal processing, the efficient handling
of diverse data streams such as images, video, and sensor data is paramount.
Model compression and multitask learning (MTL) are crucial in this field,
offering the potential to address the resource-intensive demands of processing
and interpreting multiple forms of media simultaneously. However, effectively
compressing a multitask model presents significant challenges due to the
complexities of balancing sparsity allocation and accuracy performance across
multiple tasks. To tackle these challenges, we propose AdapMTL, an adaptive
pruning framework for MTL models. AdapMTL leverages multiple learnable soft
thresholds independently assigned to the shared backbone and the task-specific
heads to capture the nuances in different components' sensitivity to pruning.
During training, it co-optimizes the soft thresholds and MTL model weights to
automatically determine the suitable sparsity level at each component to
achieve both high task accuracy and high overall sparsity. It further
incorporates an adaptive weighting mechanism that dynamically adjusts the
importance of task-specific losses based on each task's robustness to pruning.
We demonstrate the effectiveness of AdapMTL through comprehensive experiments
on popular multitask datasets, namely NYU-v2 and Tiny-Taskonomy, with different
architectures, showcasing superior performance compared to state-of-the-art
pruning methods.
]]></content:encoded>
<pubDate>2024-08-07T17:19:15Z</pubDate>
</item>
<item>
<title>LLaVA-OneVision: Easy Visual Task Transfer</title>
<link>http://arxiv.org/abs/2408.03326v1</link>
<guid>http://arxiv.org/abs/2408.03326v1</guid>
<content:encoded><![CDATA[
<div> LLaVA-OneVision, multimodal models, computer vision, transfer learning, video understanding<br />
LLaVA-OneVision是一组开放的大型多模态模型，通过整合对数据、模型和视觉表示的见解而开发，可以在三种重要的计算机视觉场景中同时推动开放LMMs的性能边界：单图像、多图像和视频场景。LLaVA-OneVision的设计允许在不同的模态/场景之间进行强大的迁移学习，产生新的新兴能力。特别地，在通过从图像到视频的任务转移中展示了强大的视频理解和跨场景能力。<br /><br />总结: LLaVA-OneVision是一组开放的大型多模态模型，通过整合对数据、模型和视觉表示的见解而开发，可以在三种重要的计算机视觉场景中同时推动开放LMMs的性能边界。其设计允许在不同的模态/场景之间进行强大的迁移学习，产生新的新兴能力。特别地，在通过从图像到视频的任务转移中展示了强大的视频理解和跨场景能力。 <div>
We present LLaVA-OneVision, a family of open large multimodal models (LMMs)
developed by consolidating our insights into data, models, and visual
representations in the LLaVA-NeXT blog series. Our experimental results
demonstrate that LLaVA-OneVision is the first single model that can
simultaneously push the performance boundaries of open LMMs in three important
computer vision scenarios: single-image, multi-image, and video scenarios.
Importantly, the design of LLaVA-OneVision allows strong transfer learning
across different modalities/scenarios, yielding new emerging capabilities. In
particular, strong video understanding and cross-scenario capabilities are
demonstrated through task transfer from images to videos.
]]></content:encoded>
<pubDate>2024-08-06T17:59:44Z</pubDate>
</item>
<item>
<title>Scaling LLM Test-Time Compute Optimally can be More Effective than
  Scaling Model Parameters</title>
<link>http://arxiv.org/abs/2408.03314v1</link>
<guid>http://arxiv.org/abs/2408.03314v1</guid>
<content:encoded><![CDATA[
<div> LLM，test-time computation，scaling，inference methods，compute-optimal strategy
<br /><br />总结:
本文研究了在语言模型中提升推理时间计算能力的方法，并关注了一个关键问题：如果一个LLM被允许在推理时间使用固定但非微不足道的计算量，它能在具有挑战性的提示上提高多少性能？我们分析了两种主要的推理时间计算扩展机制，并发现不同方法的效果取决于提示的难度。因此，提出了一种“计算最优”扩展策略，可以针对每个提示有效地分配推理时间计算。使用这种计算最优策略，我们可以提高测试时间计算的效率，达到超过最佳N基线4倍的效果。另外，在FLOPs匹配评估中，我们发现在较小的基础模型取得了较高成功率的问题上，测试时间计算可以用来胜过一个14倍大的模型。 <div>
Enabling LLMs to improve their outputs by using more test-time computation is
a critical step towards building generally self-improving agents that can
operate on open-ended natural language. In this paper, we study the scaling of
inference-time computation in LLMs, with a focus on answering the question: if
an LLM is allowed to use a fixed but non-trivial amount of inference-time
compute, how much can it improve its performance on a challenging prompt?
Answering this question has implications not only on the achievable performance
of LLMs, but also on the future of LLM pretraining and how one should tradeoff
inference-time and pre-training compute. Despite its importance, little
research attempted to understand the scaling behaviors of various test-time
inference methods. Moreover, current work largely provides negative results for
a number of these strategies. In this work, we analyze two primary mechanisms
to scale test-time computation: (1) searching against dense, process-based
verifier reward models; and (2) updating the model's distribution over a
response adaptively, given the prompt at test time. We find that in both cases,
the effectiveness of different approaches to scaling test-time compute
critically varies depending on the difficulty of the prompt. This observation
motivates applying a "compute-optimal" scaling strategy, which acts to most
effectively allocate test-time compute adaptively per prompt. Using this
compute-optimal strategy, we can improve the efficiency of test-time compute
scaling by more than 4x compared to a best-of-N baseline. Additionally, in a
FLOPs-matched evaluation, we find that on problems where a smaller base model
attains somewhat non-trivial success rates, test-time compute can be used to
outperform a 14x larger model.
]]></content:encoded>
<pubDate>2024-08-06T17:35:05Z</pubDate>
</item>
<item>
<title>Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation
  with Multimodal Generative Pretraining</title>
<link>http://arxiv.org/abs/2408.02657v1</link>
<guid>http://arxiv.org/abs/2408.02657v1</guid>
<content:encoded><![CDATA[
<div> Lumina-mGPT, multimodal autoregressive models, vision and language tasks, photorealistic images, pretrained decoder-only transformer

总结:
Lumina-mGPT是一种多模态自回归模型，能够处理多种视觉和语言任务，特别擅长根据文本描述生成灵活的逼真图像。它采用预训练的仅解码器transformer作为建模多模态token序列的统一框架，通过在大规模交织的文本-图像序列上利用下一个token预测目标进行的多模态生成预训练（mGPT），能够学习广泛和通用的多模态能力，因此可以实现逼真的文本到图像的生成。利用预训练模型，提出了灵活渐进式监督微调（FP-SFT）以释放其在任何分辨率上进行高美学图像合成的潜力，同时保持其通用的多模态能力。此外，引入了全能监督微调（Omni-SFT），将Lumina-mGPT转化为一个无缝实现全能任务统一的基础模型。最终的模型展示了多功能的多模态能力，包括灵活的文本到图像生成和可控生成等视觉生成任务，分割和深度估计等视觉识别任务，以及多轮视觉问答等视觉-语言任务。此外，还对扩散基础和自回归方法进行了直接比较，分析了它们之间的差异和相似之处。 <div>
We present Lumina-mGPT, a family of multimodal autoregressive models capable
of various vision and language tasks, particularly excelling in generating
flexible photorealistic images from text descriptions. Unlike existing
autoregressive image generation approaches, Lumina-mGPT employs a pretrained
decoder-only transformer as a unified framework for modeling multimodal token
sequences. Our key insight is that a simple decoder-only transformer with
multimodal Generative PreTraining (mGPT), utilizing the next-token prediction
objective on massive interleaved text-image sequences, can learn broad and
general multimodal capabilities, thereby illuminating photorealistic
text-to-image generation. Building on these pretrained models, we propose
Flexible Progressive Supervised Finetuning (FP-SFT) on high-quality image-text
pairs to fully unlock their potential for high-aesthetic image synthesis at any
resolution while maintaining their general multimodal capabilities.
Furthermore, we introduce Ominiponent Supervised Finetuning (Omni-SFT),
transforming Lumina-mGPT into a foundation model that seamlessly achieves
omnipotent task unification. The resulting model demonstrates versatile
multimodal capabilities, including visual generation tasks like flexible
text-to-image generation and controllable generation, visual recognition tasks
like segmentation and depth estimation, and vision-language tasks like
multiturn visual question answering. Additionally, we analyze the differences
and similarities between diffusion-based and autoregressive methods in a direct
comparison.
]]></content:encoded>
<pubDate>2024-08-05T17:46:53Z</pubDate>
</item>
<item>
<title>Talk Less, Interact Better: Evaluating In-context Conversational
  Adaptation in Multimodal LLMs</title>
<link>http://arxiv.org/abs/2408.01417v1</link>
<guid>http://arxiv.org/abs/2408.01417v1</guid>
<content:encoded><![CDATA[
<div> 关键词：语言模型，交互，效率，自适应，ICCA

总结:<br /><br />这篇文章讨论了人类在交流中如何自发地使用越来越高效的语言，并介绍了一个自动化框架ICCA来评估多模态语言模型在交互中的自适应行为。研究发现，目前的语言模型虽然能理解对话中越来越高效的语言，却不能自发地使自己的语言变得更加高效。这表明当前的训练方式并不能培养语言交互中人类常见的语言效率特性。ICCA框架可在https://github.com/lil-lab/ICCA找到。 <div>
Humans spontaneously use increasingly efficient language as interactions
progress, by adapting and forming ad-hoc conventions. This phenomenon has been
studied extensively using reference games, showing properties of human language
that go beyond relaying intents. It remains unexplored whether multimodal large
language models (MLLMs) similarly increase communication efficiency during
interactions, and what mechanisms they may adopt for this purpose. We introduce
ICCA, an automated framework to evaluate such conversational adaptation as an
in-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs, and
observe that while they may understand the increasingly efficient language of
their interlocutor, they do not spontaneously make their own language more
efficient over time. This latter ability can only be elicited in some models
(e.g., GPT-4) with heavy-handed prompting. This shows that this property of
linguistic interaction does not arise from current training regimes, even
though it is a common hallmark of human language. ICCA is available at
https://github.com/lil-lab/ICCA.
]]></content:encoded>
<pubDate>2024-08-02T17:51:57Z</pubDate>
</item>
<item>
<title>MM-Vet v2: A Challenging Benchmark to Evaluate Large Multimodal Models
  for Integrated Capabilities</title>
<link>http://arxiv.org/abs/2408.00765v1</link>
<guid>http://arxiv.org/abs/2408.00765v1</guid>
<content:encoded><![CDATA[
<div> 评估，MM-Vet v2，图像-文本序列理解，Claude 3.5 Sonnet，GPT-4o，InternVL2-Llama3-76B

<br /><br />总结：
MM-Vet v2引入了对模型处理图像-文本序列的评估，并扩大了评估样本规模。通过使用MM-Vet v2对大型多模态模型进行基准测试，发现Claude 3.5 Sonnet在评分上略优于GPT-4o，成为最佳模型，而在开放权重模型中，InternVL2-Llama3-76B以68.4的得分领先。 <div>
MM-Vet, with open-ended vision-language questions targeting at evaluating
integrated capabilities, has become one of the most popular benchmarks for
large multimodal model evaluation. MM-Vet assesses six core vision-language
(VL) capabilities: recognition, knowledge, spatial awareness, language
generation, OCR, and math. However, its question format is restricted to single
image-text pairs, lacking the interleaved image and text sequences prevalent in
real-world scenarios. To address this limitation, we introduce MM-Vet v2, which
includes a new VL capability called "image-text sequence understanding",
evaluating models' ability to process VL sequences. Furthermore, we maintain
the high quality of evaluation samples while further expanding the evaluation
set size. Using MM-Vet v2 to benchmark large multimodal models, we found that
Claude 3.5 Sonnet is the best model with a score of 71.8, slightly
outperforming GPT-4o which scored 71.0. Among open-weight models,
InternVL2-Llama3-76B leads with a score of 68.4.
]]></content:encoded>
<pubDate>2024-08-01T17:59:54Z</pubDate>
</item>
<item>
<title>AgentGen: Enhancing Planning Abilities for Large Language Model based
  Agent via Environment and Task Generation</title>
<link>http://arxiv.org/abs/2408.00764v1</link>
<guid>http://arxiv.org/abs/2408.00764v1</guid>
<content:encoded><![CDATA[
<div> agent, LLM, planning, environment, training
总结:<br /><br />这篇文章研究了如何通过指导调节来提高LLM的规划能力，即通过训练来改进代理机器人的规划能力。它介绍了一个名为AgentGen的框架，利用LLM生成环境和规划任务，以改善代理机器人的规划能力。文章指出，AgentGen大大提高了LLM的规划能力，并在某些任务中甚至表现出色于GPT-4。文章强调了环境多样性和任务难度多样性对提高规划能力的重要性，并提出了相应的解决方案。 <div>
Large Language Model (LLM) based agents have garnered significant attention
and are becoming increasingly popular. Furthermore, planning ability is a
crucial component of an LLM-based agent, involving interaction with the
environment and executing actions to complete a planning task, which generally
entails achieving a desired goal from an initial state. This paper investigates
enhancing the planning abilities of LLMs through instruction tuning, referred
to as agent training. Recent studies have demonstrated that utilizing
expert-level trajectory for instruction-tuning LLMs effectively enhances their
planning capabilities. However, existing work primarily focuses on synthesizing
trajectories from manually designed planning tasks and environments. The
labor-intensive nature of creating these environments and tasks impedes the
generation of sufficiently varied and extensive trajectories. To address this
limitation, this paper explores the automated synthesis of diverse environments
and a gradual range of planning tasks, from easy to difficult. We introduce a
framework, AgentGen, that leverages LLMs first to generate environments and
subsequently generate planning tasks conditioned on these environments.
Specifically, to improve environmental diversity, we propose using an
inspiration corpus composed of various domain-specific text segments as the
context for synthesizing environments. Moreover, to increase the difficulty
diversity of generated planning tasks, we propose a bidirectional evolution
method, Bi-Evol, that evolves planning tasks from easier and harder directions
to synthesize a task set with a smoother difficulty curve. The evaluation
results derived from AgentBoard show that AgentGen greatly improves LLMs'
planning ability, e.g., the AgentGen instruction-tuned Llama-3 8B surpasses
GPT-3.5 in overall performance. Moreover, in certain tasks, it even outperforms
GPT-4.
]]></content:encoded>
<pubDate>2024-08-01T17:59:46Z</pubDate>
</item>
<item>
<title>Smoothed Energy Guidance: Guiding Diffusion Models with Reduced Energy
  Curvature of Attention</title>
<link>http://arxiv.org/abs/2408.00760v1</link>
<guid>http://arxiv.org/abs/2408.00760v1</guid>
<content:encoded><![CDATA[
Conditional diffusion models have shown remarkable success in visual content
generation, producing high-quality samples across various domains, largely due
to classifier-free guidance (CFG). Recent attempts to extend guidance to
unconditional models have relied on heuristic techniques, resulting in
suboptimal generation quality and unintended effects. In this work, we propose
Smoothed Energy Guidance (SEG), a novel training- and condition-free approach
that leverages the energy-based perspective of the self-attention mechanism to
enhance image generation. By defining the energy of self-attention, we
introduce a method to reduce the curvature of the energy landscape of attention
and use the output as the unconditional prediction. Practically, we control the
curvature of the energy landscape by adjusting the Gaussian kernel parameter
while keeping the guidance scale parameter fixed. Additionally, we present a
query blurring method that is equivalent to blurring the entire attention
weights without incurring quadratic complexity in the number of tokens. In our
experiments, SEG achieves a Pareto improvement in both quality and the
reduction of side effects. The code is available at
\url{https://github.com/SusungHong/SEG-SDXL}.
]]></content:encoded>
<pubDate>2024-08-01T17:59:09Z</pubDate>
</item>
<item>
<title>Coarse Correspondence Elicit 3D Spacetime Understanding in Multimodal
  Language Model</title>
<link>http://arxiv.org/abs/2408.00754v1</link>
<guid>http://arxiv.org/abs/2408.00754v1</guid>
<content:encoded><![CDATA[
Multimodal language models (MLLMs) are increasingly being implemented in
real-world environments, necessitating their ability to interpret 3D spaces and
comprehend temporal dynamics. Despite their potential, current top models
within our community still fall short in adequately understanding spatial and
temporal dimensions. We introduce Coarse Correspondence, a simple,
training-free, effective, and general-purpose visual prompting method to elicit
3D and temporal understanding in multimodal LLMs. Our method uses a lightweight
tracking model to find object correspondences between frames in a video or
between sets of image viewpoints. It selects the most frequent object instances
and visualizes them with markers with unique IDs in the image. With this simple
approach, we achieve state-of-the-art results on 3D understanding benchmarks
including ScanQA (+20.5\%) and a subset of OpenEQA (+9.7\%), and on long-form
video benchmarks such as EgoSchema (+6.0\%). We also curate a small diagnostic
dataset to evaluate whether MLLMs can reason about space from a described
viewpoint other than the camera viewpoint. Again, Coarse Correspondence
improves spatial perspective-taking abilities but we highlight that MLLMs
struggle with this task. Together, we demonstrate that our simple prompting
method can significantly aid downstream tasks that require 3D or temporal
reasoning.
]]></content:encoded>
<pubDate>2024-08-01T17:57:12Z</pubDate>
</item>
<item>
<title>Tulip Agent -- Enabling LLM-Based Agents to Solve Tasks Using Large Tool
  Libraries</title>
<link>http://arxiv.org/abs/2407.21778v1</link>
<guid>http://arxiv.org/abs/2407.21778v1</guid>
<content:encoded><![CDATA[
<div> agent, architecture, tool library, inference costs, generalizability
<br />
该论文介绍了一种名为tulip agent的自主LLM（语言模型）代理架构，该架构具有对包含大量工具的工具库进行创建、读取、更新和删除访问的能力。与当前的实现不同，tulip agent不在系统提示中编码所有可用工具的描述，这减少了模型的上下文窗口，也不嵌入检索合适工具的整个提示。相反，tulip agent可以递归搜索其可扩展的工具库中的合适工具，这是通过向量存储示例实现的。tulip agent架构显著降低了推理成本，允许使用甚至是大型工具库，并使代理能够调整和扩展其工具集。作者通过数学领域的多个消融研究评估了该架构，并通过一个机器人学的应用展示了其泛化能力。GitHub上提供了参考实现和基准测试。 <div>
We introduce tulip agent, an architecture for autonomous LLM-based agents
with Create, Read, Update, and Delete access to a tool library containing a
potentially large number of tools. In contrast to state-of-the-art
implementations, tulip agent does not encode the descriptions of all available
tools in the system prompt, which counts against the model's context window, or
embed the entire prompt for retrieving suitable tools. Instead, the tulip agent
can recursively search for suitable tools in its extensible tool library,
implemented exemplarily as a vector store. The tulip agent architecture
significantly reduces inference costs, allows using even large tool libraries,
and enables the agent to adapt and extend its set of tools. We evaluate the
architecture with several ablation studies in a mathematics context and
demonstrate its generalizability with an application to robotics. A reference
implementation and the benchmark are available at
github.com/HRI-EU/tulip_agent.
]]></content:encoded>
<pubDate>2024-07-31T17:50:54Z</pubDate>
</item>
<item>
<title>FlexAttention for Efficient High-Resolution Vision-Language Models</title>
<link>http://arxiv.org/abs/2407.20228v1</link>
<guid>http://arxiv.org/abs/2407.20228v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 高分辨率视觉-语言模型, FlexAttention, 注意力机制, 计算成本, 实验结果
总结:
高分辨率视觉-语言模型中使用的注意力机制计算成本高。为解决这一问题，研究提出了FlexAttention，它通过同时将高分辨率图像编码为高分辨率标记和低分辨率标记，并仅利用低分辨率标记和少量选定的高分辨率标记来计算注意力图，大大降低了计算成本。高分辨率标记通过高分辨率选择模块进行选择，然后与低分辨率标记和文本标记连接，并输入到分层自注意力层中进行处理，生成可用于下一步选取高分辨率标记的注意力图。FlexAttention在多模态基准测试中表现优异，同时将计算成本降低了近40%。 <div>
Current high-resolution vision-language models encode images as
high-resolution image tokens and exhaustively take all these tokens to compute
attention, which significantly increases the computational cost. To address
this problem, we propose FlexAttention, a flexible attention mechanism for
efficient high-resolution vision-language models. Specifically, a
high-resolution image is encoded both as high-resolution tokens and
low-resolution tokens, where only the low-resolution tokens and a few selected
high-resolution tokens are utilized to calculate the attention map, which
greatly shrinks the computational cost. The high-resolution tokens are selected
via a high-resolution selection module which could retrieve tokens of relevant
regions based on an input attention map. The selected high-resolution tokens
are then concatenated to the low-resolution tokens and text tokens, and input
to a hierarchical self-attention layer which produces an attention map that
could be used for the next-step high-resolution token selection. The
hierarchical self-attention process and high-resolution token selection process
are performed iteratively for each attention layer. Experiments on multimodal
benchmarks prove that our FlexAttention outperforms existing high-resolution
VLMs (e.g., relatively ~9% in V* Bench, ~7% in TextVQA), while also
significantly reducing the computational cost by nearly 40%.
]]></content:encoded>
<pubDate>2024-07-29T17:59:05Z</pubDate>
</item>
<item>
<title>SOAP-RL: Sequential Option Advantage Propagation for Reinforcement
  Learning in POMDP Environments</title>
<link>http://arxiv.org/abs/2407.18913v1</link>
<guid>http://arxiv.org/abs/2407.18913v1</guid>
<content:encoded><![CDATA[
<div> POMDPs, options, reinforcement learning, SOAP, policy gradient<br />
POMDPs与选项，强化学习，SOAP算法，策略梯度<br /><br />总结: 本文比较了将强化学习算法扩展到带有选项的部分观察马尔可夫决策过程（POMDP）的方法。其中提出的两种算法，PPOEM和SOAP，旨在解决学习时间一致的选项和相关子策略的问题。PPOEM算法应用了前向-后向算法来优化带有选项的策略的预期回报，但这种学习方法在策略的实施过程中不稳定。相比之下，SOAP评估了最优选项分配的策略梯度，通过扩展广义优势估计（GAE）的概念来在时间上传播选项优势，从而获得了最坚实的表现。SOAP在POMDP环境和标准基准测试中表现出色，优于其他竞争算法，包括PPOEM，LSTM和Option-Critic。 <div>
This work compares ways of extending Reinforcement Learning algorithms to
Partially Observed Markov Decision Processes (POMDPs) with options. One view of
options is as temporally extended action, which can be realized as a memory
that allows the agent to retain historical information beyond the policy's
context window. While option assignment could be handled using heuristics and
hand-crafted objectives, learning temporally consistent options and associated
sub-policies without explicit supervision is a challenge. Two algorithms, PPOEM
and SOAP, are proposed and studied in depth to address this problem. PPOEM
applies the forward-backward algorithm (for Hidden Markov Models) to optimize
the expected returns for an option-augmented policy. However, this learning
approach is unstable during on-policy rollouts. It is also unsuited for
learning causal policies without the knowledge of future trajectories, since
option assignments are optimized for offline sequences where the entire episode
is available. As an alternative approach, SOAP evaluates the policy gradient
for an optimal option assignment. It extends the concept of the generalized
advantage estimation (GAE) to propagate option advantages through time, which
is an analytical equivalent to performing temporal back-propagation of option
policy gradients. This option policy is only conditional on the history of the
agent, not future actions. Evaluated against competing baselines, SOAP
exhibited the most robust performance, correctly discovering options for POMDP
corridor environments, as well as on standard benchmarks including Atari and
MuJoCo, outperforming PPOEM, as well as LSTM and Option-Critic baselines. The
open-sourced code is available at https://github.com/shuishida/SoapRL.
]]></content:encoded>
<pubDate>2024-07-26T17:59:55Z</pubDate>
</item>
<item>
<title>HRP: Human Affordances for Robotic Pre-Training</title>
<link>http://arxiv.org/abs/2407.18911v1</link>
<guid>http://arxiv.org/abs/2407.18911v1</guid>
<content:encoded><![CDATA[
<div> 提取关键词: 机器人代理、视觉网络、预训练、人类视频、智能机器人<br />
<br />
通过利用人类视频数据中提取的"affordances"，在环境和代理层面上进行预训练，以弥补机器人数据收集困难的问题。利用这些affordances对现有的表示进行微调，提高了机器人在各种任务上的性能。实验证明，这种预训练方案可以在5个真实世界任务中显著提高机器人的性能。同时，在不同的摄像头视角下也能够实现更高水平的泛化。这项研究的成果为智能机器人的发展提供了新的思路和方法。 <br /><br />总结: <br />该研究利用人类视频数据中提取的"affordances"进行机器人预训练，提高了机器人在各种任务上的性能。通过微调现有的表示，实现了更高水平的泛化，在不同的摄像头视角下也能得到显著提高的表现。 <div>
In order to *generalize* to various tasks in the wild, robotic agents will
need a suitable representation (i.e., vision network) that enables the robot to
predict optimal actions given high dimensional vision inputs. However, learning
such a representation requires an extreme amount of diverse training data,
which is prohibitively expensive to collect on a real robot. How can we
overcome this problem? Instead of collecting more robot data, this paper
proposes using internet-scale, human videos to extract "affordances," both at
the environment and agent level, and distill them into a pre-trained
representation. We present a simple framework for pre-training representations
on hand, object, and contact "affordance labels" that highlight relevant
objects in images and how to interact with them. These affordances are
automatically extracted from human video data (with the help of off-the-shelf
computer vision modules) and used to fine-tune existing representations. Our
approach can efficiently fine-tune *any* existing representation, and results
in models with stronger downstream robotic performance across the board. We
experimentally demonstrate (using 3000+ robot trials) that this affordance
pre-training scheme boosts performance by a minimum of 15% on 5 real-world
tasks, which consider three diverse robot morphologies (including a dexterous
hand). Unlike prior works in the space, these representations improve
performance across 3 different camera views. Quantitatively, we find that our
approach leads to higher levels of generalization in out-of-distribution
settings. For code, weights, and data check: https://hrp-robot.github.io
]]></content:encoded>
<pubDate>2024-07-26T17:59:52Z</pubDate>
</item>
<item>
<title>Sparse vs Contiguous Adversarial Pixel Perturbations in Multimodal
  Models: An Empirical Analysis</title>
<link>http://arxiv.org/abs/2407.18251v1</link>
<guid>http://arxiv.org/abs/2407.18251v1</guid>
<content:encoded><![CDATA[
<div> multimodal models, adversarial examples, L0-norm perturbation attacks, robustness, unimodal DNNs<br />
<br />
这篇文章主要评估了多模态模型对抗性示例的稳健性。作者通过在预处理的输入图像上进行L0范数扰动攻击，并在黑盒设置下对四个多模态模型和两个单模态DNN进行了攻击。攻击采用了针对少于0.04%的扰动图像区域，并结合了不同的扰动像素的空间位置：稀疏位置和以不同的连续形状排列的像素（行、列、对角线和块）。结果表明，单模态DNN比多模态模型更强壮。此外，使用基于CNN的图像编码器的模型比使用ViT的模型更容易受到攻击。在非定向攻击中，通过扰动少于0.02%的图像区域，我们获得了99%的成功率。 <br /><br />总结: 在本文中，研究人员评估了多模态模型对抗性示例的稳健性，并发现单模态DNN比多模态模型更具有鲁棒性。此外，使用CNN的图像编码器的模型更容易受到攻击。攻击可以通过扰动少于0.02%的图像区域来实现99%的成功率。 <div>
Assessing the robustness of multimodal models against adversarial examples is
an important aspect for the safety of its users. We craft L0-norm perturbation
attacks on the preprocessed input images. We launch them in a black-box setup
against four multimodal models and two unimodal DNNs, considering both targeted
and untargeted misclassification. Our attacks target less than 0.04% of
perturbed image area and integrate different spatial positioning of perturbed
pixels: sparse positioning and pixels arranged in different contiguous shapes
(row, column, diagonal, and patch). To the best of our knowledge, we are the
first to assess the robustness of three state-of-the-art multimodal models
(ALIGN, AltCLIP, GroupViT) against different sparse and contiguous pixel
distribution perturbations. The obtained results indicate that unimodal DNNs
are more robust than multimodal models. Furthermore, models using CNN-based
Image Encoder are more vulnerable than models with ViT - for untargeted
attacks, we obtain a 99% success rate by perturbing less than 0.02% of the
image area.
]]></content:encoded>
<pubDate>2024-07-25T17:59:48Z</pubDate>
</item>
<item>
<title>SV4D: Dynamic 3D Content Generation with Multi-Frame and Multi-View
  Consistency</title>
<link>http://arxiv.org/abs/2407.17470v1</link>
<guid>http://arxiv.org/abs/2407.17470v1</guid>
<content:encoded><![CDATA[
<div> 生成模型，视频合成，动态3D对象，视图生成，训练数据<br />
<br />
本文介绍了Stable Video 4D（SV4D），一种用于多帧和多视图一致动态3D内容生成的潜在视频扩散模型。与之前依赖于分别训练的视频生成模型和新视图合成的方法不同，我们设计了一个统一的扩散模型来生成动态3D对象的新视图视频。具体来说，给定一个单眼参考视频，SV4D为每个视频帧生成了时域一致的新视图。然后，我们使用生成的新视图视频来高效地优化一个隐式的4D表示（动态NeRF），而无需在大多数先前的作品中使用的繁琐的基于SDS的优化。为了训练我们统一的新视图视频生成模型，我们从现有的Objaverse数据集中筛选了一个动态3D对象数据集。在多个数据集和用户研究上的广泛实验结果证明了SV4D在新视角视频合成和4D生成方面与先前工作相比的最新性能。 <br /><br />总结: 本文提出了SV4D模型，通过统一的扩散模型生成动态3D对象的新视图视频，并能高效地优化4D表示，实验结果表明其在新视角视频合成和4D生成上表现优异。 <div>
We present Stable Video 4D (SV4D), a latent video diffusion model for
multi-frame and multi-view consistent dynamic 3D content generation. Unlike
previous methods that rely on separately trained generative models for video
generation and novel view synthesis, we design a unified diffusion model to
generate novel view videos of dynamic 3D objects. Specifically, given a
monocular reference video, SV4D generates novel views for each video frame that
are temporally consistent. We then use the generated novel view videos to
optimize an implicit 4D representation (dynamic NeRF) efficiently, without the
need for cumbersome SDS-based optimization used in most prior works. To train
our unified novel view video generation model, we curated a dynamic 3D object
dataset from the existing Objaverse dataset. Extensive experimental results on
multiple datasets and user studies demonstrate SV4D's state-of-the-art
performance on novel-view video synthesis as well as 4D generation compared to
prior works.
]]></content:encoded>
<pubDate>2024-07-24T17:59:43Z</pubDate>
</item>
<item>
<title>Toward human-centered shared autonomy AI paradigms for human-robot
  teaming in healthcare</title>
<link>http://arxiv.org/abs/2407.17464v1</link>
<guid>http://arxiv.org/abs/2407.17464v1</guid>
<content:encoded><![CDATA[
<div> 医疗机器人, 人工智能, 决策, 人机交互, 共享自治<br />
总结: 近年来，随着人工智能和计算工具的发展，智能范式已经出现，为不同领域的医疗机器人赋予了新的能力。然而，对于与动态最终用户或患者进行交互的医疗机器人来说，独立的决策和目标实现可能并不理想。因此，需要开发一种基于人类中心因素的自适应共享自治人工智能范式，以处理这一挑战，确保不会对人类造成伤害。 <div>
With recent advancements in AI and computation tools, intelligent paradigms
emerged to empower different fields such as healthcare robots with new
capabilities. Advanced AI robotic algorithms (e.g., reinforcement learning) can
be trained and developed to autonomously make individual decisions to achieve a
desired and usually fixed goal. However, such independent decisions and goal
achievements might not be ideal for a healthcare robot that usually interacts
with a dynamic end-user or a patient. In such a complex human-robot interaction
(teaming) framework, the dynamic user continuously wants to be involved in
decision-making as well as introducing new goals while interacting with their
present environment in real-time. To address this challenge, an adaptive shared
autonomy AI paradigm is required to be developed for the two interactive agents
(Human & AI agents) with a foundation based on human-centered factors to avoid
any possible ethical issues and guarantee no harm to humanity.
]]></content:encoded>
<pubDate>2024-07-24T17:58:41Z</pubDate>
</item>
<item>
<title>SoNIC: Safe Social Navigation with Adaptive Conformal Inference and
  Constrained Reinforcement Learning</title>
<link>http://arxiv.org/abs/2407.17460v1</link>
<guid>http://arxiv.org/abs/2407.17460v1</guid>
<content:encoded><![CDATA[
Reinforcement Learning (RL) has enabled social robots to generate
trajectories without human-designed rules or interventions, which makes it more
effective than hard-coded systems for generalizing to complex real-world
scenarios. However, social navigation is a safety-critical task that requires
robots to avoid collisions with pedestrians while previous RL-based solutions
fall short in safety performance in complex environments. To enhance the safety
of RL policies, to the best of our knowledge, we propose the first algorithm,
SoNIC, that integrates adaptive conformal inference (ACI) with constrained
reinforcement learning (CRL) to learn safe policies for social navigation. More
specifically, our method augments RL observations with ACI-generated
nonconformity scores and provides explicit guidance for agents to leverage the
uncertainty metrics to avoid safety-critical areas by incorporating safety
constraints with spatial relaxation. Our method outperforms state-of-the-art
baselines in terms of both safety and adherence to social norms by a large
margin and demonstrates much stronger robustness to out-of-distribution
scenarios. Our code and video demos are available on our project website:
https://sonic-social-nav.github.io/.
]]></content:encoded>
<pubDate>2024-07-24T17:57:21Z</pubDate>
</item>
<item>
<title>Can Large Language Models Automatically Jailbreak GPT-4V?</title>
<link>http://arxiv.org/abs/2407.16686v1</link>
<guid>http://arxiv.org/abs/2407.16686v1</guid>
<content:encoded><![CDATA[
<div> 多模态信息处理, 隐私泄露, 自动越狱技术, 大语言模型, 安全加固<br />
<br />研究引入了AutoJailbreak，这是一种受提示优化启发的创新自动越狱技术。通过使用大语言模型(LLMs)进行红队行动来完善越狱提示，并利用弱到强的上下文学习提示来提高效率。另外，研究提出了一种有效的搜索方法，结合了提前停止来减少优化时间和令牌消耗。实验证明，AutoJailbreak明显优于传统方法，攻击成功率(ASR)超过了95.3\%。这项研究为加强GPT-4V安全性提供了启示，并强调了潜在的大语言模型被利用来损害GPT-4V完整性的可能性。 <br /><br />总结: 多模态信息处理的GPT-4V在隐私泄露方面存在安全问题，研究介绍了一种新的自动越狱技术AutoJailbreak，利用大语言模型进行红队行动，提出了有效的搜索方法，实验证明其攻击成功率显著高于传统方法。这项研究强调了大语言模型可能被利用来损害GPT-4V的完整性。 <div>
GPT-4V has attracted considerable attention due to its extraordinary capacity
for integrating and processing multimodal information. At the same time, its
ability of face recognition raises new safety concerns of privacy leakage.
Despite researchers' efforts in safety alignment through RLHF or preprocessing
filters, vulnerabilities might still be exploited. In our study, we introduce
AutoJailbreak, an innovative automatic jailbreak technique inspired by prompt
optimization. We leverage Large Language Models (LLMs) for red-teaming to
refine the jailbreak prompt and employ weak-to-strong in-context learning
prompts to boost efficiency. Furthermore, we present an effective search method
that incorporates early stopping to minimize optimization time and token
expenditure. Our experiments demonstrate that AutoJailbreak significantly
surpasses conventional methods, achieving an Attack Success Rate (ASR)
exceeding 95.3\%. This research sheds light on strengthening GPT-4V security,
underscoring the potential for LLMs to be exploited in compromising GPT-4V
integrity.
]]></content:encoded>
<pubDate>2024-07-23T17:50:45Z</pubDate>
</item>
<item>
<title>T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video
  Generation</title>
<link>http://arxiv.org/abs/2407.14505v1</link>
<guid>http://arxiv.org/abs/2407.14505v1</guid>
<content:encoded><![CDATA[
<div> 提取关键词:
Text-to-video (T2V) generation, compositional, benchmark, metrics, models

总结:
T2V-CompBench是第一个针对组合文本到视频生成的基准测试。它包括一系列组合元素的评估，以及几种不同的评估指标。这项研究发现，目前的模型在组合文本到视频生成方面仍然面临很大挑战。通过与人工评估结果的相关性验证了提出的评估指标的有效性。我们还对各种文本到视频生成模型进行了基准测试和深入分析。这项研究为未来在这个方向的研究提供了一些启示。 <br /><br />总结:提出了T2V-CompBench基准测试，包括组合成分的评估和多种评估指标。发现当前模型在组合文本到视频生成方面面临挑战，建议未来在这方面进行更深入的研究。 <div>
Text-to-video (T2V) generation models have advanced significantly, yet their
ability to compose different objects, attributes, actions, and motions into a
video remains unexplored. Previous text-to-video benchmarks also neglect this
important ability for evaluation. In this work, we conduct the first systematic
study on compositional text-to-video generation. We propose T2V-CompBench, the
first benchmark tailored for compositional text-to-video generation.
T2V-CompBench encompasses diverse aspects of compositionality, including
consistent attribute binding, dynamic attribute binding, spatial relationships,
motion binding, action binding, object interactions, and generative numeracy.
We further carefully design evaluation metrics of MLLM-based metrics,
detection-based metrics, and tracking-based metrics, which can better reflect
the compositional text-to-video generation quality of seven proposed categories
with 700 text prompts. The effectiveness of the proposed metrics is verified by
correlation with human evaluations. We also benchmark various text-to-video
generative models and conduct in-depth analysis across different models and
different compositional categories. We find that compositional text-to-video
generation is highly challenging for current models, and we hope that our
attempt will shed light on future research in this direction.
]]></content:encoded>
<pubDate>2024-07-19T17:58:36Z</pubDate>
</item>
<item>
<title>Streetscapes: Large-scale Consistent Street View Generation Using
  Autoregressive Video Diffusion</title>
<link>http://arxiv.org/abs/2407.13759v1</link>
<guid>http://arxiv.org/abs/2407.13759v1</guid>
<content:encoded><![CDATA[
<div> 关键词: Streetscapes, 生成方法, 语言输入, 地图/布局, 训练数据<br />
总结:<br />
本文介绍了一种通过语言输入和地图/布局条件来生成Streetscapes（街景）的方法，该方法能够生成长序列的城市场景视图。与最近的视频生成或3D视图合成模型相比，该方法可以适用于跨越多个街区的长序列摄像机轨迹，并保持视觉质量和一致性。作者结合了视频扩散和自回归框架，引入了一种新的时间插值方法，以防止自回归方法偏离现实城市图像的分布。他们使用Google Street View的图像和上下文地图数据对Streetscapes系统进行训练，使用户能够根据所需的城市布局生成城市视图，并控制摄像机位置。详细结果请参阅项目页面https://boyangdeng.com/streetscapes。<br /> <div>
We present a method for generating Streetscapes-long sequences of views
through an on-the-fly synthesized city-scale scene. Our generation is
conditioned by language input (e.g., city name, weather), as well as an
underlying map/layout hosting the desired trajectory. Compared to recent models
for video generation or 3D view synthesis, our method can scale to much
longer-range camera trajectories, spanning several city blocks, while
maintaining visual quality and consistency. To achieve this goal, we build on
recent work on video diffusion, used within an autoregressive framework that
can easily scale to long sequences. In particular, we introduce a new temporal
imputation method that prevents our autoregressive approach from drifting from
the distribution of realistic city imagery. We train our Streetscapes system on
a compelling source of data-posed imagery from Google Street View, along with
contextual map data-which allows users to generate city views conditioned on
any desired city layout, with controllable camera poses. Please see more
results at our project page at https://boyangdeng.com/streetscapes.
]]></content:encoded>
<pubDate>2024-07-18T17:56:30Z</pubDate>
</item>
<item>
<title>AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge
  Bases</title>
<link>http://arxiv.org/abs/2407.12784v1</link>
<guid>http://arxiv.org/abs/2407.12784v1</guid>
<content:encoded><![CDATA[
<div> 提取关键词:
LLM agents, AgentPoison, backdoor attack, memory, RAG mechanism

总结:
LLM agents在各种应用中表现出色，但对未经验证的知识基础的依赖引发了安全和可信度问题。为此，提出了一种新颖的红队模式攻击方法AgentPoison，通过对长期记忆或RAG知识库进行毒化来攻击LLM agents。与传统的后门攻击不同，AgentPoison无需额外的模型训练或微调，并且优化的后门触发器具有出色的可转移性、上下文连贯性和隐蔽性。大量实验证明了AgentPoison对三种现实世界的LLM agents的攻击有效性。在每个agent上，AgentPoison的平均攻击成功率高于80%，对善意性能的影响很小(小于1%)，毒化率小于0.1%。 <br /><br /> <div>
LLM agents have demonstrated remarkable performance across various
applications, primarily due to their advanced capabilities in reasoning,
utilizing external knowledge and tools, calling APIs, and executing actions to
interact with environments. Current agents typically utilize a memory module or
a retrieval-augmented generation (RAG) mechanism, retrieving past knowledge and
instances with similar embeddings from knowledge bases to inform task planning
and execution. However, the reliance on unverified knowledge bases raises
significant concerns about their safety and trustworthiness. To uncover such
vulnerabilities, we propose a novel red teaming approach AgentPoison, the first
backdoor attack targeting generic and RAG-based LLM agents by poisoning their
long-term memory or RAG knowledge base. In particular, we form the trigger
generation process as a constrained optimization to optimize backdoor triggers
by mapping the triggered instances to a unique embedding space, so as to ensure
that whenever a user instruction contains the optimized backdoor trigger, the
malicious demonstrations are retrieved from the poisoned memory or knowledge
base with high probability. In the meantime, benign instructions without the
trigger will still maintain normal performance. Unlike conventional backdoor
attacks, AgentPoison requires no additional model training or fine-tuning, and
the optimized backdoor trigger exhibits superior transferability, in-context
coherence, and stealthiness. Extensive experiments demonstrate AgentPoison's
effectiveness in attacking three types of real-world LLM agents: RAG-based
autonomous driving agent, knowledge-intensive QA agent, and healthcare
EHRAgent. On each agent, AgentPoison achieves an average attack success rate
higher than 80% with minimal impact on benign performance (less than 1%) with a
poison rate less than 0.1%.
]]></content:encoded>
<pubDate>2024-07-17T17:59:47Z</pubDate>
</item>
<item>
<title>VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control</title>
<link>http://arxiv.org/abs/2407.12781v1</link>
<guid>http://arxiv.org/abs/2407.12781v1</guid>
<content:encoded><![CDATA[
<div> Transformer-based video synthesis, camera control, U-Net-based diffusion models, spatial and temporal generation, ControlNet-like conditioning mechanism<br />
<br />总结:<br />该文章介绍了现代文本到视频合成模型，这些模型能够从文本描述生成连贯、逼真的视频。然而，现有模型大多缺乏对相机运动的精细控制，这在涉及内容创作、视觉效果和3D视觉的下游应用中至关重要。最近的新方法展示了生成具有可控相机姿势的视频的能力，这些技术利用了预训练的基于U-Net的扩散模型，明确地分离了空间和时间生成。然而，目前还没有现有方法能够对处理空间和时间信息的基于transformer的视频扩散模型进行相机控制。因此，该论文提出了使用类似ControlNet的调节机制，通过Plucker坐标结合时空相机嵌入，对3D相机进行transformer的调控。在RealEstate10K数据集上进行微调后，该方法展示了在可控视频生成方面的最新性能。据我们所知，这项工作是首次为基于transformer的视频扩散模型实现了相机控制。 <div>
Modern text-to-video synthesis models demonstrate coherent, photorealistic
generation of complex videos from a text description. However, most existing
models lack fine-grained control over camera movement, which is critical for
downstream applications related to content creation, visual effects, and 3D
vision. Recently, new methods demonstrate the ability to generate videos with
controllable camera poses these techniques leverage pre-trained U-Net-based
diffusion models that explicitly disentangle spatial and temporal generation.
Still, no existing approach enables camera control for new, transformer-based
video diffusion models that process spatial and temporal information jointly.
Here, we propose to tame video transformers for 3D camera control using a
ControlNet-like conditioning mechanism that incorporates spatiotemporal camera
embeddings based on Plucker coordinates. The approach demonstrates
state-of-the-art performance for controllable video generation after
fine-tuning on the RealEstate10K dataset. To the best of our knowledge, our
work is the first to enable camera control for transformer-based video
diffusion models.
]]></content:encoded>
<pubDate>2024-07-17T17:59:05Z</pubDate>
</item>
<item>
<title>LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models</title>
<link>http://arxiv.org/abs/2407.12772v1</link>
<guid>http://arxiv.org/abs/2407.12772v1</guid>
<content:encoded><![CDATA[
The advances of large foundation models necessitate wide-coverage, low-cost,
and zero-contamination benchmarks. Despite continuous exploration of language
model evaluations, comprehensive studies on the evaluation of Large Multi-modal
Models (LMMs) remain limited. In this work, we introduce LMMS-EVAL, a unified
and standardized multimodal benchmark framework with over 50 tasks and more
than 10 models to promote transparent and reproducible evaluations. Although
LMMS-EVAL offers comprehensive coverage, we find it still falls short in
achieving low cost and zero contamination. To approach this evaluation
trilemma, we further introduce LMMS-EVAL LITE, a pruned evaluation toolkit that
emphasizes both coverage and efficiency. Additionally, we present Multimodal
LIVEBENCH that utilizes continuously updating news and online forums to assess
models' generalization abilities in the wild, featuring a low-cost and
zero-contamination evaluation approach. In summary, our work highlights the
importance of considering the evaluation trilemma and provides practical
solutions to navigate the trade-offs in evaluating large multi-modal models,
paving the way for more effective and reliable benchmarking of LMMs. We
opensource our codebase and maintain leaderboard of LIVEBENCH at
https://github.com/EvolvingLMMs-Lab/lmms-eval and
https://huggingface.co/spaces/lmms-lab/LiveBench.
]]></content:encoded>
<pubDate>2024-07-17T17:51:53Z</pubDate>
</item>
<item>
<title>Does Refusal Training in LLMs Generalize to the Past Tense?</title>
<link>http://arxiv.org/abs/2407.11969v1</link>
<guid>http://arxiv.org/abs/2407.11969v1</guid>
<content:encoded><![CDATA[
<div> 对齐技术, 拒绝训练, 过去时态, 失效, 模型攻击

拒绝训练是用来防止大型语言模型(LLMs)生成有害、不良或非法输出的方法。本文发现了目前拒绝训练方法中的一个有趣的普遍缺陷：简单地将一个有害的请求改写为过去时态(例如，“如何制作汽油弹？”改写为“人们如何制作汽油弹？”)通常足以逃过许多最先进的LLMs的限制。作者对多个模型进行了系统评估，并发现这种简单攻击对大部分模型都取得了成功。此外，作者还发现将请求改写为将来时态的方法效果较差，这表明拒绝保护栏倾向于将过去历史性问题视为比假设的将来问题更为温和。作者的实验证明，当过去时态的例子被明确地包括在微调数据中时，对抗过去时态改写是可行的。总的来说，作者的研究结果凸显出当前广泛使用的对齐技术在应对模型攻击时可能存在脆弱性，并且并不能总是按预期的那样进行泛化。 <div>
Refusal training is widely used to prevent LLMs from generating harmful,
undesirable, or illegal outputs. We reveal a curious generalization gap in the
current refusal training approaches: simply reformulating a harmful request in
the past tense (e.g., "How to make a Molotov cocktail?" to "How did people make
a Molotov cocktail?") is often sufficient to jailbreak many state-of-the-art
LLMs. We systematically evaluate this method on Llama-3 8B, GPT-3.5 Turbo,
Gemma-2 9B, Phi-3-Mini, GPT-4o, and R2D2 models using GPT-3.5 Turbo as a
reformulation model. For example, the success rate of this simple attack on
GPT-4o increases from 1% using direct requests to 88% using 20 past tense
reformulation attempts on harmful requests from JailbreakBench with GPT-4 as a
jailbreak judge. Interestingly, we also find that reformulations in the future
tense are less effective, suggesting that refusal guardrails tend to consider
past historical questions more benign than hypothetical future questions.
Moreover, our experiments on fine-tuning GPT-3.5 Turbo show that defending
against past reformulations is feasible when past tense examples are explicitly
included in the fine-tuning data. Overall, our findings highlight that the
widely used alignment techniques -- such as SFT, RLHF, and adversarial training
-- employed to align the studied models can be brittle and do not always
generalize as intended. We provide code and jailbreak artifacts at
https://github.com/tml-epfl/llm-past-tense.
]]></content:encoded>
<pubDate>2024-07-16T17:59:55Z</pubDate>
</item>
<item>
<title>Efficient Training with Denoised Neural Weights</title>
<link>http://arxiv.org/abs/2407.11966v1</link>
<guid>http://arxiv.org/abs/2407.11966v1</guid>
<content:encoded><![CDATA[
<div> 权重初始化、深度神经网络模型、生成对抗网络、图像转换任务、训练加速

总结:<br /><br />这项工作旨在解决深度神经网络模型权重初始化的挑战，通过构建一个权重生成器来合成神经网络的初始权重。作者以生成对抗网络在图像转换任务中的应用为例，收集了各种图像编辑概念及其对应的训练权重数据集，并利用这些数据集训练了一个扩散模型。该模型通过文本条件和块索引来预测权重，成功地将图像转换模型的训练时间从43.3秒大大加速，且生成质量更好，为权重初始化带来了创新的解决方案。 <div>
Good weight initialization serves as an effective measure to reduce the
training cost of a deep neural network (DNN) model. The choice of how to
initialize parameters is challenging and may require manual tuning, which can
be time-consuming and prone to human error. To overcome such limitations, this
work takes a novel step towards building a weight generator to synthesize the
neural weights for initialization. We use the image-to-image translation task
with generative adversarial networks (GANs) as an example due to the ease of
collecting model weights spanning a wide range. Specifically, we first collect
a dataset with various image editing concepts and their corresponding trained
weights, which are later used for the training of the weight generator. To
address the different characteristics among layers and the substantial number
of weights to be predicted, we divide the weights into equal-sized blocks and
assign each block an index. Subsequently, a diffusion model is trained with
such a dataset using both text conditions of the concept and the block indexes.
By initializing the image translation model with the denoised weights predicted
by our diffusion model, the training requires only 43.3 seconds. Compared to
training from scratch (i.e., Pix2pix), we achieve a 15x training time
acceleration for a new concept while obtaining even better image generation
quality.
]]></content:encoded>
<pubDate>2024-07-16T17:59:42Z</pubDate>
</item>
<item>
<title>UrbanWorld: An Urban World Model for 3D City Generation</title>
<link>http://arxiv.org/abs/2407.11965v1</link>
<guid>http://arxiv.org/abs/2407.11965v1</guid>
<content:encoded><![CDATA[
Cities, as the most fundamental environment of human life, encompass diverse
physical elements such as buildings, roads and vegetation with complex
interconnection. Crafting realistic, interactive 3D urban environments plays a
crucial role in constructing AI agents capable of perceiving, decision-making,
and acting like humans in real-world environments. However, creating
high-fidelity 3D urban environments usually entails extensive manual labor from
designers, involving intricate detailing and accurate representation of complex
urban features. Therefore, how to accomplish this in an automatical way remains
a longstanding challenge. Toward this problem, we propose UrbanWorld, the first
generative urban world model that can automatically create a customized,
realistic and interactive 3D urban world with flexible control conditions.
UrbanWorld incorporates four key stages in the automatical crafting pipeline:
3D layout generation from openly accessible OSM data, urban scene planning and
designing with a powerful urban multimodal large language model (Urban MLLM),
controllable urban asset rendering with advanced 3D diffusion techniques, and
finally the MLLM-assisted scene refinement. The crafted high-fidelity 3D urban
environments enable realistic feedback and interactions for general AI and
machine perceptual systems in simulations. We are working on contributing
UrbanWorld as an open-source and versatile platform for evaluating and
improving AI abilities in perception, decision-making, and interaction in
realistic urban environments.
]]></content:encoded>
<pubDate>2024-07-16T17:59:29Z</pubDate>
</item>
<item>
<title>Make-An-Agent: A Generalizable Policy Network Generator with
  Behavior-Prompted Diffusion</title>
<link>http://arxiv.org/abs/2407.10973v1</link>
<guid>http://arxiv.org/abs/2407.10973v1</guid>
<content:encoded><![CDATA[
<div> 生成模型，条件扩散模型，行为嵌入，策略网络，多任务学习<br />
这篇论文介绍了Make-An-Agent，一个利用条件扩散模型生成行为到策略的生成模型。该模型通过编码轨迹信息的行为嵌入指导，合成潜在的参数表示，然后解码为策略网络。该生成模型在多个任务上展现出了出色的适用性和伸缩性，并且在未见任务上具有强大的泛化能力，能够仅仅通过少量演示生成出表现良好的策略。实验展示了它在不同领域和任务上的有效性和效率，包括不同目标、行为甚至不同机器人操作器的应用。除了模拟，论文还展示了通过Make-An-Agent生成的策略直接部署在真实世界的机器人上进行运动任务。 <br /><br />总结: 该论文介绍了Make-An-Agent，一个利用条件扩散模型生成行为到策略的生成模型。文章描述了该模型的工作原理以及在多个任务上展现的出色效果。OutOfRange context限制技术的可行性。 <div>
Can we generate a control policy for an agent using just one demonstration of
desired behaviors as a prompt, as effortlessly as creating an image from a
textual description? In this paper, we present Make-An-Agent, a novel policy
parameter generator that leverages the power of conditional diffusion models
for behavior-to-policy generation. Guided by behavior embeddings that encode
trajectory information, our policy generator synthesizes latent parameter
representations, which can then be decoded into policy networks. Trained on
policy network checkpoints and their corresponding trajectories, our generation
model demonstrates remarkable versatility and scalability on multiple tasks and
has a strong generalization ability on unseen tasks to output well-performed
policies with only few-shot demonstrations as inputs. We showcase its efficacy
and efficiency on various domains and tasks, including varying objectives,
behaviors, and even across different robot manipulators. Beyond simulation, we
directly deploy policies generated by Make-An-Agent onto real-world robots on
locomotion tasks.
]]></content:encoded>
<pubDate>2024-07-15T17:59:57Z</pubDate>
</item>
<item>
<title>Ref-AVS: Refer and Segment Objects in Audio-Visual Scenes</title>
<link>http://arxiv.org/abs/2407.10957v1</link>
<guid>http://arxiv.org/abs/2407.10957v1</guid>
<content:encoded><![CDATA[
<div> Reference Audio-Visual Segmentation, multimodal perception, natural language, benchmark, segmentation guidance
<br />
该研究介绍了一项名为参考音频视觉分割（Ref-AVS）的新任务，旨在利用包含多模态提示的表达来对视觉领域内的对象进行分割。为了促进此项研究，构建了第一个Ref-AVS基准数据集，提供了相应多模态提示表达中描述的对象的像素级注释。为了应对Ref-AVS任务，提出了一种新方法，充分利用多模态提示提供精确的分割指导。最后，在三个测试子集上进行了定量和定性实验，将我们的方法与相关任务的现有方法进行了比较。结果表明了我们的方法的有效性，突出了其利用多模态提示表达精确分割对象的能力。<br /><br />总结: 该研究介绍了一个新的任务Ref-AVS，用于基于多模态提示对视觉领域内的对象进行分割。作者构建了第一个Ref-AVS基准数据集，并提出了一种新的方法来应对这一任务。实验结果表明该方法在利用多模态提示进行对象分割方面具有有效性。 <div>
Traditional reference segmentation tasks have predominantly focused on silent
visual scenes, neglecting the integral role of multimodal perception and
interaction in human experiences. In this work, we introduce a novel task
called Reference Audio-Visual Segmentation (Ref-AVS), which seeks to segment
objects within the visual domain based on expressions containing multimodal
cues. Such expressions are articulated in natural language forms but are
enriched with multimodal cues, including audio and visual descriptions. To
facilitate this research, we construct the first Ref-AVS benchmark, which
provides pixel-level annotations for objects described in corresponding
multimodal-cue expressions. To tackle the Ref-AVS task, we propose a new method
that adequately utilizes multimodal cues to offer precise segmentation
guidance. Finally, we conduct quantitative and qualitative experiments on three
test subsets to compare our approach with existing methods from related tasks.
The results demonstrate the effectiveness of our method, highlighting its
capability to precisely segment objects using multimodal-cue expressions.
Dataset is available at
\href{https://gewu-lab.github.io/Ref-AVS}{https://gewu-lab.github.io/Ref-AVS}.
]]></content:encoded>
<pubDate>2024-07-15T17:54:45Z</pubDate>
</item>
<item>
<title>FairyLandAI: Personalized Fairy Tales utilizing ChatGPT and DALLE-3</title>
<link>http://arxiv.org/abs/2407.09467v1</link>
<guid>http://arxiv.org/abs/2407.09467v1</guid>
<content:encoded><![CDATA[
<div> 关键词: FairyLandAI, OpenAI's API, 多样化故事, 儿童受众, 文化丰富

总结: 
本文介绍了FairyLandAI，这是一个通过OpenAI的API开发的创新的大语言模型（LLM），专门为儿童定制个性化童话故事。FairyLandAI具有独特的双重功能：不仅生成引人入胜、适合各种传统的故事，而且还自主生成适合于先进图像生成工具的想象力提示，从而丰富了讲故事的体验。FairyLandAI专门针对儿童的想象世界，提供既有教育意义又有娱乐性，并符合不同年龄的道德价值观的叙述。它的独特优势在于定制故事以匹配个体儿童的偏好和文化背景，开启了个性化讲故事的新时代。此外，它与图像生成技术的整合提供了全面的叙事体验，刺激了口头和视觉创意。对FairyLandAI的实证评估证明了它在为儿童打造引人入胜的故事方面的有效性，这些故事不仅娱乐，而且体现了不同传统的价值观和教义。这一模型对于父母和教育工作者来说是一种宝贵的工具，支持他们通过引人入胜的叙述传达有意义的道德教训。FairyLandAI代表了利用LLM，特别是通过OpenAI的API，进行教育和文化丰富的开创性步伐，使复杂的道德叙述对年轻、富有想象力的头脑来说变得易于接触和享受。 <div>
In the diverse world of AI-driven storytelling, there is a unique opportunity
to engage young audiences with customized, and personalized narratives. This
paper introduces FairyLandAI an innovative Large Language Model (LLM) developed
through OpenAI's API, specifically crafted to create personalized fairytales
for children. The distinctive feature of FairyLandAI is its dual capability: it
not only generates stories that are engaging, age-appropriate, and reflective
of various traditions but also autonomously produces imaginative prompts
suitable for advanced image generation tools like GenAI and Dalle-3, thereby
enriching the storytelling experience. FairyLandAI is expertly tailored to
resonate with the imaginative worlds of children, providing narratives that are
both educational and entertaining and in alignment with the moral values
inherent in different ages. Its unique strength lies in customizing stories to
match individual children's preferences and cultural backgrounds, heralding a
new era in personalized storytelling. Further, its integration with image
generation technology offers a comprehensive narrative experience that
stimulates both verbal and visual creativity. Empirical evaluations of
FairyLandAI demonstrate its effectiveness in crafting captivating stories for
children, which not only entertain but also embody the values and teachings of
diverse traditions. This model serves as an invaluable tool for parents and
educators, supporting them in imparting meaningful moral lessons through
engaging narratives. FairyLandAI represents a pioneering step in using LLMs,
particularly through OpenAI's API, for educational and cultural enrichment,
making complex moral narratives accessible and enjoyable for young, imaginative
minds.
]]></content:encoded>
<pubDate>2024-07-12T17:46:58Z</pubDate>
</item>
<item>
<title>Real-Time Anomaly Detection and Reactive Planning with Large Language
  Models</title>
<link>http://arxiv.org/abs/2407.08735v1</link>
<guid>http://arxiv.org/abs/2407.08735v1</guid>
<content:encoded><![CDATA[
<div> 大语言模型（LLM），异常检测，机器人系统，安全控制，模型预测控制<br />
<br />
本文介绍了一个两阶段推理框架，首先是快速的二元异常分类器，分析LLM嵌入空间中的观测值，然后触发更慢的后备选择阶段，利用生成LLM的推理能力。这些阶段对应于模型预测控制策略中的分支点，确保了继续沿着各种备用计划的联合可行性，以考虑慢推理者的潜伏的延迟，从而确保安全。研究表明，快速异常分类器在资源和时间约束下提高了动态机器人系统的可信度。 <div>
Foundation models, e.g., large language models (LLMs), trained on
internet-scale data possess zero-shot generalization capabilities that make
them a promising technology towards detecting and mitigating
out-of-distribution failure modes of robotic systems. Fully realizing this
promise, however, poses two challenges: (i) mitigating the considerable
computational expense of these models such that they may be applied online, and
(ii) incorporating their judgement regarding potential anomalies into a safe
control framework. In this work, we present a two-stage reasoning framework:
First is a fast binary anomaly classifier that analyzes observations in an LLM
embedding space, which may then trigger a slower fallback selection stage that
utilizes the reasoning capabilities of generative LLMs. These stages correspond
to branch points in a model predictive control strategy that maintains the
joint feasibility of continuing along various fallback plans to account for the
slow reasoner's latency as soon as an anomaly is detected, thus ensuring
safety. We show that our fast anomaly classifier outperforms autoregressive
reasoning with state-of-the-art GPT models, even when instantiated with
relatively small language models. This enables our runtime monitor to improve
the trustworthiness of dynamic robotic systems, such as quadrotors or
autonomous vehicles, under resource and time constraints. Videos illustrating
our approach in both simulation and real-world experiments are available on
this project page: https://sites.google.com/view/aesop-llm.
]]></content:encoded>
<pubDate>2024-07-11T17:59:22Z</pubDate>
</item>
<item>
<title>MetaUrban: A Simulation Platform for Embodied AI in Urban Spaces</title>
<link>http://arxiv.org/abs/2407.08725v1</link>
<guid>http://arxiv.org/abs/2407.08725v1</guid>
<content:encoded><![CDATA[
<div> Urban spaces, Robotics, Embodied AI, MetaUrban, Simulation<br />
<br />
本文介绍了MetaUrban，这是一个用于城市空间中体验智能研究的组合仿真平台。该平台可以构建无限数量的互动城市场景，涵盖广泛的地面规划、物体摆放、行人、脆弱道路使用者和其他移动代理的外观和动态。研究设计了点导航和社交导航任务作为使用MetaUrban进行体验智能研究的初步研究，并建立了强化学习和模仿学习的各种基线。实验证明，模拟环境的组合性质可以显著提高训练移动代理的泛化能力和安全性。 MetaUrban将公开提供，以提供更多研究机会，并促进城市空间中安全可靠的体验智能。 <br /><br />总结: Urban spaces, Robotics, Embodied AI, MetaUrban, Simulation <div>
Public urban spaces like streetscapes and plazas serve residents and
accommodate social life in all its vibrant variations. Recent advances in
Robotics and Embodied AI make public urban spaces no longer exclusive to
humans. Food delivery bots and electric wheelchairs have started sharing
sidewalks with pedestrians, while diverse robot dogs and humanoids have
recently emerged in the street. Ensuring the generalizability and safety of
these forthcoming mobile machines is crucial when navigating through the
bustling streets in urban spaces. In this work, we present MetaUrban, a
compositional simulation platform for Embodied AI research in urban spaces.
MetaUrban can construct an infinite number of interactive urban scenes from
compositional elements, covering a vast array of ground plans, object
placements, pedestrians, vulnerable road users, and other mobile agents'
appearances and dynamics. We design point navigation and social navigation
tasks as the pilot study using MetaUrban for embodied AI research and establish
various baselines of Reinforcement Learning and Imitation Learning. Experiments
demonstrate that the compositional nature of the simulated environments can
substantially improve the generalizability and safety of the trained mobile
agents. MetaUrban will be made publicly available to provide more research
opportunities and foster safe and trustworthy embodied AI in urban spaces.
]]></content:encoded>
<pubDate>2024-07-11T17:56:49Z</pubDate>
</item>
<item>
<title>Generative Image as Action Models</title>
<link>http://arxiv.org/abs/2407.07875v1</link>
<guid>http://arxiv.org/abs/2407.07875v1</guid>
<content:encoded><![CDATA[
<div> image-generation, visuomotor control, behavior-cloning, Stable Diffusion, RLbench
总结:<br /><br />这篇文章介绍了一种名为GENIMA的行为克隆代理，该代理使用稳定扩散模型对RGB图像进行微调，以在图像上“绘制联合动作”作为目标。这些图像被馈送到控制器中，将视觉目标映射到一系列联合位置。研究人员在25个RLBench和9个真实世界操作任务上研究了GENIMA。他们发现，通过将动作转化为图像空间，利用互联网预训练的扩散模型可以生成优于最先进的视觉运动方法的策略，特别是在对场景扰动的鲁棒性和推广到新物体方面。尽管缺乏深度、关键点或运动规划器等先验知识，我们的方法也与3D代理竞争力十足。 <div>
Image-generation diffusion models have been fine-tuned to unlock new
capabilities such as image-editing and novel view synthesis. Can we similarly
unlock image-generation models for visuomotor control? We present GENIMA, a
behavior-cloning agent that fine-tunes Stable Diffusion to 'draw joint-actions'
as targets on RGB images. These images are fed into a controller that maps the
visual targets into a sequence of joint-positions. We study GENIMA on 25
RLBench and 9 real-world manipulation tasks. We find that, by lifting actions
into image-space, internet pre-trained diffusion models can generate policies
that outperform state-of-the-art visuomotor approaches, especially in
robustness to scene perturbations and generalizing to novel objects. Our method
is also competitive with 3D agents, despite lacking priors such as depth,
keypoints, or motion-planners.
]]></content:encoded>
<pubDate>2024-07-10T17:41:10Z</pubDate>
</item>
<item>
<title>Hypothetical Minds: Scaffolding Theory of Mind for Multi-Agent Tasks
  with Large Language Models</title>
<link>http://arxiv.org/abs/2407.07086v1</link>
<guid>http://arxiv.org/abs/2407.07086v1</guid>
<content:encoded><![CDATA[
<div> 大型语言模型、多智体强化学习、认知启发式架构、心理理论模块、Melting Pot基准测试<br />
<br />
Multi-agent reinforcement learning (MARL)方法在处理多智体系统的非静态性时面临困难，并且在与新型智体进行在线测试时无法适应性学习。本文利用大型语言模型(LLMs)创建了一个自主智体Hypothetical Minds，其特点是认知启发式架构，包括感知、记忆和两个层次抽象的分层规划模块。引入了心理理论模块，通过自然语言生成关于其他智体策略的假设，然后评估和逐步完善这些假设，强化那些能正确预测其他智体行为的假设。Hypothetical Minds在Melting Pot基准测试的一系列竞争、混合动机和协作领域中显著提高了性能，包括双人和基于人口的环境。此外，与LLM-agent基线和消融实验的比较揭示了在复杂场景中成功的假设评估和完善的重要性。<br /><br />总结: 大型语言模型被用来创建一个自主智体Hypothetical Minds，它使用认知启发式架构和心理理论模块处理多智体强化学习中的挑战。在Melting Pot基准测试中取得了显著的性能提升，并且对复杂场景的成功具有重要意义。 <div>
Multi-agent reinforcement learning (MARL) methods struggle with the
non-stationarity of multi-agent systems and fail to adaptively learn online
when tested with novel agents. Here, we leverage large language models (LLMs)
to create an autonomous agent that can handle these challenges. Our agent,
Hypothetical Minds, consists of a cognitively-inspired architecture, featuring
modular components for perception, memory, and hierarchical planning over two
levels of abstraction. We introduce the Theory of Mind module that scaffolds
the high-level planning process by generating hypotheses about other agents'
strategies in natural language. It then evaluates and iteratively refines these
hypotheses by reinforcing hypotheses that make correct predictions about the
other agents' behavior. Hypothetical Minds significantly improves performance
over previous LLM-agent and RL baselines on a range of competitive, mixed
motive, and collaborative domains in the Melting Pot benchmark, including both
dyadic and population-based environments. Additionally, comparisons against
LLM-agent baselines and ablations reveal the importance of hypothesis
evaluation and refinement for succeeding on complex scenarios.
]]></content:encoded>
<pubDate>2024-07-09T17:57:15Z</pubDate>
</item>
<item>
<title>Tailor3D: Customized 3D Assets Editing and Generation with Dual-Side
  Images</title>
<link>http://arxiv.org/abs/2407.06191v1</link>
<guid>http://arxiv.org/abs/2407.06191v1</guid>
<content:encoded><![CDATA[
<div> pipeline, 3D assets, dual-side images, editing, Tailor3D
总结: 
最近，3D AIGC在从文本和图像中直接创建3D对象方面取得了进展，但编辑和定制3D资产仍然具有挑战性。为了解决这一问题，文章提出了一种名为Tailor3D的新型流程。它利用可编辑的双面图像快速创建定制的3D资产，并通过双面LRM和LoRA Triplane Transformer实现前后视图的无缝融合。实验结果表明，Tailor3D在各种3D生成和编辑任务中表现出很高的效果，并且具有用户友好、高效的特点，每个编辑步骤仅需几秒钟完成。 <div>
Recent advances in 3D AIGC have shown promise in directly creating 3D objects
from text and images, offering significant cost savings in animation and
product design. However, detailed edit and customization of 3D assets remains a
long-standing challenge. Specifically, 3D Generation methods lack the ability
to follow finely detailed instructions as precisely as their 2D image creation
counterparts. Imagine you can get a toy through 3D AIGC but with undesired
accessories and dressing. To tackle this challenge, we propose a novel pipeline
called Tailor3D, which swiftly creates customized 3D assets from editable
dual-side images. We aim to emulate a tailor's ability to locally change
objects or perform overall style transfer. Unlike creating 3D assets from
multiple views, using dual-side images eliminates conflicts on overlapping
areas that occur when editing individual views. Specifically, it begins by
editing the front view, then generates the back view of the object through
multi-view diffusion. Afterward, it proceeds to edit the back views. Finally, a
Dual-sided LRM is proposed to seamlessly stitch together the front and back 3D
features, akin to a tailor sewing together the front and back of a garment. The
Dual-sided LRM rectifies imperfect consistencies between the front and back
views, enhancing editing capabilities and reducing memory burdens while
seamlessly integrating them into a unified 3D representation with the LoRA
Triplane Transformer. Experimental results demonstrate Tailor3D's effectiveness
across various 3D generation and editing tasks, including 3D generative fill
and style transfer. It provides a user-friendly, efficient solution for editing
3D assets, with each editing step taking only seconds to complete.
]]></content:encoded>
<pubDate>2024-07-08T17:59:55Z</pubDate>
</item>
<item>
<title>JeDi: Joint-Image Diffusion Models for Finetuning-Free Personalized
  Text-to-Image Generation</title>
<link>http://arxiv.org/abs/2407.06187v1</link>
<guid>http://arxiv.org/abs/2407.06187v1</guid>
<content:encoded><![CDATA[
<div> personalization, text-to-image generation, finetuning-free, Joint-Image Diffusion, synthetic dataset generation

总结:<br /><br />这篇论文介绍了一种名为Joint-Image Diffusion（JEDI）的文本到图像生成技术，它不需要费时费力的微调过程，能够实现个性化定制。通过学习多个相关的文本-图像对的联合分布，以及采用可扩展的合成数据集生成技术，该模型能在生成质量上取得了大幅领先，并且在测试阶段可以轻松快速地进行个性化定制，无需额外的优化过程或者模块。 <div>
Personalized text-to-image generation models enable users to create images
that depict their individual possessions in diverse scenes, finding
applications in various domains. To achieve the personalization capability,
existing methods rely on finetuning a text-to-image foundation model on a
user's custom dataset, which can be non-trivial for general users,
resource-intensive, and time-consuming. Despite attempts to develop
finetuning-free methods, their generation quality is much lower compared to
their finetuning counterparts. In this paper, we propose Joint-Image Diffusion
(\jedi), an effective technique for learning a finetuning-free personalization
model. Our key idea is to learn the joint distribution of multiple related
text-image pairs that share a common subject. To facilitate learning, we
propose a scalable synthetic dataset generation technique. Once trained, our
model enables fast and easy personalization at test time by simply using
reference images as input during the sampling process. Our approach does not
require any expensive optimization process or additional modules and can
faithfully preserve the identity represented by any number of reference images.
Experimental results show that our model achieves state-of-the-art generation
quality, both quantitatively and qualitatively, significantly outperforming
both the prior finetuning-based and finetuning-free personalization baselines.
]]></content:encoded>
<pubDate>2024-07-08T17:59:02Z</pubDate>
</item>
<item>
<title>VCoME: Verbal Video Composition with Multimodal Editing Effects</title>
<link>http://arxiv.org/abs/2407.04697v1</link>
<guid>http://arxiv.org/abs/2407.04697v1</guid>
<content:encoded><![CDATA[
<div> 视频编辑效果，语音视频合成，VCoME，多模态模型，数据集<br />
总结:<br />
这篇论文介绍了一种新颖的任务——语音视频合成中的编辑效果生成，旨在通过整合文本、视觉和音频类别的多模态编辑效果，生成连贯且视觉上吸引人的视频内容。作者提出了VCoME框架，利用大型多模态模型生成视频合成的编辑效果，同时支持基于提示的构图密度和风格控制。经过广泛的定量和定性评估，证明了VCoME的有效性。同时，综合用户研究显示，该方法能够生产专业质量的视频，而效率是专业编辑人员的85倍。 <div>
Verbal videos, featuring voice-overs or text overlays, provide valuable
content but present significant challenges in composition, especially when
incorporating editing effects to enhance clarity and visual appeal. In this
paper, we introduce the novel task of verbal video composition with editing
effects. This task aims to generate coherent and visually appealing verbal
videos by integrating multimodal editing effects across textual, visual, and
audio categories. To achieve this, we curate a large-scale dataset of video
effects compositions from publicly available sources. We then formulate this
task as a generative problem, involving the identification of appropriate
positions in the verbal content and the recommendation of editing effects for
these positions. To address this task, we propose VCoME, a general framework
that employs a large multimodal model to generate editing effects for video
composition. Specifically, VCoME takes in the multimodal video context and
autoregressively outputs where to apply effects within the verbal content and
which effects are most appropriate for each position. VCoME also supports
prompt-based control of composition density and style, providing substantial
flexibility for diverse applications. Through extensive quantitative and
qualitative evaluations, we clearly demonstrate the effectiveness of VCoME. A
comprehensive user study shows that our method produces videos of professional
quality while being 85$\times$ more efficient than professional editors.
]]></content:encoded>
<pubDate>2024-07-05T17:59:02Z</pubDate>
</item>
<item>
<title>Fair Division of Indivisible Chores via Earning Restricted Equilibria</title>
<link>http://arxiv.org/abs/2407.03318v1</link>
<guid>http://arxiv.org/abs/2407.03318v1</guid>
<content:encoded><![CDATA[
<div> envy-freeness, Pareto optimality, earning-restricted, competitive equilibrium, algorithms
<br /><br />总结:
本研究首次提出了对m个不可分割家务的公平分配的常数逼近解决方案。我们改进了最佳已知的O(n^2)-EFX的解决方案，提出了5-EFX分配。此外，针对双值实例，我们提出了3-EFX和PO分配，改进了最佳已知的O(n)-EFX解决方案。我们还提出了2-EF2 + PO分配，改进了最佳已知的EFm + PO解决方案。我们引入了基于收入限制的分配竞争均衡的新概念，限制了代理人从每个家务中获得的收入。我们通过解决线性互补问题（LCP）来证明了ER均衡的存在，并设计了算法来实现这些结果。预计ER均衡的概念将在相关问题的进一步研究中发挥重要作用。 <div>
We study fair division of $m$ indivisible chores among $n$ agents with
additive preferences. We consider the desirable fairness notions of
envy-freeness up to any chore (EFX) and envy-freeness up to $k$ chores (EF$k$),
alongside the efficiency notion of Pareto optimality (PO). We present the first
constant approximations of these notions, showing the existence of:
  - 5-EFX allocations, which improve the best-known factor of $O(n^2)$-EFX.
  - 3-EFX and PO allocations for the special case of bivalued instances, which
improve the best-known factor of $O(n)$-EFX without any efficiency guarantees.
  - 2-EF2 + PO allocations, which improve the best-known factor of EF$m$ + PO.
  A notable contribution of our work is the introduction of the novel concept
of earning-restricted (ER) competitive equilibrium for fractional allocations,
which limits agents' earnings from each chore. Technically, our work addresses
two main challenges: proving the existence of an ER equilibrium and designing
algorithms that leverage ER equilibria to achieve the above results. To tackle
the first challenge, we formulate a linear complementarity problem (LCP)
formulation that captures all ER equilibria and show that the classic
complementary pivot algorithm on the LCP must terminate at an ER equilibrium.
For the second challenge, we carefully set the earning limits and use
properties of ER equilibria to design sophisticated procedures that involve
swapping and merging bundles to meet the desired fairness and efficiency
criteria. We expect that the concept of ER equilibrium will be instrumental in
deriving further results on related problems.
]]></content:encoded>
<pubDate>2024-07-03T17:58:22Z</pubDate>
</item>
<item>
<title>BACON: Supercharge Your VLM with Bag-of-Concept Graph to Mitigate
  Hallucinations</title>
<link>http://arxiv.org/abs/2407.03314v1</link>
<guid>http://arxiv.org/abs/2407.03314v1</guid>
<content:encoded><![CDATA[
<div> 关键词: BACON, VLMs, 图像注释, 结构化, 实验<br />
<br />总结:
本文提出了Bag-of-Concept Graph（BACON）模型，旨在赋予具有有限语言能力的模型以体验视觉语言模型（VLMs）的特权，并提高检测、视觉问答（VQA）和图像生成等下游任务的性能。文章将图像注释分解为基本的最小单元，并以图形结构呈现，使其易于理解，并能够解放难以定位的结构组成。通过精心设计提示，借助公开可用的VLMs和分割方法生成BACON注释。通过这种方式，构建了一个包含100K带注释图像的数据集，使VLMs具有显著的能力，如准确生成BACON、将提示转化为BACON格式、以BACON样式展现场景，以及通过交互式对话动态修改BACON中的元素等。广泛的实验包括检测、VQA和图像生成任务，证明了BACON作为一种救生索，可以实现以前无法实现的任务或在当前尖端解决方案中取得优异表现。 <div>
This paper presents Bag-of-Concept Graph (BACON) to gift models with limited
linguistic abilities to taste the privilege of Vision Language Models (VLMs)
and boost downstream tasks such as detection, visual question answering (VQA),
and image generation. Since the visual scenes in physical worlds are structured
with complex relations between objects, BACON breaks down annotations into
basic minimum elements and presents them in a graph structure. Element-wise
style enables easy understanding, and structural composition liberates
difficult locating. Careful prompt design births the BACON captions with the
help of public-available VLMs and segmentation methods. In this way, we gather
a dataset with 100K annotated images, which endow VLMs with remarkable
capabilities, such as accurately generating BACON, transforming prompts into
BACON format, envisioning scenarios in the style of BACONr, and dynamically
modifying elements within BACON through interactive dialogue and more. Wide
representative experiments, including detection, VQA, and image generation
tasks, tell BACON as a lifeline to achieve previous out-of-reach tasks or excel
in their current cutting-edge solutions.
]]></content:encoded>
<pubDate>2024-07-03T17:55:27Z</pubDate>
</item>
<item>
<title>MMedAgent: Learning to Use Medical Tools with Multi-modal Agent</title>
<link>http://arxiv.org/abs/2407.02483v1</link>
<guid>http://arxiv.org/abs/2407.02483v1</guid>
<content:encoded><![CDATA[
<div> 医疗领域, 多模态大语言模型, MMedAgent, 任务, 效率
<br /><br />总结:
本文介绍了一种名为MMedAgent的医疗领域专用代理系统，通过开发一个包含六种医疗工具解决七项任务的指示调整数据集，使得该代理系统能够根据任务选择最合适的工具。实验结果表明，MMedAgent在各种医疗任务中均实现了优越的性能，超过了开源方法甚至是闭源模型GPT-4o。此外，MMedAgent在更新和集成新的医疗工具方面也表现出高效性。 <div>
Multi-Modal Large Language Models (MLLMs), despite being successful, exhibit
limited generality and often fall short when compared to specialized models.
Recently, LLM-based agents have been developed to address these challenges by
selecting appropriate specialized models as tools based on user inputs.
However, such advancements have not been extensively explored within the
medical domain. To bridge this gap, this paper introduces the first agent
explicitly designed for the medical field, named \textbf{M}ulti-modal
\textbf{Med}ical \textbf{Agent} (MMedAgent). We curate an instruction-tuning
dataset comprising six medical tools solving seven tasks, enabling the agent to
choose the most suitable tools for a given task. Comprehensive experiments
demonstrate that MMedAgent achieves superior performance across a variety of
medical tasks compared to state-of-the-art open-source methods and even the
closed-source model, GPT-4o. Furthermore, MMedAgent exhibits efficiency in
updating and integrating new medical tools.
]]></content:encoded>
<pubDate>2024-07-02T17:58:23Z</pubDate>
</item>
<item>
<title>Understanding Alignment in Multimodal LLMs: A Comprehensive Study</title>
<link>http://arxiv.org/abs/2407.02477v1</link>
<guid>http://arxiv.org/abs/2407.02477v1</guid>
<content:encoded><![CDATA[
<div> 多模态大语言模型（MLLMs）, 对齐算法, 偏好数据集, 图像理解, 幻觉<br />
本文主要研究了多模态大语言模型中偏好对齐的各个方面。首先对对齐算法进行了分类，包括离线和在线方法，并指出在某些情况下结合这两种方法可以改善模型性能。其次，对多种已发表的多模态偏好数据集进行了回顾，并讨论了它们构建细节对模型性能的影响。接着，介绍了一种新颖的多模态偏好数据创建方法——偏好驱动的幻觉采样（BDHS），并展示了它在各种基准测试中取得了与先前已发表的对齐工作竞争性的性能。<br /><br />总结:多模态大语言模型的对齐算法包括离线和在线方法，结合两种方法可改善模型性能。已发表的多模态偏好数据集对模型性能有影响。偏好驱动的幻觉采样（BDHS）是一种新颖的数据创建方法，能够取得与先前对齐工作相媲美的性能。 <div>
Preference alignment has become a crucial component in enhancing the
performance of Large Language Models (LLMs), yet its impact in Multimodal Large
Language Models (MLLMs) remains comparatively underexplored. Similar to
language models, MLLMs for image understanding tasks encounter challenges like
hallucination. In MLLMs, hallucination can occur not only by stating incorrect
facts but also by producing responses that are inconsistent with the image
content. A primary objective of alignment for MLLMs is to encourage these
models to align responses more closely with image information. Recently,
multiple works have introduced preference datasets for MLLMs and examined
different alignment methods, including Direct Preference Optimization (DPO) and
Proximal Policy Optimization (PPO). However, due to variations in datasets,
base model types, and alignment methods, it remains unclear which specific
elements contribute most significantly to the reported improvements in these
works. In this paper, we independently analyze each aspect of preference
alignment in MLLMs. We start by categorizing the alignment algorithms into two
groups, offline (such as DPO), and online (such as online-DPO), and show that
combining offline and online methods can improve the performance of the model
in certain scenarios. We review a variety of published multimodal preference
datasets and discuss how the details of their construction impact model
performance. Based on these insights, we introduce a novel way of creating
multimodal preference data called Bias-Driven Hallucination Sampling (BDHS)
that needs neither additional annotation nor external models, and show that it
can achieve competitive performance to previously published alignment work for
multimodal models across a range of benchmarks.
]]></content:encoded>
<pubDate>2024-07-02T17:55:03Z</pubDate>
</item>
<item>
<title>PoliFormer: Scaling On-Policy RL with Transformers Results in Masterful
  Navigators</title>
<link>http://arxiv.org/abs/2406.20083v1</link>
<guid>http://arxiv.org/abs/2406.20083v1</guid>
<content:encoded><![CDATA[
<div> 强化学习、视觉变压器编码器、长期记忆、导航机器人、性能突破<br />
这篇文章介绍了一种名为PoliFormer的室内导航Agent，它是通过强化学习在模拟环境中进行训练的，并且可以在无需适应的情况下在真实世界中实现泛化。PoliFormer使用了视觉变压器编码器和因果变压器解码器，能够实现长期记忆和推理。它在多台机器上进行了数亿次的交互训练，实现了高效的训练和高吞吐量。PoliFormer在LoCoBot和Stretch RE-1机器人以及四个导航基准测试中都取得了最先进的结果，尤其在CHORES-S基准测试的目标导航中表现出了非凡的成绩，成功率达到了85.5%，比以往提升了28.5%。此外，PoliFormer还可以轻松扩展到各种下游应用领域，包括目标追踪、多目标导航和开放词汇的导航，无需微调即可实现。<br /><br />总结: 该文章介绍了PoliFormer，一种强化学习训练的导航Agent，通过视觉变压器编码器和因果变压器解码器实现长期记忆和推理。在多个机器上进行了数亿次的交互训练，取得了在真实世界中泛化的先进结果，尤其在CHORES-S基准测试中表现突出。该技术还能轻松应用于多个领域，无需微调即可实现。 <div>
We present PoliFormer (Policy Transformer), an RGB-only indoor navigation
agent trained end-to-end with reinforcement learning at scale that generalizes
to the real-world without adaptation despite being trained purely in
simulation. PoliFormer uses a foundational vision transformer encoder with a
causal transformer decoder enabling long-term memory and reasoning. It is
trained for hundreds of millions of interactions across diverse environments,
leveraging parallelized, multi-machine rollouts for efficient training with
high throughput. PoliFormer is a masterful navigator, producing
state-of-the-art results across two distinct embodiments, the LoCoBot and
Stretch RE-1 robots, and four navigation benchmarks. It breaks through the
plateaus of previous work, achieving an unprecedented 85.5% success rate in
object goal navigation on the CHORES-S benchmark, a 28.5% absolute improvement.
PoliFormer can also be trivially extended to a variety of downstream
applications such as object tracking, multi-object navigation, and
open-vocabulary navigation with no finetuning.
]]></content:encoded>
<pubDate>2024-06-28T17:51:10Z</pubDate>
</item>
<item>
<title>ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos</title>
<link>http://arxiv.org/abs/2406.19392v1</link>
<guid>http://arxiv.org/abs/2406.19392v1</guid>
<content:encoded><![CDATA[
<div> 关键词: ReXTime, temporal reasoning, video events, benchmark, large language models <br />
<br /> 
本文介绍了ReXTime基准测试，旨在严格测试AI模型在视频事件中进行时间推理的能力。ReXTime专注于跨时间推理，即在问题和相应答案出现在不同视频片段时的人类理解能力。这种推理需要对视频片段之间的因果关系有高级理解，即使是最前沿的多模态大语言模型也会面临重大挑战。为了促进这种评估，作者们开发了一个自动化流水线，用于生成时间推理问题-答案对，显著减少了对繁重的手工注释的需求。他们的基准测试包括921个精心审核的验证样本和2,143个测试样本，每个样本都经过手工筛选以确保准确性和相关性。评估结果显示，尽管最前沿的大语言模型表现优于学术模型，但它们仍然落后于人类性能，有着14.3%的显著准确度差距。此外，作者的流水线创建了一个由9695个机器生成样本组成的训练数据集，而不需要手工努力，经验研究表明这可以通过微调来增强跨时间推理能力。<br /><br />总结: 本文介绍了ReXTime基准测试的设计和评估结果，以及作者们通过自动化流水线生成训练数据集的方法和作用。 <div>
We introduce ReXTime, a benchmark designed to rigorously test AI models'
ability to perform temporal reasoning within video events. Specifically,
ReXTime focuses on reasoning across time, i.e. human-like understanding when
the question and its corresponding answer occur in different video segments.
This form of reasoning, requiring advanced understanding of cause-and-effect
relationships across video segments, poses significant challenges to even the
frontier multimodal large language models. To facilitate this evaluation, we
develop an automated pipeline for generating temporal reasoning question-answer
pairs, significantly reducing the need for labor-intensive manual annotations.
Our benchmark includes 921 carefully vetted validation samples and 2,143 test
samples, each manually curated for accuracy and relevance. Evaluation results
show that while frontier large language models outperform academic models, they
still lag behind human performance by a significant 14.3% accuracy gap.
Additionally, our pipeline creates a training dataset of 9,695 machine
generated samples without manual effort, which empirical studies suggest can
enhance the across-time reasoning via fine-tuning.
]]></content:encoded>
<pubDate>2024-06-27T17:59:45Z</pubDate>
</item>
<item>
<title>OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and
  Understanding</title>
<link>http://arxiv.org/abs/2406.19389v1</link>
<guid>http://arxiv.org/abs/2406.19389v1</guid>
<content:encoded><![CDATA[
<div> 像素级理解，推理能力，文本指示，OMG-LLaVA，视觉多模态模型
<br /><br />
总结:
本文提出了OMG-LLaVA框架，将强大的像素级视觉理解能力与推理能力相结合。该框架可以接受各种视觉和文本指示，实现灵活的用户交互。作者使用通用分割方法作为视觉编码器，将图像信息、感知先验和视觉提示整合成LLM所需的视觉令牌。LLM负责理解用户的文本指示，并基于视觉信息提供文本响应和像素级分割结果。作者提出了感知先验嵌入来更好地整合感知先验和图像特征。OMG-LLaVA在单一模型中实现了图像级、对象级和像素级推理和理解能力，在多个基准测试中达到或超越了专业方法的性能。与使用LLM连接各专业方法的做法不同，本文旨在对一个编码器、一个解码器和一个LLM进行端到端训练。该代码和模型已发布供进一步研究使用。 <div>
Current universal segmentation methods demonstrate strong capabilities in
pixel-level image and video understanding. However, they lack reasoning
abilities and cannot be controlled via text instructions. In contrast, large
vision-language multimodal models exhibit powerful vision-based conversation
and reasoning capabilities but lack pixel-level understanding and have
difficulty accepting visual prompts for flexible user interaction. This paper
proposes OMG-LLaVA, a new and elegant framework combining powerful pixel-level
vision understanding with reasoning abilities. It can accept various visual and
text prompts for flexible user interaction. Specifically, we use a universal
segmentation method as the visual encoder, integrating image information,
perception priors, and visual prompts into visual tokens provided to the LLM.
The LLM is responsible for understanding the user's text instructions and
providing text responses and pixel-level segmentation results based on the
visual information. We propose perception prior embedding to better integrate
perception priors with image features. OMG-LLaVA achieves image-level,
object-level, and pixel-level reasoning and understanding in a single model,
matching or surpassing the performance of specialized methods on multiple
benchmarks. Rather than using LLM to connect each specialist, our work aims at
end-to-end training on one encoder, one decoder, and one LLM. The code and
model have been released for further research.
]]></content:encoded>
<pubDate>2024-06-27T17:59:01Z</pubDate>
</item>
<item>
<title>Symbolic Learning Enables Self-Evolving Agents</title>
<link>http://arxiv.org/abs/2406.18532v1</link>
<guid>http://arxiv.org/abs/2406.18532v1</guid>
<content:encoded><![CDATA[
<div> agent symbolic learning, language agents, data-centric, artificial general intelligence, back-propagation and gradient descent
<br /><br />
agent symbolic learning 是一种系统框架，使得语言代理能够以数据为中心地使用符号优化器自行优化。它利用符号网络和模仿连接主义学习的两种基本算法：反向传播和梯度下降，以自然语言的方式处理权重、损失和梯度。这种方法使得语言代理能够在创建和部署后自行更新，并成为“自我进化的代理”。这一方法的引入使得语言代理的研究从模型和工程为中心转向了以数据为中心的学习，是通往人工通用智能的关键。 <br />agent symbolic learning, data-centric, 自我进化的代理<br />包含所有要点的总结:
agent symbolic learning 是一种系统框架，使得语言代理能够以数据为中心地使用符号优化器自行优化。它利用符号网络和模仿连接主义学习的两种基本算法：反向传播和梯度下降，以自然语言的方式处理权重、损失和梯度。这种方法使得语言代理能够在创建和部署后自行更新，并成为“自我进化的代理”。这一方法的引入使得语言代理的研究从模型和工程为中心转向了以数据为中心的学习，是通往人工通用智能的关键。 <div>
The AI community has been exploring a pathway to artificial general
intelligence (AGI) by developing "language agents", which are complex large
language models (LLMs) pipelines involving both prompting techniques and tool
usage methods. While language agents have demonstrated impressive capabilities
for many real-world tasks, a fundamental limitation of current language agents
research is that they are model-centric, or engineering-centric. That's to say,
the progress on prompts, tools, and pipelines of language agents requires
substantial manual engineering efforts from human experts rather than
automatically learning from data. We believe the transition from model-centric,
or engineering-centric, to data-centric, i.e., the ability of language agents
to autonomously learn and evolve in environments, is the key for them to
possibly achieve AGI.
  In this work, we introduce agent symbolic learning, a systematic framework
that enables language agents to optimize themselves on their own in a
data-centric way using symbolic optimizers. Specifically, we consider agents as
symbolic networks where learnable weights are defined by prompts, tools, and
the way they are stacked together. Agent symbolic learning is designed to
optimize the symbolic network within language agents by mimicking two
fundamental algorithms in connectionist learning: back-propagation and gradient
descent. Instead of dealing with numeric weights, agent symbolic learning works
with natural language simulacrums of weights, loss, and gradients. We conduct
proof-of-concept experiments on both standard benchmarks and complex real-world
tasks and show that agent symbolic learning enables language agents to update
themselves after being created and deployed in the wild, resulting in
"self-evolving agents".
]]></content:encoded>
<pubDate>2024-06-26T17:59:18Z</pubDate>
</item>
<item>
<title>MultiDiff: Consistent Novel View Synthesis from a Single Image</title>
<link>http://arxiv.org/abs/2406.18524v1</link>
<guid>http://arxiv.org/abs/2406.18524v1</guid>
<content:encoded><![CDATA[
<div> 深度预测，视频扩散，多视角一致，生成图像，实时编辑
<br />
本文介绍了一种名为MultiDiff的新方法，用于从单个RGB图像中一致地合成新视图。该方法结合了单目深度预测和视频扩散模型的强先验知识，提高了几何稳定性，并允许模型学习生成图像之间的连续、像素精确的对应关系。与依赖自回归图像生成的方法相比，MultiDiff可以同时合成一系列帧，产生高质量、多视角一致的结果，甚至对于有着大摄像机移动的长期场景生成，也能减少推断时间。此外，MultiDiff还引入了一种新颖的结构化噪声分布，用于提高一致性和图像质量。实验结果表明，MultiDiff在RealEstate10K和ScanNet这两个具有挑战性的真实世界数据集上表现优于现有方法。最后，该模型自然支持多视角一致的编辑，无需进一步调整。
<br /><br />总结: MultiDiff方法结合了深度预测和视频扩散模型的先验知识，实现了高质量、多视角一致的图像生成，并且能够实时编辑。 <div>
We introduce MultiDiff, a novel approach for consistent novel view synthesis
of scenes from a single RGB image. The task of synthesizing novel views from a
single reference image is highly ill-posed by nature, as there exist multiple,
plausible explanations for unobserved areas. To address this issue, we
incorporate strong priors in form of monocular depth predictors and
video-diffusion models. Monocular depth enables us to condition our model on
warped reference images for the target views, increasing geometric stability.
The video-diffusion prior provides a strong proxy for 3D scenes, allowing the
model to learn continuous and pixel-accurate correspondences across generated
images. In contrast to approaches relying on autoregressive image generation
that are prone to drifts and error accumulation, MultiDiff jointly synthesizes
a sequence of frames yielding high-quality and multi-view consistent results --
even for long-term scene generation with large camera movements, while reducing
inference time by an order of magnitude. For additional consistency and image
quality improvements, we introduce a novel, structured noise distribution. Our
experimental results demonstrate that MultiDiff outperforms state-of-the-art
methods on the challenging, real-world datasets RealEstate10K and ScanNet.
Finally, our model naturally supports multi-view consistent editing without the
need for further tuning.
]]></content:encoded>
<pubDate>2024-06-26T17:53:51Z</pubDate>
</item>
<item>
<title>ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of
  Text-to-Time-lapse Video Generation</title>
<link>http://arxiv.org/abs/2406.18522v1</link>
<guid>http://arxiv.org/abs/2406.18522v1</guid>
<content:encoded><![CDATA[
We propose a novel text-to-video (T2V) generation benchmark,
ChronoMagic-Bench, to evaluate the temporal and metamorphic capabilities of the
T2V models (e.g. Sora and Lumiere) in time-lapse video generation. In contrast
to existing benchmarks that focus on the visual quality and textual relevance
of generated videos, ChronoMagic-Bench focuses on the model's ability to
generate time-lapse videos with significant metamorphic amplitude and temporal
coherence. The benchmark probes T2V models for their physics, biology, and
chemistry capabilities, in a free-form text query. For these purposes,
ChronoMagic-Bench introduces 1,649 prompts and real-world videos as references,
categorized into four major types of time-lapse videos: biological,
human-created, meteorological, and physical phenomena, which are further
divided into 75 subcategories. This categorization comprehensively evaluates
the model's capacity to handle diverse and complex transformations. To
accurately align human preference with the benchmark, we introduce two new
automatic metrics, MTScore and CHScore, to evaluate the videos' metamorphic
attributes and temporal coherence. MTScore measures the metamorphic amplitude,
reflecting the degree of change over time, while CHScore assesses the temporal
coherence, ensuring the generated videos maintain logical progression and
continuity. Based on the ChronoMagic-Bench, we conduct comprehensive manual
evaluations of ten representative T2V models, revealing their strengths and
weaknesses across different categories of prompts, and providing a thorough
evaluation framework that addresses current gaps in video generation research.
Moreover, we create a large-scale ChronoMagic-Pro dataset, containing 460k
high-quality pairs of 720p time-lapse videos and detailed captions ensuring
high physical pertinence and large metamorphic amplitude.
]]></content:encoded>
<pubDate>2024-06-26T17:50:47Z</pubDate>
</item>
<item>
<title>Text-Animator: Controllable Visual Text Video Generation</title>
<link>http://arxiv.org/abs/2406.17777v1</link>
<guid>http://arxiv.org/abs/2406.17777v1</guid>
<content:encoded><![CDATA[
<div> Text-to-Video, Visualization, Text-Animator, Video generation, Visual text
<br /><br />本研究提出了一种名为Text-Animator的创新方法，用于视觉文本视频生成。该方法通过文本嵌入注入模块准确地描述生成视频中的视觉文本结构。此外，研究团队还开发了一个摄像头控制模块和一个文本精化模块，以改善所生成的视觉文本的稳定性。实验结果定量和定性地展示了该方法相比于现有视频生成方法在生成视觉文本准确性方面的优越性。 Text-Animator对于文本在视频中的可视化具有重要意义，尤其是在游戏、电子商务和广告等行业中。虽然现有的文本到视频生成方法取得了进展，但仍然面临着许多问题。本研究提出的Text-Animator方法填补了这一空白，并展现了出色的表现。总结: <br />Text-Animator方法针对文本到视频生成问题提出了创新的解决方案，实现了在生成视频中准确地展现视觉文本的结构。 <div>
Video generation is a challenging yet pivotal task in various industries,
such as gaming, e-commerce, and advertising. One significant unresolved aspect
within T2V is the effective visualization of text within generated videos.
Despite the progress achieved in Text-to-Video~(T2V) generation, current
methods still cannot effectively visualize texts in videos directly, as they
mainly focus on summarizing semantic scene information, understanding, and
depicting actions. While recent advances in image-level visual text generation
show promise, transitioning these techniques into the video domain faces
problems, notably in preserving textual fidelity and motion coherence. In this
paper, we propose an innovative approach termed Text-Animator for visual text
video generation. Text-Animator contains a text embedding injection module to
precisely depict the structures of visual text in generated videos. Besides, we
develop a camera control module and a text refinement module to improve the
stability of generated visual text by controlling the camera movement as well
as the motion of visualized text. Quantitative and qualitative experimental
results demonstrate the superiority of our approach to the accuracy of
generated visual text over state-of-the-art video generation methods. The
project page can be found at https://laulampaul.github.io/text-animator.html.
]]></content:encoded>
<pubDate>2024-06-25T17:59:41Z</pubDate>
</item>
<item>
<title>MG-LLaVA: Towards Multi-Granularity Visual Instruction Tuning</title>
<link>http://arxiv.org/abs/2406.17770v1</link>
<guid>http://arxiv.org/abs/2406.17770v1</guid>
<content:encoded><![CDATA[
<div> 多模态大型语言模型、MG-LLaVA、视觉处理、细节信息、性能评估
<br />
多模态大型语言模型MG-LLaVA结合了多个视觉流程，包括低分辨率、高分辨率和物体中心特征，以增强模型的视觉处理能力。该模型引入了额外的高分辨率视觉编码器和经过Conv-Gate融合网络与基础视觉特征相结合，以捕捉细节信息。此外，还通过离线检测器识别的边界框来整合物体级特征以进一步提高模型的对象识别能力。经过指导调整，MG-LLaVA仅在公开可用的多模态数据上训练，展现出了卓越的感知能力。经过广泛的评估，MG-LLaVA在多个基准测试中优于相似参数大小的现有多模态大型语言模型，展现出了卓越的效能。<br /><br />总结: 多模态大型语言模型MG-LLaVA结合了多个视觉流程，包括低分辨率、高分辨率和物体中心特征，以增强模型的视觉处理能力。与现有模型相比，MG-LLaVA表现出卓越的性能。 <div>
Multi-modal large language models (MLLMs) have made significant strides in
various visual understanding tasks. However, the majority of these models are
constrained to process low-resolution images, which limits their effectiveness
in perception tasks that necessitate detailed visual information. In our study,
we present MG-LLaVA, an innovative MLLM that enhances the model's visual
processing capabilities by incorporating a multi-granularity vision flow, which
includes low-resolution, high-resolution, and object-centric features. We
propose the integration of an additional high-resolution visual encoder to
capture fine-grained details, which are then fused with base visual features
through a Conv-Gate fusion network. To further refine the model's object
recognition abilities, we incorporate object-level features derived from
bounding boxes identified by offline detectors. Being trained solely on
publicly available multimodal data through instruction tuning, MG-LLaVA
demonstrates exceptional perception skills. We instantiate MG-LLaVA with a wide
variety of language encoders, ranging from 3.8B to 34B, to evaluate the model's
performance comprehensively. Extensive evaluations across multiple benchmarks
demonstrate that MG-LLaVA outperforms existing MLLMs of comparable parameter
sizes, showcasing its remarkable efficacy. The code will be available at
https://github.com/PhoenixZ810/MG-LLaVA.
]]></content:encoded>
<pubDate>2024-06-25T17:55:11Z</pubDate>
</item>
<item>
<title>EXTRACT: Efficient Policy Learning by Extracting Transferrable Robot
  Skills from Offline Data</title>
<link>http://arxiv.org/abs/2406.17768v1</link>
<guid>http://arxiv.org/abs/2406.17768v1</guid>
<content:encoded><![CDATA[
Most reinforcement learning (RL) methods focus on learning optimal policies
over low-level action spaces. While these methods can perform well in their
training environments, they lack the flexibility to transfer to new tasks.
Instead, RL agents that can act over useful, temporally extended skills rather
than low-level actions can learn new tasks more easily. Prior work in
skill-based RL either requires expert supervision to define useful skills,
which is hard to scale, or learns a skill-space from offline data with
heuristics that limit the adaptability of the skills, making them difficult to
transfer during downstream RL. Our approach, EXTRACT, instead utilizes
pre-trained vision language models to extract a discrete set of semantically
meaningful skills from offline data, each of which is parameterized by
continuous arguments, without human supervision. This skill parameterization
allows robots to learn new tasks by only needing to learn when to select a
specific skill and how to modify its arguments for the specific task. We
demonstrate through experiments in sparse-reward, image-based, robot
manipulation environments that EXTRACT can more quickly learn new tasks than
prior works, with major gains in sample efficiency and performance over prior
skill-based RL. Website at https://www.jessezhang.net/projects/extract/.
]]></content:encoded>
<pubDate>2024-06-25T17:50:03Z</pubDate>
</item>
<item>
<title>Revisiting Referring Expression Comprehension Evaluation in the Era of
  Large Multimodal Models</title>
<link>http://arxiv.org/abs/2406.16866v1</link>
<guid>http://arxiv.org/abs/2406.16866v1</guid>
<content:encoded><![CDATA[
<div> 关键词: Referring expression comprehension, large multimodal models, benchmarks, labeling error rates, Ref-L4

总结:<br /><br />这篇文章主要介绍了针对文本描述定位目标实例的Referring expression comprehension (REC)技术。最近，大型多模态模型（LMMs）如CogVLM的出现推动了REC技术的进展，取得了92.44%的准确率。然而，研究质疑现有的基准数据集如RefCOCO，RefCOCO+和RefCOCOg是否能充分评估LMMs的能力。作者通过对这些基准数据集进行手动检查，发现了高达14%到24%的标注错误率，这削弱了评估的真实性。为了解决这一问题，他们排除了问题实例，并重新评估了多个能够处理REC任务的LMMs，显示出显著的准确率提高，突显了基准数据集噪音的影响。作为回应，他们推出了Ref-L4，这是一个专门设计用于评估现代REC模型的全面基准数据集，具有大样本量、多样的物体类别、长的指代表达和丰富的词汇量等特点。他们在Ref-L4上评估了总共24个大型模型，并提供了有价值的见解。原始的RefCOCO，RefCOCO+和RefCOCOg数据集的清理版本，以及他们的Ref-L4基准数据集和评估代码，均可在https://github.com/JierunChen/Ref-L4上获得。 <div>
Referring expression comprehension (REC) involves localizing a target
instance based on a textual description. Recent advancements in REC have been
driven by large multimodal models (LMMs) like CogVLM, which achieved 92.44%
accuracy on RefCOCO. However, this study questions whether existing benchmarks
such as RefCOCO, RefCOCO+, and RefCOCOg, capture LMMs' comprehensive
capabilities. We begin with a manual examination of these benchmarks, revealing
high labeling error rates: 14% in RefCOCO, 24% in RefCOCO+, and 5% in RefCOCOg,
which undermines the authenticity of evaluations. We address this by excluding
problematic instances and reevaluating several LMMs capable of handling the REC
task, showing significant accuracy improvements, thus highlighting the impact
of benchmark noise. In response, we introduce Ref-L4, a comprehensive REC
benchmark, specifically designed to evaluate modern REC models. Ref-L4 is
distinguished by four key features: 1) a substantial sample size with 45,341
annotations; 2) a diverse range of object categories with 365 distinct types
and varying instance scales from 30 to 3,767; 3) lengthy referring expressions
averaging 24.2 words; and 4) an extensive vocabulary comprising 22,813 unique
words. We evaluate a total of 24 large models on Ref-L4 and provide valuable
insights. The cleaned versions of RefCOCO, RefCOCO+, and RefCOCOg, as well as
our Ref-L4 benchmark and evaluation code, are available at
https://github.com/JierunChen/Ref-L4.
]]></content:encoded>
<pubDate>2024-06-24T17:59:58Z</pubDate>
</item>
<item>
<title>FreeTraj: Tuning-Free Trajectory Control in Video Diffusion Models</title>
<link>http://arxiv.org/abs/2406.16863v1</link>
<guid>http://arxiv.org/abs/2406.16863v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 扩散模型, 视频生成, 轨迹控制, 噪音构造, 注意力计算

总结:<br /><br />
这篇文章介绍了一个新的方法，不需经过训练就能实现视频生成过程的轨迹控制。首先分析了初始噪音对生成内容运动轨迹的影响，并提出了一个名为 FreeTraj 的方法，通过修改噪音采样和注意力机制来实现轨迹控制。接着，扩展了 FreeTraj 方法，实现了更长、更大的视频生成，并可以手动或自动生成轨迹。实验结果表明，该方法在增强视频扩散模型轨迹可控性方面取得了很好的效果。 <div>
Diffusion model has demonstrated remarkable capability in video generation,
which further sparks interest in introducing trajectory control into the
generation process. While existing works mainly focus on training-based methods
(e.g., conditional adapter), we argue that diffusion model itself allows decent
control over the generated content without requiring any training. In this
study, we introduce a tuning-free framework to achieve trajectory-controllable
video generation, by imposing guidance on both noise construction and attention
computation. Specifically, 1) we first show several instructive phenomenons and
analyze how initial noises influence the motion trajectory of generated
content. 2) Subsequently, we propose FreeTraj, a tuning-free approach that
enables trajectory control by modifying noise sampling and attention
mechanisms. 3) Furthermore, we extend FreeTraj to facilitate longer and larger
video generation with controllable trajectories. Equipped with these designs,
users have the flexibility to provide trajectories manually or opt for
trajectories automatically generated by the LLM trajectory planner. Extensive
experiments validate the efficacy of our approach in enhancing the trajectory
controllability of video diffusion models.
]]></content:encoded>
<pubDate>2024-06-24T17:59:56Z</pubDate>
</item>
<item>
<title>Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs</title>
<link>http://arxiv.org/abs/2406.16860v1</link>
<guid>http://arxiv.org/abs/2406.16860v1</guid>
<content:encoded><![CDATA[
We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a
vision-centric approach. While stronger language models can enhance multimodal
capabilities, the design choices for vision components are often insufficiently
explored and disconnected from visual representation learning research. This
gap hinders accurate sensory grounding in real-world scenarios. Our study uses
LLMs and visual instruction tuning as an interface to evaluate various visual
representations, offering new insights into different models and architectures
-- self-supervised, strongly supervised, or combinations thereof -- based on
experiments with over 20 vision encoders. We critically examine existing MLLM
benchmarks, addressing the difficulties involved in consolidating and
interpreting results from various tasks, and introduce a new vision-centric
benchmark, CV-Bench. To further improve visual grounding, we propose the
Spatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that
integrates high-resolution vision features with LLMs while reducing the number
of tokens. Additionally, we discuss the curation of high-quality visual
instruction-tuning data from publicly available sources, emphasizing the
importance of data source balancing and distribution ratio. Collectively,
Cambrian-1 not only achieves state-of-the-art performance but also serves as a
comprehensive, open cookbook for instruction-tuned MLLMs. We provide model
weights, code, supporting tools, datasets, and detailed instruction-tuning and
evaluation recipes. We hope our release will inspire and accelerate
advancements in multimodal systems and visual representation learning.
]]></content:encoded>
<pubDate>2024-06-24T17:59:42Z</pubDate>
</item>
<item>
<title>DreamBench++: A Human-Aligned Benchmark for Personalized Image
  Generation</title>
<link>http://arxiv.org/abs/2406.16855v1</link>
<guid>http://arxiv.org/abs/2406.16855v1</guid>
<content:encoded><![CDATA[
Personalized image generation holds great promise in assisting humans in
everyday work and life due to its impressive function in creatively generating
personalized content. However, current evaluations either are automated but
misalign with humans or require human evaluations that are time-consuming and
expensive. In this work, we present DreamBench++, a human-aligned benchmark
automated by advanced multimodal GPT models. Specifically, we systematically
design the prompts to let GPT be both human-aligned and self-aligned, empowered
with task reinforcement. Further, we construct a comprehensive dataset
comprising diverse images and prompts. By benchmarking 7 modern generative
models, we demonstrate that DreamBench++ results in significantly more
human-aligned evaluation, helping boost the community with innovative findings.
]]></content:encoded>
<pubDate>2024-06-24T17:58:47Z</pubDate>
</item>
<item>
<title>Long Context Transfer from Language to Vision</title>
<link>http://arxiv.org/abs/2406.16852v1</link>
<guid>http://arxiv.org/abs/2406.16852v1</guid>
<content:encoded><![CDATA[
Video sequences offer valuable temporal information, but existing large
multimodal models (LMMs) fall short in understanding extremely long videos.
Many works address this by reducing the number of visual tokens using visual
resamplers. Alternatively, in this paper, we approach this problem from the
perspective of the language model. By simply extrapolating the context length
of the language backbone, we enable LMMs to comprehend orders of magnitude more
visual tokens without any video training. We call this phenomenon long context
transfer and carefully ablate its properties. To effectively measure LMMs'
ability to generalize to long contexts in the vision modality, we develop
V-NIAH (Visual Needle-In-A-Haystack), a purely synthetic long vision benchmark
inspired by the language model's NIAH test. Our proposed Long Video Assistant
(LongVA) can process 2000 frames or over 200K visual tokens without additional
complexities. With its extended context length, LongVA achieves
state-of-the-art performance on Video-MME among 7B-scale models by densely
sampling more input frames. Our work is open-sourced at
https://github.com/EvolvingLMMs-Lab/LongVA.
]]></content:encoded>
<pubDate>2024-06-24T17:58:06Z</pubDate>
</item>
<item>
<title>GenoTEX: A Benchmark for Evaluating LLM-Based Exploration of Gene
  Expression Data in Alignment with Bioinformaticians</title>
<link>http://arxiv.org/abs/2406.15341v1</link>
<guid>http://arxiv.org/abs/2406.15341v1</guid>
<content:encoded><![CDATA[
<div> 基因表达数据，机器学习，大型语言模型，基准数据集，基因组数据分析
<br /><br />
研究介绍了一个名为GenoTEX的基准数据集，用于自动探索基因表达数据，包括数据集选择、预处理和统计分析等任务。GenoTEX由人类生物信息学家精心筛选和注释，提供了解决各种基因识别问题的代码和结果。同时，研究还提出了基于大型语言模型的GenoAgents团队，他们使用上下文感知规划、迭代校正和领域专家协商的方式共同探索基因数据集。实验表明了基于大型语言模型的方法在基因组数据分析中的潜力，同时也指出了存在的挑战和改进的方向。研究者们将GenoTEX公开发布在GitHub上，作为基于人工智能的基因组数据分析方法的基准和改进的新资源。 <div>
Recent advancements in machine learning have significantly improved the
identification of disease-associated genes from gene expression datasets.
However, these processes often require extensive expertise and manual effort,
limiting their scalability. Large Language Model (LLM)-based agents have shown
promise in automating these tasks due to their increasing problem-solving
abilities. To support the evaluation and development of such methods, we
introduce GenoTEX, a benchmark dataset for the automatic exploration of gene
expression data, involving the tasks of dataset selection, preprocessing, and
statistical analysis. GenoTEX provides annotated code and results for solving a
wide range of gene identification problems, in a full analysis pipeline that
follows the standard of computational genomics. These annotations are curated
by human bioinformaticians who carefully analyze the datasets to ensure
accuracy and reliability. To provide baselines for these tasks, we present
GenoAgents, a team of LLM-based agents designed with context-aware planning,
iterative correction, and domain expert consultation to collaboratively explore
gene datasets. Our experiments with GenoAgents demonstrate the potential of
LLM-based approaches in genomics data analysis, while error analysis highlights
the challenges and areas for future improvement. We propose GenoTEX as a
promising resource for benchmarking and enhancing AI-driven methods for
genomics data analysis. We make our benchmark publicly available at
\url{https://github.com/Liu-Hy/GenoTex}.
]]></content:encoded>
<pubDate>2024-06-21T17:55:24Z</pubDate>
</item>
<item>
<title>Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning</title>
<link>http://arxiv.org/abs/2406.15334v1</link>
<guid>http://arxiv.org/abs/2406.15334v1</guid>
<content:encoded><![CDATA[
<div> 多模态学习, 多示例学习, 在上下文学习, 注意力头, 多模态任务向量

本文研究了多模态学习中的多示例学习问题，指出了现有模型在预训练阶段设定的上下文长度限制了多示例学习的效果。为了解决这一问题，研究者们引入了多模态任务向量（MTV），并利用这些提取出的MTV来实现多示例学习。实验证明，MTV可以随着压缩示例的数量而提高性能，并且在推理阶段可以推广到类似的跨领域任务。通过这种方法，LMMs可以实现多模态、多示例的上下文学习，为视觉与语言任务提供了新的可能性。<br /><br />总结: 多模态学习中存在多示例学习问题，研究者引入了多模态任务向量（MTV）来解决这一问题。实验证明MTV可以提高性能并推广到类似的跨领域任务。 <div>
The recent success of interleaved Large Multimodal Models (LMMs) in few-shot
learning suggests that in-context learning (ICL) with many examples can be
promising for learning new tasks. However, this many-shot multimodal ICL
setting has one crucial problem: it is fundamentally limited by the model's
context length set at pretraining. The problem is especially prominent in the
multimodal domain, which processes both text and images, requiring additional
tokens. This motivates the need for a multimodal method to compress many shots
into fewer tokens without finetuning. In this work, we enable LMMs to perform
multimodal, many-shot in-context learning by leveraging Multimodal Task Vectors
(MTV)--compact implicit representations of in-context examples compressed in
the model's attention heads. Specifically, we first demonstrate the existence
of such MTV in LMMs and then leverage these extracted MTV to enable many-shot
in-context learning for various vision-and-language tasks. Our experiments
suggest that MTV can scale in performance with the number of compressed shots
and generalize to similar out-of-domain tasks without additional context length
for inference.
]]></content:encoded>
<pubDate>2024-06-21T17:50:02Z</pubDate>
</item>
<item>
<title>GeoLRM: Geometry-Aware Large Reconstruction Model for High-Quality 3D
  Gaussian Generation</title>
<link>http://arxiv.org/abs/2406.15333v1</link>
<guid>http://arxiv.org/abs/2406.15333v1</guid>
<content:encoded><![CDATA[
In this work, we introduce the Geometry-Aware Large Reconstruction Model
(GeoLRM), an approach which can predict high-quality assets with 512k Gaussians
and 21 input images in only 11 GB GPU memory. Previous works neglect the
inherent sparsity of 3D structure and do not utilize explicit geometric
relationships between 3D and 2D images. This limits these methods to a
low-resolution representation and makes it difficult to scale up to the dense
views for better quality. GeoLRM tackles these issues by incorporating a novel
3D-aware transformer structure that directly processes 3D points and uses
deformable cross-attention mechanisms to effectively integrate image features
into 3D representations. We implement this solution through a two-stage
pipeline: initially, a lightweight proposal network generates a sparse set of
3D anchor points from the posed image inputs; subsequently, a specialized
reconstruction transformer refines the geometry and retrieves textural details.
Extensive experimental results demonstrate that GeoLRM significantly
outperforms existing models, especially for dense view inputs. We also
demonstrate the practical applicability of our model with 3D generation tasks,
showcasing its versatility and potential for broader adoption in real-world
applications.
]]></content:encoded>
<pubDate>2024-06-21T17:49:31Z</pubDate>
</item>
<item>
<title>Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities</title>
<link>http://arxiv.org/abs/2406.14562v1</link>
<guid>http://arxiv.org/abs/2406.14562v1</guid>
<content:encoded><![CDATA[
<div> 多模态大型语言模型、视觉推理、白板思维提示、结果、分析<br />
总结:<br />
这篇论文介绍了一种简单的方法，即白板思维提示，用于在多模态大型语言模型中解锁视觉推理能力。通过为模型提供一个“白板”，让其将推理步骤以图像的形式展示，然后将这些图像返回给模型进行进一步处理。该方法在四项涉及视觉和空间推理的自然语言任务中取得了最先进的结果。论文还探讨了该技术成功的领域以及错误的来源。白板思维提示能够显著提高模型的准确性，而传统的“链式思维”方法在某些情况下表现不佳。  <div>
When presented with questions involving visual thinking, humans naturally
switch reasoning modalities, often forming mental images or drawing visual
aids. Large language models have shown promising results in arithmetic and
symbolic reasoning by expressing intermediate reasoning in text as a chain of
thought, yet struggle to extend this capability to answer text queries that are
easily solved by visual reasoning, even with extensive multimodal pretraining.
We introduce a simple method, whiteboard-of-thought prompting, to unlock the
visual reasoning capabilities of multimodal large language models across
modalities. Whiteboard-of-thought prompting provides multimodal large language
models with a metaphorical `whiteboard' to draw out reasoning steps as images,
then returns these images back to the model for further processing. We find
this can be accomplished with no demonstrations or specialized modules, instead
leveraging models' existing ability to write code with libraries such as
Matplotlib and Turtle. This simple approach shows state-of-the-art results on
four difficult natural language tasks that involve visual and spatial
reasoning. We identify multiple settings where GPT-4o using chain-of-thought
fails dramatically, including more than one where it achieves $0\%$ accuracy,
while whiteboard-of-thought enables up to $92\%$ accuracy in these same
settings. We present a detailed exploration of where the technique succeeds as
well as its sources of error.
]]></content:encoded>
<pubDate>2024-06-20T17:59:45Z</pubDate>
</item>
<item>
<title>CooHOI: Learning Cooperative Human-Object Interaction with Manipulated
  Object Dynamics</title>
<link>http://arxiv.org/abs/2406.14558v1</link>
<guid>http://arxiv.org/abs/2406.14558v1</guid>
<content:encoded><![CDATA[
<div> 多人合作，物体运输，学习范式，CooHOI框架，AMP框架

多年来，由于大规模动作捕捉数据的可用性和强化学习方法的应用，人形控制取得了显著进展。然而，许多现实世界的任务，如移动大型和沉重的家具，需要多角色协作。由于多角色协作数据的稀缺性和多代理学习所带来的效率挑战，这些任务不能简单地使用为单一代理场景设计的训练范式来解决。在本文中，我们介绍了合作人物物体交互（CooHOI）框架，这是一个新颖的框架，通过两阶段学习范式来解决多人物体运输问题：个体技能获取和随后的转移。最初，一个单一代理使用对抗性运动先验（AMP）框架学习执行任务。随后，代理通过考虑并行训练中操纵对象的共享动力学来学习与其他人合作，使用多代理近端策略优化（MAPPO）方法。当一个代理与对象互动，导致特定物体动力学变化时，其他代理学习做出适当的响应，从而实现了队友之间的隐式沟通和协调。与先前依赖基于跟踪的方法进行多字符HOI的方法不同，CooHOI本质上是高效的，不依赖于多角色交互的动作捕捉数据，并且可以无缝地扩展到包括更多参与者和广泛的对象类型。<br /><br />总结: 该研究介绍了CoоHOI框架，通过AMP框架进行个体技能获取，随后使用MAPPO方法进行多角色协作学习。该框架不依赖于动作捕捉数据，能够实现隐式沟通和协调。 <div>
Recent years have seen significant advancements in humanoid control, largely
due to the availability of large-scale motion capture data and the application
of reinforcement learning methodologies. However, many real-world tasks, such
as moving large and heavy furniture, require multi-character collaboration.
Given the scarcity of data on multi-character collaboration and the efficiency
challenges associated with multi-agent learning, these tasks cannot be
straightforwardly addressed using training paradigms designed for single-agent
scenarios. In this paper, we introduce Cooperative Human-Object Interaction
(CooHOI), a novel framework that addresses multi-character objects transporting
through a two-phase learning paradigm: individual skill acquisition and
subsequent transfer. Initially, a single agent learns to perform tasks using
the Adversarial Motion Priors (AMP) framework. Following this, the agent learns
to collaborate with others by considering the shared dynamics of the
manipulated object during parallel training using Multi Agent Proximal Policy
Optimization (MAPPO). When one agent interacts with the object, resulting in
specific object dynamics changes, the other agents learn to respond
appropriately, thereby achieving implicit communication and coordination
between teammates. Unlike previous approaches that relied on tracking-based
methods for multi-character HOI, CooHOI is inherently efficient, does not
depend on motion capture data of multi-character interactions, and can be
seamlessly extended to include more participants and a wide range of object
types
]]></content:encoded>
<pubDate>2024-06-20T17:59:22Z</pubDate>
</item>
<item>
<title>A Survey of Multimodal-Guided Image Editing with Text-to-Image Diffusion
  Models</title>
<link>http://arxiv.org/abs/2406.14555v1</link>
<guid>http://arxiv.org/abs/2406.14555v1</guid>
<content:encoded><![CDATA[
Image editing aims to edit the given synthetic or real image to meet the
specific requirements from users. It is widely studied in recent years as a
promising and challenging field of Artificial Intelligence Generative Content
(AIGC). Recent significant advancement in this field is based on the
development of text-to-image (T2I) diffusion models, which generate images
according to text prompts. These models demonstrate remarkable generative
capabilities and have become widely used tools for image editing. T2I-based
image editing methods significantly enhance editing performance and offer a
user-friendly interface for modifying content guided by multimodal inputs. In
this survey, we provide a comprehensive review of multimodal-guided image
editing techniques that leverage T2I diffusion models. First, we define the
scope of image editing from a holistic perspective and detail various control
signals and editing scenarios. We then propose a unified framework to formalize
the editing process, categorizing it into two primary algorithm families. This
framework offers a design space for users to achieve specific goals.
Subsequently, we present an in-depth analysis of each component within this
framework, examining the characteristics and applicable scenarios of different
combinations. Given that training-based methods learn to directly map the
source image to target one under user guidance, we discuss them separately, and
introduce injection schemes of source image in different scenarios.
Additionally, we review the application of 2D techniques to video editing,
highlighting solutions for inter-frame inconsistency. Finally, we discuss open
challenges in the field and suggest potential future research directions. We
keep tracing related works at
https://github.com/xinchengshuai/Awesome-Image-Editing.
]]></content:encoded>
<pubDate>2024-06-20T17:58:52Z</pubDate>
</item>
<item>
<title>DrVideo: Document Retrieval Based Long Video Understanding</title>
<link>http://arxiv.org/abs/2406.12846v1</link>
<guid>http://arxiv.org/abs/2406.12846v1</guid>
<content:encoded><![CDATA[
<div> 长视频理解，document-retrieval-based system, large language models, agent-based iterative loop, 实验证实方法有效。<br /><br />长视频理解在现有方法中存在挑战，难以定位关键信息和进行长距离推理。为了解决这一问题，研究人员提出了DrVideo，这是一个基于文档检索的系统，旨在利用大型语言模型的力量。具体来说，DrVideo将长视频转换为基于文本的长文档，以便最初检索关键帧并增加这些帧的信息。随后，它采用基于代理的迭代循环不断搜索缺失信息，增加相关数据，并一旦收集到足够的与问题相关的信息，就以链式思维的方式提供最终预测。对长视频基准进行的实验证实了该方法的有效性。DrVideo在EgoSchema基准（3分钟）上的准确率高出3.8，MovieChat-1K break模式上高出17.9，在MovieChat-1K全局模式（10分钟）上高出38.0，在LLama-Vid QA数据集（超过60分钟）上高出30.2。 <br /><br />总结: 该研究提出了DrVideo系统，利用大型语言模型将长视频转换为文本以进行理解，证实其在长视频理解任务中的有效性。 <div>
Existing methods for long video understanding primarily focus on videos only
lasting tens of seconds, with limited exploration of techniques for handling
longer videos. The increased number of frames in longer videos presents two
main challenges: difficulty in locating key information and performing
long-range reasoning. Thus, we propose DrVideo, a document-retrieval-based
system designed for long video understanding. Our key idea is to convert the
long-video understanding problem into a long-document understanding task so as
to effectively leverage the power of large language models. Specifically,
DrVideo transforms a long video into a text-based long document to initially
retrieve key frames and augment the information of these frames, which is used
this as the system's starting point. It then employs an agent-based iterative
loop to continuously search for missing information, augment relevant data, and
provide final predictions in a chain-of-thought manner once sufficient
question-related information is gathered. Extensive experiments on long video
benchmarks confirm the effectiveness of our method. DrVideo outperforms
existing state-of-the-art methods with +3.8 accuracy on EgoSchema benchmark (3
minutes), +17.9 in MovieChat-1K break mode, +38.0 in MovieChat-1K global mode
(10 minutes), and +30.2 on the LLama-Vid QA dataset (over 60 minutes).
]]></content:encoded>
<pubDate>2024-06-18T17:59:03Z</pubDate>
</item>
<item>
<title>Synergizing Foundation Models and Federated Learning: A Survey</title>
<link>http://arxiv.org/abs/2406.12844v1</link>
<guid>http://arxiv.org/abs/2406.12844v1</guid>
<content:encoded><![CDATA[
<div> 关键词: Foundation Models, Federated Learning, 数据隐私, 协作学习, 应用领域
<br />
总结:<br />
本文主要讨论了基金会模型（FMs）和联邦学习（FL）的潜力和挑战。FMs在预训练阶段需要大量的高质量数据，而FL是一种协作学习范式，可以突破数据可用性的障碍。通过FL，FMs可以定制和适应各种领域特定任务，同时保护数据隐私。文章还总结了核心技术、未来方向和应用，并提供了一个关于FM-FL的定期更新的文集。 <div>
The recent development of Foundation Models (FMs), represented by large
language models, vision transformers, and multimodal models, has been making a
significant impact on both academia and industry. Compared with small-scale
models, FMs have a much stronger demand for high-volume data during the
pre-training phase. Although general FMs can be pre-trained on data collected
from open sources such as the Internet, domain-specific FMs need proprietary
data, posing a practical challenge regarding the amount of data available due
to privacy concerns. Federated Learning (FL) is a collaborative learning
paradigm that breaks the barrier of data availability from different
participants. Therefore, it provides a promising solution to customize and
adapt FMs to a wide range of domain-specific tasks using distributed datasets
whilst preserving privacy. This survey paper discusses the potentials and
challenges of synergizing FL and FMs and summarizes core techniques, future
directions, and applications. A periodically updated paper collection on FM-FL
is available at https://github.com/lishenghui/awesome-fm-fl.
]]></content:encoded>
<pubDate>2024-06-18T17:58:09Z</pubDate>
</item>
<item>
<title>Can Go AIs be adversarially robust?</title>
<link>http://arxiv.org/abs/2406.12843v1</link>
<guid>http://arxiv.org/abs/2406.12843v1</guid>
<content:encoded><![CDATA[
Prior work found that superhuman Go AIs like KataGo can be defeated by simple
adversarial strategies. In this paper, we study if simple defenses can improve
KataGo's worst-case performance. We test three natural defenses: adversarial
training on hand-constructed positions, iterated adversarial training, and
changing the network architecture. We find that some of these defenses are able
to protect against previously discovered attacks. Unfortunately, we also find
that none of these defenses are able to withstand adaptive attacks. In
particular, we are able to train new adversaries that reliably defeat our
defended agents by causing them to blunder in ways humans would not. Our
results suggest that building robust AI systems is challenging even in narrow
domains such as Go. For interactive examples of attacks and a link to our
codebase, see https://goattack.far.ai.
]]></content:encoded>
<pubDate>2024-06-18T17:57:49Z</pubDate>
</item>
<item>
<title>Autoregressive Image Generation without Vector Quantization</title>
<link>http://arxiv.org/abs/2406.11838v1</link>
<guid>http://arxiv.org/abs/2406.11838v1</guid>
<content:encoded><![CDATA[
<div> Diffusion, Autoregressive modeling, Continuous-valued space, Diffusion Loss function, Image generation
总结: 本文提出了一种在连续数值空间中使用扩散过程来建模每个标记概率分布的方法，从而消除了离散值标记器的需求。这种方法在标准自回归模型和广义掩模自回归（MAR）变体中取得了良好的效果，并且在序列建模方面具有速度优势。作者希望这项工作能够激发在其他连续数值领域和应用中使用自回归生成的动力。 <div>
Conventional wisdom holds that autoregressive models for image generation are
typically accompanied by vector-quantized tokens. We observe that while a
discrete-valued space can facilitate representing a categorical distribution,
it is not a necessity for autoregressive modeling. In this work, we propose to
model the per-token probability distribution using a diffusion procedure, which
allows us to apply autoregressive models in a continuous-valued space. Rather
than using categorical cross-entropy loss, we define a Diffusion Loss function
to model the per-token probability. This approach eliminates the need for
discrete-valued tokenizers. We evaluate its effectiveness across a wide range
of cases, including standard autoregressive models and generalized masked
autoregressive (MAR) variants. By removing vector quantization, our image
generator achieves strong results while enjoying the speed advantage of
sequence modeling. We hope this work will motivate the use of autoregressive
generation in other continuous-valued domains and applications.
]]></content:encoded>
<pubDate>2024-06-17T17:59:58Z</pubDate>
</item>
<item>
<title>mDPO: Conditional Preference Optimization for Multimodal Large Language
  Models</title>
<link>http://arxiv.org/abs/2406.11839v1</link>
<guid>http://arxiv.org/abs/2406.11839v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 多模态，偏好优化，大语言模型，mDPO，奖励锚定<br />
<br />
在多模态情景下，直接偏好优化（DPO）已被证明是对齐大语言模型（LLM）的一个有效方法。最近的研究尝试将DPO应用于多模态场景，但发现难以实现一致的改进。通过一项比较实验，我们确定了多模态偏好优化中的无条件偏好问题，即模型忽视了图像条件。为了解决这个问题，我们提出了mDPO，这是一个多模态DPO目标，通过优化图像偏好来防止语言偏好的过度优先。此外，我们引入了奖励锚定，强制奖励对于所选的响应保持为正值，从而避免相对偏好优化中降低其可能性的问题。在两个不同大小的多模态LLM和三个广泛使用的基准测试上的实验证明，mDPO有效解决了多模态偏好优化中的无条件偏好问题，并显著提升了模型性能，特别是减少了臆想现象。<br /><br />总结:多模态直接偏好优化（DPO）有效解决了无条件偏好问题，通过优化图像偏好防止语言偏好的过度优先。引入奖励锚定避免了相对偏好优化中降低其可能性的问题。实验表明mDPO显著提升了模型性能，特别是在减少臆想现象方面。 <div>
Direct preference optimization (DPO) has shown to be an effective method for
large language model (LLM) alignment. Recent works have attempted to apply DPO
to multimodal scenarios but have found it challenging to achieve consistent
improvement. Through a comparative experiment, we identify the unconditional
preference problem in multimodal preference optimization, where the model
overlooks the image condition. To address this problem, we propose mDPO, a
multimodal DPO objective that prevents the over-prioritization of language-only
preferences by also optimizing image preference. Moreover, we introduce a
reward anchor that forces the reward to be positive for chosen responses,
thereby avoiding the decrease in their likelihood -- an intrinsic problem of
relative preference optimization. Experiments on two multimodal LLMs of
different sizes and three widely used benchmarks demonstrate that mDPO
effectively addresses the unconditional preference problem in multimodal
preference optimization and significantly improves model performance,
particularly in reducing hallucination.
]]></content:encoded>
<pubDate>2024-06-17T17:59:58Z</pubDate>
</item>
<item>
<title>Scaling the Codebook Size of VQGAN to 100,000 with a Utilization Rate of
  99%</title>
<link>http://arxiv.org/abs/2406.11837v1</link>
<guid>http://arxiv.org/abs/2406.11837v1</guid>
<content:encoded><![CDATA[
In the realm of image quantization exemplified by VQGAN, the process encodes
images into discrete tokens drawn from a codebook with a predefined size.
Recent advancements, particularly with LLAMA 3, reveal that enlarging the
codebook significantly enhances model performance. However, VQGAN and its
derivatives, such as VQGAN-FC (Factorized Codes) and VQGAN-EMA, continue to
grapple with challenges related to expanding the codebook size and enhancing
codebook utilization. For instance, VQGAN-FC is restricted to learning a
codebook with a maximum size of 16,384, maintaining a typically low utilization
rate of less than 12% on ImageNet. In this work, we propose a novel image
quantization model named VQGAN-LC (Large Codebook), which extends the codebook
size to 100,000, achieving an utilization rate exceeding 99%. Unlike previous
methods that optimize each codebook entry, our approach begins with a codebook
initialized with 100,000 features extracted by a pre-trained vision encoder.
Optimization then focuses on training a projector that aligns the entire
codebook with the feature distributions of the encoder in VQGAN-LC. We
demonstrate the superior performance of our model over its counterparts across
a variety of tasks, including image reconstruction, image classification,
auto-regressive image generation using GPT, and image creation with diffusion-
and flow-based generative models. Code and models are available at
https://github.com/zh460045050/VQGAN-LC.
]]></content:encoded>
<pubDate>2024-06-17T17:59:57Z</pubDate>
</item>
<item>
<title>Exploring the Role of Large Language Models in Prompt Encoding for
  Diffusion Models</title>
<link>http://arxiv.org/abs/2406.11831v1</link>
<guid>http://arxiv.org/abs/2406.11831v1</guid>
<content:encoded><![CDATA[
Large language models (LLMs) based on decoder-only transformers have
demonstrated superior text understanding capabilities compared to CLIP and
T5-series models. However, the paradigm for utilizing current advanced LLMs in
text-to-image diffusion models remains to be explored. We observed an unusual
phenomenon: directly using a large language model as the prompt encoder
significantly degrades the prompt-following ability in image generation. We
identified two main obstacles behind this issue. One is the misalignment
between the next token prediction training in LLM and the requirement for
discriminative prompt features in diffusion models. The other is the intrinsic
positional bias introduced by the decoder-only architecture. To deal with this
issue, we propose a novel framework to fully harness the capabilities of LLMs.
Through the carefully designed usage guidance, we effectively enhance the text
representation capability for prompt encoding and eliminate its inherent
positional bias. This allows us to integrate state-of-the-art LLMs into the
text-to-image generation model flexibly. Furthermore, we also provide an
effective manner to fuse multiple LLMs into our framework. Considering the
excellent performance and scaling capabilities demonstrated by the transformer
architecture, we further design an LLM-Infused Diffusion Transformer (LI-DiT)
based on the framework. We conduct extensive experiments to validate LI-DiT
across model size and data size. Benefiting from the inherent ability of the
LLMs and our innovative designs, the prompt understanding performance of LI-DiT
easily surpasses state-of-the-art open-source models as well as mainstream
closed-source commercial models including Stable Diffusion 3, DALL-E 3, and
Midjourney V6. The powerful LI-DiT-10B will be available after further
optimization and security checks.
]]></content:encoded>
<pubDate>2024-06-17T17:59:43Z</pubDate>
</item>
<item>
<title>VideoGUI: A Benchmark for GUI Automation from Instructional Videos</title>
<link>http://arxiv.org/abs/2406.10227v1</link>
<guid>http://arxiv.org/abs/2406.10227v1</guid>
<content:encoded><![CDATA[
<div> GUI automation, VideoGUI, multi-modal benchmark, visual-centric tasks, evaluation metrics
<br /><br />本研究引入了VideoGUI，这是一个新颖的多模态基准，旨在评估GUI助手在以视觉为中心的GUI任务上的表现。该基准源自高质量的网络教学视频，重点关注涉及专业和新颖软件（例如Adobe Photoshop或Stable Diffusion WebUI）以及复杂活动（例如视频编辑）的任务。VideoGUI通过分层过程评估GUI助手，允许识别它们可能失败的具体级别：（i）高级规划：从视觉条件重构程序子任务，而无需语言描述；（ii）中级规划：根据视觉状态（即屏幕截图）和目标生成精确操作序列；（iii）原子操作执行：执行诸如准确点击指定元素之类的特定操作。对于每个级别，我们设计了跨个体维度的评估指标，以提供清晰的信号，例如在原子操作执行中点击、拖动、输入和滚动的个体表现。我们在VideoGUI上的评估显示，即使是当前最先进的大型多模态模型GPT4o在视觉为中心的GUI任务上表现不佳，特别是在高级规划方面。
<br /><br />总结: 本研究介绍了VideoGUI基准，其通过多层次过程评估GUI助手的性能，特别关注视觉为中心的GUI任务。该基准涵盖了复杂软件和活动，为评估提供了详细的指标，揭示了当前先进模型在面对视觉为中心的GUI任务时的不足之处。 <div>
Graphical User Interface (GUI) automation holds significant promise for
enhancing human productivity by assisting with computer tasks. Existing task
formulations primarily focus on simple tasks that can be specified by a single,
language-only instruction, such as "Insert a new slide." In this work, we
introduce VideoGUI, a novel multi-modal benchmark designed to evaluate GUI
assistants on visual-centric GUI tasks. Sourced from high-quality web
instructional videos, our benchmark focuses on tasks involving professional and
novel software (e.g., Adobe Photoshop or Stable Diffusion WebUI) and complex
activities (e.g., video editing). VideoGUI evaluates GUI assistants through a
hierarchical process, allowing for identification of the specific levels at
which they may fail: (i) high-level planning: reconstruct procedural subtasks
from visual conditions without language descriptions; (ii) middle-level
planning: generate sequences of precise action narrations based on visual state
(i.e., screenshot) and goals; (iii) atomic action execution: perform specific
actions such as accurately clicking designated elements. For each level, we
design evaluation metrics across individual dimensions to provide clear
signals, such as individual performance in clicking, dragging, typing, and
scrolling for atomic action execution. Our evaluation on VideoGUI reveals that
even the SoTA large multimodal model GPT4o performs poorly on visual-centric
GUI tasks, especially for high-level planning.
]]></content:encoded>
<pubDate>2024-06-14T17:59:08Z</pubDate>
</item>
<item>
<title>Alleviating Distortion in Image Generation via Multi-Resolution
  Diffusion Models</title>
<link>http://arxiv.org/abs/2406.09416v1</link>
<guid>http://arxiv.org/abs/2406.09416v1</guid>
<content:encoded><![CDATA[
<div> 高清图像生成, 扩散模型, 多分辨率网络, 时间相关层归一化, Transformer架构<br />
<br />
扩散模型在高保真图像生成方面非常有效。传统的方法使用卷积U-Net架构，而最近基于Transformer的设计表现出卓越的性能和可扩展性。然而，Transformer架构通过“分块化”输入数据（通过“分块化”）所面临的问题是在于输入标记长度对自注意力操作的二次复杂性方面存在着视觉保真度和计算复杂度之间的权衡。为了解决这一挑战，作者提出了使用DiMR多分辨率网络和TD-LN时间相关层归一化增强扩散模型。他们的方法在类相关的ImageNet生成基准测试中取得了良好的效果，超越了先前的扩散模型，并在ImageNet 256x256和512x512上取得了新的FID分数。 <br /><br />总结: <br />本文提出了一种结合了DiMR多分辨率网络和TD-LN时间相关层归一化的增强扩散模型，以应对Transformer架构在保真度和计算复杂度之间的权衡问题。作者的方法在ImageNet生成基准测试中取得了良好的效果，并取得了新的FID分数。 <div>
This paper presents innovative enhancements to diffusion models by
integrating a novel multi-resolution network and time-dependent layer
normalization. Diffusion models have gained prominence for their effectiveness
in high-fidelity image generation. While conventional approaches rely on
convolutional U-Net architectures, recent Transformer-based designs have
demonstrated superior performance and scalability. However, Transformer
architectures, which tokenize input data (via "patchification"), face a
trade-off between visual fidelity and computational complexity due to the
quadratic nature of self-attention operations concerning token length. While
larger patch sizes enable attention computation efficiency, they struggle to
capture fine-grained visual details, leading to image distortions. To address
this challenge, we propose augmenting the Diffusion model with the
Multi-Resolution network (DiMR), a framework that refines features across
multiple resolutions, progressively enhancing detail from low to high
resolution. Additionally, we introduce Time-Dependent Layer Normalization
(TD-LN), a parameter-efficient approach that incorporates time-dependent
parameters into layer normalization to inject time information and achieve
superior performance. Our method's efficacy is demonstrated on the
class-conditional ImageNet generation benchmark, where DiMR-XL variants
outperform prior diffusion models, setting new state-of-the-art FID scores of
1.70 on ImageNet 256 x 256 and 2.89 on ImageNet 512 x 512. Project page:
https://qihao067.github.io/projects/DiMR
]]></content:encoded>
<pubDate>2024-06-13T17:59:58Z</pubDate>
</item>
<item>
<title>Explore the Limits of Omni-modal Pretraining at Scale</title>
<link>http://arxiv.org/abs/2406.09412v1</link>
<guid>http://arxiv.org/abs/2406.09412v1</guid>
<content:encoded><![CDATA[
<div> 多模态智能、预训练、MiCo、性能记录、GitHub
<br /><br />
总结:
本研究提出了一种名为MiCo的可扩展预训练范式，旨在构建多模态智能。该方法能够在不同的模态和数据量下进行预训练，并在各种任务中展现出显著的能力，包括对10种不同模态的单模态感知基准任务、25种跨模态理解任务以及18种多模态大型语言模型基准任务。我们的模型在性能上建立了37项最新记录。希望本研究能够为全模态智能的发展做出贡献。可以在https://github.com/invictus717/MiCo找到代码和模型。 <div>
We propose to build omni-modal intelligence, which is capable of
understanding any modality and learning universal representations. In specific,
we propose a scalable pretraining paradigm, named Multimodal Context (MiCo),
which can scale up the numbers of modalities and amount of data, together with
the model parameters, in the pretraining process. With MiCo, the pretrained
models show significant emergent abilities in multimodal learning, which are
evaluated on the following tasks: i) single-modality perception benchmarks of
10 different modalities, ii) 25 cross-modality understanding tasks of
retrieval, question-answering, captioning, and iii) 18 multimodal large
language model benchmarks. Our models establish 37 new records for
state-of-the-art performance. We hope that our research could contribute to the
development of omni-modal intelligence. Code and Models are at
https://github.com/invictus717/MiCo
]]></content:encoded>
<pubDate>2024-06-13T17:59:53Z</pubDate>
</item>
<item>
<title>MuirBench: A Comprehensive Benchmark for Robust Multi-image
  Understanding</title>
<link>http://arxiv.org/abs/2406.09411v1</link>
<guid>http://arxiv.org/abs/2406.09411v1</guid>
<content:encoded><![CDATA[
We introduce MuirBench, a comprehensive benchmark that focuses on robust
multi-image understanding capabilities of multimodal LLMs. MuirBench consists
of 12 diverse multi-image tasks (e.g., scene understanding, ordering) that
involve 10 categories of multi-image relations (e.g., multiview, temporal
relations). Comprising 11,264 images and 2,600 multiple-choice questions,
MuirBench is created in a pairwise manner, where each standard instance is
paired with an unanswerable variant that has minimal semantic differences, in
order for a reliable assessment. Evaluated upon 20 recent multi-modal LLMs, our
results reveal that even the best-performing models like GPT-4o and Gemini Pro
find it challenging to solve MuirBench, achieving 68.0% and 49.3% in accuracy.
Open-source multimodal LLMs trained on single images can hardly generalize to
multi-image questions, hovering below 33.3% in accuracy. These results
highlight the importance of MuirBench in encouraging the community to develop
multimodal LLMs that can look beyond a single image, suggesting potential
pathways for future improvements.
]]></content:encoded>
<pubDate>2024-06-13T17:59:52Z</pubDate>
</item>
<item>
<title>Words Worth a Thousand Pictures: Measuring and Understanding Perceptual
  Variability in Text-to-Image Generation</title>
<link>http://arxiv.org/abs/2406.08482v1</link>
<guid>http://arxiv.org/abs/2406.08482v1</guid>
<content:encoded><![CDATA[
<div> 黑盒扩散模型、图像可变性、W1KP、提示重用、语言特征分析<br />
扩散模型是文本到图像生成的最新技术，但其感知可变性尚未得到充分研究。本文研究了提示如何影响基于黑盒扩散模型的图像可变性。我们提出了W1KP，这是一种经过人类校准的图像可变性度量，可以从现有的图像对感知距离中提取。由于当前数据集不包括最新的扩散模型，因此我们为评估而策划了三个测试集。我们的最佳感知距离在准确性方面比九个基线模型提高了高达18个百分点，我们的校准与人类的判断相匹配的概率达到了78%。利用W1KP，我们研究了提示的重复使用，并表明Imagen提示可以在生成的图像变得与已生成的图像太相似之前重复使用10-50次，而Stable Diffusion XL和DALL-E 3可以重复使用50-200次。最后，我们分析了真实提示的56个语言特征，发现提示的长度、CLIP嵌入范数、具体性和词义对可变性影响最大。据我们所知，我们是首批从视觉语言学角度分析扩散可变性的研究者。我们的项目页面位于http://w1kp.com。<br /><br />总结: 本文研究了扩散模型的图像可变性，提出了W1KP度量及其重要性，以及对提示重复使用和语言特征分析的结果进行了详细研究，为扩散模型的进一步发展提出了有益的见解。 <div>
Diffusion models are the state of the art in text-to-image generation, but
their perceptual variability remains understudied. In this paper, we examine
how prompts affect image variability in black-box diffusion-based models. We
propose W1KP, a human-calibrated measure of variability in a set of images,
bootstrapped from existing image-pair perceptual distances. Current datasets do
not cover recent diffusion models, thus we curate three test sets for
evaluation. Our best perceptual distance outperforms nine baselines by up to 18
points in accuracy, and our calibration matches graded human judgements 78% of
the time. Using W1KP, we study prompt reusability and show that Imagen prompts
can be reused for 10-50 random seeds before new images become too similar to
already generated images, while Stable Diffusion XL and DALL-E 3 can be reused
50-200 times. Lastly, we analyze 56 linguistic features of real prompts,
finding that the prompt's length, CLIP embedding norm, concreteness, and word
senses influence variability most. As far as we are aware, we are the first to
analyze diffusion variability from a visuolinguistic perspective. Our project
page is at http://w1kp.com
]]></content:encoded>
<pubDate>2024-06-12T17:59:27Z</pubDate>
</item>
<item>
<title>What If We Recaption Billions of Web Images with LLaMA-3?</title>
<link>http://arxiv.org/abs/2406.08478v1</link>
<guid>http://arxiv.org/abs/2406.08478v1</guid>
<content:encoded><![CDATA[
<div> LLaMA-3, 图像-文本对, 数据增强, 生成模型, 判别模型
<br /> 
本文介绍了如何利用开源的LLaMA-3进行图像-文本对的数据增强，并且在数据集Recap-DataComp-1B上进行了实验。实验证明，经过增强的数据集可以显著提升训练先进的视觉-语言模型，在判别模型（如CLIP）中表现出增强的零-shot性能，在生成模型（如文本到图像Diffusion Transformers）中生成的图像与用户的文本指令更加吻合，特别是在处理复杂查询时。 通过这篇文章，我们可以了解到数据增强对于提升视觉-语言模型的作用，以及LLaMA-3的重要性和应用价值。
<br /><br />总结: 
数据增强的重要性和作用、LLaMA-3的应用、Recap-DataComp-1B数据集的实验效果、判别模型和生成模型的表现提升、复杂查询的处理。 <div>
Web-crawled image-text pairs are inherently noisy. Prior studies demonstrate
that semantically aligning and enriching textual descriptions of these pairs
can significantly enhance model training across various vision-language tasks,
particularly text-to-image generation. However, large-scale investigations in
this area remain predominantly closed-source. Our paper aims to bridge this
community effort, leveraging the powerful and \textit{open-sourced} LLaMA-3, a
GPT-4 level LLM. Our recaptioning pipeline is simple: first, we fine-tune a
LLaMA-3-8B powered LLaVA-1.5 and then employ it to recaption 1.3 billion images
from the DataComp-1B dataset. Our empirical results confirm that this enhanced
dataset, Recap-DataComp-1B, offers substantial benefits in training advanced
vision-language models. For discriminative models like CLIP, we observe
enhanced zero-shot performance in cross-modal retrieval tasks. For generative
models like text-to-image Diffusion Transformers, the generated images exhibit
a significant improvement in alignment with users' text instructions,
especially in following complex queries. Our project page is
https://www.haqtu.me/Recap-Datacomp-1B/
]]></content:encoded>
<pubDate>2024-06-12T17:59:07Z</pubDate>
</item>
<item>
<title>Commonsense-T2I Challenge: Can Text-to-Image Generation Models
  Understand Commonsense?</title>
<link>http://arxiv.org/abs/2406.07546v1</link>
<guid>http://arxiv.org/abs/2406.07546v1</guid>
<content:encoded><![CDATA[
<div> 文本-图像生成；常识推理；评估基准；挑战性数据集；模型分析<br />
<br />
挑战性的Commonsense-T2I数据集是一个新颖的任务和基准，用于评估文本-图像生成模型产生符合现实生活常识的能力。数据集由专家手动筛选和标注，提供对模型行为的细粒度分析。研究发现，即使是DALL-E 3模型在Commonsense-T2I上也只能达到48.92%的准确率，而稳定的Diffusion XL模型仅实现24.92%的准确率。实验结果表明，GPT丰富的提示无法解决这一挑战，并进行了详细的分析。Commonsense-T2I旨在成为T2I常识检查的高质量评估基准，促进现实生活图像生成的发展。<br /><br />总结: 挑战性的Commonsense-T2I数据集是一个评估文本-图像生成模型的新基准，旨在促进现实生活图像生成的进步。研究发现目前的模型在该数据集上仍存在较大差距，即使是最先进的模型也只能实现有限的准确率。对于GPT-enriched提示也无法解决这一挑战，需要进行更深入的分析和改进。 <div>
We present a novel task and benchmark for evaluating the ability of
text-to-image(T2I) generation models to produce images that fit commonsense in
real life, which we call Commonsense-T2I. Given two adversarial text prompts
containing an identical set of action words with minor differences, such as "a
lightbulb without electricity" v.s. "a lightbulb with electricity", we evaluate
whether T2I models can conduct visual-commonsense reasoning, e.g. produce
images that fit "the lightbulb is unlit" vs. "the lightbulb is lit"
correspondingly. Commonsense-T2I presents an adversarial challenge, providing
pairwise text prompts along with expected outputs. The dataset is carefully
hand-curated by experts and annotated with fine-grained labels, such as
commonsense type and likelihood of the expected outputs, to assist analyzing
model behavior. We benchmark a variety of state-of-the-art (sota) T2I models
and surprisingly find that, there is still a large gap between image synthesis
and real life photos--even the DALL-E 3 model could only achieve 48.92% on
Commonsense-T2I, and the stable diffusion XL model only achieves 24.92%
accuracy. Our experiments show that GPT-enriched prompts cannot solve this
challenge, and we include a detailed analysis about possible reasons for such
deficiency. We aim for Commonsense-T2I to serve as a high-quality evaluation
benchmark for T2I commonsense checking, fostering advancements in real life
image generation.
]]></content:encoded>
<pubDate>2024-06-11T17:59:48Z</pubDate>
</item>
<item>
<title>Situational Awareness Matters in 3D Vision Language Reasoning</title>
<link>http://arxiv.org/abs/2406.07544v1</link>
<guid>http://arxiv.org/abs/2406.07544v1</guid>
<content:encoded><![CDATA[
<div> 3D vision language reasoning, household robots, embodied AI, SIG3D, situational awareness<br />
<br />
该研究展示了在3D空间内进行复杂的视觉语言推理任务对于开发家庭机器人和以人为中心的智能AI具有重要意义。作者提出了SIG3D模型，该模型能够自主地在基于语言提示的情境中确定自身位置，并以此为基础回答开放性问题。通过在SQA3D和ScanQA数据集上的实验，发现SIG3D在情境估计和问题回答方面优于现有模型，情境估计准确率提升超过30%。进一步分析验证了模型设计的合理性，探究了视觉和文本标记的不同功能，并突出了在3D问题回答领域中情境意识的重要性。<br /><br />总结: 该研究围绕着3D视觉语言推理展开，提出了SIG3D模型以解决情境感知和问题回答的挑战。实验证明SIG3D在这两方面表现优异，突显了情境意识在3D问题回答中的重要性。 <div>
Being able to carry out complicated vision language reasoning tasks in 3D
space represents a significant milestone in developing household robots and
human-centered embodied AI. In this work, we demonstrate that a critical and
distinct challenge in 3D vision language reasoning is situational awareness,
which incorporates two key components: (1) The autonomous agent grounds its
self-location based on a language prompt. (2) The agent answers open-ended
questions from the perspective of its calculated position. To address this
challenge, we introduce SIG3D, an end-to-end Situation-Grounded model for 3D
vision language reasoning. We tokenize the 3D scene into sparse voxel
representation and propose a language-grounded situation estimator, followed by
a situated question answering module. Experiments on the SQA3D and ScanQA
datasets show that SIG3D outperforms state-of-the-art models in situation
estimation and question answering by a large margin (e.g., an enhancement of
over 30% on situation estimation accuracy). Subsequent analysis corroborates
our architectural design choices, explores the distinct functions of visual and
textual tokens, and highlights the importance of situational awareness in the
domain of 3D question answering.
]]></content:encoded>
<pubDate>2024-06-11T17:59:45Z</pubDate>
</item>
<item>
<title>Cognitive Insights Across Languages: Enhancing Multimodal Interview
  Analysis</title>
<link>http://arxiv.org/abs/2406.07542v1</link>
<guid>http://arxiv.org/abs/2406.07542v1</guid>
<content:encoded><![CDATA[
Cognitive decline is a natural process that occurs as individuals age. Early
diagnosis of anomalous decline is crucial for initiating professional treatment
that can enhance the quality of life of those affected. To address this issue,
we propose a multimodal model capable of predicting Mild Cognitive Impairment
and cognitive scores. The TAUKADIAL dataset is used to conduct the evaluation,
which comprises audio recordings of clinical interviews. The proposed model
demonstrates the ability to transcribe and differentiate between languages used
in the interviews. Subsequently, the model extracts audio and text features,
combining them into a multimodal architecture to achieve robust and generalized
results. Our approach involves in-depth research to implement various features
obtained from the proposed modalities.
]]></content:encoded>
<pubDate>2024-06-11T17:59:31Z</pubDate>
</item>
<item>
<title>GaussianCity: Generative Gaussian Splatting for Unbounded 3D City
  Generation</title>
<link>http://arxiv.org/abs/2406.06526v1</link>
<guid>http://arxiv.org/abs/2406.06526v1</guid>
<content:encoded><![CDATA[
<div> GaussianCity, 3D city generation, NeRF, 3D-GS, unbounded 3D scenes

GaussianCity是一种用于高效合成无限规模3D城市的生成高斯分布点框架。文章提出了两个关键见解：1）紧凑的3D场景表示：引入BEV-Point作为高度紧凑的中间表示，确保对于无限场景的VRAM使用量保持恒定，从而实现无限城市的生成。2）空间感知的高斯属性解码器：提出了空间感知的BEV-Point解码器，以产生3D高斯属性，利用Point Serializer集成BEV点的结构和上下文特征。大量实验证明，GaussianCity在俯视和街景3D城市生成方面取得了最先进的结果。值得注意的是，与CityDreamer相比，GaussianCity表现出60倍的速度提升（10.72 FPS对0.18 FPS）。<br /><br />总结: GaussianCity 提出了一种用于高效生成无限规模3D城市的高斯分布点框架，具有紧凑的3D场景表示和空间感知的高斯属性解码器。在实验中表现出了优越的性能。 <div>
3D city generation with NeRF-based methods shows promising generation results
but is computationally inefficient. Recently 3D Gaussian Splatting (3D-GS) has
emerged as a highly efficient alternative for object-level 3D generation.
However, adapting 3D-GS from finite-scale 3D objects and humans to
infinite-scale 3D cities is non-trivial. Unbounded 3D city generation entails
significant storage overhead (out-of-memory issues), arising from the need to
expand points to billions, often demanding hundreds of Gigabytes of VRAM for a
city scene spanning 10km^2. In this paper, we propose GaussianCity, a
generative Gaussian Splatting framework dedicated to efficiently synthesizing
unbounded 3D cities with a single feed-forward pass. Our key insights are
two-fold: 1) Compact 3D Scene Representation: We introduce BEV-Point as a
highly compact intermediate representation, ensuring that the growth in VRAM
usage for unbounded scenes remains constant, thus enabling unbounded city
generation. 2) Spatial-aware Gaussian Attribute Decoder: We present
spatial-aware BEV-Point decoder to produce 3D Gaussian attributes, which
leverages Point Serializer to integrate the structural and contextual
characteristics of BEV points. Extensive experiments demonstrate that
GaussianCity achieves state-of-the-art results in both drone-view and
street-view 3D city generation. Notably, compared to CityDreamer, GaussianCity
exhibits superior performance with a speedup of 60 times (10.72 FPS v.s. 0.18
FPS).
]]></content:encoded>
<pubDate>2024-06-10T17:59:55Z</pubDate>
</item>
<item>
<title>Autoregressive Model Beats Diffusion: Llama for Scalable Image
  Generation</title>
<link>http://arxiv.org/abs/2406.06525v1</link>
<guid>http://arxiv.org/abs/2406.06525v1</guid>
<content:encoded><![CDATA[
<div> 关键词: LlamaGen, 图像生成模型, 自回归模型, 缩放, 训练数据<br />
总结: <br />本文介绍了LlamaGen，这是一种新的图像生成模型，将大型语言模型的原始“下一个标记预测”范式应用于视觉生成领域。作者发现，即使没有对视觉信号进行归纳偏差，例如Llama这样的纯自回归模型，只要适当地进行缩放，也可以实现最先进的图像生成性能。他们重新审视了图像标记器的设计空间、图像生成模型的可扩展性属性以及它们的训练数据质量。在这次探索中，他们取得了一系列成果：（1）一种图像标记器，其下采样比为16，重构质量为0.94 rFID，ImageNet基准上的代码本使用率为97%；（2）一系列有条件类别的图像生成模型，参数范围从1.11亿到31亿，在ImageNet 256x256基准测试上实现了2.18 FID，优于流行的扩散模型，如LDM、DiT；（3）一种文本有条件的图像生成模型，参数为7.75亿，经过LAION-COCO和高美学质量图像的两阶段训练，展现出了与文本对齐的竞争性视觉质量。他们验证了LLM服务框架在优化图像生成模型推理速度方面的有效性，并实现了326% - 414%的加速。他们发布了所有模型和代码，以促进视觉生成和多模式基础模型的开源社区。 <div>
We introduce LlamaGen, a new family of image generation models that apply
original ``next-token prediction'' paradigm of large language models to visual
generation domain. It is an affirmative answer to whether vanilla
autoregressive models, e.g., Llama, without inductive biases on visual signals
can achieve state-of-the-art image generation performance if scaling properly.
We reexamine design spaces of image tokenizers, scalability properties of image
generation models, and their training data quality. The outcome of this
exploration consists of: (1) An image tokenizer with downsample ratio of 16,
reconstruction quality of 0.94 rFID and codebook usage of 97% on ImageNet
benchmark. (2) A series of class-conditional image generation models ranging
from 111M to 3.1B parameters, achieving 2.18 FID on ImageNet 256x256
benchmarks, outperforming the popular diffusion models such as LDM, DiT. (3) A
text-conditional image generation model with 775M parameters, from two-stage
training on LAION-COCO and high aesthetics quality images, demonstrating
competitive performance of visual quality and text alignment. (4) We verify the
effectiveness of LLM serving frameworks in optimizing the inference speed of
image generation models and achieve 326% - 414% speedup. We release all models
and codes to facilitate open-source community of visual generation and
multimodal foundation models.
]]></content:encoded>
<pubDate>2024-06-10T17:59:52Z</pubDate>
</item>
<item>
<title>3D-GRAND: Towards Better Grounding and Less Hallucination for 3D-LLMs</title>
<link>http://arxiv.org/abs/2406.05132v1</link>
<guid>http://arxiv.org/abs/2406.05132v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 语言理解, 3D感知, 大规模数据集, 3D-LLMs, 综合评估<br />
总结:<br />
这篇论文讨论了语言和3D感知的整合对于发展具有身体感知能力并能理解和互动物理世界的智能体和机器人的重要性。作者介绍了一个包含大规模家庭场景和密集语言-场景指令的数据集3D-GRAND，结果显示使用这个数据集可以显著提高3D-LLMs的语境能力，并减少幻觉。作者还提出了一个综合评估标准3D-POPE来系统评估3D-LLMs的幻觉情况，结果表明数据集大小和3D-LLM性能之间存在着扩展效应。最后，作者还展示了大规模合成数据训练的模型在真实3D扫描中表现良好的初步信号。通过3D-GRAND和3D-POPE，作者旨在为身体感知人工智能社区提供必要的资源和见解，为更可靠、更基础的3D-LLMs铺平道路。 <div>
The integration of language and 3D perception is crucial for developing
embodied agents and robots that comprehend and interact with the physical
world. While large language models (LLMs) have demonstrated impressive language
understanding and generation capabilities, their adaptation to 3D environments
(3D-LLMs) remains in its early stages. A primary challenge is the absence of
large-scale datasets that provide dense grounding between language and 3D
scenes. In this paper, we introduce 3D-GRAND, a pioneering large-scale dataset
comprising 40,087 household scenes paired with 6.2 million densely-grounded
scene-language instructions. Our results show that instruction tuning with
3D-GRAND significantly enhances grounding capabilities and reduces
hallucinations in 3D-LLMs. As part of our contributions, we propose a
comprehensive benchmark 3D-POPE to systematically evaluate hallucination in
3D-LLMs, enabling fair comparisons among future models. Our experiments
highlight a scaling effect between dataset size and 3D-LLM performance,
emphasizing the critical role of large-scale 3D-text datasets in advancing
embodied AI research. Notably, our results demonstrate early signals for
effective sim-to-real transfer, indicating that models trained on large
synthetic data can perform well on real-world 3D scans. Through 3D-GRAND and
3D-POPE, we aim to equip the embodied AI community with essential resources and
insights, setting the stage for more reliable and better-grounded 3D-LLMs.
Project website: https://3d-grand.github.io
]]></content:encoded>
<pubDate>2024-06-07T17:59:59Z</pubDate>
</item>
<item>
<title>An Empirical Study on Parameter-Efficient Fine-Tuning for MultiModal
  Large Language Models</title>
<link>http://arxiv.org/abs/2406.05130v1</link>
<guid>http://arxiv.org/abs/2406.05130v1</guid>
<content:encoded><![CDATA[
<div> 参数-efficient fine-tuning, MLLM, PEFT methods, multimodal instruction datasets, 模型性能

参数-efficient fine-tuning（PEFT）方法是为了解决大型语言模型（MLLMs）参数过多的问题，通过对开源MLLMs的LLM组件进行实证研究，比较了四种流行的PEFT方法在多个方面的影响：对各种模型的影响、参数和PEFT模块位置的影响、微调数据的规模对模型性能的影响、PEFT方法对模型稳定性、MLLM的泛化能力以及错觉现象。研究发现，在各种实验中，adapter是表现最佳的PEFT方法，同时微调连接器层可以提高大多数MLLM的性能。总结：PEFT方法对MLLM的性能影响显著，adapter方法表现最佳，微调连接器层也能改善模型性能。 <div>
Multimodal large language models (MLLMs) fine-tuned with multimodal
instruction datasets have demonstrated remarkable capabilities in multimodal
tasks. However, fine-tuning all parameters of MLLMs has become challenging as
they usually contain billions of parameters. To address this issue, we study
parameter-efficient fine-tuning (PEFT) methods for MLLMs. We aim to identify
effective methods for enhancing the performance of MLLMs in scenarios where
only a limited number of parameters are trained. This paper conducts empirical
studies using four popular PEFT methods to fine-tune the LLM component of
open-source MLLMs. We present a comprehensive analysis that encompasses various
aspects, including the impact of PEFT methods on various models, parameters and
location of the PEFT module, size of fine-tuning data, model stability based on
PEFT methods, MLLM's generalization, and hallucination. We evaluated four PEFT
methods on seven datasets from two different categories: unseen and seen
datasets. Across all experiments, we show that the adapter is the
best-performing PEFT method. At the same time, fine-tuning the connector layers
leads to improved performance in most MLLMs. Code and data are available at
https://github.com/alenai97/PEFT-MLLM.git.
]]></content:encoded>
<pubDate>2024-06-07T17:58:11Z</pubDate>
</item>
<item>
<title>Physics3D: Learning Physical Properties of 3D Gaussians via Video
  Diffusion</title>
<link>http://arxiv.org/abs/2406.04338v2</link>
<guid>http://arxiv.org/abs/2406.04338v2</guid>
<content:encoded><![CDATA[
<div> 物理3D，生成模型，物理特性，视频扩散模型，材料模拟

<br /><br />总结: 
近年来，3D生成模型得到了快速发展，为模拟3D物体的动态运动和自定义行为等应用打开了新的可能性。然而，当前的3D生成模型往往只关注颜色和形状等表面特征，忽略了真实世界中影响物体行为的固有物理特性。为了准确模拟与物理对齐的动力学特性，预测物体的物理特性并将其纳入行为预测过程是至关重要的。本文提出了一种名为Physics3D的新方法，通过视频扩散模型学习3D物体的各种物理特性。我们的方法涉及设计一个基于粘弹性材料模型的高度通用的物理模拟系统，使我们能够以高保真度模拟各种材料。此外，我们从视频扩散模型中提取了物理先验知识，更加了解真实物体材料的特性。大量实验证明了我们的方法在弹性和塑性材料方面的有效性。Physics3D显示出了在虚拟神经空间和物理世界之间弥合差距的巨大潜力，提供了在虚拟环境中更好地整合和应用真实物理原理的可能性。 <div>
In recent years, there has been rapid development in 3D generation models,
opening up new possibilities for applications such as simulating the dynamic
movements of 3D objects and customizing their behaviors. However, current 3D
generative models tend to focus only on surface features such as color and
shape, neglecting the inherent physical properties that govern the behavior of
objects in the real world. To accurately simulate physics-aligned dynamics, it
is essential to predict the physical properties of materials and incorporate
them into the behavior prediction process. Nonetheless, predicting the diverse
materials of real-world objects is still challenging due to the complex nature
of their physical attributes. In this paper, we propose \textbf{Physics3D}, a
novel method for learning various physical properties of 3D objects through a
video diffusion model. Our approach involves designing a highly generalizable
physical simulation system based on a viscoelastic material model, which
enables us to simulate a wide range of materials with high-fidelity
capabilities. Moreover, we distill the physical priors from a video diffusion
model that contains more understanding of realistic object materials. Extensive
experiments demonstrate the effectiveness of our method with both elastic and
plastic materials. Physics3D shows great potential for bridging the gap between
the physical world and virtual neural space, providing a better integration and
application of realistic physical principles in virtual environments. Project
page: https://liuff19.github.io/Physics3D.
]]></content:encoded>
<pubDate>2024-06-07T01:30:11Z</pubDate>
</item>
<item>
<title>Physics3D: Learning Physical Properties of 3D Gaussians via Video
  Diffusion</title>
<link>http://arxiv.org/abs/2406.04338v1</link>
<guid>http://arxiv.org/abs/2406.04338v1</guid>
<content:encoded><![CDATA[
<div> 物理属性，3D对象，视频扩散模型，物理仿真，材料模型<br />
总结:<br />
近年来，3D生成模型发展迅速，但目前的模型偏重于表面特征，忽视了真实世界中物体行为的内在物理特性。为了准确模拟物理对齐的动态行为，有必要预测材料的物理属性并将其纳入行为预测过程。本文提出了一种通过视频扩散模型学习3D对象各种物理属性的新方法，该方法基于粘弹性材料模型设计了高度通用的物理仿真系统，能够模拟广泛的材料，并利用视频扩散模型中的物理先验知识。大量实验证明了我们方法的有效性，展示了Physics3D在弹性和塑性材料方面的巨大潜力，有助于弥合物理世界与虚拟神经空间之间的差距，提供更好的虚拟环境中真实物理原则的整合和应用。 <div>
In recent years, there has been rapid development in 3D generation models,
opening up new possibilities for applications such as simulating the dynamic
movements of 3D objects and customizing their behaviors. However, current 3D
generative models tend to focus only on surface features such as color and
shape, neglecting the inherent physical properties that govern the behavior of
objects in the real world. To accurately simulate physics-aligned dynamics, it
is essential to predict the physical properties of materials and incorporate
them into the behavior prediction process. Nonetheless, predicting the diverse
materials of real-world objects is still challenging due to the complex nature
of their physical attributes. In this paper, we propose \textbf{Physics3D}, a
novel method for learning various physical properties of 3D objects through a
video diffusion model. Our approach involves designing a highly generalizable
physical simulation system based on a viscoelastic material model, which
enables us to simulate a wide range of materials with high-fidelity
capabilities. Moreover, we distill the physical priors from a video diffusion
model that contains more understanding of realistic object materials. Extensive
experiments demonstrate the effectiveness of our method with both elastic and
plastic materials. Physics3D shows great potential for bridging the gap between
the physical world and virtual neural space, providing a better integration and
application of realistic physical principles in virtual environments. Project
page: https://liuff19.github.io/Physics3D.
]]></content:encoded>
<pubDate>2024-06-06T17:59:47Z</pubDate>
</item>
<item>
<title>Coherent Zero-Shot Visual Instruction Generation</title>
<link>http://arxiv.org/abs/2406.04337v1</link>
<guid>http://arxiv.org/abs/2406.04337v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 文本到图像合成, 扩散模型, 图像生成, 文本理解, 大型语言模型

扩散模型和大型语言模型的进展使得文本到图像合成取得了巨大进展，但生成需要保持一致性的视觉说明仍然是一个巨大挑战。本文引入了一个简单的、无需训练的框架来解决这些问题，利用了扩散模型和大型语言模型的进步。我们的方法系统地整合了文本理解和图像生成，以确保视觉说明在整个指导序列中既具有视觉吸引力，又保持一致性和准确性。我们通过测试多步说明并与几个基准进行文本对齐和一致性比较来验证其有效性。实验结果表明，我们的方法能够可视化一致和视觉上令人愉悦的说明。<br /><br />总结: 本文介绍了一个简单的、无需训练的框架，利用了扩散模型和大型语言模型的进步，系统地整合了文本理解和图像生成，解决了文本到图像合成中的一致性和准确性问题。 <div>
Despite the advances in text-to-image synthesis, particularly with diffusion
models, generating visual instructions that require consistent representation
and smooth state transitions of objects across sequential steps remains a
formidable challenge. This paper introduces a simple, training-free framework
to tackle the issues, capitalizing on the advancements in diffusion models and
large language models (LLMs). Our approach systematically integrates text
comprehension and image generation to ensure visual instructions are visually
appealing and maintain consistency and accuracy throughout the instruction
sequence. We validate the effectiveness by testing multi-step instructions and
comparing the text alignment and consistency with several baselines. Our
experiments show that our approach can visualize coherent and visually pleasing
instructions
]]></content:encoded>
<pubDate>2024-06-06T17:59:44Z</pubDate>
</item>
<item>
<title>Wings: Learning Multimodal LLMs without Text-only Forgetting</title>
<link>http://arxiv.org/abs/2406.03496v1</link>
<guid>http://arxiv.org/abs/2406.03496v1</guid>
<content:encoded><![CDATA[
<div> Wings, MLLMs, attention, text-only forgetting, multimodal comprehension<br />
<br />
Wings是一种新型的MLLM，它在文本对话和多模态理解方面表现出色。通过分析MLLM在多模态指令中的注意力，我们发现文本遗忘与注意力从图像前到图像后的转移有关。因此，我们设计了额外的模块作为增强学习器，以补偿注意力转移。视觉和文本学习者在每一层的注意力模块内并行连接，以平衡对视觉元素的关注，并在后续阶段通过基于注意力的路由与文本学习者合作整合输出。我们设计了低秩残差注意力（LoRRA）来保证学习效率。实验证明，Wings在纯文本和视觉问答任务中均优于同等规模的MLLM。在新建的交错式图像文本（IIT）基准测试上，Wings在纯文本丰富和多模态丰富问答任务中表现出卓越的性能。<br /><br />总结:Wings是一种新型的MLLM，它在文本对话和多模态理解方面表现出色。同时，通过对MLLM在多模态指令中的注意力分析，发现了文本遗忘与注意力转移的关联，从而设计了补偿模块。Wings在实验中表现出比同等规模的MLLM更优异的性能，在纯文本丰富和多模态丰富问答任务中均表现出色。 <div>
Multimodal large language models (MLLMs), initiated with a trained LLM, first
align images with text and then fine-tune on multimodal mixed inputs. However,
the MLLM catastrophically forgets the text-only instructions, which do not
include images and can be addressed within the initial LLM. In this paper, we
present Wings, a novel MLLM that excels in both text-only dialogues and
multimodal comprehension. Analyzing MLLM attention in multimodal instructions
reveals that text-only forgetting is related to the attention shifts from
pre-image to post-image text. From that, we construct extra modules that act as
the boosted learner to compensate for the attention shift. The complementary
visual and textual learners, like "wings" on either side, are connected in
parallel within each layer's attention block. Initially, image and text inputs
are aligned with visual learners operating alongside the main attention,
balancing focus on visual elements. Textual learners are later collaboratively
integrated with attention-based routing to blend the outputs of the visual and
textual learners. We design the Low-Rank Residual Attention (LoRRA) to
guarantee high efficiency for learners. Our experimental results demonstrate
that Wings outperforms equally-scaled MLLMs in both text-only and visual
question-answering tasks. On a newly constructed Interleaved Image-Text (IIT)
benchmark, Wings exhibits superior performance from text-only-rich to
multimodal-rich question-answering tasks.
]]></content:encoded>
<pubDate>2024-06-05T17:59:40Z</pubDate>
</item>
<item>
<title>Leveraging Visual Tokens for Extended Text Contexts in Multi-Modal
  Learning</title>
<link>http://arxiv.org/abs/2406.02547v1</link>
<guid>http://arxiv.org/abs/2406.02547v1</guid>
<content:encoded><![CDATA[
<div> 多模态模型，训练，长上下文长度，GPU内存，计算成本<br />
探索性研究，视觉化上下文文本处理，长文本处理，FLOPs，下游基准性能<br />
VisInContext方法显著减少了GPU内存使用和FLOPs<br />
研究表明，VisInContext训练的模型在下游基准测试中表现优异<br />
VisInContext方法对增加上下文文本长度和增强文档理解能力有潜力

<br /><br />总结:
本研究探索了一种新的多模态大语言模型中增加长上下文文本长度的方法——视觉化上下文文本处理（VisInContext）。该方法显著降低了GPU内存使用和浮点运算（FLOPs），使得在训练和推断阶段，可以将预训练的上下文文本长度从256扩展到2048个标记，而几乎保持相同的FLOPs。实验结果表明，采用VisInContext训练的模型在常见的下游基准测试中表现优异，还显示出在增加上下文文本长度和增强文档理解能力方面有巨大潜力。 <div>
Training models with longer in-context lengths is a significant challenge for
multimodal model due to substantial GPU memory and computational costs. This
exploratory study does not present state-of-the-art models; rather, it
introduces an innovative method designed to increase in-context text length in
multi-modality large language models (MLLMs) efficiently. We present Visualized
In-Context Text Processing (VisInContext), which processes long in-context text
using visual tokens. This technique significantly reduces GPU memory usage and
floating point operations (FLOPs) for both training and inferenceing stage. For
instance, our method expands the pre-training in-context text length from 256
to 2048 tokens with nearly same FLOPs for a 56 billion parameter MOE model.
Experimental results demonstrate that model trained with VisInContext delivers
superior performance on common downstream benchmarks for in-context few-shot
evaluation. Additionally, VisInContext is complementary to existing methods for
increasing in-context text length and enhances document understanding
capabilities, showing great potential in document QA tasks and sequential
document retrieval.
]]></content:encoded>
<pubDate>2024-06-04T17:59:25Z</pubDate>
</item>
<item>
<title>Robust and highly scalable estimation of directional couplings from
  time-shifted signals</title>
<link>http://arxiv.org/abs/2406.02545v1</link>
<guid>http://arxiv.org/abs/2406.02545v1</guid>
<content:encoded><![CDATA[
<div> 网络、方向耦合、延迟、变分贝叶斯、测量参数

这篇论文介绍了如何通过变分贝叶斯框架来估计网络节点之间的方向耦合，解决了因测量中可能存在未知延迟而导致问题不适定的情况。通过边际化延迟的不确定性，得到了保守的耦合估计。为了克服传统变分方法的过于自信的问题，采用了混合变分推断方案，其中测量参数的后验分布使用前向KL loss进行估计，而耦合的条件后验分布则使用高度可扩展的基于梯度的变分推断方法进行估计。在对实际数据进行的实验证实中，表明该网络能够提供可靠且保守的耦合估计，远远优于回归DCM等类似方法。 <br /><br />总结: 网络节点间的方向耦合估计在存在未知延迟的情况下是一个挑战，本文提出了使用变分贝叶斯框架来边际化延迟的不确定性以获得保守的耦合估计。通过混合变分推断方案克服了传统方法的问题，在实验中展示了该方法的可靠性和优越性。 <div>
The estimation of directed couplings between the nodes of a network from
indirect measurements is a central methodological challenge in scientific
fields such as neuroscience, systems biology and economics. Unfortunately, the
problem is generally ill-posed due to the possible presence of unknown delays
in the measurements. In this paper, we offer a solution of this problem by
using a variational Bayes framework, where the uncertainty over the delays is
marginalized in order to obtain conservative coupling estimates. To overcome
the well-known overconfidence of classical variational methods, we use a
hybrid-VI scheme where the (possibly flat or multimodal) posterior over the
measurement parameters is estimated using a forward KL loss while the (nearly
convex) conditional posterior over the couplings is estimated using the highly
scalable gradient-based VI. In our ground-truth experiments, we show that the
network provides reliable and conservative estimates of the couplings, greatly
outperforming similar methods such as regression DCM.
]]></content:encoded>
<pubDate>2024-06-04T17:58:33Z</pubDate>
</item>
<item>
<title>ViDiT-Q: Efficient and Accurate Quantization of Diffusion Transformers
  for Image and Video Generation</title>
<link>http://arxiv.org/abs/2406.02540v1</link>
<guid>http://arxiv.org/abs/2406.02540v1</guid>
<content:encoded><![CDATA[
Diffusion transformers (DiTs) have exhibited remarkable performance in visual
generation tasks, such as generating realistic images or videos based on
textual instructions. However, larger model sizes and multi-frame processing
for video generation lead to increased computational and memory costs, posing
challenges for practical deployment on edge devices. Post-Training Quantization
(PTQ) is an effective method for reducing memory costs and computational
complexity. When quantizing diffusion transformers, we find that applying
existing diffusion quantization methods designed for U-Net faces challenges in
preserving quality. After analyzing the major challenges for quantizing
diffusion transformers, we design an improved quantization scheme: "ViDiT-Q":
Video and Image Diffusion Transformer Quantization) to address these issues.
Furthermore, we identify highly sensitive layers and timesteps hinder
quantization for lower bit-widths. To tackle this, we improve ViDiT-Q with a
novel metric-decoupled mixed-precision quantization method (ViDiT-Q-MP). We
validate the effectiveness of ViDiT-Q across a variety of text-to-image and
video models. While baseline quantization methods fail at W8A8 and produce
unreadable content at W4A8, ViDiT-Q achieves lossless W8A8 quantization.
ViDiTQ-MP achieves W4A8 with negligible visual quality degradation, resulting
in a 2.5x memory optimization and a 1.5x latency speedup.
]]></content:encoded>
<pubDate>2024-06-04T17:57:10Z</pubDate>
</item>
<item>
<title>RapVerse: Coherent Vocals and Whole-Body Motions Generations from Text</title>
<link>http://arxiv.org/abs/2405.20336v1</link>
<guid>http://arxiv.org/abs/2405.20336v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 3D holistic body motions, singing vocals, RapVerse dataset, autoregressive multimodal transformers, vector-quantized variational autoencoder

生成音频和人体运动的统一模型，使用RapVerse数据集进行研究，在语言、音频和动作之间进行变换。通过变换编码整体运动序列和音频，确保音频和人体运动的无缝和真实混合。实验证明，该统一生成框架不仅可以直接从文本输入中产生连贯和逼真的歌唱声音和人体运动，而且可以与专门的单模态生成系统相匹敌，为联合声音-运动生成建立了新的基准。 项目页面可用于研究目的，网址为https://vis-www.cs.umass.edu/RapVerse。<br /><br />总结: 通过使用RapVerse数据集，这项工作介绍了一个新的挑战性任务，即从文本歌词直接生成3D整体人体动作和歌唱声音。研究人员使用变换编码模型统一处理语言、音频和动作，并通过实验证明了该框架的有效性和性能。 <div>
In this work, we introduce a challenging task for simultaneously generating
3D holistic body motions and singing vocals directly from textual lyrics
inputs, advancing beyond existing works that typically address these two
modalities in isolation. To facilitate this, we first collect the RapVerse
dataset, a large dataset containing synchronous rapping vocals, lyrics, and
high-quality 3D holistic body meshes. With the RapVerse dataset, we investigate
the extent to which scaling autoregressive multimodal transformers across
language, audio, and motion can enhance the coherent and realistic generation
of vocals and whole-body human motions. For modality unification, a
vector-quantized variational autoencoder is employed to encode whole-body
motion sequences into discrete motion tokens, while a vocal-to-unit model is
leveraged to obtain quantized audio tokens preserving content, prosodic
information, and singer identity. By jointly performing transformer modeling on
these three modalities in a unified way, our framework ensures a seamless and
realistic blend of vocals and human motions. Extensive experiments demonstrate
that our unified generation framework not only produces coherent and realistic
singing vocals alongside human motions directly from textual inputs but also
rivals the performance of specialized single-modality generation systems,
establishing new benchmarks for joint vocal-motion generation. The project page
is available for research purposes at https://vis-www.cs.umass.edu/RapVerse.
]]></content:encoded>
<pubDate>2024-05-30T17:59:39Z</pubDate>
</item>
<item>
<title>LLMs Meet Multimodal Generation and Editing: A Survey</title>
<link>http://arxiv.org/abs/2405.19334v1</link>
<guid>http://arxiv.org/abs/2405.19334v1</guid>
<content:encoded><![CDATA[
<div> 大型语言模型，多模态学习，生成模型，技术组件，人机交互<br />
本篇文章对多模态生成进行了系统性综述，重点在不同领域（包括图像、视频、3D和音频）的多模态生成上进行了详细阐述，突出了这些领域的里程碑式进展。具体来说，文章详尽调查了这些研究中使用的方法和多模态数据集的关键技术组成部分。此外，文章还深入探讨了工具增强的多模态代理，这些代理可以利用现有的生成模型进行人机交互。最后，文章还全面讨论了AI安全性的进展，研究了新兴应用以及未来展望。我们的工作为多模态生成提供了系统性和富有洞察力的概述，预计将推动生成内容人工智能（AIGC）和世界模型的发展。 <div>
With the recent advancement in large language models (LLMs), there is a
growing interest in combining LLMs with multimodal learning. Previous surveys
of multimodal large language models (MLLMs) mainly focus on understanding. This
survey elaborates on multimodal generation across different domains, including
image, video, 3D, and audio, where we highlight the notable advancements with
milestone works in these fields. Specifically, we exhaustively investigate the
key technical components behind methods and multimodal datasets utilized in
these studies. Moreover, we dig into tool-augmented multimodal agents that can
use existing generative models for human-computer interaction. Lastly, we also
comprehensively discuss the advancement in AI safety and investigate emerging
applications as well as future prospects. Our work provides a systematic and
insightful overview of multimodal generation, which is expected to advance the
development of Artificial Intelligence for Generative Content (AIGC) and world
models. A curated list of all related papers can be found at
https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation
]]></content:encoded>
<pubDate>2024-05-29T17:59:20Z</pubDate>
</item>
<item>
<title>Multi-Modal Generative Embedding Model</title>
<link>http://arxiv.org/abs/2405.19333v1</link>
<guid>http://arxiv.org/abs/2405.19333v1</guid>
<content:encoded><![CDATA[
<div> 生成，嵌入，Multi-Modal Generative Embedding Model (MM-GEM)，PoolAggregator，ViT-Large

该研究提出了一种多模态生成嵌入模型(MM-GEM)，将生成和嵌入两个目标整合到一个大型语言模型中。他们还提出了一种PoolAggregator来提高效率，并实现细粒度的嵌入和生成能力。研究发现，这两个目标并不会显著冲突。MM-GEM在跨模态检索和零样本分类等多模态嵌入模型基准测试上表现出色，同时具有良好的图像字幕生成能力。此外，MM-GEM可以无缝执行区域级图像字幕生成和检索任务。MM-GEM中的高级文本模型还提高了长文本和图像检索的召回率超过5%。 <br /><br />总结: 该研究提出了一种多模态生成嵌入模型(MM-GEM)，实现了在一个大型语言模型中整合生成和嵌入两个目标，并通过PoolAggregator实现了高效率和细粒度的嵌入和生成能力。研究发现，这种模型在多种基准测试上表现出色，并且在图像字幕生成和检索任务中也能够达到良好的效果。 <div>
Most multi-modal tasks can be formulated into problems of either generation
or embedding. Existing models usually tackle these two types of problems by
decoupling language modules into a text decoder for generation, and a text
encoder for embedding. To explore the minimalism of multi-modal paradigms, we
attempt to achieve only one model per modality in this work. We propose a
Multi-Modal Generative Embedding Model (MM-GEM), whereby the generative and
embedding objectives are encapsulated in one Large Language Model. We also
propose a PoolAggregator to boost efficiency and enable the ability of
fine-grained embedding and generation. A surprising finding is that these two
objectives do not significantly conflict with each other. For example, MM-GEM
instantiated from ViT-Large and TinyLlama shows competitive performance on
benchmarks for multimodal embedding models such as cross-modal retrieval and
zero-shot classification, while has good ability of image captioning.
Additionally, MM-GEM can seamlessly execute region-level image caption
generation and retrieval tasks. Besides, the advanced text model in MM-GEM
brings over 5% improvement in Recall@1 for long text and image retrieval.
]]></content:encoded>
<pubDate>2024-05-29T17:59:10Z</pubDate>
</item>
<item>
<title>Normative Modules: A Generative Agent Architecture for Learning Norms
  that Supports Multi-Agent Cooperation</title>
<link>http://arxiv.org/abs/2405.19328v1</link>
<guid>http://arxiv.org/abs/2405.19328v1</guid>
<content:encoded><![CDATA[
Generative agents, which implement behaviors using a large language model
(LLM) to interpret and evaluate an environment, has demonstrated the capacity
to solve complex tasks across many social and technological domains. However,
when these agents interact with other agents and humans in presence of social
structures such as existing norms, fostering cooperation between them is a
fundamental challenge. In this paper, we develop the framework of a 'Normative
Module': an architecture designed to enhance cooperation by enabling agents to
recognize and adapt to the normative infrastructure of a given environment. We
focus on the equilibrium selection aspect of the cooperation problem and inform
our agent design based on the existence of classification institutions that
implement correlated equilibrium to provide effective resolution of the
equilibrium selection problem. Specifically, the normative module enables
agents to learn through peer interactions which of multiple candidate
institutions in the environment, does a group treat as authoritative. By
enabling normative competence in this sense, agents gain ability to coordinate
their sanctioning behaviour; coordinated sanctioning behaviour in turn shapes
primary behaviour within a social environment, leading to higher average
welfare. We design a new environment that supports institutions and evaluate
the proposed framework based on two key criteria derived from agent
interactions with peers and institutions: (i) the agent's ability to disregard
non-authoritative institutions and (ii) the agent's ability to identify
authoritative institutions among several options. We show that these
capabilities allow the agent to achieve more stable cooperative outcomes
compared to baseline agents without the normative module, paving the way for
research in a new avenue of designing environments and agents that account for
normative infrastructure.
]]></content:encoded>
<pubDate>2024-05-29T17:57:30Z</pubDate>
</item>
<item>
<title>Hierarchical World Models as Visual Whole-Body Humanoid Controllers</title>
<link>http://arxiv.org/abs/2405.18418v1</link>
<guid>http://arxiv.org/abs/2405.18418v1</guid>
<content:encoded><![CDATA[
<div> 强化学习，全身控制，视觉观察，层次世界模型，仿真人形机器人<br />本文探讨了基于强化学习的高度数据驱动的视觉全身人形机器人控制方法。研究采用分层世界模型，由高级代理根据视觉观察生成指令，低级代理执行这些指令，通过奖励进行训练。该方法在模拟环境下成功实现了8项任务的控制，并生成了广受人类喜爱的动作。详细信息可在https://nicklashansen.com/rlpuppeteer 网站找到。<br /><br />总结: 本文使用强化学习方法，提出了一种基于视觉观察的全身人形机器人控制方法，通过分层世界模型进行训练，取得了良好效果。 <div>
Whole-body control for humanoids is challenging due to the high-dimensional
nature of the problem, coupled with the inherent instability of a bipedal
morphology. Learning from visual observations further exacerbates this
difficulty. In this work, we explore highly data-driven approaches to visual
whole-body humanoid control based on reinforcement learning, without any
simplifying assumptions, reward design, or skill primitives. Specifically, we
propose a hierarchical world model in which a high-level agent generates
commands based on visual observations for a low-level agent to execute, both of
which are trained with rewards. Our approach produces highly performant control
policies in 8 tasks with a simulated 56-DoF humanoid, while synthesizing
motions that are broadly preferred by humans. Code and videos:
https://nicklashansen.com/rlpuppeteer
]]></content:encoded>
<pubDate>2024-05-28T17:57:23Z</pubDate>
</item>
<item>
<title>Reason3D: Searching and Reasoning 3D Segmentation via Large Language
  Model</title>
<link>http://arxiv.org/abs/2405.17427v1</link>
<guid>http://arxiv.org/abs/2405.17427v1</guid>
<content:encoded><![CDATA[
<div> 大型语言模型、3D理解、Reason3D、分割、层次掩模<br />
本文介绍了一种名为Reason3D的新型大型语言模型，用于全面理解3D环境。Reason3D接受点云数据和文本提示作为输入，生成文本响应和分割掩模，实现了高级任务，如3D推理分割、层次搜索、表达引用和问题回答。文章提出了一种分层掩模解码器，用于定位广阔场景中的小物体。实验验证了Reason3D在大规模ScanNet和Matterport3D数据集上在3D表达引用、3D问题回答和3D推理分割任务上取得了显著成果。可从https://github.com/KuanchihHuang/Reason3D获取代码和模型。<br /><br />总结: 本文介绍了一种新型大型语言模型Reason3D，用于理解3D环境。Reason3D能够处理点云数据和文本提示，实现了3D推理分割、层次搜索、表达引用和问题回答等任务。文章还提出了分层掩模解码器，用于在广阔场景中定位小物体。实验证实了Reason3D在大规模数据集上取得了显著的成果。 <div>
Recent advancements in multimodal large language models (LLMs) have shown
their potential in various domains, especially concept reasoning. Despite these
developments, applications in understanding 3D environments remain limited.
This paper introduces Reason3D, a novel LLM designed for comprehensive 3D
understanding. Reason3D takes point cloud data and text prompts as input to
produce textual responses and segmentation masks, facilitating advanced tasks
like 3D reasoning segmentation, hierarchical searching, express referring, and
question answering with detailed mask outputs. Specifically, we propose a
hierarchical mask decoder to locate small objects within expansive scenes. This
decoder initially generates a coarse location estimate covering the object's
general area. This foundational estimation facilitates a detailed,
coarse-to-fine segmentation strategy that significantly enhances the precision
of object identification and segmentation. Experiments validate that Reason3D
achieves remarkable results on large-scale ScanNet and Matterport3D datasets
for 3D express referring, 3D question answering, and 3D reasoning segmentation
tasks. Code and models are available at:
https://github.com/KuanchihHuang/Reason3D.
]]></content:encoded>
<pubDate>2024-05-27T17:59:41Z</pubDate>
</item>
<item>
<title>LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence</title>
<link>http://arxiv.org/abs/2405.17424v1</link>
<guid>http://arxiv.org/abs/2405.17424v1</guid>
<content:encoded><![CDATA[
<div> 大型自回归模型、综合先验知识、长程规划、决策链、Minecraft。

总结:<br />
本文介绍了一种名为大型自回归模型（LARM）的新型代理技术。该技术结合了文本和多视角图像输入，并能够以自回归方式预测后续行动。为了训练LARM，研究人员开发了一种名为自回归节点传输结构的新型数据格式，并组建了相应的数据集。通过采用两阶段训练方法，LARM成功收获了Minecraft游戏中的强化装备，这需要比以往最佳方法更复杂的决策链。此外，LARM的速度是以前方法的6.8倍。这表明LARM在处理现实世界互动时具有很大的潜力。 <div>
Due to the need to interact with the real world, embodied agents are required
to possess comprehensive prior knowledge, long-horizon planning capability, and
a swift response speed. Despite recent large language model (LLM) based agents
achieving promising performance, they still exhibit several limitations. For
instance, the output of LLMs is a descriptive sentence, which is ambiguous when
determining specific actions. To address these limitations, we introduce the
large auto-regressive model (LARM). LARM leverages both text and multi-view
images as input and predicts subsequent actions in an auto-regressive manner.
To train LARM, we develop a novel data format named auto-regressive node
transmission structure and assemble a corresponding dataset. Adopting a
two-phase training regimen, LARM successfully harvests enchanted equipment in
Minecraft, which demands significantly more complex decision-making chains than
the highest achievements of prior best methods. Besides, the speed of LARM is
6.8x faster.
]]></content:encoded>
<pubDate>2024-05-27T17:59:32Z</pubDate>
</item>
<item>
<title>Improved Distribution Matching Distillation for Fast Image Synthesis</title>
<link>http://arxiv.org/abs/2405.14867v1</link>
<guid>http://arxiv.org/abs/2405.14867v1</guid>
<content:encoded><![CDATA[
<div> Distribution Matching Distillation, DMD, regression loss, GAN loss, multi-step sampling <br />
总结: 
本文介绍了DMD2技术，旨在提高分布匹配蒸馏（DMD）训练的质量和效率。首先，通过消除回归损失以及昂贵的数据集构建需求，解决了DMD的稳定性问题。接着，引入了GAN损失来使学生模型在真实数据上进行训练，提升了生成图片的质量。最后，修改了训练程序以实现多步采样，并解决了训练-推断输入不匹配的问题。这些改进使得该方法在一步图片生成领域取得了新的成果，且在推断成本减少500倍的情况下，FID分数在ImageNet-64x64达到了1.28，在零样例COCO 2014上达到了8.35，超过了原始教师模型。此外，通过对SDXL进行蒸馏，展示了优异的视觉质量，尤其在少步方法中能够生成百万像素的大型图片。 <div>
Recent approaches have shown promises distilling diffusion models into
efficient one-step generators. Among them, Distribution Matching Distillation
(DMD) produces one-step generators that match their teacher in distribution,
without enforcing a one-to-one correspondence with the sampling trajectories of
their teachers. However, to ensure stable training, DMD requires an additional
regression loss computed using a large set of noise-image pairs generated by
the teacher with many steps of a deterministic sampler. This is costly for
large-scale text-to-image synthesis and limits the student's quality, tying it
too closely to the teacher's original sampling paths. We introduce DMD2, a set
of techniques that lift this limitation and improve DMD training. First, we
eliminate the regression loss and the need for expensive dataset construction.
We show that the resulting instability is due to the fake critic not estimating
the distribution of generated samples accurately and propose a two time-scale
update rule as a remedy. Second, we integrate a GAN loss into the distillation
procedure, discriminating between generated samples and real images. This lets
us train the student model on real data, mitigating the imperfect real score
estimation from the teacher model, and enhancing quality. Lastly, we modify the
training procedure to enable multi-step sampling. We identify and address the
training-inference input mismatch problem in this setting, by simulating
inference-time generator samples during training time. Taken together, our
improvements set new benchmarks in one-step image generation, with FID scores
of 1.28 on ImageNet-64x64 and 8.35 on zero-shot COCO 2014, surpassing the
original teacher despite a 500X reduction in inference cost. Further, we show
our approach can generate megapixel images by distilling SDXL, demonstrating
exceptional visual quality among few-step methods.
]]></content:encoded>
<pubDate>2024-05-23T17:59:49Z</pubDate>
</item>
<item>
<title>Comprehensive Multimodal Deep Learning Survival Prediction Enabled by a
  Transformer Architecture: A Multicenter Study in Glioblastoma</title>
<link>http://arxiv.org/abs/2405.12963v1</link>
<guid>http://arxiv.org/abs/2405.12963v1</guid>
<content:encoded><![CDATA[
<div> 深度学习模型, 胶质母细胞瘤, 图像数据, 临床数据, 生存预测<br />
<br />
胶质母细胞瘤是一种致命的脑肿瘤，预测其生存率对治疗和临床决策至关重要。本研究提出了一种基于transformer的深度学习模型，结合了核磁共振图像、临床和分子病理数据，以改善胶质母细胞瘤的生存预测。该模型利用自监督学习技术对高维MRI输入进行编码，并通过交叉注意力有效地整合了图像和非图像数据。实验结果表明，该模型在多个独立测试集上表现稳定，且显著优于目前的基于3D-CNN的模型。结论指出，该transformer模型整合了多种输入模态的信息，为改善胶质母细胞瘤生存预测提供了重要贡献，并具有较强的泛化能力。 <br /><br />总结: <br />胶质母细胞瘤是一种致命的脑肿瘤，预测其生存率对治疗和临床决策至关重要。<br />本研究提出了一种基于transformer的深度学习模型，结合了核磁共振图像、临床和分子病理数据，以改善胶质母细胞瘤的生存预测。<br />实验结果表明，该模型在多个独立测试集上表现稳定，且显著优于目前的基于3D-CNN的模型。<br />结论指出，该transformer模型整合了多种输入模态的信息，为改善胶质母细胞瘤生存预测提供了重要贡献，并具有较强的泛化能力。 <div>
Background: This research aims to improve glioblastoma survival prediction by
integrating MR images, clinical and molecular-pathologic data in a
transformer-based deep learning model, addressing data heterogeneity and
performance generalizability. Method: We propose and evaluate a
transformer-based non-linear and non-proportional survival prediction model.
The model employs self-supervised learning techniques to effectively encode the
high-dimensional MRI input for integration with non-imaging data using
cross-attention. To demonstrate model generalizability, the model is assessed
with the time-dependent concordance index (Cdt) in two training setups using
three independent public test sets: UPenn-GBM, UCSF-PDGM, and RHUH-GBM, each
comprising 378, 366, and 36 cases, respectively. Results: The proposed
transformer model achieved promising performance for imaging as well as
non-imaging data, effectively integrating both modalities for enhanced
performance (UPenn-GBM test-set, imaging Cdt 0.645, multimodal Cdt 0.707) while
outperforming state-of-the-art late-fusion 3D-CNN-based models. Consistent
performance was observed across the three independent multicenter test sets
with Cdt values of 0.707 (UPenn-GBM, internal test set), 0.672 (UCSF-PDGM,
first external test set) and 0.618 (RHUH-GBM, second external test set). The
model achieved significant discrimination between patients with favorable and
unfavorable survival for all three datasets (logrank p 1.9\times{10}^{-8},
9.7\times{10}^{-3}, and 1.2\times{10}^{-2}). Conclusions: The proposed
transformer-based survival prediction model integrates complementary
information from diverse input modalities, contributing to improved
glioblastoma survival prediction compared to state-of-the-art methods.
Consistent performance was observed across institutions supporting model
generalizability.
]]></content:encoded>
<pubDate>2024-05-21T17:44:48Z</pubDate>
</item>
<item>
<title>Adapting Large Multimodal Models to Distribution Shifts: The Role of
  In-Context Learning</title>
<link>http://arxiv.org/abs/2405.12217v1</link>
<guid>http://arxiv.org/abs/2405.12217v1</guid>
<content:encoded><![CDATA[
<div> LMMs, adaptability, in-context learning, TopKNearestPR, InvariantSelectPR
LMM大型多模型，具有高鲁棒性，适应性强，但在特定领域需要领域特定的适应性。提出了在上下文中学习(ICL)作为增强LMM适应性的有效替代方法，以及选择示例的困难和方法。通过评估无监督ICL方法TopKNearestPR，发现其效果受制于预训练视觉编码器的不足。因此提出了一种新方法InvariantSelectPR，利用Class-conditioned Contrastive Invariance（CCI）来增强预训练视觉编码器，提高其鲁棒性。实验证明，InvariantSelectPR显著提高了LMM的适应性，在基准数据集上取得了显著的性能提高。总结: LMM具有高鲁棒性但需要特定领域的适应性；ICL是一种有效的增强LMM适应性的替代方法；TopKNearestPR方法的效果受限于预训练视觉编码器的不足；InvariantSelectPR利用CCI提高了LMM的适应性。 <div>
Recent studies indicate that large multimodal models (LMMs) are highly robust
against natural distribution shifts, often surpassing previous baselines.
Despite this, domain-specific adaptation is still necessary, particularly in
specialized areas like healthcare. Due to the impracticality of fine-tuning
LMMs given their vast parameter space, this work investigates in-context
learning (ICL) as an effective alternative for enhancing LMMs' adaptability. We
find that the success of ICL heavily relies on the choice of demonstration,
mirroring challenges seen in large language models but introducing unique
complexities for LMMs facing distribution shifts. Our study addresses this by
evaluating an unsupervised ICL method, TopKNearestPR, which selects in-context
examples through a nearest example search based on feature similarity. We
uncover that its effectiveness is limited by the deficiencies of pre-trained
vision encoders under distribution shift scenarios. To address these
challenges, we propose InvariantSelectPR, a novel method leveraging
Class-conditioned Contrastive Invariance (CCI) for more robust demonstration
selection. Specifically, CCI enhances pre-trained vision encoders by improving
their discriminative capabilities across different classes and ensuring
invariance to domain-specific variations. This enhancement allows the encoders
to effectively identify and retrieve the most informative examples, which are
then used to guide LMMs in adapting to new query samples under varying
distributions. Our experiments show that InvariantSelectPR substantially
improves the adaptability of LMMs, achieving significant performance gains on
benchmark datasets, with a 34.2%$\uparrow$ accuracy increase in 7-shot on
Camelyon17 and 16.9%$\uparrow$ increase in 7-shot on HAM10000 compared to the
baseline zero-shot performance.
]]></content:encoded>
<pubDate>2024-05-20T17:59:21Z</pubDate>
</item>
<item>
<title>Observational Scaling Laws and the Predictability of Language Model
  Performance</title>
<link>http://arxiv.org/abs/2405.10938v1</link>
<guid>http://arxiv.org/abs/2405.10938v1</guid>
<content:encoded><![CDATA[
<div> scaling laws, language model performance, observational approach, training compute efficiencies, predictability<br />
<br />总结:<br />文章主要讨论了语言模型性能随规模变化的规律性以及如何利用观测方法构建缩放定律。作者提出了通过观察已有的约80个公开模型来构建缩放定律的替代方法，并指出了模型家族在训练计算效率和能力方面的差异。作者还展示了复杂的缩放现象是可以预测的，包括一些新现象的平滑、S形行为以及如何预测后训练干预对语言模型能力的影响。 <div>
Understanding how language model performance varies with scale is critical to
benchmark and algorithm development. Scaling laws are one approach to building
this understanding, but the requirement of training models across many
different scales has limited their use. We propose an alternative,
observational approach that bypasses model training and instead builds scaling
laws from ~80 publically available models. Building a single scaling law from
multiple model families is challenging due to large variations in their
training compute efficiencies and capabilities. However, we show that these
variations are consistent with a simple, generalized scaling law where language
model performance is a function of a low-dimensional capability space, and
model families only vary in their efficiency in converting training compute to
capabilities. Using this approach, we show the surprising predictability of
complex scaling phenomena: we show that several emergent phenomena follow a
smooth, sigmoidal behavior and are predictable from small models; we show that
the agent performance of models such as GPT-4 can be precisely predicted from
simpler non-agentic benchmarks; and we show how to predict the impact of
post-training interventions like Chain-of-Thought and Self-Consistency as
language model capabilities continue to improve.
]]></content:encoded>
<pubDate>2024-05-17T17:49:44Z</pubDate>
</item>
<item>
<title>CinePile: A Long Video Question Answering Dataset and Benchmark</title>
<link>http://arxiv.org/abs/2405.08813v1</link>
<guid>http://arxiv.org/abs/2405.08813v1</guid>
<content:encoded><![CDATA[
<div> 长视频理解，数据集，CinePile，问题-答案数据集，LLMs，视频理解模型
<br /><br />
本文介绍了针对真实长视频理解的创新数据集和基准测试CinePile。该数据集包含了30.5万道多项选择题，涵盖了视觉和多模态方面的各种内容，包括时间理解，人物与物体的互动，以及场景中事件或动作的推理。作者利用先进的LLMs与人类相结合的方法构建了这个问题-答案数据集。此外，他们还评估了最近的视频中心化LLMs在数据集的测试部分上的表现，结果显示即使最先进的视频中心化LLMs在这些任务上的表现也明显落后于人类表现，突显了视频理解中的复杂性和挑战。数据集可以在链接https://hf.co/datasets/tomg-group-umd/cinepile上获取。
<br /><br />总结: 本文介绍了CinePile数据集，包括多项选择题和LLMs评估结果，突出了视频理解的复杂性和挑战。 <div>
Current datasets for long-form video understanding often fall short of
providing genuine long-form comprehension challenges, as many tasks derived
from these datasets can be successfully tackled by analyzing just one or a few
random frames from a video. To address this issue, we present a novel dataset
and benchmark, CinePile, specifically designed for authentic long-form video
understanding. This paper details our innovative approach for creating a
question-answer dataset, utilizing advanced LLMs with human-in-the-loop and
building upon human-generated raw data. Our comprehensive dataset comprises
305,000 multiple-choice questions (MCQs), covering various visual and
multimodal aspects, including temporal comprehension, understanding
human-object interactions, and reasoning about events or actions within a
scene. Additionally, we evaluate recent video-centric LLMs, both open-source
and proprietary, on the test split of our dataset. The findings reveal that
even state-of-the-art video-centric LLMs significantly lag behind human
performance in these tasks, highlighting the complexity and challenge inherent
in video understanding. The dataset is available at
https://hf.co/datasets/tomg-group-umd/cinepile
]]></content:encoded>
<pubDate>2024-05-14T17:59:02Z</pubDate>
</item>
<item>
<title>SciFIBench: Benchmarking Large Multimodal Models for Scientific Figure
  Interpretation</title>
<link>http://arxiv.org/abs/2405.08807v1</link>
<guid>http://arxiv.org/abs/2405.08807v1</guid>
<content:encoded><![CDATA[
<div> 科学研究, 图形解释, 多模态模型, 基准测试, CS arXiv paper

多模态模型在科学研究中的应用潜力巨大，但在图形解释方面的能力尚未得到很好的表征。本文介绍了SciFIBench，这是一个科学图形解释基准测试，包括1000个多项选择题和12个类别的两项任务。题目经过人工筛选和质量控制，对26个多模态模型进行了评估，结果显示这是一个具有挑战性的基准测试。最后，研究了多模态模型在扩展问题集上的表现。SciFIBench的发布旨在推动该领域的进展。 <br /><br />总结: <div>
Large multimodal models (LMMs) have proven flexible and generalisable across
many tasks and fields. Although they have strong potential to aid scientific
research, their capabilities in this domain are not well characterised. A key
aspect of scientific research is the ability to understand and interpret
figures, which serve as a rich, compressed source of complex information. In
this work, we present SciFIBench, a scientific figure interpretation benchmark.
Our main benchmark consists of a 1000-question gold set of multiple-choice
questions split between two tasks across 12 categories. The questions are
curated from CS arXiv paper figures and captions, using adversarial filtering
to find hard negatives and human verification for quality control. We evaluate
26 LMMs on SciFIBench, finding it to be a challenging benchmark. Finally, we
investigate the alignment and reasoning faithfulness of the LMMs on augmented
question sets from our benchmark. We release SciFIBench to encourage progress
in this domain.
]]></content:encoded>
<pubDate>2024-05-14T17:54:17Z</pubDate>
</item>
<item>
<title>SPIN: Simultaneous Perception, Interaction and Navigation</title>
<link>http://arxiv.org/abs/2405.07991v1</link>
<guid>http://arxiv.org/abs/2405.07991v1</guid>
<content:encoded><![CDATA[
<div> 移动操作，机器人，视觉，整体协调，感知
<br />
该文章介绍了移动操作系统的挑战以及提出的解决方案。移动操作相比于静态操作或者运动操作来说更加具有挑战性，需要在无序动态环境中完成长期任务。文章提出了一种活动视觉系统来感知和反应环境，类似于人类利用整体协调和手眼协调来完成任务。这种系统不仅可以在复杂的环境中移动，还可以通过活动视觉系统选择何时感知何物。通过这种方法，机器人学会了在复杂杂乱的环境中移动，并展示了灵活的整体协调能力，而无需创建环境地图。文章还提供了结果可视化和视频。 <div>
While there has been remarkable progress recently in the fields of
manipulation and locomotion, mobile manipulation remains a long-standing
challenge. Compared to locomotion or static manipulation, a mobile system must
make a diverse range of long-horizon tasks feasible in unstructured and dynamic
environments. While the applications are broad and interesting, there are a
plethora of challenges in developing these systems such as coordination between
the base and arm, reliance on onboard perception for perceiving and interacting
with the environment, and most importantly, simultaneously integrating all
these parts together. Prior works approach the problem using disentangled
modular skills for mobility and manipulation that are trivially tied together.
This causes several limitations such as compounding errors, delays in
decision-making, and no whole-body coordination. In this work, we present a
reactive mobile manipulation framework that uses an active visual system to
consciously perceive and react to its environment. Similar to how humans
leverage whole-body and hand-eye coordination, we develop a mobile manipulator
that exploits its ability to move and see, more specifically -- to move in
order to see and to see in order to move. This allows it to not only move
around and interact with its environment but also, choose "when" to perceive
"what" using an active visual system. We observe that such an agent learns to
navigate around complex cluttered scenarios while displaying agile whole-body
coordination using only ego-vision without needing to create environment maps.
Results visualizations and videos at https://spin-robot.github.io/
]]></content:encoded>
<pubDate>2024-05-13T17:59:36Z</pubDate>
</item>
<item>
<title>A Generalist Learner for Multifaceted Medical Image Interpretation</title>
<link>http://arxiv.org/abs/2405.07988v1</link>
<guid>http://arxiv.org/abs/2405.07988v1</guid>
<content:encoded><![CDATA[
<div> 医疗人工智能，MedVersa，多模态学习，医学图像解释，数据集
<br /><br />
提出了MedVersa，一个通用的学习系统，可以学习和处理医学图像的多个任务。它利用大型语言模型作为可学习的协调器，允许从视觉和语言监督中学习，支持多模态输入，并且可以实时进行任务规范。通过引入MedInterp数据集，包括超过1300万个注释实例，涵盖了11个任务跨越3种模态，来支持MedVersa的开发。实验表明，MedVersa在9个任务中实现了最先进的性能，有时优于专家对手超过10％。这种通用方法为医学图像解释打开了新的可能性，为更适应和高效的人工智能辅助临床决策铺平了道路。 
<br />MedVersa是第一个展示多模态生成医学人工智能在实现多模态输出、输入和动态任务规范方面的可行性，突显了它作为全面医学图像分析多功能系统的潜力。 <div>
Current medical artificial intelligence systems are often limited to narrow
applications, hindering their widespread adoption in clinical practice. To
address this limitation, we propose MedVersa, a generalist learner that enables
flexible learning and tasking for medical image interpretation. By leveraging a
large language model as a learnable orchestrator, MedVersa can learn from both
visual and linguistic supervision, support multimodal inputs, and perform
real-time task specification. This versatility allows MedVersa to adapt to
various clinical scenarios and perform multifaceted medical image analysis. We
introduce MedInterp, the largest multimodal dataset to date for medical image
interpretation, consisting of over 13 million annotated instances spanning 11
tasks across 3 modalities, to support the development of MedVersa. Our
experiments demonstrate that MedVersa achieves state-of-the-art performance in
9 tasks, sometimes outperforming specialist counterparts by over 10%. MedVersa
is the first to showcase the viability of multimodal generative medical AI in
implementing multimodal outputs, inputs, and dynamic task specification,
highlighting its potential as a multifunctional system for comprehensive
medical image analysis. This generalist approach to medical image
interpretation paves the way for more adaptable and efficient AI-assisted
clinical decision-making.
]]></content:encoded>
<pubDate>2024-05-13T17:58:51Z</pubDate>
</item>
<item>
<title>Conformal Validity Guarantees Exist for Any Data Distribution</title>
<link>http://arxiv.org/abs/2405.06627v1</link>
<guid>http://arxiv.org/abs/2405.06627v1</guid>
<content:encoded><![CDATA[
<div> 黑盒优化, 主动学习, 数据分布, 不确定性, 风险量化
总结:
本文讨论了随着机器学习的普及，从业者日益寻求量化和控制这些系统所承担风险的手段。特别是在黑盒优化和主动学习中，机器学习系统具有自主收集数据的能力，其行为引发了数据分布的顺序反馈循环转变，这一挑战尤为突出。虽然符合预测已成为一种有前途的不确定性和风险量化方法，但现有的变种要么无法适应数据依赖性转变的序列，要么没有充分利用由我们控制的代理引发的转变事实。本文证明了符合预测在理论上可以扩展到\textit{任何}联合数据分布，而不仅仅是可交换或准可交换的数据分布，尽管在最一般的情况下计算极为不切实际。对于实际应用，我们概述了一种推导任何数据分布的具体符合算法的程序，并利用这一程序导出了一系列关于代理引发的协变量转变的易处理的算法。我们在合成黑盒优化和主动学习任务上对所提出的算法进行了实证评估。 <div>
As machine learning (ML) gains widespread adoption, practitioners are
increasingly seeking means to quantify and control the risk these systems
incur. This challenge is especially salient when ML systems have autonomy to
collect their own data, such as in black-box optimization and active learning,
where their actions induce sequential feedback-loop shifts in the data
distribution. Conformal prediction has emerged as a promising approach to
uncertainty and risk quantification, but existing variants either fail to
accommodate sequences of data-dependent shifts, or do not fully exploit the
fact that agent-induced shift is under our control. In this work we prove that
conformal prediction can theoretically be extended to \textit{any} joint data
distribution, not just exchangeable or quasi-exchangeable ones, although it is
exceedingly impractical to compute in the most general case. For practical
applications, we outline a procedure for deriving specific conformal algorithms
for any data distribution, and we use this procedure to derive tractable
algorithms for a series of agent-induced covariate shifts. We evaluate the
proposed algorithms empirically on synthetic black-box optimization and active
learning tasks.
]]></content:encoded>
<pubDate>2024-05-10T17:40:24Z</pubDate>
</item>
<item>
<title>Characterizing the Accuracy - Efficiency Trade-off of Low-rank
  Decomposition in Language Models</title>
<link>http://arxiv.org/abs/2405.06626v1</link>
<guid>http://arxiv.org/abs/2405.06626v1</guid>
<content:encoded><![CDATA[
<div> 低秩分解、大型语言模型、内存优化、模型压缩、精度-效率权衡
<br /><br />
在大型语言模型中，模型大小的激增使得计算与模型大小的比率大幅降低，从而将大型语言模型推向内存受限的领域。为了优化内存占用和流量，研究者们积极探索模型压缩方法，如量化和参数修剪。然而，针对低秩分解在大型语言模型中的精度与效率权衡尚未得到充分理解。因此，本文对低秩分解方法，特别是Tucker分解，在最近的语言模型上进行了精度与效率权衡的研究。通过对BERT和Llama 2模型在六个广泛使用的语言模型基准上进行案例研究，结果显示，我们可以在几乎不影响精度的情况下实现9%的模型大小减少。这表明低秩分解可能是大型语言模型应用的一个有前景的方向，尤其是对于需要实时服务并且延迟和模型精度同等重要的应用场景，比如人工智能辅助和实时编程助手。 
<br />
总结: <br />
1. 模型压缩是为了优化大型语言模型的内存占用和流量。
2. 低秩分解方法有望在不影响精度的情况下减小模型大小。
3. 通过对Tucker分解在BERT和Llama 2模型上进行研究，证明了低秩分解方法的潜力。 <div>
Large language models (LLMs) have emerged and presented their general
problem-solving capabilities with one model. However, the model size has
increased dramatically with billions of parameters to enable such broad
problem-solving capabilities. In addition, due to the dominance of
matrix-matrix and matrix-vector multiplications in LLMs, the compute-to-model
size ratio is significantly lower than that of CNNs. This shift pushes LLMs
from a computation-bound regime to a memory-bound regime. Therefore, optimizing
the memory footprint and traffic is an important optimization direction for
LLMs today.
  Model compression methods such as quantization and parameter pruning have
been actively explored for achieving the memory footprint and traffic
optimization. However, the accuracy-efficiency trade-off of rank pruning for
LLMs is not well-understood yet. Therefore, we characterize the
accuracy-efficiency trade-off of a low-rank decomposition method, specifically
Tucker decomposition, on recent language models, including an open-source LLM,
Llama 2.
  We formalize the low-rank decomposition design space and show that the
decomposition design space is enormous (e.g., O($2^{37}$) for Llama2-7B). To
navigate such a vast design space, we formulate the design space and perform
thorough case studies of accuracy-efficiency trade-offs using six widely used
LLM benchmarks on BERT and Llama 2 models. Our results show that we can achieve
a 9\% model size reduction with minimal accuracy drops, which range from 4\%p
to 10\%p, depending on the difficulty of the benchmark, without any retraining
to recover accuracy after decomposition. The results show that low-rank
decomposition can be a promising direction for LLM-based applications that
require real-time service in scale (e.g., AI agent assist and real-time coding
assistant), where the latency is as important as the model accuracy.
]]></content:encoded>
<pubDate>2024-05-10T17:40:02Z</pubDate>
</item>
<item>
<title>Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency
  for Tool Planning</title>
<link>http://arxiv.org/abs/2405.05955v1</link>
<guid>http://arxiv.org/abs/2405.05955v1</guid>
<content:encoded><![CDATA[
<div> 模型、Smurfs、多智能体框架、任务分解、工具利用<br />
在本文中，介绍了一个名为"Smurfs"的多智能体框架，旨在将传统的大型语言模型转化为协同作用的多智能体集合，以提高任务分解和执行效率。该框架通过创新的提示策略，在模型内分配不同的角色，从而促进专业智能体之间的合作。实证研究以mistral-7b-instruct模型为案例，展示了Smurfs在复杂工具利用场景中的卓越能力。值得注意的是，在ToolBench I2和I3基准测试中，Smurfs以惊人的84.4%胜率超过了ChatGPT-ReACT和GPT-4模型的最高记录。此外，通过全面的割除研究，验证了多智能体框架核心组件对其整体有效性的贡献，并为未来探索多智能体大型语言模型系统铺平了道路。 <br /><br />总结: <br />模型的介绍，Smurfs框架的设计目的和主要功能<br />基于mistral-7b-instruct模型的应用实证研究，展示了Smurfs框架在复杂工具利用场景中的优越能力和表现<br />通过全面的割除研究，验证了多智能体框架核心组件对其整体有效性的贡献<br />对比了Smurfs和其他模型在ToolBench I2和I3基准测试中的表现，突出了Smurfs的优越性能<br />为未来探索多智能体大型语言模型系统提供了新的思路和方向 <div>
The emergence of large language models (LLMs) has opened up unprecedented
possibilities for automating complex tasks that are often comparable to human
performance. Despite their capabilities, LLMs still encounter difficulties in
completing tasks that require high levels of accuracy and complexity due to
their inherent limitations in handling multifaceted problems single-handedly.
This paper introduces "Smurfs", a cutting-edge multi-agent framework designed
to revolutionize the application of LLMs. By transforming a conventional LLM
into a synergistic multi-agent ensemble, Smurfs enhances task decomposition and
execution without necessitating extra training. This is achieved through
innovative prompting strategies that allocate distinct roles within the model,
thereby facilitating collaboration among specialized agents. The framework
gives access to external tools to efficiently solve complex tasks. Our
empirical investigation, featuring the mistral-7b-instruct model as a case
study, showcases Smurfs' superior capability in intricate tool utilization
scenarios. Notably, Smurfs outmatches the ChatGPT-ReACT in the ToolBench I2 and
I3 benchmark with a remarkable 84.4% win rate, surpassing the highest recorded
performance of a GPT-4 model at 73.5%. Furthermore, through comprehensive
ablation studies, we dissect the contribution of the core components of the
multi-agent framework to its overall efficacy. This not only verifies the
effectiveness of the framework, but also sets a route for future exploration of
multi-agent LLM systems.
]]></content:encoded>
<pubDate>2024-05-09T17:49:04Z</pubDate>
</item>
<item>
<title>Frame Interpolation with Consecutive Brownian Bridge Diffusion</title>
<link>http://arxiv.org/abs/2405.05953v1</link>
<guid>http://arxiv.org/abs/2405.05953v1</guid>
<content:encoded><![CDATA[
<div> Latent Diffusion Models, Video Frame Interpolation, Conditional Image Generation, Random Generation, Deterministic Output
总结: 最近视频帧插值的研究采用了基于扩散的条件图像生成方法，利用潜在扩散模型作为条件生成模型。然而，潜在扩散模型会产生多样化的输出，导致输出不确定性。为了解决这个问题，研究提出了连续布朗桥扩散方法，能够减小生成的累积方差，从而改善视频帧插值的性能。 <div>
Recent work in Video Frame Interpolation (VFI) tries to formulate VFI as a
diffusion-based conditional image generation problem, synthesizing the
intermediate frame given a random noise and neighboring frames. Due to the
relatively high resolution of videos, Latent Diffusion Models (LDMs) are
employed as the conditional generation model, where the autoencoder compresses
images into latent representations for diffusion and then reconstructs images
from these latent representations. Such a formulation poses a crucial
challenge: VFI expects that the output is deterministically equal to the ground
truth intermediate frame, but LDMs randomly generate a diverse set of different
images when the model runs multiple times. The reason for the diverse
generation is that the cumulative variance (variance accumulated at each step
of generation) of generated latent representations in LDMs is large. This makes
the sampling trajectory random, resulting in diverse rather than deterministic
generations. To address this problem, we propose our unique solution: Frame
Interpolation with Consecutive Brownian Bridge Diffusion. Specifically, we
propose consecutive Brownian Bridge diffusion that takes a deterministic
initial value as input, resulting in a much smaller cumulative variance of
generated latent representations. Our experiments suggest that our method can
improve together with the improvement of the autoencoder and achieve
state-of-the-art performance in VFI, leaving strong potential for further
enhancement.
]]></content:encoded>
<pubDate>2024-05-09T17:46:22Z</pubDate>
</item>
<item>
<title>LLMs with Personalities in Multi-issue Negotiation Games</title>
<link>http://arxiv.org/abs/2405.05248v2</link>
<guid>http://arxiv.org/abs/2405.05248v2</guid>
<content:encoded><![CDATA[
<div> 人工智能, 大语言模型, 谈判, 个性特质, 游戏理论
<br />
这篇文章使用大型语言模型（LLMs）来调查人工智能代理人在谈判中的表现，测量他们对游戏理论中的谈判能力以及衡量公平和风险概念的方法论挑战。研究使用模拟方法进行了1500次单问题和多问题谈判的实验，结果显示对称性和不对称性的议题估值增加了领域复杂性，提高了协议达成率，但减少了侵略性谈判的剩余价值。通过梯度提升回归和Shapley解释器，研究发现高度的开放性、尽责性和神经质与公平倾向相关，而低宜人性和低开放性与理性倾向相关。低尽责性与高毒性相关。这些结果表明LLMs可能具有内置的防护措施，以默认公平行为，但可以"越狱"以利用易于相处的对手。此外，研究还提出了关于如何设计谈判机器人以及基于博弈论和计算社会科学的谈判行为评估框架的实用见解。
<br /><br />总结: 本研究使用大型语言模型检验了人工智能代理人在谈判中的表现，并得出了一些重要结果。通过实验发现了LLMs在谈判中的行为特质，以及如何设计谈判机器人和评估谈判行为的框架。 <div>
Powered by large language models (LLMs), AI agents have become capable of
many human tasks. Using the most canonical definitions of the Big Five
personality, we measure the ability of LLMs to negotiate within a
game-theoretical framework, as well as methodological challenges to measuring
notions of fairness and risk. Simulations (n=1,500) for both single-issue and
multi-issue negotiation reveal increase in domain complexity with asymmetric
issue valuations improve agreement rates but decrease surplus from aggressive
negotiation. Through gradient-boosted regression and Shapley explainers, we
find high openness, conscientiousness, and neuroticism are associated with fair
tendencies; low agreeableness and low openness are associated with rational
tendencies. Low conscientiousness is associated with high toxicity. These
results indicate that LLMs may have built-in guardrails that default to fair
behavior, but can be "jail broken" to exploit agreeable opponents. We also
offer pragmatic insight in how negotiation bots can be designed, and a
framework of assessing negotiation behavior based on game theory and
computational social science.
]]></content:encoded>
<pubDate>2024-05-09T01:09:09Z</pubDate>
</item>
<item>
<title>Diffusion-HMC: Parameter Inference with Diffusion Model driven
  Hamiltonian Monte Carlo</title>
<link>http://arxiv.org/abs/2405.05255v1</link>
<guid>http://arxiv.org/abs/2405.05255v1</guid>
<content:encoded><![CDATA[
<div> 生成模型，宇宙学，参数推断，暗物质密度场，哈密顿蒙特卡洛<br />
<br />
本研究利用扩散生成模型作为宇宙学参数推断模型和冷暗物质密度场的仿真模型。该模型能够仿真与目标分布一致的总结统计信息的场景。进一步利用扩散生成模型的近似似然性质，通过哈密顿蒙特卡洛方法对给定的测试图像进行宇宙学参数的后验采样，从而获得对宇宙学的紧密限制。最后，本研究证明了与基线参数推断网络相比，这种参数推断方法对噪声的影响更为稳健。 <br /><br />总结: <br />生成模型在宇宙学中的应用，模拟暗物质密度场，参数推断，哈密顿蒙特卡洛方法，对噪声的鲁棒性。 <div>
Diffusion generative models have excelled at diverse image generation and
reconstruction tasks across fields. A less explored avenue is their application
to discriminative tasks involving regression or classification problems. The
cornerstone of modern cosmology is the ability to generate predictions for
observed astrophysical fields from theory and constrain physical models from
observations using these predictions. This work uses a single diffusion
generative model to address these interlinked objectives -- as a surrogate
model or emulator for cold dark matter density fields conditional on input
cosmological parameters, and as a parameter inference model that solves the
inverse problem of constraining the cosmological parameters of an input field.
The model is able to emulate fields with summary statistics consistent with
those of the simulated target distribution. We then leverage the approximate
likelihood of the diffusion generative model to derive tight constraints on
cosmology by using the Hamiltonian Monte Carlo method to sample the posterior
on cosmological parameters for a given test image. Finally, we demonstrate that
this parameter inference approach is more robust to the addition of noise than
baseline parameter inference networks.
]]></content:encoded>
<pubDate>2024-05-08T17:59:03Z</pubDate>
</item>
<item>
<title>LLMs with Personalities in Multi-issue Negotiation Games</title>
<link>http://arxiv.org/abs/2405.05248v1</link>
<guid>http://arxiv.org/abs/2405.05248v1</guid>
<content:encoded><![CDATA[
<div> Negotiation, Large Language Models, Personality, Fairness, Risk

AI代理人通过大型语言模型（LLM）已经可以完成很多人类任务。本研究使用大五人格的经典定义，衡量了LLM在博弈理论框架内进行谈判的能力，以及衡量公平和风险概念的方法论挑战。针对单一议题和多议题谈判的1500次模拟显示，领域复杂性的增加以及对称议题价值的不平衡会提高协议率，但会降低激进谈判的剩余价值。通过梯度提升回归和Shapley解释器，我们发现高的开放性、责任心和神经质与公平倾向相关；低的宜人性和低的开放性与理性倾向相关。低的责任心与高的毒性相关。这些结果表明，LLM可能具有默认公平行为的内置防护，但可以被"越狱"以利用易被说服的对手。我们还提供了关于如何设计谈判机器人以及基于博弈论和计算社会科学的评估谈判行为的实用见解。 <br /><br />总结: 本研究通过大型语言模型对谈判能力进行了量化分析，发现LLM可能内置了默认公平行为的保障，但也可以被“越狱”以利用易被说服的对手。同时，提出了设计谈判机器人和评估谈判行为的实用见解。 <div>
Powered by large language models (LLMs), AI agents have become capable of
many human tasks. Using the most canonical definitions of the Big Five
personality, we measure the ability of LLMs to negotiate within a
game-theoretical framework, as well as methodological challenges to measuring
notions of fairness and risk. Simulations (n=1,500) for both single-issue and
multi-issue negotiation reveal increase in domain complexity with asymmetric
issue valuations improve agreement rates but decrease surplus from aggressive
negotiation. Through gradient-boosted regression and Shapley explainers, we
find high openness, conscientiousness, and neuroticism are associated with fair
tendencies; low agreeableness and low openness are associated with rational
tendencies. Low conscientiousness is associated with high toxicity. These
results indicate that LLMs may have built-in guardrails that default to fair
behavior, but can be "jail broken" to exploit agreeable opponents. We also
offer pragmatic insight in how negotiation bots can be designed, and a
framework of assessing negotiation behavior based on game theory and
computational social science.
]]></content:encoded>
<pubDate>2024-05-08T17:51:53Z</pubDate>
</item>
<item>
<title>Pose Priors from Language Models</title>
<link>http://arxiv.org/abs/2405.03689v1</link>
<guid>http://arxiv.org/abs/2405.03689v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 零-shot姿势优化, 3D姿势估计, 文本模型, 物理接触约束, 社交互动

总结: 
零-shot姿势优化是一种利用大型预训练文本模型的先验知识来改进3D姿势估计的方法。通过将自然语言描述转换为可控制的损失，可以约束3D姿势优化，从而正确捕捉人们在近距离互动时的语义。该方法与更复杂的最先进方法相媲美，而且不需要昂贵的人工标注接触点和训练专门模型。此外，与以往的方法不同，该方法提供了一个统一框架来解决自身接触和人与人之间的接触。 <div>
We present a zero-shot pose optimization method that enforces accurate
physical contact constraints when estimating the 3D pose of humans. Our central
insight is that since language is often used to describe physical interaction,
large pretrained text-based models can act as priors on pose estimation.
  We can thus leverage this insight to improve pose estimation by converting
natural language descriptors, generated by a large multimodal model (LMM), into
tractable losses to constrain the 3D pose optimization. Despite its simplicity,
our method produces surprisingly compelling pose reconstructions of people in
close contact, correctly capturing the semantics of the social and physical
interactions. We demonstrate that our method rivals more complex
state-of-the-art approaches that require expensive human annotation of contact
points and training specialized models. Moreover, unlike previous approaches,
our method provides a unified framework for resolving self-contact and
person-to-person contact.
]]></content:encoded>
<pubDate>2024-05-06T17:59:36Z</pubDate>
</item>
<item>
<title>Vibe-Eval: A hard evaluation suite for measuring progress of multimodal
  language models</title>
<link>http://arxiv.org/abs/2405.02287v1</link>
<guid>http://arxiv.org/abs/2405.02287v1</guid>
<content:encoded><![CDATA[
<div> 评估框架, 多模态对话模型, 困难难度, 人工评估, 自动评估

评估框架Vibe-Eval是一个新的开放式基准测试和评估多模态对话模型的框架。它包括269个视觉理解提示，其中包括100个高难度提示，并配有专家撰写的标准答案。该框架旨在进行日常任务的多模态对话模型的评估，并严格测试和探索当前前沿模型的能力。特别地，高难度提示集中包含了超过50%的问题，所有前沿模型都回答错误。文章探讨了设计、评估和排名模型在极具挑战性提示上的微妙之处。同时讨论了人工评估和自动评估之间的权衡，并显示了使用Reka Core进行自动模型评估与人类判断之间的大致相关性。为了轻量级评估目的，他们提供了免费的API访问，并计划对在Vibe-Eval的自动评分上表现良好的公共模型进行正式的人工评估。他们还发布了评估代码和数据，网址为https://github.com/reka-ai/reka-vibe-eval。<br /><br />总结: 评估框架Vibe-Eval旨在评估多模态对话模型的日常任务能力，包含了大量高难度提示，并探讨了人工评估和自动评估之间的相互关系。他们提供了免费的API访问，并计划对在自动评分上表现良好的公共模型进行正式的人工评估。 <div>
We introduce Vibe-Eval: a new open benchmark and framework for evaluating
multimodal chat models. Vibe-Eval consists of 269 visual understanding prompts,
including 100 of hard difficulty, complete with gold-standard responses
authored by experts. Vibe-Eval is open-ended and challenging with dual
objectives: (i) vibe checking multimodal chat models for day-to-day tasks and
(ii) rigorously testing and probing the capabilities of present frontier
models. Notably, our hard set contains >50% questions that all frontier models
answer incorrectly. We explore the nuances of designing, evaluating, and
ranking models on ultra challenging prompts. We also discuss trade-offs between
human and automatic evaluation, and show that automatic model evaluation using
Reka Core roughly correlates to human judgment. We offer free API access for
the purpose of lightweight evaluation and plan to conduct formal human
evaluations for public models that perform well on the Vibe-Eval's automatic
scores. We release the evaluation code and data, see
https://github.com/reka-ai/reka-vibe-eval
]]></content:encoded>
<pubDate>2024-05-03T17:59:55Z</pubDate>
</item>
<item>
<title>Geometric Fabrics: a Safe Guiding Medium for Policy Learning</title>
<link>http://arxiv.org/abs/2405.02250v1</link>
<guid>http://arxiv.org/abs/2405.02250v1</guid>
<content:encoded><![CDATA[
<div> 强化学习, 机器人政策, 控制器, 二阶动力学, 几何结构

强化学习中的机器人政策必须解密复杂的交互作用，学习如何完成任务。传统控制器只能实现直线运动，无法捕捉机器人需要展现的非线性行为，因此引入了几何结构来实现更丰富和期望的行为。通过人工的二阶动力学来控制机器人的未受控动力学，形成行为动力学，从而解锁新的行为空间并帮助训练强化学习政策。这种行为动力学使得强化学习政策能够安全地执行动作，简化奖励设计，并帮助组合高性能政策。文章将这个框架具体应用到一个问题上，即通过高度驱动的机器人手对一个立方体进行灵巧的手部重新定位。 <div>
Robotics policies are always subjected to complex, second order dynamics that
entangle their actions with resulting states. In reinforcement learning (RL)
contexts, policies have the burden of deciphering these complicated
interactions over massive amounts of experience and complex reward functions to
learn how to accomplish tasks. Moreover, policies typically issue actions
directly to controllers like Operational Space Control (OSC) or joint PD
control, which induces straightline motion towards these action targets in task
or joint space. However, straightline motion in these spaces for the most part
do not capture the rich, nonlinear behavior our robots need to exhibit,
shifting the burden of discovering these behaviors more completely to the
agent. Unlike these simpler controllers, geometric fabrics capture a much
richer and desirable set of behaviors via artificial, second order dynamics
grounded in nonlinear geometry. These artificial dynamics shift the
uncontrolled dynamics of a robot via an appropriate control law to form
behavioral dynamics. Behavioral dynamics unlock a new action space and safe,
guiding behavior over which RL policies are trained. Behavioral dynamics enable
bang-bang-like RL policy actions that are still safe for real robots, simplify
reward engineering, and help sequence real-world, high-performance policies. We
describe the framework more generally and create a specific instantiation for
the problem of dexterous, in-hand reorientation of a cube by a highly actuated
robot hand.
]]></content:encoded>
<pubDate>2024-05-03T17:07:45Z</pubDate>
</item>
<item>
<title>Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon
  Robotics Tasks</title>
<link>http://arxiv.org/abs/2405.01534v1</link>
<guid>http://arxiv.org/abs/2405.01534v1</guid>
<content:encoded><![CDATA[
<div> 高级语言模型，机器人任务规划，技能库，机器人控制任务，Plan-Seq-Learn。

高级语言模型（LLMs）已经表明能够执行长期规划的机器人任务，然而现有方法需要访问预定义的技能库（例如拾取、放置、拉动、推动、导航等）。然而，LLM规划并不解决如何设计或学习这些行为，尤其是在长期规划环境中仍然具有挑战性。此外，对于许多感兴趣的任务，机器人需要能够以细粒度的方式调整其行为，这要求代理能够修改低级控制动作。我们是否可以利用LLMs的互联网规模知识来指导强化学习（RL）策略，从而在不需要预先确定的技能集的情况下在线有效解决机器人控制任务？在本文中，我们提出了Plan-Seq-Learn（PSL）：一种模块化方法，利用运动规划来弥合摘要语言与学习低级控制之间的鸿沟，从头开始解决长期规划的机器人任务。我们展示了PSL在超过25项具有10个阶段的挑战性机器人任务上实现了最先进的成果。PSL利用原始视觉输入解决长期规划任务，覆盖了四个基准任务，在成功率超过85%，优于基于语言、经典和端到端方法。视频结果和代码可在https://mihdalal.github.io/planseqlearn/找到。<br /><br />总结: 高级语言模型在机器人领域具有潜在应用，但现有方法存在一些挑战，Plan-Seq-Learn提出了一种新的模块化方法，有效地解决了长期规划的机器人任务，取得了很好的结果。 <div>
Large Language Models (LLMs) have been shown to be capable of performing
high-level planning for long-horizon robotics tasks, yet existing methods
require access to a pre-defined skill library (e.g. picking, placing, pulling,
pushing, navigating). However, LLM planning does not address how to design or
learn those behaviors, which remains challenging particularly in long-horizon
settings. Furthermore, for many tasks of interest, the robot needs to be able
to adjust its behavior in a fine-grained manner, requiring the agent to be
capable of modifying low-level control actions. Can we instead use the
internet-scale knowledge from LLMs for high-level policies, guiding
reinforcement learning (RL) policies to efficiently solve robotic control tasks
online without requiring a pre-determined set of skills? In this paper, we
propose Plan-Seq-Learn (PSL): a modular approach that uses motion planning to
bridge the gap between abstract language and learned low-level control for
solving long-horizon robotics tasks from scratch. We demonstrate that PSL
achieves state-of-the-art results on over 25 challenging robotics tasks with up
to 10 stages. PSL solves long-horizon tasks from raw visual input spanning four
benchmarks at success rates of over 85%, out-performing language-based,
classical, and end-to-end approaches. Video results and code at
https://mihdalal.github.io/planseqlearn/
]]></content:encoded>
<pubDate>2024-05-02T17:59:31Z</pubDate>
</item>
<item>
<title>OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D
  Perception, Reasoning and Planning</title>
<link>http://arxiv.org/abs/2405.01533v1</link>
<guid>http://arxiv.org/abs/2405.01533v1</guid>
<content:encoded><![CDATA[
<div> 3D MLLM，autonomous driving agents，OmniDrive-nuScenes，visual question-answering，3D situational awareness<br />
<br />
提出了一个整体框架，通过使用稀疏查询将视觉表示转换成3D形式，结合动态对象和静态地图元素的信息，为3D世界模型提供压缩表示，以实现3D感知-行动对齐。还提出了OmniDrive-nuScenes数据集，通过视觉问答等多种任务评估模型的真实3D情境认知能力。研究表明，所提出的架构有效性强，同时视觉问答任务对复杂3D场景中的推理和规划至关重要。<br /><br />总结: <div>
The advances in multimodal large language models (MLLMs) have led to growing
interests in LLM-based autonomous driving agents to leverage their strong
reasoning capabilities. However, capitalizing on MLLMs' strong reasoning
capabilities for improved planning behavior is challenging since planning
requires full 3D situational awareness beyond 2D reasoning. To address this
challenge, our work proposes a holistic framework for strong alignment between
agent models and 3D driving tasks. Our framework starts with a novel 3D MLLM
architecture that uses sparse queries to lift and compress visual
representations into 3D before feeding them into an LLM. This query-based
representation allows us to jointly encode dynamic objects and static map
elements (e.g., traffic lanes), providing a condensed world model for
perception-action alignment in 3D. We further propose OmniDrive-nuScenes, a new
visual question-answering dataset challenging the true 3D situational awareness
of a model with comprehensive visual question-answering (VQA) tasks, including
scene description, traffic regulation, 3D grounding, counterfactual reasoning,
decision making and planning. Extensive studies show the effectiveness of the
proposed architecture as well as the importance of the VQA tasks for reasoning
and planning in complex 3D scenes.
]]></content:encoded>
<pubDate>2024-05-02T17:59:24Z</pubDate>
</item>
<item>
<title>No Representation, No Trust: Connecting Representation, Collapse, and
  Trust Issues in PPO</title>
<link>http://arxiv.org/abs/2405.00662v1</link>
<guid>http://arxiv.org/abs/2405.00662v1</guid>
<content:encoded><![CDATA[
<div> 非稳态、深度强化学习、表示动态、Proximal Policy Optimization、性能崩溃<br />
在这项工作中，研究人员对Proximal Policy Optimization（PPO）在Atari和MuJoCo环境中的表示动态进行了实证研究，发现PPO代理也受到特征排名恶化和可塑性丧失的影响。他们展示了这种情况在非稳态性较强时会加剧，最终导致actor的性能崩溃，而与评论家的性能无关。作者还指出了表示崩溃、性能崩溃和PPO中的信任区域问题之间的联系，并提出了一种新的辅助损失Proximal Feature Optimization（PFO），通过其他干预措施表明，规范化表示动态可以改善PPO代理的性能。<br /><br />总结: 非稳态环境中，深度强化学习中的表示动态对性能具有重要影响，特别是在Proximal Policy Optimization中，这种影响会导致性能崩溃。通过实证研究，研究人员发现了这种影响，并提出了Proximal Feature Optimization作为解决方案，通过规范化表示动态来改善代理的性能。 <div>
Reinforcement learning (RL) is inherently rife with non-stationarity since
the states and rewards the agent observes during training depend on its
changing policy. Therefore, networks in deep RL must be capable of adapting to
new observations and fitting new targets. However, previous works have observed
that networks in off-policy deep value-based methods exhibit a decrease in
representation rank, often correlated with an inability to continue learning or
a collapse in performance. Although this phenomenon has generally been
attributed to neural network learning under non-stationarity, it has been
overlooked in on-policy policy optimization methods which are often thought
capable of training indefinitely. In this work, we empirically study
representation dynamics in Proximal Policy Optimization (PPO) on the Atari and
MuJoCo environments, revealing that PPO agents are also affected by feature
rank deterioration and loss of plasticity. We show that this is aggravated with
stronger non-stationarity, ultimately driving the actor's performance to
collapse, regardless of the performance of the critic. We draw connections
between representation collapse, performance collapse, and trust region issues
in PPO, and present Proximal Feature Optimization (PFO), a novel auxiliary
loss, that along with other interventions shows that regularizing the
representation dynamics improves the performance of PPO agents.
]]></content:encoded>
<pubDate>2024-05-01T17:50:16Z</pubDate>
</item>
<item>
<title>DOCCI: Descriptions of Connected and Contrasting Images</title>
<link>http://arxiv.org/abs/2404.19753v1</link>
<guid>http://arxiv.org/abs/2404.19753v1</guid>
<content:encoded><![CDATA[
<div> 视觉-语言数据集；描述；DOCCI；图像到文本生成；文本到图像生成

<br /><br />总结:
视觉-语言数据集对于文本到图像（T2I）和图像到文本（I2T）研究至关重要。然而，目前的数据集缺乏细致详细的描述，这些描述可以让模型学习到更丰富的关联信息。为了填补这一空白，我们介绍了DOCCI（Descriptions of Connected and Contrasting Images），这是一个包含长篇人工注释英文描述的数据集，涵盖了1.5万张图像。我们指示人工注释者为每个图像创建全面的描述，这些描述平均有136个词，并且被精心设计，以清晰地区分每个图像与相关或相似的其他图像。通过定量和定性分析，我们证明DOCCI是一个有效的图像到文本生成训练资源。此外，我们还展示了DOCCI对于文本到图像生成的有用性，并凸显了当前文本到图像模型在捕捉长篇描述和细节方面的局限性。 <div>
Vision-language datasets are vital for both text-to-image (T2I) and
image-to-text (I2T) research. However, current datasets lack descriptions with
fine-grained detail that would allow for richer associations to be learned by
models. To fill the gap, we introduce Descriptions of Connected and Contrasting
Images (DOCCI), a dataset with long, human-annotated English descriptions for
15k images that were taken, curated and donated by a single researcher intent
on capturing key challenges such as spatial relations, counting, text
rendering, world knowledge, and more. We instruct human annotators to create
comprehensive descriptions for each image; these average 136 words in length
and are crafted to clearly distinguish each image from those that are related
or similar. Each description is highly compositional and typically encompasses
multiple challenges. Through both quantitative and qualitative analyses, we
demonstrate that DOCCI serves as an effective training resource for
image-to-text generation -- a PaLI 5B model finetuned on DOCCI shows equal or
superior results compared to highly-performant larger models like LLaVA-1.5 7B
and InstructBLIP 7B. Furthermore, we show that DOCCI is a useful testbed for
text-to-image generation, highlighting the limitations of current text-to-image
models in capturing long descriptions and fine details.
]]></content:encoded>
<pubDate>2024-04-30T17:56:24Z</pubDate>
</item>
<item>
<title>Hallucination of Multimodal Large Language Models: A Survey</title>
<link>http://arxiv.org/abs/2404.18930v1</link>
<guid>http://arxiv.org/abs/2404.18930v1</guid>
<content:encoded><![CDATA[
<div> 语言模型；视觉-语言模型；幻觉；评估方法；挑战

幻觉在多模态大语言模型（MLLMs）中是一个严重的问题，也被称为大视觉-语言模型（LVLMs）。文章综述了在MLLMs中出现幻觉的原因、评估方法以及缓解策略。同时指出了当前挑战和限制，并提出了未来研究的方向。通过深入的分析和细致的分类，为进一步提升MLLMs的健壮性和可靠性提供了宝贵的见解和资源。 <div>
This survey presents a comprehensive analysis of the phenomenon of
hallucination in multimodal large language models (MLLMs), also known as Large
Vision-Language Models (LVLMs), which have demonstrated significant
advancements and remarkable abilities in multimodal tasks. Despite these
promising developments, MLLMs often generate outputs that are inconsistent with
the visual content, a challenge known as hallucination, which poses substantial
obstacles to their practical deployment and raises concerns regarding their
reliability in real-world applications. This problem has attracted increasing
attention, prompting efforts to detect and mitigate such inaccuracies. We
review recent advances in identifying, evaluating, and mitigating these
hallucinations, offering a detailed overview of the underlying causes,
evaluation benchmarks, metrics, and strategies developed to address this issue.
Additionally, we analyze the current challenges and limitations, formulating
open questions that delineate potential pathways for future research. By
drawing the granular classification and landscapes of hallucination causes,
evaluation benchmarks, and mitigation methods, this survey aims to deepen the
understanding of hallucinations in MLLMs and inspire further advancements in
the field. Through our thorough and in-depth review, we contribute to the
ongoing dialogue on enhancing the robustness and reliability of MLLMs,
providing valuable insights and resources for researchers and practitioners
alike. Resources are available at:
https://github.com/showlab/Awesome-MLLM-Hallucination.
]]></content:encoded>
<pubDate>2024-04-29T17:59:41Z</pubDate>
</item>
<item>
<title>Stylus: Automatic Adapter Selection for Diffusion Models</title>
<link>http://arxiv.org/abs/2404.18928v1</link>
<guid>http://arxiv.org/abs/2404.18928v1</guid>
<content:encoded><![CDATA[
<div> Stylus, adapters, fine-tuned, matching, prompt

总结: 本文介绍了使用适配器来生成高保真度定制图像的替代方法，以降低成本。文章探讨了将适配器与提示匹配的问题，并提出了一种名为Stylus的解决方案。Stylus通过三个阶段的方法来高效地选择和自动组合任务特定的适配器，从而提高了模型的性能，并在评估中取得了良好的效果。作者还开发了一个包含75K适配器的数据集StylusDocs，并在常见的Stable Diffusion检查点上进行了评估，结果表明Stylus在CLIP-FID Pareto效率方面表现更好，并且被人类和多模型评估者比基础模型更受欢迎。 <div>
Beyond scaling base models with more data or parameters, fine-tuned adapters
provide an alternative way to generate high fidelity, custom images at reduced
costs. As such, adapters have been widely adopted by open-source communities,
accumulating a database of over 100K adapters-most of which are highly
customized with insufficient descriptions. This paper explores the problem of
matching the prompt to a set of relevant adapters, built on recent work that
highlight the performance gains of composing adapters. We introduce Stylus,
which efficiently selects and automatically composes task-specific adapters
based on a prompt's keywords. Stylus outlines a three-stage approach that first
summarizes adapters with improved descriptions and embeddings, retrieves
relevant adapters, and then further assembles adapters based on prompts'
keywords by checking how well they fit the prompt. To evaluate Stylus, we
developed StylusDocs, a curated dataset featuring 75K adapters with
pre-computed adapter embeddings. In our evaluation on popular Stable Diffusion
checkpoints, Stylus achieves greater CLIP-FID Pareto efficiency and is twice as
preferred, with humans and multimodal models as evaluators, over the base
model. See stylus-diffusion.github.io for more.
]]></content:encoded>
<pubDate>2024-04-29T17:59:16Z</pubDate>
</item>
<item>
<title>TheaterGen: Character Management with LLM for Consistent Multi-turn
  Image Generation</title>
<link>http://arxiv.org/abs/2404.18919v1</link>
<guid>http://arxiv.org/abs/2404.18919v1</guid>
<content:encoded><![CDATA[
Recent advances in diffusion models can generate high-quality and stunning
images from text. However, multi-turn image generation, which is of high demand
in real-world scenarios, still faces challenges in maintaining semantic
consistency between images and texts, as well as contextual consistency of the
same subject across multiple interactive turns. To address this issue, we
introduce TheaterGen, a training-free framework that integrates large language
models (LLMs) and text-to-image (T2I) models to provide the capability of
multi-turn image generation. Within this framework, LLMs, acting as a
"Screenwriter", engage in multi-turn interaction, generating and managing a
standardized prompt book that encompasses prompts and layout designs for each
character in the target image. Based on these, Theatergen generate a list of
character images and extract guidance information, akin to the "Rehearsal".
Subsequently, through incorporating the prompt book and guidance information
into the reverse denoising process of T2I diffusion models, Theatergen generate
the final image, as conducting the "Final Performance". With the effective
management of prompt books and character images, TheaterGen significantly
improves semantic and contextual consistency in synthesized images.
Furthermore, we introduce a dedicated benchmark, CMIGBench (Consistent
Multi-turn Image Generation Benchmark) with 8000 multi-turn instructions.
Different from previous multi-turn benchmarks, CMIGBench does not define
characters in advance. Both the tasks of story generation and multi-turn
editing are included on CMIGBench for comprehensive evaluation. Extensive
experimental results show that TheaterGen outperforms state-of-the-art methods
significantly. It raises the performance bar of the cutting-edge Mini DALLE 3
model by 21% in average character-character similarity and 19% in average
text-image similarity.
]]></content:encoded>
<pubDate>2024-04-29T17:58:14Z</pubDate>
</item>
<item>
<title>Sample-Efficient Robust Multi-Agent Reinforcement Learning in the Face
  of Environmental Uncertainty</title>
<link>http://arxiv.org/abs/2404.18909v1</link>
<guid>http://arxiv.org/abs/2404.18909v1</guid>
<content:encoded><![CDATA[
To overcome the sim-to-real gap in reinforcement learning (RL), learned
policies must maintain robustness against environmental uncertainties. While
robust RL has been widely studied in single-agent regimes, in multi-agent
environments, the problem remains understudied -- despite the fact that the
problems posed by environmental uncertainties are often exacerbated by
strategic interactions. This work focuses on learning in distributionally
robust Markov games (RMGs), a robust variant of standard Markov games, wherein
each agent aims to learn a policy that maximizes its own worst-case performance
when the deployed environment deviates within its own prescribed uncertainty
set. This results in a set of robust equilibrium strategies for all agents that
align with classic notions of game-theoretic equilibria. Assuming a
non-adaptive sampling mechanism from a generative model, we propose a
sample-efficient model-based algorithm (DRNVI) with finite-sample complexity
guarantees for learning robust variants of various notions of game-theoretic
equilibria. We also establish an information-theoretic lower bound for solving
RMGs, which confirms the near-optimal sample complexity of DRNVI with respect
to problem-dependent factors such as the size of the state space, the target
accuracy, and the horizon length.
]]></content:encoded>
<pubDate>2024-04-29T17:51:47Z</pubDate>
</item>
<item>
<title>Learning Visuotactile Skills with Two Multifingered Hands</title>
<link>http://arxiv.org/abs/2404.16823v1</link>
<guid>http://arxiv.org/abs/2404.16823v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 人体灵巧性，双手系统，触觉数据，数据收集，机械手
总结:
人们致力于通过双手系统和视触觉数据来模拟人类的灵巧性、感知体验和运动模式。他们面临两个挑战：缺乏适用于多指手的双臂设置的可负担和可访问的远程操作系统，以及缺乏配备触觉传感器的多指手硬件。为了解决这些问题，他们开发了一种低成本的双手-臂远程操作系统HATO，同时引入了一种新的硬件适应方法，利用触觉传感器装备了两个假肢手来进行研究。利用他们系统收集的视触觉数据，他们学习完成长时间、高精度任务的技能，进一步实证研究了数据集大小、感知模式和视觉输入预处理对策略学习的影响。他们的研究结果标志着从视触觉数据中实现双手多指操作迈出了一大步。 <div>
Aiming to replicate human-like dexterity, perceptual experiences, and motion
patterns, we explore learning from human demonstrations using a bimanual system
with multifingered hands and visuotactile data. Two significant challenges
exist: the lack of an affordable and accessible teleoperation system suitable
for a dual-arm setup with multifingered hands, and the scarcity of
multifingered hand hardware equipped with touch sensing. To tackle the first
challenge, we develop HATO, a low-cost hands-arms teleoperation system that
leverages off-the-shelf electronics, complemented with a software suite that
enables efficient data collection; the comprehensive software suite also
supports multimodal data processing, scalable policy learning, and smooth
policy deployment. To tackle the latter challenge, we introduce a novel
hardware adaptation by repurposing two prosthetic hands equipped with touch
sensors for research. Using visuotactile data collected from our system, we
learn skills to complete long-horizon, high-precision tasks which are difficult
to achieve without multifingered dexterity and touch feedback. Furthermore, we
empirically investigate the effects of dataset size, sensing modality, and
visual input preprocessing on policy learning. Our results mark a promising
step forward in bimanual multifingered manipulation from visuotactile data.
Videos, code, and datasets can be found at https://toruowo.github.io/hato/ .
]]></content:encoded>
<pubDate>2024-04-25T17:59:41Z</pubDate>
</item>
<item>
<title>How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal
  Models with Open-Source Suites</title>
<link>http://arxiv.org/abs/2404.16821v1</link>
<guid>http://arxiv.org/abs/2404.16821v1</guid>
<content:encoded><![CDATA[
<div> 关键词: InternVL 1.5, 多模态大语言模型, 视觉编码器, 高分辨率, 双语数据集

总结:
InternVL 1.5是一个开源的多模态大语言模型，通过引入三项简单的改进，架起了开源和专有商业模型在多模态理解方面的巨大差距。首先是强大的视觉编码器，通过不断学习的策略提升了视觉基础模型InternViT-6B的能力，使其可以在不同的LLM中转移和重复使用。其次是动态高分辨率，将图像根据输入图像的长宽比和分辨率划分为1到40个448×448像素的图块，支持高达4K分辨率的输入。第三是高质量的双语数据集，精心收集了涵盖常见场景、文档图像的双语数据集，并用英文和中文问答对进行注释，显著提升了OCR和中文相关任务的性能。通过一系列基准测试和比较研究，InternVL 1.5表现出了与开源和专有模型的竞争性能，成绩在18项基准测试中有8项达到了最先进的水平。Code已发布在https://github.com/OpenGVLab/InternVL。<br /><br /> <div>
In this report, we introduce InternVL 1.5, an open-source multimodal large
language model (MLLM) to bridge the capability gap between open-source and
proprietary commercial models in multimodal understanding. We introduce three
simple improvements: (1) Strong Vision Encoder: we explored a continuous
learning strategy for the large-scale vision foundation model -- InternViT-6B,
boosting its visual understanding capabilities, and making it can be
transferred and reused in different LLMs. (2) Dynamic High-Resolution: we
divide images into tiles ranging from 1 to 40 of 448$\times$448 pixels
according to the aspect ratio and resolution of the input images, which
supports up to 4K resolution input. (3) High-Quality Bilingual Dataset: we
carefully collected a high-quality bilingual dataset that covers common scenes,
document images, and annotated them with English and Chinese question-answer
pairs, significantly enhancing performance in OCR- and Chinese-related tasks.
We evaluate InternVL 1.5 through a series of benchmarks and comparative
studies. Compared to both open-source and proprietary models, InternVL 1.5
shows competitive performance, achieving state-of-the-art results in 8 of 18
benchmarks. Code has been released at https://github.com/OpenGVLab/InternVL.
]]></content:encoded>
<pubDate>2024-04-25T17:59:19Z</pubDate>
</item>
<item>
<title>DPO: Differential reinforcement learning with application to optimal
  configuration search</title>
<link>http://arxiv.org/abs/2404.15617v1</link>
<guid>http://arxiv.org/abs/2404.15617v1</guid>
<content:encoded><![CDATA[
<div> 强化学习、连续状态空间、连续动作空间、Differential Policy Optimization (DPO)、遗憾上界
<br />
本文提出了第一个可以处理训练样本有限且具有短长度回合的差分强化学习框架DPO。该方法引入了差分策略优化（DPO），这是一种逐点和逐阶段迭代方法，可以优化由局部移动算子编码的策略。研究证明了DPO的逐点收敛估计，并提供了与当前理论研究相当的遗憾上界。此类逐点估计确保了学习的策略在不同步骤间能够均匀匹配最优路径。然后，DPO被应用于一类具有Lagrangian奖励的实际强化学习问题。DPO易于实现，可扩展，并在对比基准实验中表现出与几种流行的强化学习方法相媲美的结果。
<br /><br />总结: <br />本文介绍了一种新的差分强化学习框架DPO，能够处理具有有限训练样本和短长度回合的情况。该方法通过优化局部移动算子编码的策略，实现了逐点收敛和与当前理论研究相当的遗憾上界。在实际应用中，DPO表现出了可扩展性和竞争力。 <div>
Reinforcement learning (RL) with continuous state and action spaces remains
one of the most challenging problems within the field. Most current learning
methods focus on integral identities such as value functions to derive an
optimal strategy for the learning agent. In this paper, we instead study the
dual form of the original RL formulation to propose the first differential RL
framework that can handle settings with limited training samples and
short-length episodes. Our approach introduces Differential Policy Optimization
(DPO), a pointwise and stage-wise iteration method that optimizes policies
encoded by local-movement operators. We prove a pointwise convergence estimate
for DPO and provide a regret bound comparable with current theoretical works.
Such pointwise estimate ensures that the learned policy matches the optimal
path uniformly across different steps. We then apply DPO to a class of
practical RL problems which search for optimal configurations with Lagrangian
rewards. DPO is easy to implement, scalable, and shows competitive results on
benchmarking experiments against several popular RL methods.
]]></content:encoded>
<pubDate>2024-04-24T03:11:12Z</pubDate>
</item>
<item>
<title>ID-Animator: Zero-Shot Identity-Preserving Human Video Generation</title>
<link>http://arxiv.org/abs/2404.15275v1</link>
<guid>http://arxiv.org/abs/2404.15275v1</guid>
<content:encoded><![CDATA[
<div> 视频生成、人脸识别、数据集构建、模型优势、代码开源
<br /><br />
总结:
本研究提出了一种零样本人体视频生成方法ID-Animator，能在没有额外训练的情况下根据单个面部参考图像执行个性化视频生成。该方法利用面部适配器从可学习的面部潜在查询中编码身份相关嵌入，以促进视频生成过程中的身份信息提取。此外，研究还介绍了一个面向身份的数据集构建流水线，利用构建的面部图像池的解耦人类属性和行为字幕技术。在此基础上，进一步设计了随机面部参考训练方法，以精确捕获参考图像中的身份相关嵌入，从而提高模型对个性化视频生成的保真度和泛化能力。大量实验证明，ID-Animator相对于以前的模型在生成个性化人体视频方面具有显著优势。此外，我们的方法与流行的预训练T2V模型（如animatediff）和各种社区骨干模型高度兼容，在视频生成的实际应用中显示出很高的可扩展性，这在需要高度保留身份的视频生成中非常理想。我们的代码和检查点将在https://github.com/ID-Animator/ID-Animator上发布。 <div>
Generating high fidelity human video with specified identities has attracted
significant attention in the content generation community. However, existing
techniques struggle to strike a balance between training efficiency and
identity preservation, either requiring tedious case-by-case finetuning or
usually missing the identity details in video generation process. In this
study, we present ID-Animator, a zero-shot human-video generation approach that
can perform personalized video generation given single reference facial image
without further training. ID-Animator inherits existing diffusion-based video
generation backbones with a face adapter to encode the ID-relevant embeddings
from learnable facial latent queries. To facilitate the extraction of identity
information in video generation, we introduce an ID-oriented dataset
construction pipeline, which incorporates decoupled human attribute and action
captioning technique from a constructed facial image pool. Based on this
pipeline, a random face reference training method is further devised to
precisely capture the ID-relevant embeddings from reference images, thus
improving the fidelity and generalization capacity of our model for ID-specific
video generation. Extensive experiments demonstrate the superiority of
ID-Animator to generate personalized human videos over previous models.
Moreover, our method is highly compatible with popular pre-trained T2V models
like animatediff and various community backbone models, showing high
extendability in real-world applications for video generation where identity
preservation is highly desired. Our codes and checkpoints will be released at
https://github.com/ID-Animator/ID-Animator.
]]></content:encoded>
<pubDate>2024-04-23T17:59:43Z</pubDate>
</item>
<item>
<title>Estimation Network Design framework for efficient distributed
  optimization</title>
<link>http://arxiv.org/abs/2404.15273v1</link>
<guid>http://arxiv.org/abs/2404.15273v1</guid>
<content:encoded><![CDATA[
<div> 分布式决策问题, 代理, 估计网络设计, 稀疏性, 优化

分布式决策问题涉及一组只能通过点对点网络进行通信的代理，没有中央内存。在应用中，如网络控制和数据排名，每个代理只受决策向量的一小部分影响：这种稀疏性通常在分布式算法中被忽略，而它可以提高效率和可扩展性。为了解决这个问题，我们最近的论文引入了估计网络设计(END)，这是一个用于分析和设计分布式迭代的图论语言。END算法可以调整以利用特定问题实例的稀疏性，减少通信开销，最小化冗余，而不需要逐个案例的收敛分析。在本文中，我们展示了END在分布式优化领域的灵活性。特别是，我们研究了许多已有方法的稀疏感知版本，包括ADMM、AugDGM和Push-Sum DGD。对传感器网络中的估计问题进行的模拟表明，END算法可以提高收敛速度，并大大减少通信和内存成本。

<br /><br />总结: 本文介绍了一种新的图论语言END，用于分析和设计分布式迭代算法。这种算法可以根据具体问题实例的稀疏性进行调整，减少通信开销和冗余，不需要逐个案例的收敛分析。通过模拟传感器网络中的估计问题，展示了END算法可以提高收敛速度并大大降低通信和内存成本。 <div>
Distributed decision problems features a group of agents that can only
communicate over a peer-to-peer network, without a central memory. In
applications such as network control and data ranking, each agent is only
affected by a small portion of the decision vector: this sparsity is typically
ignored in distributed algorithms, while it could be leveraged to improve
efficiency and scalability. To address this issue, our recent paper introduces
Estimation Network Design (END), a graph theoretical language for the analysis
and design of distributed iterations. END algorithms can be tuned to exploit
the sparsity of specific problem instances, reducing communication overhead and
minimizing redundancy, yet without requiring case-by-case convergence analysis.
In this paper, we showcase the flexility of END in the context of distributed
optimization. In particular, we study the sparsity-aware version of many
established methods, including ADMM, AugDGM and Push-Sum DGD. Simulations on an
estimation problem in sensor networks demonstrate that END algorithms can boost
convergence speed and greatly reduce the communication and memory cost.
]]></content:encoded>
<pubDate>2024-04-23T17:59:09Z</pubDate>
</item>
<item>
<title>CT-GLIP: 3D Grounded Language-Image Pretraining with CT Scans and
  Radiology Reports for Full-Body Scenarios</title>
<link>http://arxiv.org/abs/2404.15272v1</link>
<guid>http://arxiv.org/abs/2404.15272v1</guid>
<content:encoded><![CDATA[
Medical Vision-Language Pretraining (Med-VLP) establishes a connection
between visual content from medical images and the relevant textual
descriptions. Existing Med-VLP methods primarily focus on 2D images depicting a
single body part, notably chest X-rays. In this paper, we extend the scope of
Med-VLP to encompass 3D images, specifically targeting full-body scenarios, by
using a multimodal dataset of CT images and reports. Compared with the 2D
counterpart, 3D VLP is required to effectively capture essential semantics from
significantly sparser representation in 3D imaging. In this paper, we introduce
CT-GLIP (Grounded Language-Image Pretraining with CT scans), a novel method
that constructs organ-level image-text pairs to enhance multimodal contrastive
learning, aligning grounded visual features with precise diagnostic text.
Additionally, we developed an abnormality dictionary to augment contrastive
learning with diverse negative samples. Our method, trained on a multimodal CT
dataset comprising 44,011 organ-level vision-text pairs from 17,702 patients
across 104 organs, demonstrates it can identify organs and abnormalities in a
zero-shot manner using natural languages. The performance of CT-GLIP is
validated on a separate test set of 1,130 patients, focusing on the 16 most
frequent abnormalities across 7 organs. The experimental results show our
model's superior performance over the standard CLIP framework across zero-shot
and fine-tuning scenarios, using both CNN and ViT architectures.
]]></content:encoded>
<pubDate>2024-04-23T17:59:01Z</pubDate>
</item>
<item>
<title>Automatic Layout Planning for Visually-Rich Documents with
  Instruction-Following Models</title>
<link>http://arxiv.org/abs/2404.15271v1</link>
<guid>http://arxiv.org/abs/2404.15271v1</guid>
<content:encoded><![CDATA[
Recent advancements in instruction-following models have made user
interactions with models more user-friendly and efficient, broadening their
applicability. In graphic design, non-professional users often struggle to
create visually appealing layouts due to limited skills and resources. In this
work, we introduce a novel multimodal instruction-following framework for
layout planning, allowing users to easily arrange visual elements into tailored
layouts by specifying canvas size and design purpose, such as for book covers,
posters, brochures, or menus. We developed three layout reasoning tasks to
train the model in understanding and executing layout instructions. Experiments
on two benchmarks show that our method not only simplifies the design process
for non-professionals but also surpasses the performance of few-shot GPT-4V
models, with mIoU higher by 12% on Crello. This progress highlights the
potential of multimodal instruction-following models to automate and simplify
the design process, providing an approachable solution for a wide range of
design tasks on visually-rich documents.
]]></content:encoded>
<pubDate>2024-04-23T17:58:33Z</pubDate>
</item>
<item>
<title>Aligning LLM Agents by Learning Latent Preference from User Edits</title>
<link>http://arxiv.org/abs/2404.15269v1</link>
<guid>http://arxiv.org/abs/2404.15269v1</guid>
<content:encoded><![CDATA[
We study interactive learning of language agents based on user edits made to
the agent's output. In a typical setting such as writing assistants, the user
interacts with a language agent to generate a response given a context, and may
optionally edit the agent response to personalize it based on their latent
preference, in addition to improving the correctness. The edit feedback is
naturally generated, making it a suitable candidate for improving the agent's
alignment with the user's preference, and for reducing the cost of user edits
over time. We propose a learning framework, PRELUDE that infers a description
of the user's latent preference based on historic edit data and using it to
define a prompt policy that drives future response generation. This avoids
fine-tuning the agent, which is costly, challenging to scale with the number of
users, and may even degrade its performance on other tasks. Furthermore,
learning descriptive preference improves interpretability, allowing the user to
view and modify the learned preference. However, user preference can be complex
and vary based on context, making it challenging to learn. To address this, we
propose a simple yet effective algorithm named CIPHER that leverages a large
language model (LLM) to infer the user preference for a given context based on
user edits. In the future, CIPHER retrieves inferred preferences from the
k-closest contexts in the history, and forms an aggregate preference for
response generation. We introduce two interactive environments -- summarization
and email writing, for evaluation using a GPT-4 simulated user. We compare with
algorithms that directly retrieve user edits but do not learn descriptive
preference, and algorithms that learn context-agnostic preference. On both
tasks, CIPHER achieves the lowest edit distance cost and learns preferences
that show significant similarity to the ground truth preferences
]]></content:encoded>
<pubDate>2024-04-23T17:57:47Z</pubDate>
</item>
<item>
<title>From Parts to Whole: A Unified Reference Framework for Controllable
  Human Image Generation</title>
<link>http://arxiv.org/abs/2404.15267v1</link>
<guid>http://arxiv.org/abs/2404.15267v1</guid>
<content:encoded><![CDATA[
Recent advancements in controllable human image generation have led to
zero-shot generation using structural signals (e.g., pose, depth) or facial
appearance. Yet, generating human images conditioned on multiple parts of human
appearance remains challenging. Addressing this, we introduce Parts2Whole, a
novel framework designed for generating customized portraits from multiple
reference images, including pose images and various aspects of human
appearance. To achieve this, we first develop a semantic-aware appearance
encoder to retain details of different human parts, which processes each image
based on its textual label to a series of multi-scale feature maps rather than
one image token, preserving the image dimension. Second, our framework supports
multi-image conditioned generation through a shared self-attention mechanism
that operates across reference and target features during the diffusion
process. We enhance the vanilla attention mechanism by incorporating mask
information from the reference human images, allowing for the precise selection
of any part. Extensive experiments demonstrate the superiority of our approach
over existing alternatives, offering advanced capabilities for multi-part
controllable human image customization. See our project page at
https://huanngzh.github.io/Parts2Whole/.
]]></content:encoded>
<pubDate>2024-04-23T17:56:08Z</pubDate>
</item>
<item>
<title>MoVA: Adapting Mixture of Vision Experts to Multimodal Context</title>
<link>http://arxiv.org/abs/2404.13046v1</link>
<guid>http://arxiv.org/abs/2404.13046v1</guid>
<content:encoded><![CDATA[
<div> 大语言模型、视觉编码器、MoVA、多模态理解、图像内容理解<br />
本文介绍了MoVA，这是一种新型的多模态大语言模型，通过采用粗到细的机制，自适应地路由和融合任务特定的视觉专家。在粗粒度阶段，使用上下文感知的专家路由策略动态选择最合适的视觉专家，利用大语言模型具有的专家路由低秩自适应（LoRA）能力。在细粒度阶段，使用混合视觉专家适配器（MoV-Adapter）从各种专家中提取和融合特定任务的知识。这种粗到细的范式有效地利用了基于多模态上下文和模型专业知识的专家表示，进一步增强了泛化能力。在广泛的实验中，MoVA在各种具有挑战性的多模态基准测试中取得了显著的性能提升，而无需任何额外复杂的设置。MoVA的代码和模型可以在https://github.com/TempleX98/MoVA上找到。 <br /><br />总结: 大语言模型在多模态理解中发挥重要作用，MoVA通过粗到细的机制，自适应地路由和融合任务特定的视觉专家，取得了显著的性能提升。 <div>
As the key component in multimodal large language models (MLLMs), the ability
of the visual encoder greatly affects MLLM's understanding on diverse image
content. Although some large-scale pretrained vision encoders such as vision
encoders in CLIP and DINOv2 have brought promising performance, we found that
there is still no single vision encoder that can dominate various image content
understanding, e.g., the CLIP vision encoder leads to outstanding results on
general image understanding but poor performance on document or chart content.
To alleviate the bias of CLIP vision encoder, we first delve into the inherent
behavior of different pre-trained vision encoders and then propose the MoVA, a
powerful and novel MLLM, adaptively routing and fusing task-specific vision
experts with a coarse-to-fine mechanism. In the coarse-grained stage, we design
a context-aware expert routing strategy to dynamically select the most suitable
vision experts according to the user instruction, input image, and expertise of
vision experts. This benefits from the powerful model function understanding
ability of the large language model (LLM) equipped with expert-routing low-rank
adaptation (LoRA). In the fine-grained stage, we elaborately conduct the
mixture-of-vision-expert adapter (MoV-Adapter) to extract and fuse
task-specific knowledge from various experts. This coarse-to-fine paradigm
effectively leverages representations from experts based on multimodal context
and model expertise, further enhancing the generalization ability. We conduct
extensive experiments to evaluate the effectiveness of the proposed approach.
Without any bells and whistles, MoVA can achieve significant performance gains
over current state-of-the-art methods in a wide range of challenging multimodal
benchmarks. Codes and models will be available at
https://github.com/TempleX98/MoVA.
]]></content:encoded>
<pubDate>2024-04-19T17:59:48Z</pubDate>
</item>
<item>
<title>BLINK: Multimodal Large Language Models Can See but Not Perceive</title>
<link>http://arxiv.org/abs/2404.12390v1</link>
<guid>http://arxiv.org/abs/2404.12390v1</guid>
<content:encoded><![CDATA[
<div> benchmark, multimodal language models, visual perception, challenges, improvement
<br /><br />总结:
研究介绍了Blink，一个针对多模态语言模型(LLMs)的新基准，专注于核心视觉感知能力，这在其他评估中找不到。Blink的任务大多可以在人类“眨眼”之内解决，例如相对深度估计、视觉对应、取证检测和多视角推理。然而，研究发现这些对感知能力要求很高的任务对当前多模态LLMs构成重大挑战，因为它们无法通过自然语言进行调解。Blink将14个经典计算机视觉任务重新格式化为3,807个多项选择题，配备单个或多个图像和视觉提示。人类平均准确率达到95.70%，但对现有多模态LLMs来说却具有挑战性。即使是性能最好的GPT-4V和Gemini的准确率也分别只有51.26%和45.72%，仅比随机猜测高出13.17%和7.63%，这表明这些感知能力在最近的多模态LLMs中尚未“出现”。研究还强调，专业的CV模型可以更好地解决这些问题，为未来的改进提供了潜在途径。我们相信Blink将激发社区帮助多模态LLMs赶上人类级别的视觉感知水平。 <div>
We introduce Blink, a new benchmark for multimodal language models (LLMs)
that focuses on core visual perception abilities not found in other
evaluations. Most of the Blink tasks can be solved by humans "within a blink"
(e.g., relative depth estimation, visual correspondence, forensics detection,
and multi-view reasoning). However, we find these perception-demanding tasks
cast significant challenges for current multimodal LLMs because they resist
mediation through natural language. Blink reformats 14 classic computer vision
tasks into 3,807 multiple-choice questions, paired with single or multiple
images and visual prompting. While humans get 95.70% accuracy on average, Blink
is surprisingly challenging for existing multimodal LLMs: even the
best-performing GPT-4V and Gemini achieve accuracies of 51.26% and 45.72%, only
13.17% and 7.63% higher than random guessing, indicating that such perception
abilities have not "emerged" yet in recent multimodal LLMs. Our analysis also
highlights that specialist CV models could solve these problems much better,
suggesting potential pathways for future improvements. We believe Blink will
stimulate the community to help multimodal LLMs catch up with human-level
visual perception.
]]></content:encoded>
<pubDate>2024-04-18T17:59:54Z</pubDate>
</item>
<item>
<title>Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language
  Models</title>
<link>http://arxiv.org/abs/2404.12387v1</link>
<guid>http://arxiv.org/abs/2404.12387v1</guid>
<content:encoded><![CDATA[
<div> 模型, 训练, 评估, 多模态, Reka
<br /><br />
总结: 
这篇技术报告介绍了Reka Core, Flash和Edge，它们是强大的多模态语言模型，能够处理和推理文本、图像、视频和音频输入。报告讨论了其中一些模型的训练细节，并提供了全面的评估结果。Reka Edge和Reka Flash不仅达到了行业最新水平，而且在性能上超过了许多更大的模型，在其计算类别上获得了巨大的价值。与此同时，Reka最强大和最大的模型Reka Core在自动评估和盲目人类评估方面接近最先进的模型。在图像问答基准测试（如MMMU、VQAv2）上，Core的表现与GPT4-V相媲美。在多模态聊天方面，Core在盲目第三方人类评估设置下排名第二，超越了其他模型，如Claude 3 Opus。在文本基准测试上，Core不仅在一系列公认的基准测试（如MMLU、GSM8K）上表现出色，而且在人类评估上超越了GPT4-0613。在视频问答（Perception-Test）方面，Core的表现超过了Gemini Ultra。这些模型已经在http://chat.reka.ai 上投入生产。您还可以在http://showcase.reka.ai 上找到一些高质量的定性示例。 <div>
We introduce Reka Core, Flash, and Edge, a series of powerful multimodal
language models trained from scratch by Reka. Reka models are able to process
and reason with text, images, video, and audio inputs. This technical report
discusses details of training some of these models and provides comprehensive
evaluation results. We show that Reka Edge and Reka Flash are not only
state-of-the-art but also outperform many much larger models, delivering
outsized values for their respective compute class. Meanwhile, our most capable
and largest model, Reka Core, approaches the best frontier models on both
automatic evaluations and blind human evaluations. On image question answering
benchmarks (e.g. MMMU, VQAv2), Core performs competitively to GPT4-V.
Meanwhile, on multimodal chat, Core ranks as the second most preferred model
under a blind third-party human evaluation setup, outperforming other models
such as Claude 3 Opus. On text benchmarks, Core not only performs competitively
to other frontier models on a set of well-established benchmarks (e.g. MMLU,
GSM8K) but also outperforms GPT4-0613 on human evaluation. On video question
answering (Perception-Test), Core outperforms Gemini Ultra. Models are shipped
in production at http://chat.reka.ai . A showcase of non cherry picked
qualitative examples can also be found at http://showcase.reka.ai .
]]></content:encoded>
<pubDate>2024-04-18T17:59:48Z</pubDate>
</item>
<item>
<title>MeshLRM: Large Reconstruction Model for High-Quality Mesh</title>
<link>http://arxiv.org/abs/2404.12385v1</link>
<guid>http://arxiv.org/abs/2404.12385v1</guid>
<content:encoded><![CDATA[
We propose MeshLRM, a novel LRM-based approach that can reconstruct a
high-quality mesh from merely four input images in less than one second.
Different from previous large reconstruction models (LRMs) that focus on
NeRF-based reconstruction, MeshLRM incorporates differentiable mesh extraction
and rendering within the LRM framework. This allows for end-to-end mesh
reconstruction by fine-tuning a pre-trained NeRF LRM with mesh rendering.
Moreover, we improve the LRM architecture by simplifying several complex
designs in previous LRMs. MeshLRM's NeRF initialization is sequentially trained
with low- and high-resolution images; this new LRM training strategy enables
significantly faster convergence and thereby leads to better quality with less
compute. Our approach achieves state-of-the-art mesh reconstruction from
sparse-view inputs and also allows for many downstream applications, including
text-to-3D and single-image-to-3D generation. Project page:
https://sarahweiii.github.io/meshlrm/
]]></content:encoded>
<pubDate>2024-04-18T17:59:41Z</pubDate>
</item>
<item>
<title>COMBO: Compositional World Models for Embodied Multi-Agent Cooperation</title>
<link>http://arxiv.org/abs/2404.10775v1</link>
<guid>http://arxiv.org/abs/2404.10775v1</guid>
<content:encoded><![CDATA[
<div> 多智能体协作, 分布式智能, 生成模型, 视觉观测, 规划
<br />
这篇论文研究了多智能体协作的问题，这些智能体在只有部分视角观测到世界的情况下必须进行合作。为了有效地在这种设置中进行规划，我们首先训练生成模型来估计部分视角观测的整体世界状态。然后，我们提出学习一个组合式世界模型，通过对多个智能体的联合动作进行分解，并组合生成视频，以精确模拟这个世界状态上的多组动作。最后，结合视觉语言模型推断其他智能体的动作，我们可以使用树搜索过程集成这些模块，促进在线协作规划。在ThreeDWorld模拟器上创建了两个具有挑战性的多智能体长时间跨度协作任务，并进行了2-4个智能体的实验。结果表明我们的组合式世界模型是有效的，该框架使得多智能体能够在各种任务和任意数量的其他智能体之间高效协作，展现了我们提出框架的未来前景。 <div>
In this paper, we investigate the problem of embodied multi-agent
cooperation, where decentralized agents must cooperate given only partial
egocentric views of the world. To effectively plan in this setting, in contrast
to learning world dynamics in a single-agent scenario, we must simulate world
dynamics conditioned on an arbitrary number of agents' actions given only
partial egocentric visual observations of the world. To address this issue of
partial observability, we first train generative models to estimate the overall
world state given partial egocentric observations. To enable accurate
simulation of multiple sets of actions on this world state, we then propose to
learn a compositional world model for multi-agent cooperation by factorizing
the naturally composable joint actions of multiple agents and compositionally
generating the video. By leveraging this compositional world model, in
combination with Vision Language Models to infer the actions of other agents,
we can use a tree search procedure to integrate these modules and facilitate
online cooperative planning. To evaluate the efficacy of our methods, we create
two challenging embodied multi-agent long-horizon cooperation tasks using the
ThreeDWorld simulator and conduct experiments with 2-4 agents. The results show
our compositional world model is effective and the framework enables the
embodied agents to cooperate efficiently with different agents across various
tasks and an arbitrary number of agents, showing the promising future of our
proposed framework. More videos can be found at
https://vis-www.cs.umass.edu/combo/.
]]></content:encoded>
<pubDate>2024-04-16T17:59:11Z</pubDate>
</item>
<item>
<title>A Conceptual Framework for Conversational Search and Recommendation:
  Conceptualizing Agent-Human Interactions During the Conversational Search
  Process</title>
<link>http://arxiv.org/abs/2404.08630v1</link>
<guid>http://arxiv.org/abs/2404.08630v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 对话式搜索, 用户意图, 搜索空间, 决策点, 代理人<br /> 
总结: 
本文旨在发展用户和代理人的行为和意图的概念框架，解释这些行为如何使用户能够探索搜索空间并满足他们的信息需求。我们概述了不同的行为和意图，然后讨论了在对话过程中代理人需要决策如何引导对话搜索过程以达到成功和/或令人满意的结论的关键决策点。本文实质上提供了代理人和用户之间对话搜索过程的概念化，为对话搜索代理人的研究、开发和评估提供了框架和起点。<br /> <div>
The conversational search task aims to enable a user to resolve information
needs via natural language dialogue with an agent. In this paper, we aim to
develop a conceptual framework of the actions and intents of users and agents
explaining how these actions enable the user to explore the search space and
resolve their information need. We outline the different actions and intents,
before discussing key decision points in the conversation where the agent needs
to decide how to steer the conversational search process to a successful and/or
satisfactory conclusion. Essentially, this paper provides a conceptualization
of the conversational search process between an agent and user, which provides
a framework and a starting point for research, development and evaluation of
conversational search agents.
]]></content:encoded>
<pubDate>2024-04-12T17:48:18Z</pubDate>
</item>
<item>
<title>Connecting NeRFs, Images, and Text</title>
<link>http://arxiv.org/abs/2404.07993v1</link>
<guid>http://arxiv.org/abs/2404.07993v1</guid>
<content:encoded><![CDATA[
<div> Neural Radiance Fields, 3D scenes, multimodal representation learning, NeRF embeddings, bidirectional mapping<br />
<br />
本文探讨了将NeRF模态与其他模态相连的新研究方向，类似于图像和文本的建立方法。作者提出了一个简单的框架，利用预训练模型用于NeRF表示，同时结合文本和图像处理的多模态模型。这个框架学习了NeRF嵌入和与之对应的图像和文本嵌入之间的双向映射。这种映射解锁了几种新颖且有用的应用，包括NeRF零样本分类以及从图像或文本中检索NeRF。 <br /><br />总结: <br />NeRF模态与其他模态相连的新研究方向；提出了简单的框架，利用预训练模型用于NeRF表示，同时结合文本和图像处理的多模态模型；学习了NeRF嵌入和与之对应的图像和文本嵌入之间的双向映射；解锁了几种新颖且有用的应用，包括NeRF零样本分类以及从图像或文本中检索NeRF。 <div>
Neural Radiance Fields (NeRFs) have emerged as a standard framework for
representing 3D scenes and objects, introducing a novel data type for
information exchange and storage. Concurrently, significant progress has been
made in multimodal representation learning for text and image data. This paper
explores a novel research direction that aims to connect the NeRF modality with
other modalities, similar to established methodologies for images and text. To
this end, we propose a simple framework that exploits pre-trained models for
NeRF representations alongside multimodal models for text and image processing.
Our framework learns a bidirectional mapping between NeRF embeddings and those
obtained from corresponding images and text. This mapping unlocks several novel
and useful applications, including NeRF zero-shot classification and NeRF
retrieval from images or text.
]]></content:encoded>
<pubDate>2024-04-11T17:59:59Z</pubDate>
</item>
<item>
<title>UMBRAE: Unified Multimodal Decoding of Brain Signals</title>
<link>http://arxiv.org/abs/2404.07202v1</link>
<guid>http://arxiv.org/abs/2404.07202v1</guid>
<content:encoded><![CDATA[
<div> 多模态解码、脑信号、空间信息、跨学科训练、UMBRAE
总结:
多模态解码UMBRACE旨在解决大脑研究中的挑战，提取脑信号的概念和空间细节，并引入跨学科训练策略，使模型能够在多个学科上进行训练，并在新任务中取得优越结果。实验证明，UMBRAE不仅在新任务中取得了优异的成绩，并且胜过了现有方法，在综合大脑理解基准BrainHub上也展现出了良好的表现。 <div>
We address prevailing challenges of the brain-powered research, departing
from the observation that the literature hardly recover accurate spatial
information and require subject-specific models. To address these challenges,
we propose UMBRAE, a unified multimodal decoding of brain signals. First, to
extract instance-level conceptual and spatial details from neural signals, we
introduce an efficient universal brain encoder for multimodal-brain alignment
and recover object descriptions at multiple levels of granularity from
subsequent multimodal large language model (MLLM). Second, we introduce a
cross-subject training strategy mapping subject-specific features to a common
feature space. This allows a model to be trained on multiple subjects without
extra resources, even yielding superior results compared to subject-specific
models. Further, we demonstrate this supports weakly-supervised adaptation to
new subjects, with only a fraction of the total training data. Experiments
demonstrate that UMBRAE not only achieves superior results in the newly
introduced tasks but also outperforms methods in well established tasks. To
assess our method, we construct and share with the community a comprehensive
brain understanding benchmark BrainHub. Our code and benchmark are available at
https://weihaox.github.io/UMBRAE.
]]></content:encoded>
<pubDate>2024-04-10T17:59:20Z</pubDate>
</item>
<item>
<title>MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video
  Understanding</title>
<link>http://arxiv.org/abs/2404.05726v1</link>
<guid>http://arxiv.org/abs/2404.05726v1</guid>
<content:encoded><![CDATA[
<div> 长视频理解 模型设计 在线处理 内存存储 多模态模型

该研究提出了一种针对长视频理解的高效有效模型设计。与现有工作不同的是，该模型采用在线处理视频的方式，并将过去的视频信息存储在内存银行中。这使得模型能够在不超过LLMs上下文长度限制或GPU内存限制的情况下参考历史视频内容进行长期分析。我们的内存银行可以轻松集成到当前多模态LLMs中。我们在各种视频理解任务上进行了大量实验，如长视频理解、视频问题回答和视频字幕生成，我们的模型在多个数据集上均取得了最先进的性能。具体代码可在https://boheumd.github.io/MA-LMM/上找到。 <br /><br />总结: <br />长视频理解的挑战，提出了一种针对长视频理解的高效有效模型设计，通过在线处理视频并将信息存储在内存中，实现长期视频分析。在多个数据集上取得了最先进的性能。 <div>
With the success of large language models (LLMs), integrating the vision
model into LLMs to build vision-language foundation models has gained much more
interest recently. However, existing LLM-based large multimodal models (e.g.,
Video-LLaMA, VideoChat) can only take in a limited number of frames for short
video understanding. In this study, we mainly focus on designing an efficient
and effective model for long-term video understanding. Instead of trying to
process more frames simultaneously like most existing work, we propose to
process videos in an online manner and store past video information in a memory
bank. This allows our model to reference historical video content for long-term
analysis without exceeding LLMs' context length constraints or GPU memory
limits. Our memory bank can be seamlessly integrated into current multimodal
LLMs in an off-the-shelf manner. We conduct extensive experiments on various
video understanding tasks, such as long-video understanding, video question
answering, and video captioning, and our model can achieve state-of-the-art
performances across multiple datasets. Code available at
https://boheumd.github.io/MA-LMM/.
]]></content:encoded>
<pubDate>2024-04-08T17:59:24Z</pubDate>
</item>
<item>
<title>Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs</title>
<link>http://arxiv.org/abs/2404.05719v1</link>
<guid>http://arxiv.org/abs/2404.05719v1</guid>
<content:encoded><![CDATA[
<div> 关键词: Multimodal large language models, Ferret-UI, user interface, training samples, model evaluation

Ferret-UI是一种新型的Multimodal large language model，专门用于增强对移动用户界面的理解能力。它通过引入了"any resolution"技术来放大细节，提供了更好的视觉特性。该模型通过收集大量基本UI任务的训练样本，并对其进行格式化处理，以便进行精确的指代和基础。此外，还编制了一个包括详细描述、感知/交互对话和功能推断在内的数据集，以增强模型的推理能力。经过训练后，Ferret-UI在UI界面的理解和执行开放式指令方面表现出色。针对模型评估，作者建立了一个全面的基准，涵盖了所有前述任务。Ferret-UI不仅在大部分开源UI MLLMs方面表现出色，而且在所有基本UI任务上都超越了GPT-4V。

<br /><br />总结:
Ferret-UI是一款专门针对移动用户界面的Multimodal large language model，具有增强理解能力。作者收集了大量UI任务的训练样本，并编制了用于增强模型推理能力的数据集。经过训练后，Ferret-UI在理解和执行UI界面任务方面表现出色，超越了开源UI MLLMs和GPT-4V。 <div>
Recent advancements in multimodal large language models (MLLMs) have been
noteworthy, yet, these general-domain MLLMs often fall short in their ability
to comprehend and interact effectively with user interface (UI) screens. In
this paper, we present Ferret-UI, a new MLLM tailored for enhanced
understanding of mobile UI screens, equipped with referring, grounding, and
reasoning capabilities. Given that UI screens typically exhibit a more
elongated aspect ratio and contain smaller objects of interest (e.g., icons,
texts) than natural images, we incorporate "any resolution" on top of Ferret to
magnify details and leverage enhanced visual features. Specifically, each
screen is divided into 2 sub-images based on the original aspect ratio (i.e.,
horizontal division for portrait screens and vertical division for landscape
screens). Both sub-images are encoded separately before being sent to LLMs. We
meticulously gather training samples from an extensive range of elementary UI
tasks, such as icon recognition, find text, and widget listing. These samples
are formatted for instruction-following with region annotations to facilitate
precise referring and grounding. To augment the model's reasoning ability, we
further compile a dataset for advanced tasks, including detailed description,
perception/interaction conversations, and function inference. After training on
the curated datasets, Ferret-UI exhibits outstanding comprehension of UI
screens and the capability to execute open-ended instructions. For model
evaluation, we establish a comprehensive benchmark encompassing all the
aforementioned tasks. Ferret-UI excels not only beyond most open-source UI
MLLMs, but also surpasses GPT-4V on all the elementary UI tasks.
]]></content:encoded>
<pubDate>2024-04-08T17:55:44Z</pubDate>
</item>
<item>
<title>SwapAnything: Enabling Arbitrary Object Swapping in Personalized Visual
  Editing</title>
<link>http://arxiv.org/abs/2404.05717v1</link>
<guid>http://arxiv.org/abs/2404.05717v1</guid>
<content:encoded><![CDATA[
Effective editing of personal content holds a pivotal role in enabling
individuals to express their creativity, weaving captivating narratives within
their visual stories, and elevate the overall quality and impact of their
visual content. Therefore, in this work, we introduce SwapAnything, a novel
framework that can swap any objects in an image with personalized concepts
given by the reference, while keeping the context unchanged. Compared with
existing methods for personalized subject swapping, SwapAnything has three
unique advantages: (1) precise control of arbitrary objects and parts rather
than the main subject, (2) more faithful preservation of context pixels, (3)
better adaptation of the personalized concept to the image. First, we propose
targeted variable swapping to apply region control over latent feature maps and
swap masked variables for faithful context preservation and initial semantic
concept swapping. Then, we introduce appearance adaptation, to seamlessly adapt
the semantic concept into the original image in terms of target location,
shape, style, and content during the image generation process. Extensive
results on both human and automatic evaluation demonstrate significant
improvements of our approach over baseline methods on personalized swapping.
Furthermore, SwapAnything shows its precise and faithful swapping abilities
across single object, multiple objects, partial object, and cross-domain
swapping tasks. SwapAnything also achieves great performance on text-based
swapping and tasks beyond swapping such as object insertion.
]]></content:encoded>
<pubDate>2024-04-08T17:52:29Z</pubDate>
</item>
<item>
<title>Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation</title>
<link>http://arxiv.org/abs/2404.04256v1</link>
<guid>http://arxiv.org/abs/2404.04256v1</guid>
<content:encoded><![CDATA[
<div> 多模态语义分割, AI代理, 低光环境, 过曝环境, Sigma, Siamese Mamba网络<br />
<br />
多模态语义分割方法Sigma利用Selective Structured State Space Model, Mamba, 实现了全局感知域与线性复杂度的结合。通过Siamese编码器和创新的Mamba融合机制有效地选择不同模态的关键信息。再结合解码器增强模型的通道建模能力。该方法在RGB-热像和RGB-深度语义分割任务上得到了有效评估，并展示了其优越性，标志着State Space Models (SSMs)在多模态感知任务中的首次成功应用。 代码可在https://github.com/zifuwan/Sigma找到。<br />
<br />总结: 多模态语义分割方法Sigma利用了新颖的Siamese Mamba网络和State Space Models (SSMs)，在低光和过曝环境下实现了更可靠的场景理解和全局感知，且具有线性复杂度，有效结合了不同模态的关键信息，在RGB-热像和RGB-深度语义分割任务上得到了有效验证。 <div>
Multi-modal semantic segmentation significantly enhances AI agents'
perception and scene understanding, especially under adverse conditions like
low-light or overexposed environments. Leveraging additional modalities
(X-modality) like thermal and depth alongside traditional RGB provides
complementary information, enabling more robust and reliable segmentation. In
this work, we introduce Sigma, a Siamese Mamba network for multi-modal semantic
segmentation, utilizing the Selective Structured State Space Model, Mamba.
Unlike conventional methods that rely on CNNs, with their limited local
receptive fields, or Vision Transformers (ViTs), which offer global receptive
fields at the cost of quadratic complexity, our model achieves global receptive
fields coverage with linear complexity. By employing a Siamese encoder and
innovating a Mamba fusion mechanism, we effectively select essential
information from different modalities. A decoder is then developed to enhance
the channel-wise modeling ability of the model. Our method, Sigma, is
rigorously evaluated on both RGB-Thermal and RGB-Depth segmentation tasks,
demonstrating its superiority and marking the first successful application of
State Space Models (SSMs) in multi-modal perception tasks. Code is available at
https://github.com/zifuwan/Sigma.
]]></content:encoded>
<pubDate>2024-04-05T17:59:44Z</pubDate>
</item>
<item>
<title>CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept
  Matching</title>
<link>http://arxiv.org/abs/2404.03653v1</link>
<guid>http://arxiv.org/abs/2404.03653v1</guid>
<content:encoded><![CDATA[
<div> 提取关键词: Diffusion models, text-to-image generation, CoMat, fine-tuning strategy, image captioning

总结:
Diffusion模型在文本到图像生成领域取得了巨大成功，但是文本提示和图像之间的不匹配仍然是一个挑战。主要原因是不足的标记注意激活导致的。研究人员提出了CoMat，这是一个端到端的扩散模型微调策略，具有图像到文本概念匹配机制。他们利用图像字幕模型来测量图像到文本的对齐并引导扩散模型重新审视被忽略的标记。另外，他们还提出了一种新颖的属性集中模块来解决属性绑定问题。通过对SDXL进行微调，他们得到了CoMat-SDXL模型，在两个文本到图像对齐基准测试中明显优于基线模型SDXL，并取得了领先的性能。<br /><br />总结: 该研究提出了一种新的文本到图像生成模型微调策略，通过引入图像到文本对齐机制和属性集中模块，显著改善了现有模型中存在的文本提示和图像不匹配的问题，取得了领先的性能。 <div>
Diffusion models have demonstrated great success in the field of
text-to-image generation. However, alleviating the misalignment between the
text prompts and images is still challenging. The root reason behind the
misalignment has not been extensively investigated. We observe that the
misalignment is caused by inadequate token attention activation. We further
attribute this phenomenon to the diffusion model's insufficient condition
utilization, which is caused by its training paradigm. To address the issue, we
propose CoMat, an end-to-end diffusion model fine-tuning strategy with an
image-to-text concept matching mechanism. We leverage an image captioning model
to measure image-to-text alignment and guide the diffusion model to revisit
ignored tokens. A novel attribute concentration module is also proposed to
address the attribute binding problem. Without any image or human preference
data, we use only 20K text prompts to fine-tune SDXL to obtain CoMat-SDXL.
Extensive experiments show that CoMat-SDXL significantly outperforms the
baseline model SDXL in two text-to-image alignment benchmarks and achieves
start-of-the-art performance.
]]></content:encoded>
<pubDate>2024-04-04T17:59:46Z</pubDate>
</item>
<item>
<title>AutoWebGLM: Bootstrap And Reinforce A Large Language Model-based Web
  Navigating Agent</title>
<link>http://arxiv.org/abs/2404.03648v1</link>
<guid>http://arxiv.org/abs/2404.03648v1</guid>
<content:encoded><![CDATA[
<div> GPT-4,Automated Web Navigation, AutoWebGLM, ChatGLM3-6B, Reinforcement Learning
<br /><br />总结:
本文介绍了基于大语言模型的自动网页导航代理的开发。现有的代理在处理真实世界的网页时存在三个问题：网页操作的多样性、HTML文本超出模型处理能力以及决策复杂性。为了解决这一挑战，研究人员开发了AutoWebGLM，这是一种建立在ChatGLM3-6B基础上的GPT-4智能代理，用于自动网页导航。他们设计了一种HTML简化算法，以保留重要信息的同时简洁地表示网页，并采用混合人工智能方法构建网页浏览数据进行课程培训。随后，他们通过强化学习和拒绝抽样来进一步促进模型对网页的理解、浏览器操作和高效任务分解。最后，他们建立了一个双语基准测试集AutoWebBench，用于进行真实世界的网页浏览任务。他们评估了AutoWebGLM在各种网页导航基准测试中的表现，并揭示了改进之处以及需要解决的问题。 <div>
Large language models (LLMs) have fueled many intelligent agent tasks, such
as web navigation -- but most existing agents perform far from satisfying in
real-world webpages due to three factors: (1) the versatility of actions on
webpages, (2) HTML text exceeding model processing capacity, and (3) the
complexity of decision-making due to the open-domain nature of web. In light of
the challenge, we develop AutoWebGLM, a GPT-4-outperforming automated web
navigation agent built upon ChatGLM3-6B. Inspired by human browsing patterns,
we design an HTML simplification algorithm to represent webpages, preserving
vital information succinctly. We employ a hybrid human-AI method to build web
browsing data for curriculum training. Then, we bootstrap the model by
reinforcement learning and rejection sampling to further facilitate webpage
comprehension, browser operations, and efficient task decomposition by itself.
For testing, we establish a bilingual benchmark -- AutoWebBench -- for
real-world web browsing tasks. We evaluate AutoWebGLM across diverse web
navigation benchmarks, revealing its improvements but also underlying
challenges to tackle real environments. Related code, model, and data will be
released at \url{https://github.com/THUDM/AutoWebGLM}.
]]></content:encoded>
<pubDate>2024-04-04T17:58:40Z</pubDate>
</item>
<item>
<title>Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale
  Prediction</title>
<link>http://arxiv.org/abs/2404.02905v1</link>
<guid>http://arxiv.org/abs/2404.02905v1</guid>
<content:encoded><![CDATA[
<div> 关键词: Visual AutoRegressive modeling, next-scale prediction, image generation, scalability, zero-shot generalization<br />
<br />
这篇文章介绍了Visual AutoRegressive modeling (VAR)作为一种新的图像自回归学习范式，重新定义了图像上的自回归学习，将其视为粗到细的“下一尺度预测”或“下一分辨率预测”，而不是标准的光栅扫描“下一个标记预测”。这种简单直观的方法使得自回归(AR)变换器可以快速学习视觉分布并具有良好的泛化能力：VAR首次使得AR模型在图像生成方面超过了扩散变压器。在ImageNet 256x256基准测试中，VAR通过将Frechet inception距离(FID)从18.65提高到1.80，Inception分数(IS)从80.4提高到356.4，且推理速度约快20倍，显著改善了AR基线。实验证实，VAR在多个维度上均优于扩散变压器(DiT)，包括图像质量、推理速度、数据效率和可扩展性。扩大VAR模型呈现出与LLMs中观察到的类似的幂律缩放规律，相关系数接近-0.998，为其提供了坚实的证据。VAR在下游任务中展示了零样本泛化能力，包括图像修补、外扩和编辑。这些结果表明VAR已经模拟出了LLMs的两个重要特性：缩放规律和零样本任务泛化。我们已发布所有模型和代码，以促进对AR/VAR模型在视觉生成和统一学习方面的探索。<br /><br />总结:Visual AutoRegressive modeling (VAR)重新定义了图像上的自回归学习范式，实现了对视觉分布的快速学习和良好的泛化能力。在图像生成方面，VAR优于扩散变压器，并呈现出幂律缩放规律。此外，VAR还展示了在下游任务中的零样本泛化能力。我们已发布相关模型和代码，以推动AR/VAR模型在视觉生成和统一学习中的进一步探索。 <div>
We present Visual AutoRegressive modeling (VAR), a new generation paradigm
that redefines the autoregressive learning on images as coarse-to-fine
"next-scale prediction" or "next-resolution prediction", diverging from the
standard raster-scan "next-token prediction". This simple, intuitive
methodology allows autoregressive (AR) transformers to learn visual
distributions fast and generalize well: VAR, for the first time, makes AR
models surpass diffusion transformers in image generation. On ImageNet 256x256
benchmark, VAR significantly improve AR baseline by improving Frechet inception
distance (FID) from 18.65 to 1.80, inception score (IS) from 80.4 to 356.4,
with around 20x faster inference speed. It is also empirically verified that
VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions
including image quality, inference speed, data efficiency, and scalability.
Scaling up VAR models exhibits clear power-law scaling laws similar to those
observed in LLMs, with linear correlation coefficients near -0.998 as solid
evidence. VAR further showcases zero-shot generalization ability in downstream
tasks including image in-painting, out-painting, and editing. These results
suggest VAR has initially emulated the two important properties of LLMs:
Scaling Laws and zero-shot task generalization. We have released all models and
codes to promote the exploration of AR/VAR models for visual generation and
unified learning.
]]></content:encoded>
<pubDate>2024-04-03T17:59:53Z</pubDate>
</item>
<item>
<title>ALOHa: A New Measure for Hallucination in Captioning Models</title>
<link>http://arxiv.org/abs/2404.02904v1</link>
<guid>http://arxiv.org/abs/2404.02904v1</guid>
<content:encoded><![CDATA[
<div> 提取关键词：multimodal pre-training, visual description, object hallucination, ALOHa, large language models（LLMs）

在这项工作中，作者提出了一种现代化的开放词汇量度量标准ALOHa，用于测量物体的幻觉。他们使用大型语言模型来从候选字幕中提取可接地的物体，并测量它们与参考物体的语义相似性，最终使用匈牙利匹配生成最终的幻觉分数。 ALoha在HAT和nocaps数据集上的表现优于现有的CHIAR度量标准。总的来说，这项工作提出了一个有效的新方法来评估图像描述中的对象幻觉，并且在两个不同的数据集上都取得了令人满意的结果。

<br /><br />总结: 本文提出了一种现代化的开放词汇量度量标准ALOHa，用于衡量图像描述中的对象幻觉。该方法利用大型语言模型来提取候选字幕中的可接地物体，并测量它们与参考物体的语义相似性，最终使用匈牙利匹配生成最终的幻觉分数。ALOHa在HAT和nocaps数据集上的表现优于现有的CHIAR度量标准。这项工作提出了一个有效的新方法来评估图像描述中的对象幻觉，并且在两个不同的数据集上都取得了令人满意的结果。 <div>
Despite recent advances in multimodal pre-training for visual description,
state-of-the-art models still produce captions containing errors, such as
hallucinating objects not present in a scene. The existing prominent metric for
object hallucination, CHAIR, is limited to a fixed set of MS COCO objects and
synonyms. In this work, we propose a modernized open-vocabulary metric, ALOHa,
which leverages large language models (LLMs) to measure object hallucinations.
Specifically, we use an LLM to extract groundable objects from a candidate
caption, measure their semantic similarity to reference objects from captions
and object detections, and use Hungarian matching to produce a final
hallucination score. We show that ALOHa correctly identifies 13.6% more
hallucinated objects than CHAIR on HAT, a new gold-standard subset of MS COCO
Captions annotated for hallucinations, and 30.8% more on nocaps, where objects
extend beyond MS COCO categories. Our code is available at
https://davidmchan.github.io/aloha/.
]]></content:encoded>
<pubDate>2024-04-03T17:59:36Z</pubDate>
</item>
<item>
<title>MatAtlas: Text-driven Consistent Geometry Texturing and Material
  Assignment</title>
<link>http://arxiv.org/abs/2404.02899v1</link>
<guid>http://arxiv.org/abs/2404.02899v1</guid>
<content:encoded><![CDATA[
We present MatAtlas, a method for consistent text-guided 3D model texturing.
Following recent progress we leverage a large scale text-to-image generation
model (e.g., Stable Diffusion) as a prior to texture a 3D model. We carefully
design an RGB texturing pipeline that leverages a grid pattern diffusion,
driven by depth and edges. By proposing a multi-step texture refinement
process, we significantly improve the quality and 3D consistency of the
texturing output. To further address the problem of baked-in lighting, we move
beyond RGB colors and pursue assigning parametric materials to the assets.
Given the high-quality initial RGB texture, we propose a novel material
retrieval method capitalized on Large Language Models (LLM), enabling
editabiliy and relightability. We evaluate our method on a wide variety of
geometries and show that our method significantly outperform prior arts. We
also analyze the role of each component through a detailed ablation study.
]]></content:encoded>
<pubDate>2024-04-03T17:57:15Z</pubDate>
</item>
<item>
<title>A Mean Field Game Model for Timely Computation in Edge Computing Systems</title>
<link>http://arxiv.org/abs/2404.02898v1</link>
<guid>http://arxiv.org/abs/2404.02898v1</guid>
<content:encoded><![CDATA[
We consider the problem of task offloading in multi-access edge computing
(MEC) systems constituting $N$ devices assisted by an edge server (ES), where
the devices can split task execution between a local processor and the ES.
Since the local task execution and communication with the ES both consume
power, each device must judiciously choose between the two. We model the
problem as a large population non-cooperative game among the $N$ devices. Since
computation of an equilibrium in this scenario is difficult due to the presence
of a large number of devices, we employ the mean-field game framework to reduce
the finite-agent game problem to a generic user's multi-objective optimization
problem, with a coupled consistency condition. By leveraging the novel age of
information (AoI) metric, we invoke techniques from stochastic hybrid systems
(SHS) theory and study the tradeoffs between increasing information freshness
and reducing power consumption. In numerical simulations, we validate that a
higher load at the ES may lead devices to upload their task to the ES less
often.
]]></content:encoded>
<pubDate>2024-04-03T17:55:20Z</pubDate>
</item>
<item>
<title>Segment Any 3D Object with Language</title>
<link>http://arxiv.org/abs/2404.02157v1</link>
<guid>http://arxiv.org/abs/2404.02157v1</guid>
<content:encoded><![CDATA[
<div> Open-Vocabulary 3D Instance Segmentation, free-form language instructions, semantic-aware, geometric-aware, multimodal fusion network<br />
<br />
本文研究了使用自由形式语言指令的开放词汇3D实例分割（OV-3DIS）。先前的研究依赖于只使用标注的基本类别进行训练，但容易在未知的新领域中受限。最近的研究通过生成类别不可知的掩模或从2D到3D的投影泛化掩模，但忽略了语义或几何信息，导致性能不佳。相反，直接从3D点云中生成具有语义相关性的可泛化掩模会产生更好的结果。本文引入了Segment any 3D Object with LanguagE（SOLE），它是一个具有强大泛化能力的语义和几何感知的视觉-语言学习框架，通过直接从3D点云生成与语义相关的掩模。我们提出一个多模态融合网络来在骨干和解码器中融合多模态语义。另外，为了与各种语言指令对齐3D分割模型并增强掩模质量，我们引入了三种类型的多模态关联作为监督。我们的SOLE在ScanNetv2、ScanNet200和Replica基准测试上的表现大大超过先前方法，即使在训练中缺乏类别注释，结果甚至接近于完全监督的对照组。此外，大量的定性结果证明了我们的SOLE对语言指令的多功能性。<br /><br />总结: 本文研究了开放词汇3D实例分割（OV-3DIS）和自由形式语言指令。作者提出了SOLE框架，它是一个具有强大泛化能力的语义和几何感知的视觉-语言学习框架。SOLE在基准测试上表现出色，并且在缺乏类别注释的训练中也取得了良好的效果。最终的结果证明了SOLE对语言指令的多功能性。 <div>
In this paper, we investigate Open-Vocabulary 3D Instance Segmentation
(OV-3DIS) with free-form language instructions. Earlier works that rely on only
annotated base categories for training suffer from limited generalization to
unseen novel categories. Recent works mitigate poor generalizability to novel
categories by generating class-agnostic masks or projecting generalized masks
from 2D to 3D, but disregard semantic or geometry information, leading to
sub-optimal performance. Instead, generating generalizable but semantic-related
masks directly from 3D point clouds would result in superior outcomes. In this
paper, we introduce Segment any 3D Object with LanguagE (SOLE), which is a
semantic and geometric-aware visual-language learning framework with strong
generalizability by generating semantic-related masks directly from 3D point
clouds. Specifically, we propose a multimodal fusion network to incorporate
multimodal semantics in both backbone and decoder. In addition, to align the 3D
segmentation model with various language instructions and enhance the mask
quality, we introduce three types of multimodal associations as supervision.
Our SOLE outperforms previous methods by a large margin on ScanNetv2,
ScanNet200, and Replica benchmarks, and the results are even close to the
fully-supervised counterpart despite the absence of class annotations in the
training. Furthermore, extensive qualitative results demonstrate the
versatility of our SOLE to language instructions.
]]></content:encoded>
<pubDate>2024-04-02T17:59:10Z</pubDate>
</item>
<item>
<title>Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks</title>
<link>http://arxiv.org/abs/2404.02151v1</link>
<guid>http://arxiv.org/abs/2404.02151v1</guid>
<content:encoded><![CDATA[
<div> 关键词: LLMs, adaptive attacks, jailbreaking, logprobs, vulnerabilities

总结: 
本文研究表明，即使是最新的安全对齐的LLMs也无法抵御简单的自适应越狱攻击。首先，作者展示了成功利用对logprobs的访问进行越狱的方法，然后介绍了对不公开logprobs的Claude模型进行迁移或预填攻击的方法。另外，还展示了如何在受毒害的模型中使用有限的令牌搜索来查找特洛伊木马字符串。攻击的共同主题是适应性至关重要：不同的模型对不同的提示模板敏感，某些模型则基于其API具有独特的弱点，有时候根据先前的知识来限制令牌搜索空间也是至关重要的。详细攻击代码、提示和日志可在https://github.com/tml-epfl/llm-adaptive-attacks找到。<br /><br /> <div>
We show that even the most recent safety-aligned LLMs are not robust to
simple adaptive jailbreaking attacks. First, we demonstrate how to successfully
leverage access to logprobs for jailbreaking: we initially design an
adversarial prompt template (sometimes adapted to the target LLM), and then we
apply random search on a suffix to maximize the target logprob (e.g., of the
token "Sure"), potentially with multiple restarts. In this way, we achieve
nearly 100\% attack success rate -- according to GPT-4 as a judge -- on
GPT-3.5/4, Llama-2-Chat-7B/13B/70B, Gemma-7B, and R2D2 from HarmBench that was
adversarially trained against the GCG attack. We also show how to jailbreak all
Claude models -- that do not expose logprobs -- via either a transfer or
prefilling attack with 100\% success rate. In addition, we show how to use
random search on a restricted set of tokens for finding trojan strings in
poisoned models -- a task that shares many similarities with jailbreaking --
which is the algorithm that brought us the first place in the SaTML'24 Trojan
Detection Competition. The common theme behind these attacks is that adaptivity
is crucial: different models are vulnerable to different prompting templates
(e.g., R2D2 is very sensitive to in-context learning prompts), some models have
unique vulnerabilities based on their APIs (e.g., prefilling for Claude), and
in some settings it is crucial to restrict the token search space based on
prior knowledge (e.g., for trojan detection). We provide the code, prompts, and
logs of the attacks at https://github.com/tml-epfl/llm-adaptive-attacks.
]]></content:encoded>
<pubDate>2024-04-02T17:58:27Z</pubDate>
</item>
<item>
<title>Diffusion$^2$: Dynamic 3D Content Generation via Score Composition of
  Orthogonal Diffusion Models</title>
<link>http://arxiv.org/abs/2404.02148v1</link>
<guid>http://arxiv.org/abs/2404.02148v1</guid>
<content:encoded><![CDATA[
Recent advancements in 3D generation are predominantly propelled by
improvements in 3D-aware image diffusion models which are pretrained on
Internet-scale image data and fine-tuned on massive 3D data, offering the
capability of producing highly consistent multi-view images. However, due to
the scarcity of synchronized multi-view video data, it is impractical to adapt
this paradigm to 4D generation directly. Despite that, the available video and
3D data are adequate for training video and multi-view diffusion models that
can provide satisfactory dynamic and geometric priors respectively. In this
paper, we present Diffusion$^2$, a novel framework for dynamic 3D content
creation that leverages the knowledge about geometric consistency and temporal
smoothness from these models to directly sample dense multi-view and
multi-frame images which can be employed to optimize continuous 4D
representation. Specifically, we design a simple yet effective denoising
strategy via score composition of video and multi-view diffusion models based
on the probability structure of the images to be generated. Owing to the high
parallelism of the image generation and the efficiency of the modern 4D
reconstruction pipeline, our framework can generate 4D content within few
minutes. Furthermore, our method circumvents the reliance on 4D data, thereby
having the potential to benefit from the scalability of the foundation video
and multi-view diffusion models. Extensive experiments demonstrate the efficacy
of our proposed framework and its capability to flexibly adapt to various types
of prompts.
]]></content:encoded>
<pubDate>2024-04-02T17:58:03Z</pubDate>
</item>
<item>
<title>Iterated Learning Improves Compositionality in Large Vision-Language
  Models</title>
<link>http://arxiv.org/abs/2404.02145v1</link>
<guid>http://arxiv.org/abs/2404.02145v1</guid>
<content:encoded><![CDATA[
A fundamental characteristic common to both human vision and natural language
is their compositional nature. Yet, despite the performance gains contributed
by large vision and language pretraining, recent investigations find that
most-if not all-our state-of-the-art vision-language models struggle at
compositionality. They are unable to distinguish between images of " a girl in
white facing a man in black" and "a girl in black facing a man in white".
Moreover, prior work suggests that compositionality doesn't arise with scale:
larger model sizes or training data don't help. This paper develops a new
iterated training algorithm that incentivizes compositionality. We draw on
decades of cognitive science research that identifies cultural transmission-the
need to teach a new generation-as a necessary inductive prior that incentivizes
humans to develop compositional languages. Specifically, we reframe
vision-language contrastive learning as the Lewis Signaling Game between a
vision agent and a language agent, and operationalize cultural transmission by
iteratively resetting one of the agent's weights during training. After every
iteration, this training paradigm induces representations that become "easier
to learn", a property of compositional languages: e.g. our model trained on
CC3M and CC12M improves standard CLIP by 4.7%, 4.0% respectfully in the
SugarCrepe benchmark.
]]></content:encoded>
<pubDate>2024-04-02T17:57:31Z</pubDate>
</item>
<item>
<title>Detecting Image Attribution for Text-to-Image Diffusion Models in RGB
  and Beyond</title>
<link>http://arxiv.org/abs/2403.19653v1</link>
<guid>http://arxiv.org/abs/2403.19653v1</guid>
<content:encoded><![CDATA[
<div> 文本到图像，T2I模型，真实主义，图像生成，图像检测，可追溯性

T2I模型在生成图像方面取得了巨大进步，具有惊人的真实主义和创造力。这种进展引发了对假图像检测和归因的研究，但先前的研究并未完全探讨这一任务的实际和科学维度。除了将图像归因给12种最先进的T2I生成器外，我们还对推断阶段的超参数和图像修改进行了广泛的分析。我们的实验揭示了初始化种子是高度可检测的，以及图像生成过程中的其他细微变化在一定程度上是可辨识的。我们进一步调查了对图像归因所利用的视觉痕迹，通过扰动高频细节和利用图像风格和结构的中等级表示。值得注意的是，改变高频信息只会稍微降低准确性，并且在风格表示上训练的归因器表现优于在RGB图像上进行训练。我们的分析强调了与先前探索的各种视觉粒度级别相比，假图像在不同级别上是可检测和可归因的。 <br /><br />总结: 文本到图像（T2I）模型取得了巨大进步，可以生成具有惊人真实主义和创造力的图像。研究人员对假图像检测和归因进行了深入探讨，发现初始化种子和图像生成过程中的微小变化都是可辨识的。此外，通过扰动高频细节和利用图像风格和结构的中等级表示，也可以追踪图像的可信度。这些分析结果强调了实验发现的各种视觉粒度级别，进一步证明了假图像在不同级别上是可检测和可归因的。 <div>
Modern text-to-image (T2I) diffusion models can generate images with
remarkable realism and creativity. These advancements have sparked research in
fake image detection and attribution, yet prior studies have not fully explored
the practical and scientific dimensions of this task. In addition to
attributing images to 12 state-of-the-art T2I generators, we provide extensive
analyses on what inference stage hyperparameters and image modifications are
discernible. Our experiments reveal that initialization seeds are highly
detectable, along with other subtle variations in the image generation process
to some extent. We further investigate what visual traces are leveraged in
image attribution by perturbing high-frequency details and employing mid-level
representations of image style and structure. Notably, altering high-frequency
information causes only slight reductions in accuracy, and training an
attributor on style representations outperforms training on RGB images. Our
analyses underscore that fake images are detectable and attributable at various
levels of visual granularity than previously explored.
]]></content:encoded>
<pubDate>2024-03-28T17:59:42Z</pubDate>
</item>
<item>
<title>MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions</title>
<link>http://arxiv.org/abs/2403.19651v1</link>
<guid>http://arxiv.org/abs/2403.19651v1</guid>
<content:encoded><![CDATA[
<div> 自监督图像检索模型，文本指令，MagicLens，多模态模型，语言模型
<br /><br />总结:
本文介绍了一种名为MagicLens的自监督图像检索模型，利用文本指令来实现更丰富的图像检索。通过在同一网页上自然出现的图像对中挖掘丰富的语义关系，并利用大型多模态模型和大型语言模型来合成指令，使得模型能够支持开放式指令的图像检索。MagicLens在3600万个（查询图像，指令，目标图像）三元组上进行训练，并在八个不同的图像检索任务基准上取得可比或更好的结果，超过之前最先进的方法，同时模型规模减小了50倍。另外，对140万个未见过的图像进行人类分析，进一步展示了MagicLens支持的多样化搜索意图。 <div>
Image retrieval, i.e., finding desired images given a reference image,
inherently encompasses rich, multi-faceted search intents that are difficult to
capture solely using image-based measures. Recent work leverages text
instructions to allow users to more freely express their search intents.
However, existing work primarily focuses on image pairs that are visually
similar and/or can be characterized by a small set of pre-defined relations.
The core thesis of this paper is that text instructions can enable retrieving
images with richer relations beyond visual similarity. To show this, we
introduce MagicLens, a series of self-supervised image retrieval models that
support open-ended instructions. MagicLens is built on a key novel insight:
image pairs that naturally occur on the same web pages contain a wide range of
implicit relations (e.g., inside view of), and we can bring those implicit
relations explicit by synthesizing instructions via large multimodal models
(LMMs) and large language models (LLMs). Trained on 36.7M (query image,
instruction, target image) triplets with rich semantic relations mined from the
web, MagicLens achieves comparable or better results on eight benchmarks of
various image retrieval tasks than prior state-of-the-art (SOTA) methods.
Remarkably, it outperforms previous SOTA but with a 50X smaller model size on
multiple benchmarks. Additional human analyses on a 1.4M-image unseen corpus
further demonstrate the diversity of search intents supported by MagicLens.
]]></content:encoded>
<pubDate>2024-03-28T17:59:20Z</pubDate>
</item>
<item>
<title>Human-compatible driving partners through data-regularized self-play
  reinforcement learning</title>
<link>http://arxiv.org/abs/2403.19648v1</link>
<guid>http://arxiv.org/abs/2403.19648v1</guid>
<content:encoded><![CDATA[
A central challenge for autonomous vehicles is coordinating with humans.
Therefore, incorporating realistic human agents is essential for scalable
training and evaluation of autonomous driving systems in simulation. Simulation
agents are typically developed by imitating large-scale, high-quality datasets
of human driving. However, pure imitation learning agents empirically have high
collision rates when executed in a multi-agent closed-loop setting. To build
agents that are realistic and effective in closed-loop settings, we propose
Human-Regularized PPO (HR-PPO), a multi-agent algorithm where agents are
trained through self-play with a small penalty for deviating from a human
reference policy. In contrast to prior work, our approach is RL-first and only
uses 30 minutes of imperfect human demonstrations. We evaluate agents in a
large set of multi-agent traffic scenes. Results show our HR-PPO agents are
highly effective in achieving goals, with a success rate of 93%, an off-road
rate of 3.5%, and a collision rate of 3%. At the same time, the agents drive in
a human-like manner, as measured by their similarity to existing human driving
logs. We also find that HR-PPO agents show considerable improvements on proxy
measures for coordination with human driving, particularly in highly
interactive scenarios. We open-source our code and trained agents at
https://github.com/Emerge-Lab/nocturne_lab and provide demonstrations of agent
behaviors at https://sites.google.com/view/driving-partners.
]]></content:encoded>
<pubDate>2024-03-28T17:56:56Z</pubDate>
</item>
<item>
<title>GANTASTIC: GAN-based Transfer of Interpretable Directions for
  Disentangled Image Editing in Text-to-Image Diffusion Models</title>
<link>http://arxiv.org/abs/2403.19645v1</link>
<guid>http://arxiv.org/abs/2403.19645v1</guid>
<content:encoded><![CDATA[
The rapid advancement in image generation models has predominantly been
driven by diffusion models, which have demonstrated unparalleled success in
generating high-fidelity, diverse images from textual prompts. Despite their
success, diffusion models encounter substantial challenges in the domain of
image editing, particularly in executing disentangled edits-changes that target
specific attributes of an image while leaving irrelevant parts untouched. In
contrast, Generative Adversarial Networks (GANs) have been recognized for their
success in disentangled edits through their interpretable latent spaces. We
introduce GANTASTIC, a novel framework that takes existing directions from
pre-trained GAN models-representative of specific, controllable attributes-and
transfers these directions into diffusion-based models. This novel approach not
only maintains the generative quality and diversity that diffusion models are
known for but also significantly enhances their capability to perform precise,
targeted image edits, thereby leveraging the best of both worlds.
]]></content:encoded>
<pubDate>2024-03-28T17:55:16Z</pubDate>
</item>
<item>
<title>SLEDGE: Synthesizing Simulation Environments for Driving Agents with
  Generative Models</title>
<link>http://arxiv.org/abs/2403.17933v1</link>
<guid>http://arxiv.org/abs/2403.17933v1</guid>
<content:encoded><![CDATA[
<div> 训练、模拟、车辆、生成模型、交通仿真
总结：<br /><br />这篇文章介绍了SLEDGE，它是第一个在真实驾驶记录上训练的车辆运动规划生成器。其核心组件是一个学习模型，能够生成车辆包围框和车道图。通过对现有车道图表示的系统研究，引入了一种新颖的栅格到矢量自编码器(RVAE)。RVAE能够将代理和车道图编码成栅格化潜在地图的不同通道，这有助于车道条件下的代理生成，以及使用扩散变压器同时生成车道和代理。SLEDGE的生成实体能够更好地控制仿真，例如增加转弯或提高交通密度。与nuPlan相比，SLEDGE设置存储量减少了500倍(<4GB)，更易于获取，有助于民主化这一领域的未来研究。在测试SLEDGE生成的困难路线和密集交通时，nuPlan比赛获胜者PDM的失败率超过40%，这为规划算法带来了新的挑战。 <div>
SLEDGE is the first generative simulator for vehicle motion planning trained
on real-world driving logs. Its core component is a learned model that is able
to generate agent bounding boxes and lane graphs. The model's outputs serve as
an initial state for traffic simulation. The unique properties of the entities
to be generated for SLEDGE, such as their connectivity and variable count per
scene, render the naive application of most modern generative models to this
task non-trivial. Therefore, together with a systematic study of existing lane
graph representations, we introduce a novel raster-to-vector autoencoder
(RVAE). It encodes agents and the lane graph into distinct channels in a
rasterized latent map. This facilitates both lane-conditioned agent generation
and combined generation of lanes and agents with a Diffusion Transformer. Using
generated entities in SLEDGE enables greater control over the simulation, e.g.
upsampling turns or increasing traffic density. Further, SLEDGE can support
500m long routes, a capability not found in existing data-driven simulators
like nuPlan. It presents new challenges for planning algorithms, evidenced by
failure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge,
when tested on hard routes and dense traffic generated by our model. Compared
to nuPlan, SLEDGE requires 500$\times$ less storage to set up (<4GB), making it
a more accessible option and helping with democratizing future research in this
field.
]]></content:encoded>
<pubDate>2024-03-26T17:58:29Z</pubDate>
</item>
<item>
<title>MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution</title>
<link>http://arxiv.org/abs/2403.17927v1</link>
<guid>http://arxiv.org/abs/2403.17927v1</guid>
<content:encoded><![CDATA[
<div> GitHub, LLMs, MAGIS, software evolution, agents
总结:<br />
这篇文章介绍了在GitHub存储库中解决出现的问题是一个复杂的挑战，需要不断地更新代码并维护现有功能。大型语言模型（LLMs）在代码生成和理解方面表现出了潜力，但在存储库级别的代码更改上遇到了困难。为了克服这些挑战，作者通过实证研究了LLMs通常无法解决GitHub问题的原因，并分析了一些影响因素。在此基础上，他们提出了基于LLM的多Agent框架MAGIS，该框架包括四种个性化的代理：经理、存储库管理员、开发人员和质量保证工程师代理。在实验中，他们使用SWE-bench基准来比较MAGIS与流行的LLMs，包括GPT-3.5，GPT-4和Claude-2。结果显示，MAGIS能够解决13.94%的GitHub问题，明显优于基线。具体来说，MAGIS在解决比率上比直接应用GPT-4提高了八倍。文章还分析了提高GitHub问题解决率的因素，如代码行位置、任务分配等。 <div>
In software evolution, resolving the emergent issues within GitHub
repositories is a complex challenge that involves not only the incorporation of
new code but also the maintenance of existing functionalities. Large Language
Models (LLMs) have shown promise in code generation and understanding but face
difficulties in code change, particularly at the repository level. To overcome
these challenges, we empirically study the reason why LLMs mostly fail to
resolve GitHub issues and analyze some impact factors. Motivated by the
empirical findings, we propose a novel LLM-based Multi-Agent framework for
GitHub Issue reSolution, MAGIS, consisting of four kinds of agents customized
for the software evolution: Manager, Repository Custodian, Developer, and
Quality Assurance Engineer agents. This framework leverages the collaboration
of various agents in the planning and coding process to unlock the potential of
LLMs to resolve GitHub issues. In experiments, we employ the SWE-bench
benchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4, and
Claude-2. MAGIS can resolve 13.94% GitHub issues, which significantly
outperforms the baselines. Specifically, MAGIS achieves an eight-fold increase
in resolved ratio over the direct application of GPT-4, the based LLM of our
method. We also analyze the factors for improving GitHub issue resolution
rates, such as line location, task allocation, etc.
]]></content:encoded>
<pubDate>2024-03-26T17:57:57Z</pubDate>
</item>
<item>
<title>An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems</title>
<link>http://arxiv.org/abs/2403.16809v1</link>
<guid>http://arxiv.org/abs/2403.16809v1</guid>
<content:encoded><![CDATA[
<div> 关键词: CPS-IoT, 大型语言模型(LLMs), 控制环境, 感知优化, 强化学习<br />
<br />
本文介绍了大型语言模型(LLMs)在Cyber-Physical Systems and the Internet of Things (CPS-IoT) 应用中的作用。文章提出了利用LLMs模拟人群的行为和热舒适度偏好，并将其集成到基于代理的强化学习算法中，以实现能源节约和居住者舒适度的平衡。研究结果表明，LLMs能够模拟大型开放空间中复杂的人群运动，而基于代理的强化学习算法相对于现有的设定点控制策略表现更优，显示出在CPS-IoT应用中，自适应和个性化的决策是至关重要的。通过这个案例研究，文章展示了将先进的Foundation Models如LLMs集成到CPS-IoT中以增强系统适应性和效率的潜力。 <div>
The increasing prevalence of Cyber-Physical Systems and the Internet of
Things (CPS-IoT) applications and Foundation Models are enabling new
applications that leverage real-time control of the environment. For example,
real-time control of Heating, Ventilation and Air-Conditioning (HVAC) systems
can reduce its usage when not needed for the comfort of human occupants, hence
reducing energy consumption. Collecting real-time feedback on human preferences
in such human-in-the-loop (HITL) systems, however, is difficult in practice. We
propose the use of large language models (LLMs) to deal with the challenges of
dynamic environments and difficult-to-obtain data in CPS optimization. In this
paper, we present a case study that employs LLM agents to mimic the behaviors
and thermal preferences of various population groups (e.g. young families, the
elderly) in a shopping mall. The aggregated thermal preferences are integrated
into an agent-in-the-loop based reinforcement learning algorithm AitL-RL, which
employs the LLM as a dynamic simulation of the physical environment to learn
how to balance between energy savings and occupant comfort. Our results show
that LLMs are capable of simulating complex population movements within large
open spaces. Besides, AitL-RL demonstrates superior performance compared to the
popular existing policy of set point control, suggesting that adaptive and
personalized decision-making is critical for efficient optimization in CPS-IoT
applications. Through this case study, we demonstrate the potential of
integrating advanced Foundation Models like LLMs into CPS-IoT to enhance system
adaptability and efficiency. The project's code can be found on our GitHub
repository.
]]></content:encoded>
<pubDate>2024-03-25T14:32:28Z</pubDate>
</item>
<item>
<title>Exploiting Priors from 3D Diffusion Models for RGB-Based One-Shot View
  Planning</title>
<link>http://arxiv.org/abs/2403.16803v1</link>
<guid>http://arxiv.org/abs/2403.16803v1</guid>
<content:encoded><![CDATA[
<div> 一次性视角规划, 机器人, 环境交互, 3D生成模型, 几何先验

利用扩散模型的3D生成能力，我们提出了一种新颖的一次性视角规划方法。这种方法利用几何先验，能够有效地规划出全局最短路径，从而实现仅凭物体的单个RGB图像即可进行高效的一次性数据收集和物体重建。我们在模拟和真实环境中进行了规划实验，结果表明我们的方法在物体重建质量和移动成本之间取得了很好的平衡。<br /><br />总结: 本研究提出了一种利用3D生成模型的新一次性视角规划方法，通过几何先验实现了高效的物体重建，同时在实验中取得了良好的效果。 <div>
Object reconstruction is relevant for many autonomous robotic tasks that
require interaction with the environment. A key challenge in such scenarios is
planning view configurations to collect informative measurements for
reconstructing an initially unknown object. One-shot view planning enables
efficient data collection by predicting view configurations and planning the
globally shortest path connecting all views at once. However, geometric priors
about the object are required to conduct one-shot view planning. In this work,
we propose a novel one-shot view planning approach that utilizes the powerful
3D generation capabilities of diffusion models as priors. By incorporating such
geometric priors into our pipeline, we achieve effective one-shot view planning
starting with only a single RGB image of the object to be reconstructed. Our
planning experiments in simulation and real-world setups indicate that our
approach balances well between object reconstruction quality and movement cost.
]]></content:encoded>
<pubDate>2024-03-25T14:21:49Z</pubDate>
</item>
<item>
<title>LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis</title>
<link>http://arxiv.org/abs/2403.15385v1</link>
<guid>http://arxiv.org/abs/2403.15385v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 文本到3D生成, LATTE3D, 优化, 3D数据, 高效性

总结:<br /><br />
LATTE3D是一个新的文本到3D生成方法，通过建立可伸缩的架构和利用3D数据进行优化，实现了快速高质量的生成。与以往方法相比，LATTE3D能够在单次前向传递中产生高度详细的纹理网格，生成速度为400毫秒。该方法进一步可以通过快速的测试时间优化来提升效果。LATTE3D克服了以往方法的缺点，能够在更大的输入数据集上表现良好。 <div>
Recent text-to-3D generation approaches produce impressive 3D results but
require time-consuming optimization that can take up to an hour per prompt.
Amortized methods like ATT3D optimize multiple prompts simultaneously to
improve efficiency, enabling fast text-to-3D synthesis. However, they cannot
capture high-frequency geometry and texture details and struggle to scale to
large prompt sets, so they generalize poorly. We introduce LATTE3D, addressing
these limitations to achieve fast, high-quality generation on a significantly
larger prompt set. Key to our method is 1) building a scalable architecture and
2) leveraging 3D data during optimization through 3D-aware diffusion priors,
shape regularization, and model initialization to achieve robustness to diverse
and complex training prompts. LATTE3D amortizes both neural field and textured
surface generation to produce highly detailed textured meshes in a single
forward pass. LATTE3D generates 3D objects in 400ms, and can be further
enhanced with fast test-time optimization.
]]></content:encoded>
<pubDate>2024-03-22T17:59:37Z</pubDate>
</item>
<item>
<title>ThemeStation: Generating Theme-Aware 3D Assets from Few Exemplars</title>
<link>http://arxiv.org/abs/2403.15383v1</link>
<guid>http://arxiv.org/abs/2403.15383v1</guid>
<content:encoded><![CDATA[
<div> 3D assets, ThemeStation, generation, diversity, quality
<br /><br />
本文介绍了一种名为ThemeStation的方法，用于主题感知的3D到3D生成。该方法通过两个阶段的框架设计来实现自定义3D资产的合成，首先生成概念图像，然后进行参考信息辅助的3D建模。文章提出了一种新颖的双重分数蒸馏（DSD）损失，以共同利用来自输入示例和合成概念图像的先验知识。大量实验和用户研究证实，ThemeStation在生成多样化的主题感知3D模型方面优于先前的工作，并且质量令人印象深刻。ThemeStation还能够实现各种应用，比如可控的3D到3D生成。
<br /> 
总结:本文介绍了一种新颖的方法ThemeStation用于主题感知的3D到3D生成，通过两个阶段的框架设计实现自定义3D资产的合成。提出了一种新颖的双重分数蒸馏（DSD）损失以共同利用先验知识。实验证实，ThemeStation在生成多样化的主题感知3D模型方面优于先前的工作，并且质量令人印象深刻。 <div>
Real-world applications often require a large gallery of 3D assets that share
a consistent theme. While remarkable advances have been made in general 3D
content creation from text or image, synthesizing customized 3D assets
following the shared theme of input 3D exemplars remains an open and
challenging problem. In this work, we present ThemeStation, a novel approach
for theme-aware 3D-to-3D generation. ThemeStation synthesizes customized 3D
assets based on given few exemplars with two goals: 1) unity for generating 3D
assets that thematically align with the given exemplars and 2) diversity for
generating 3D assets with a high degree of variations. To this end, we design a
two-stage framework that draws a concept image first, followed by a
reference-informed 3D modeling stage. We propose a novel dual score
distillation (DSD) loss to jointly leverage priors from both the input
exemplars and the synthesized concept image. Extensive experiments and user
studies confirm that ThemeStation surpasses prior works in producing diverse
theme-aware 3D models with impressive quality. ThemeStation also enables
various applications such as controllable 3D-to-3D generation.
]]></content:encoded>
<pubDate>2024-03-22T17:59:01Z</pubDate>
</item>
<item>
<title>Long-CLIP: Unlocking the Long-Text Capability of CLIP</title>
<link>http://arxiv.org/abs/2403.15378v1</link>
<guid>http://arxiv.org/abs/2403.15378v1</guid>
<content:encoded><![CDATA[
Contrastive Language-Image Pre-training (CLIP) has been the cornerstone for
zero-shot classification, text-image retrieval, and text-image generation by
aligning image and text modalities. Despite its widespread adoption, a
significant limitation of CLIP lies in the inadequate length of text input. The
length of the text token is restricted to 77, and an empirical study shows the
actual effective length is even less than 20. This prevents CLIP from handling
detailed descriptions, limiting its applications for image retrieval and
text-to-image generation with extensive prerequisites. To this end, we propose
Long-CLIP as a plug-and-play alternative to CLIP that supports long-text input,
retains or even surpasses its zero-shot generalizability, and aligns the CLIP
latent space, making it readily replace CLIP without any further adaptation in
downstream frameworks. Nevertheless, achieving this goal is far from
straightforward, as simplistic fine-tuning can result in a significant
degradation of CLIP's performance. Moreover, substituting the text encoder with
a language model supporting longer contexts necessitates pretraining with vast
amounts of data, incurring significant expenses. Accordingly, Long-CLIP
introduces an efficient fine-tuning solution on CLIP with two novel strategies
designed to maintain the original capabilities, including (1) a
knowledge-preserved stretching of positional embedding and (2) a primary
component matching of CLIP features. With leveraging just one million extra
long text-image pairs, Long-CLIP has shown the superiority to CLIP for about
20% in long caption text-image retrieval and 6% in traditional text-image
retrieval tasks, e.g., COCO and Flickr30k. Furthermore, Long-CLIP offers
enhanced capabilities for generating images from detailed text descriptions by
replacing CLIP in a plug-and-play manner.
]]></content:encoded>
<pubDate>2024-03-22T17:58:16Z</pubDate>
</item>
<item>
<title>Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language
  Models</title>
<link>http://arxiv.org/abs/2403.12966v1</link>
<guid>http://arxiv.org/abs/2403.12966v1</guid>
<content:encoded><![CDATA[
<div> 关键词：视觉-语言理解、大型视觉-语言模型、交互式推理、细节信息、多模态数据集

总结：<br /><br />本文介绍了大型视觉-语言模型中视觉编码器在提取有针对性的特征方面所面临的挑战，以及现有模型普遍采用低分辨率图像限制了视觉识别能力的问题。作者提出了Chain-of-Spot (CoS)方法，即交互式推理，通过关注图像中与问题或指令相对应的关键区域，从而增强特征提取，实现了多粒度图像特征的获取。将Chain-of-Spot与LLaVA-1.5模型相结合，显著提升了模型在多模态数据集和基准测试中的性能，取得了新的最先进结果。作者的实证研究表明，该方法显著改善了大型视觉-语言模型理解和推理视觉内容的能力，为更复杂的视觉指令跟随应用铺平了道路。 <div>
In the realm of vision-language understanding, the proficiency of models in
interpreting and reasoning over visual content has become a cornerstone for
numerous applications. However, it is challenging for the visual encoder in
Large Vision-Language Models (LVLMs) to extract useful features tailored to
questions that aid the language model's response. Furthermore, a common
practice among existing LVLMs is to utilize lower-resolution images, which
restricts the ability for visual recognition. Our work introduces the
Chain-of-Spot (CoS) method, which we describe as Interactive Reasoning, a novel
approach that enhances feature extraction by focusing on key regions of
interest (ROI) within the image, corresponding to the posed questions or
instructions. This technique allows LVLMs to access more detailed visual
information without altering the original image resolution, thereby offering
multi-granularity image features. By integrating Chain-of-Spot with
instruct-following LLaVA-1.5 models, the process of image reasoning
consistently improves performance across a wide range of multimodal datasets
and benchmarks without bells and whistles and achieves new state-of-the-art
results. Our empirical findings demonstrate a significant improvement in LVLMs'
ability to understand and reason about visual content, paving the way for more
sophisticated visual instruction-following applications. Code and models are
available at https://github.com/dongyh20/Chain-of-Spot
]]></content:encoded>
<pubDate>2024-03-19T17:59:52Z</pubDate>
</item>
<item>
<title>LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images</title>
<link>http://arxiv.org/abs/2403.11703v1</link>
<guid>http://arxiv.org/abs/2403.11703v1</guid>
<content:encoded><![CDATA[
<div> 关键词: Visual encoding, Large multimodal model (LMM), LLaVA-UHD, 图像模块化策略, 压缩模块

LLaVA-UHD是一个能够高效地感知任何宽高比和高分辨率图像的大型多模式模型。它包括图像模块化策略、压缩模块和空间模式等关键组件，能够在9个基准测试中胜过已有LMM模型。LLaVA-UHD在仅使用94%的推理计算的情况下，支持比LLaVA-1.5 336x336大6倍的分辨率图像，并在TextVQA上取得了6.4%的准确性提升。此外，该模型能够在学术环境下高效训练，仅需在8个A100 GPU上花费23小时，相比LLaVA-1.5的26小时。具体数据和代码可在https://github.com/thunlp/LLaVA-UHD上公开获取。<br /><br />总结: 本文介绍了LLaVA-UHD，一个针对视觉编码策略的大型多模式模型。通过图像模块化策略、压缩模块和空间模式等关键组件，LLaVA-UHD能够在高分辨率和任何宽高比的图像上表现出色，在多个基准测试中超越已有的LMM模型。 <div>
Visual encoding constitutes the basis of large multimodal models (LMMs) in
understanding the visual world. Conventional LMMs process images in fixed sizes
and limited resolutions, while recent explorations in this direction are
limited in adaptivity, efficiency, and even correctness. In this work, we first
take GPT-4V and LLaVA-1.5 as representative examples and expose systematic
flaws rooted in their visual encoding strategy. To address the challenges, we
present LLaVA-UHD, a large multimodal model that can efficiently perceive
images in any aspect ratio and high resolution. LLaVA-UHD includes three key
components: (1) An image modularization strategy that divides native-resolution
images into smaller variable-sized slices for efficient and extensible
encoding, (2) a compression module that further condenses image tokens from
visual encoders, and (3) a spatial schema to organize slice tokens for LLMs.
Comprehensive experiments show that LLaVA-UHD outperforms established LMMs
trained with 2-3 orders of magnitude more data on 9 benchmarks. Notably, our
model built on LLaVA-1.5 336x336 supports 6 times larger (i.e., 672x1088)
resolution images using only 94% inference computation, and achieves 6.4
accuracy improvement on TextVQA. Moreover, the model can be efficiently trained
in academic settings, within 23 hours on 8 A100 GPUs (vs. 26 hours of
LLaVA-1.5). We make the data and code publicly available at
https://github.com/thunlp/LLaVA-UHD.
]]></content:encoded>
<pubDate>2024-03-18T12:04:11Z</pubDate>
</item>
<item>
<title>Virbo: Multimodal Multilingual Avatar Video Generation in Digital
  Marketing</title>
<link>http://arxiv.org/abs/2403.11700v1</link>
<guid>http://arxiv.org/abs/2403.11700v1</guid>
<content:encoded><![CDATA[
<div> 智能系统、短视频制作、多语言定制、视频营销、成本降低
<br /><br />总结:
本文介绍了一个名为Virbo的智能系统，支持自动生成讲话头像视频。用户只需提供指定的脚本，Virbo便可以使用深度生成模型生成目标讲话视频。同时，该系统还支持多模态输入，可定制具有指定面部、指定声音和特效的视频。此外，系统还集成了一个多语言定制模块，支持批量生成数百种精美模板和创意特效的多语言讲话头像视频。通过一系列用户研究和演示测试，我们发现Virbo能够生成保持高质量的视频，而整个制作成本大大降低。这一智能系统将有效推动视频制作行业的发展，解决语言障碍和成本挑战，促进互联网营销。 <div>
With the widespread popularity of internet celebrity marketing all over the
world, short video production has gradually become a popular way of presenting
products information. However, the traditional video production industry
usually includes series of procedures as script writing, video filming in a
professional studio, video clipping, special effects rendering, customized
post-processing, and so forth. Not to mention that multilingual videos is not
accessible for those who could not speak multilingual languages. These
complicated procedures usually needs a professional team to complete, and this
made short video production costly in both time and money. This paper presents
an intelligent system that supports the automatic generation of talking avatar
videos, namely Virbo. With simply a user-specified script, Virbo could use a
deep generative model to generate a target talking videos. Meanwhile, the
system also supports multimodal inputs to customize the video with specified
face, specified voice and special effects. This system also integrated a
multilingual customization module that supports generate multilingual talking
avatar videos in a batch with hundreds of delicate templates and creative
special effects. Through a series of user studies and demo tests, we found that
Virbo can generate talking avatar videos that maintained a high quality of
videos as those from a professional team while reducing the entire production
costs significantly. This intelligent system will effectively promote the video
production industry and facilitate the internet marketing neglecting of
language barriers and cost challenges.
]]></content:encoded>
<pubDate>2024-03-18T11:56:35Z</pubDate>
</item>
<item>
<title>VideoAgent: Long-form Video Understanding with Large Language Model as
  Agent</title>
<link>http://arxiv.org/abs/2403.10517v1</link>
<guid>http://arxiv.org/abs/2403.10517v1</guid>
<content:encoded><![CDATA[
<div> 长形视频理解，计算机视觉，VideoAgent，互动推理和规划，EgoSchema和NExT-QA benchmarks<br />
总结:<br />
这篇文章介绍了长形视频理解方面的挑战，提出了一种名为VideoAgent的代理系统，它采用大型语言模型作为中央代理来迭代地识别和编译关键信息以回答问题，并利用视觉-语言基础模型来翻译和检索视觉信息。经过在具有挑战性的EgoSchema和NExT-QA基准测试上的评估，VideoAgent取得了优秀的成绩，证明了该方法的有效性和效率，突显了代理式方法在推进长形视频理解方面的潜力。 <div>
Long-form video understanding represents a significant challenge within
computer vision, demanding a model capable of reasoning over long multi-modal
sequences. Motivated by the human cognitive process for long-form video
understanding, we emphasize interactive reasoning and planning over the ability
to process lengthy visual inputs. We introduce a novel agent-based system,
VideoAgent, that employs a large language model as a central agent to
iteratively identify and compile crucial information to answer a question, with
vision-language foundation models serving as tools to translate and retrieve
visual information. Evaluated on the challenging EgoSchema and NExT-QA
benchmarks, VideoAgent achieves 54.1% and 71.3% zero-shot accuracy with only
8.4 and 8.2 frames used on average. These results demonstrate superior
effectiveness and efficiency of our method over the current state-of-the-art
methods, highlighting the potential of agent-based approaches in advancing
long-form video understanding.
]]></content:encoded>
<pubDate>2024-03-15T17:57:52Z</pubDate>
</item>
<item>
<title>SCP-Diff: Photo-Realistic Semantic Image Synthesis with
  Spatial-Categorical Joint Prior</title>
<link>http://arxiv.org/abs/2403.09638v1</link>
<guid>http://arxiv.org/abs/2403.09638v1</guid>
<content:encoded><![CDATA[
<div> 关键词: Semantic image synthesis, ControlNet, latent diffusion models, noise priors, SCP-Diff

总结:
语义图像合成是传感器模拟领域的一个有希望的技术，但目前基于GAN的最佳实践在质量上还未达到期望的水平。然而，随着潜在扩散模型在图像生成方面取得了重大进展，有必要评估ControlNet这一密集控制能力显著的方法。我们的研究发现了ControlNet结果的两个主要问题：大语义区域内存在奇怪的子结构，以及内容与语义掩模不匹配。通过实证研究，我们找到了这些问题的原因，即训练数据分布与推断阶段应用的标准正态先验之间的不匹配。为了解决这一挑战，我们为SIS开发了特定的噪声先验，包括空间、分类和新颖的空间-分类联合先验。这种方法被我们称为SCP-Diff，并取得了卓越的成果，在Cityscapes上达到了10.53的FID，在ADE20K上达到了12.66的FID。项目页面上可以访问到代码和模型。 <br /><br /> <div>
Semantic image synthesis (SIS) shows good promises for sensor simulation.
However, current best practices in this field, based on GANs, have not yet
reached the desired level of quality. As latent diffusion models make
significant strides in image generation, we are prompted to evaluate
ControlNet, a notable method for its dense control capabilities. Our
investigation uncovered two primary issues with its results: the presence of
weird sub-structures within large semantic areas and the misalignment of
content with the semantic mask. Through empirical study, we pinpointed the
cause of these problems as a mismatch between the noised training data
distribution and the standard normal prior applied at the inference stage. To
address this challenge, we developed specific noise priors for SIS,
encompassing spatial, categorical, and a novel spatial-categorical joint prior
for inference. This approach, which we have named SCP-Diff, has yielded
exceptional results, achieving an FID of 10.53 on Cityscapes and 12.66 on
ADE20K.The code and models can be accessed via the project page.
]]></content:encoded>
<pubDate>2024-03-14T17:59:55Z</pubDate>
</item>
<item>
<title>3D-VLA: A 3D Vision-Language-Action Generative World Model</title>
<link>http://arxiv.org/abs/2403.09631v1</link>
<guid>http://arxiv.org/abs/2403.09631v1</guid>
<content:encoded><![CDATA[
<div> 3D-VLA, perception, action, world model, embodied foundation model

3D-VLA是一种新型的视觉-语言-行动模型，与以往的2D输入不同，它能够无缝地连接3D感知、推理和行动。该模型引入了一系列交互令牌，以便在具体环境中进行交互，并通过训练一系列体现扩散模型，使模型具备生成能力。此外，模型还建立了一个大规模的3D具体指令数据集，通过提取现有机器人数据集中的大量3D相关信息进行训练。实验表明，3D-VLA显著改善了在具体环境中的推理、多模态生成和规划能力，展现出其在真实世界应用中的潜力。<br /><br />总结: <div>
Recent vision-language-action (VLA) models rely on 2D inputs, lacking
integration with the broader realm of the 3D physical world. Furthermore, they
perform action prediction by learning a direct mapping from perception to
action, neglecting the vast dynamics of the world and the relations between
actions and dynamics. In contrast, human beings are endowed with world models
that depict imagination about future scenarios to plan actions accordingly. To
this end, we propose 3D-VLA by introducing a new family of embodied foundation
models that seamlessly link 3D perception, reasoning, and action through a
generative world model. Specifically, 3D-VLA is built on top of a 3D-based
large language model (LLM), and a set of interaction tokens is introduced to
engage with the embodied environment. Furthermore, to inject generation
abilities into the model, we train a series of embodied diffusion models and
align them into the LLM for predicting the goal images and point clouds. To
train our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by
extracting vast 3D-related information from existing robotics datasets. Our
experiments on held-in datasets demonstrate that 3D-VLA significantly improves
the reasoning, multimodal generation, and planning capabilities in embodied
environments, showcasing its potential in real-world applications.
]]></content:encoded>
<pubDate>2024-03-14T17:58:41Z</pubDate>
</item>
<item>
<title>Quiet-STaR: Language Models Can Teach Themselves to Think Before
  Speaking</title>
<link>http://arxiv.org/abs/2403.09629v1</link>
<guid>http://arxiv.org/abs/2403.09629v1</guid>
<content:encoded><![CDATA[
When writing and talking, people sometimes pause to think. Although
reasoning-focused works have often framed reasoning as a method of answering
questions or completing agentic tasks, reasoning is implicit in almost all
written text. For example, this applies to the steps not stated between the
lines of a proof or to the theory of mind underlying a conversation. In the
Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned
by inferring rationales from few-shot examples in question-answering and
learning from those that lead to a correct answer. This is a highly constrained
setting -- ideally, a language model could instead learn to infer unstated
rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR
in which LMs learn to generate rationales at each token to explain future text,
improving their predictions. We address key challenges, including 1) the
computational cost of generating continuations, 2) the fact that the LM does
not initially know how to generate or use internal thoughts, and 3) the need to
predict beyond individual next tokens. To resolve these, we propose a tokenwise
parallel sampling algorithm, using learnable tokens indicating a thought's
start and end, and an extended teacher-forcing technique. Encouragingly,
generated rationales disproportionately help model difficult-to-predict tokens
and improve the LM's ability to directly answer difficult questions. In
particular, after continued pretraining of an LM on a corpus of internet text
with Quiet-STaR, we find zero-shot improvements on GSM8K
(5.9%$\rightarrow$10.9%) and CommonsenseQA (36.3%$\rightarrow$47.2%) and
observe a perplexity improvement of difficult tokens in natural text.
Crucially, these improvements require no fine-tuning on these tasks. Quiet-STaR
marks a step towards LMs that can learn to reason in a more general and
scalable way.
]]></content:encoded>
<pubDate>2024-03-14T17:58:16Z</pubDate>
</item>
<item>
<title>VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis</title>
<link>http://arxiv.org/abs/2403.08764v1</link>
<guid>http://arxiv.org/abs/2403.08764v1</guid>
<content:encoded><![CDATA[
<div> VLOGGER, audio-driven, human video generation, diffusion model, MENTOR

VLOGGER提出了一种基于音频驱动的单张人物图像生成视频的方法，借鉴了最近生成扩散模型的成功。该方法包括1）随机的人体到三维运动扩散模型，2）一种新颖的基于扩散的架构，将文本到图像模型与空间和时间控制相结合。这支持生成可变长度的高质量视频，通过人脸和身体的高级表示轻松控制。与先前的工作不同，该方法不需要为每个人进行训练，也不依赖于人脸检测和裁剪，并且生成完整的图像（不仅仅是脸部或嘴唇），考虑到关键的场景（例如可见的躯干或多样化的主体身份），以正确合成交流的人类。他们还提出了MENTOR，这是一个新的多样化数据集，具有3D姿势和表情注释，比以前的数据集大一个数量级（800,000个身份），并具有动态手势，他们在这个数据集上训练并消融了他们的主要技术贡献。VLOGGER在三个公开基准测试中表现优越，考虑到图像质量，身份保留和时间一致性，同时生成上半身手势。他们分析了VLOGGER的性能，关于多个多样性指标，显示出他们的架构选择和MENTOR的使用有利于以规模训练公正和无偏差的模型。最后，他们展示了视频编辑和个性化的应用。

<br /><br />总结: VLOGGER是一种基于音频驱动的人类视频生成方法，利用扩散模型和大规模数据集MENTOR，能够生成高质量、可变长度的视频，并具有上半身手势。它不需要为每个人进行训练，能够生成完整的人物图像，并考虑到不同的场景和主体身份。该方法在公开基准测试中表现优越，同时能够训练公正和无偏差的模型。 <div>
We propose VLOGGER, a method for audio-driven human video generation from a
single input image of a person, which builds on the success of recent
generative diffusion models. Our method consists of 1) a stochastic
human-to-3d-motion diffusion model, and 2) a novel diffusion-based architecture
that augments text-to-image models with both spatial and temporal controls.
This supports the generation of high quality video of variable length, easily
controllable through high-level representations of human faces and bodies. In
contrast to previous work, our method does not require training for each
person, does not rely on face detection and cropping, generates the complete
image (not just the face or the lips), and considers a broad spectrum of
scenarios (e.g. visible torso or diverse subject identities) that are critical
to correctly synthesize humans who communicate. We also curate MENTOR, a new
and diverse dataset with 3d pose and expression annotations, one order of
magnitude larger than previous ones (800,000 identities) and with dynamic
gestures, on which we train and ablate our main technical contributions.
  VLOGGER outperforms state-of-the-art methods in three public benchmarks,
considering image quality, identity preservation and temporal consistency while
also generating upper-body gestures. We analyze the performance of VLOGGER with
respect to multiple diversity metrics, showing that our architectural choices
and the use of MENTOR benefit training a fair and unbiased model at scale.
Finally we show applications in video editing and personalization.
]]></content:encoded>
<pubDate>2024-03-13T17:59:02Z</pubDate>
</item>
<item>
<title>Bridging Different Language Models and Generative Vision Models for
  Text-to-Image Generation</title>
<link>http://arxiv.org/abs/2403.07860v1</link>
<guid>http://arxiv.org/abs/2403.07860v1</guid>
<content:encoded><![CDATA[
<div> text-to-image generation, language model, vision model, integration, LaVi-Bridge

总结: 
本文介绍了文本到图像生成的最新发展，即文本到图像扩散模型。这些模型通常包括一个语言模型，用于解释用户提示，以及一个视觉模型，用于生成相应的图像。研究的目标是探索如何整合任意两个不相关的语言和生成视觉模型，以提高文本到图像生成的质量。文中提出了LaVi-Bridge，这是一个能够整合各种预训练语言模型和生成视觉模型的流程。通过利用LoRA和适配器，LaVi-Bridge提供了一种灵活的插拔式方法，无需修改语言和视觉模型的原始权重。研究表明，整合更先进的模块，如更先进的语言模型或生成视觉模型，可以显着提高文本对齐和图像质量等能力。通过广泛的评估验证了LaVi-Bridge的有效性。 该项目的代码可在https://github.com/ShihaoZhaoZSH/LaVi-Bridge 上找到。 <div>
Text-to-image generation has made significant advancements with the
introduction of text-to-image diffusion models. These models typically consist
of a language model that interprets user prompts and a vision model that
generates corresponding images. As language and vision models continue to
progress in their respective domains, there is a great potential in exploring
the replacement of components in text-to-image diffusion models with more
advanced counterparts. A broader research objective would therefore be to
investigate the integration of any two unrelated language and generative vision
models for text-to-image generation. In this paper, we explore this objective
and propose LaVi-Bridge, a pipeline that enables the integration of diverse
pre-trained language models and generative vision models for text-to-image
generation. By leveraging LoRA and adapters, LaVi-Bridge offers a flexible and
plug-and-play approach without requiring modifications to the original weights
of the language and vision models. Our pipeline is compatible with various
language models and generative vision models, accommodating different
structures. Within this framework, we demonstrate that incorporating superior
modules, such as more advanced language models or generative vision models,
results in notable improvements in capabilities like text alignment or image
quality. Extensive evaluations have been conducted to verify the effectiveness
of LaVi-Bridge. Code is available at
https://github.com/ShihaoZhaoZSH/LaVi-Bridge.
]]></content:encoded>
<pubDate>2024-03-12T17:50:11Z</pubDate>
</item>
<item>
<title>Gemini 1.5: Unlocking multimodal understanding across millions of tokens
  of context</title>
<link>http://arxiv.org/abs/2403.05530v1</link>
<guid>http://arxiv.org/abs/2403.05530v1</guid>
<content:encoded><![CDATA[
<div> Gemini 1.5 Pro, multimodal, long-context, recall, state-of-the-art

Gemini 1.5 Pro是Gemini家族的最新模型，是一种高效的多模态专家混合模型，能够在包括多个长文档和数小时的视频和音频在内的上下文中召回和推理细粒度信息。在长上下文检索任务中，Gemini 1.5 Pro实现了接近完美的召回率，提高了长文档QA、长视频QA和长上下文ASR的最新水平，并在各种基准测试中达到或超过了Gemini 1.0 Ultra的最新表现。研究了Gemini 1.5 Pro在长上下文能力方面的极限，发现了在接下来的标记预测和近乎完美的（>99%）检索方面的持续改进，可达到至少10M个标记，比现有模型如Claude 2.1（20万）和GPT-4 Turbo（12.8万）有了一代的飞跃。最后，我们突出介绍了大型语言模型在前沿的惊人新能力；当给定一个Kalamang语法手册时，这个模型学会了将英语翻译成Kalamang，其水平与从相同内容学习的人相似。 <br /><br />总结: Gemimi 1.5 Pro是一款高效的多模态专家混合模型，能够处理长上下文任务，并在长文档QA、长视频QA和长上下文ASR方面取得了最新的突破。此外，该模型还在标记预测和检索方面表现出非常优越的能力，超越了现有的一些模型。最后，该模型还展示了在小语种翻译方面的惊人能力。 <div>
In this report, we present the latest model of the Gemini family, Gemini 1.5
Pro, a highly compute-efficient multimodal mixture-of-experts model capable of
recalling and reasoning over fine-grained information from millions of tokens
of context, including multiple long documents and hours of video and audio.
Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks
across modalities, improves the state-of-the-art in long-document QA,
long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's
state-of-the-art performance across a broad set of benchmarks. Studying the
limits of Gemini 1.5 Pro's long-context ability, we find continued improvement
in next-token prediction and near-perfect retrieval (>99%) up to at least 10M
tokens, a generational leap over existing models such as Claude 2.1 (200k) and
GPT-4 Turbo (128k). Finally, we highlight surprising new capabilities of large
language models at the frontier; when given a grammar manual for Kalamang, a
language with fewer than 200 speakers worldwide, the model learns to translate
English to Kalamang at a similar level to a person who learned from the same
content.
]]></content:encoded>
<pubDate>2024-03-08T18:54:20Z</pubDate>
</item>
<item>
<title>Mechanism for Decision-aware Collaborative Federated Learning: A Pitfall
  of Shapley Values</title>
<link>http://arxiv.org/abs/2403.04753v1</link>
<guid>http://arxiv.org/abs/2403.04753v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 机制设计, 协作学习, 联邦学习平台, 决策感知, Shapley value

总结: 本论文研究了通过联邦学习平台实现决策感知协作学习的机制设计。文章介绍了一个由数字平台和多个决策感知代理组成的框架，平台提供基础设施，使代理可以访问数据，为协作学习创造激励，进行联邦学习以避免直接共享原始数据。文章引入了多行动协作联邦学习（MCFL）框架，分析了著名的Shapley value机制的均衡问题。研究发现，虽然Shapley value通过鼓励充分参与有效地最大化了联合创造的剩余价值，但不慎促进了虚假身份操纵，进一步增加了平台进行联邦学习时的通信成本。因此，Shapley value机制有一个严重的缺陷，即暗示着数据分割和身份复制，最终损害了联邦学习系统的整体效率。<br /><br />总结: 本论文研究了决策感知协作学习的机制设计，介绍了联邦学习平台的框架，分析了Shapley value机制的均衡问题，并发现其存在的缺陷。 <div>
This paper investigates mechanism design for decision-aware collaboration via
federated learning (FL) platforms. Our framework consists of a digital platform
and multiple decision-aware agents, each endowed with proprietary data sets.
The platform offers an infrastructure that enables access to the data, creates
incentives for collaborative learning aimed at operational decision-making, and
conducts FL to avoid direct raw data sharing. The computation and communication
efficiency of the FL process is inherently influenced by the agent
participation equilibrium induced by the mechanism. Therefore, assessing the
system's efficiency involves two critical factors: the surplus created by
coalition formation and the communication costs incurred across the coalition
during FL. To evaluate the system efficiency under the intricate interplay
between mechanism design, agent participation, operational decision-making, and
the performance of FL algorithms, we introduce a multi-action collaborative
federated learning (MCFL) framework for decision-aware agents. Under this
framework, we further analyze the equilibrium for the renowned Shapley value
based mechanisms. Specifically, we examine the issue of false-name
manipulation, a form of dishonest behavior where participating agents create
duplicate fake identities to split their original data among these identities.
By solving the agent participation equilibrium, we demonstrate that while
Shapley value effectively maximizes coalition-generated surplus by encouraging
full participation, it inadvertently promotes false-name manipulation. This
further significantly increases the communication costs when the platform
conducts FL. Thus, we highlight a significant pitfall of Shapley value based
mechanisms, which implicitly incentivizes data splitting and identity
duplication, ultimately impairing the overall efficiency in FL systems.
]]></content:encoded>
<pubDate>2024-03-07T18:54:59Z</pubDate>
</item>
<item>
<title>Stop Regressing: Training Value Functions via Classification for
  Scalable Deep RL</title>
<link>http://arxiv.org/abs/2403.03950v1</link>
<guid>http://arxiv.org/abs/2403.03950v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 深度强化学习, 值函数, 分类交叉熵, 可扩展性, 改进性能

深度强化学习中的值函数是一个核心组件，通过神经网络参数化，采用均方误差回归目标进行训练以匹配自举目标值。然而，将使用回归的基于值的强化学习方法扩展到大型网络（如高容量Transformer）的难度已被证明是具有挑战性的。观察到这一差异，本文研究了是否通过在训练值函数时仅使用分类代替回归可以改进深度强化学习的可扩展性。我们证明，使用分类交叉熵训练的值函数显著改善了多个领域的性能和可扩展性。其中包括：在Atari 2600游戏中使用SoftMoEs进行单任务强化学习，使用大规模ResNets进行Atari的多任务强化学习，在Q-transformers中进行机器人操纵，无搜索下的国际象棋对局，以及使用高容量Transformer进行语言代理Wordle任务，在这些领域取得了最先进的结果。通过仔细分析，我们展示了分类交叉熵的好处主要源于其减轻了值为基础的强化学习固有的问题，如嘈杂的目标和非稳态性。总体而言，我们认为简单地将值函数训练与分类交叉熵相结合可以在几乎没有成本的情况下显著提高深度强化学习的可扩展性。
<br /><br />总结: 本文研究了通过使用分类交叉熵代替回归来训练值函数，从而改善深度强化学习的可扩展性。作者证明了这种方法在多个领域均取得了显著的性能提升，包括Atari游戏、机器人操纵和语言代理任务。通过分析，作者指出分类交叉熵的好处主要来自于其减轻值函数训练中的固有问题。因此，本文的贡献在于提出了一种简单且高效的方法来提高深度强化学习的可扩展性，为相关领域的研究和应用带来了新的启发。 <div>
Value functions are a central component of deep reinforcement learning (RL).
These functions, parameterized by neural networks, are trained using a mean
squared error regression objective to match bootstrapped target values.
However, scaling value-based RL methods that use regression to large networks,
such as high-capacity Transformers, has proven challenging. This difficulty is
in stark contrast to supervised learning: by leveraging a cross-entropy
classification loss, supervised methods have scaled reliably to massive
networks. Observing this discrepancy, in this paper, we investigate whether the
scalability of deep RL can also be improved simply by using classification in
place of regression for training value functions. We demonstrate that value
functions trained with categorical cross-entropy significantly improves
performance and scalability in a variety of domains. These include: single-task
RL on Atari 2600 games with SoftMoEs, multi-task RL on Atari with large-scale
ResNets, robotic manipulation with Q-transformers, playing Chess without
search, and a language-agent Wordle task with high-capacity Transformers,
achieving state-of-the-art results on these domains. Through careful analysis,
we show that the benefits of categorical cross-entropy primarily stem from its
ability to mitigate issues inherent to value-based RL, such as noisy targets
and non-stationarity. Overall, we argue that a simple shift to training value
functions with categorical cross-entropy can yield substantial improvements in
the scalability of deep RL at little-to-no cost.
]]></content:encoded>
<pubDate>2024-03-06T18:55:47Z</pubDate>
</item>
<item>
<title>Can Audio Reveal Music Performance Difficulty? Insights from the Piano
  Syllabus Dataset</title>
<link>http://arxiv.org/abs/2403.03947v1</link>
<guid>http://arxiv.org/abs/2403.03947v1</guid>
<content:encoded><![CDATA[
<div> 音乐教育, 音乐信息检索, 音频分析, 数据集, 难度估计
<br /><br />总结:
音乐教育中自动估计音乐作品难度的重要性，音乐信息检索领域已经有一些工作致力于这一任务。本文首次提出了一个基于音频录音的音乐作品难度估计数据集，并开发了一个识别框架来处理不同的输入表示。实验证明了该提议的有效性，并将数据集、代码和训练模型公开分享，以促进该领域的进一步研究。 <div>
Automatically estimating the performance difficulty of a music piece
represents a key process in music education to create tailored curricula
according to the individual needs of the students. Given its relevance, the
Music Information Retrieval (MIR) field depicts some proof-of-concept works
addressing this task that mainly focuses on high-level music abstractions such
as machine-readable scores or music sheet images. In this regard, the potential
of directly analyzing audio recordings has been generally neglected, which
prevents students from exploring diverse music pieces that may not have a
formal symbolic-level transcription. This work pioneers in the automatic
estimation of performance difficulty of music pieces on audio recordings with
two precise contributions: (i) the first audio-based difficulty estimation
dataset -- namely, Piano Syllabus (PSyllabus) dataset -- featuring 7,901 piano
pieces across 11 difficulty levels from 1,233 composers; and (ii) a recognition
framework capable of managing different input representations -- both unimodal
and multimodal manners -- directly derived from audio to perform the difficulty
estimation task. The comprehensive experimentation comprising different
pre-training schemes, input modalities, and multi-task scenarios prove the
validity of the proposal and establishes PSyllabus as a reference dataset for
audio-based difficulty estimation in the MIR field. The dataset as well as the
developed code and trained models are publicly shared to promote further
research in the field.
]]></content:encoded>
<pubDate>2024-03-06T18:54:13Z</pubDate>
</item>
<item>
<title>Scaling Rectified Flow Transformers for High-Resolution Image Synthesis</title>
<link>http://arxiv.org/abs/2403.03206v1</link>
<guid>http://arxiv.org/abs/2403.03206v1</guid>
<content:encoded><![CDATA[
<div> 高维数据，扩散模型，噪音采样，文本到图像生成，变压器架构
<br />
本文介绍了扩散模型和矫正流模型对高维感知数据的生成建模技术，矫正流模型为直线连接了数据和噪音，具有更好的理论特性和概念简单性，但尚未成为标准做法。作者提出了改进的噪音采样技术，通过偏向感知相关尺度的噪音，提高了对于高分辨率文本到图像合成的性能。此外，作者提出了一个基于变压器的架构，实现了文本到图像的双向信息流，提高了文本理解能力、排版效果和人类评价。最后，作者强调了他们的最大模型优于现有模型，并将实验数据、代码和模型权重公开发布。 
<br /><br />总结: 
<br />扩散模型和矫正流模型为高维感知数据的生成建模提供了有效手段，提出了改进的噪音采样技术，以提高对高分辨率文本到图像合成的性能。新的基于变压器的架构实现了文本到图像的双向信息流，提高了文本理解能力、排版效果和人类评价。作者的最大模型优于现有模型，并将实验数据、代码和模型权重公开发布。 <div>
Diffusion models create data from noise by inverting the forward paths of
data towards noise and have emerged as a powerful generative modeling technique
for high-dimensional, perceptual data such as images and videos. Rectified flow
is a recent generative model formulation that connects data and noise in a
straight line. Despite its better theoretical properties and conceptual
simplicity, it is not yet decisively established as standard practice. In this
work, we improve existing noise sampling techniques for training rectified flow
models by biasing them towards perceptually relevant scales. Through a
large-scale study, we demonstrate the superior performance of this approach
compared to established diffusion formulations for high-resolution
text-to-image synthesis. Additionally, we present a novel transformer-based
architecture for text-to-image generation that uses separate weights for the
two modalities and enables a bidirectional flow of information between image
and text tokens, improving text comprehension, typography, and human preference
ratings. We demonstrate that this architecture follows predictable scaling
trends and correlates lower validation loss to improved text-to-image synthesis
as measured by various metrics and human evaluations. Our largest models
outperform state-of-the-art models, and we will make our experimental data,
code, and model weights publicly available.
]]></content:encoded>
<pubDate>2024-03-05T18:45:39Z</pubDate>
</item>
<item>
<title>Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers</title>
<link>http://arxiv.org/abs/2402.19479v1</link>
<guid>http://arxiv.org/abs/2402.19479v1</guid>
<content:encoded><![CDATA[
<div> 数据质量、视频标注、Panda-70M数据集、下游任务、模型训练

数据质量和标注质量上限了下游模型的质量。视频文本数据难以收集，因为手动标注耗时多，需要观看整个视频，而视频包含多个场景和动作。为了建立高质量字幕视频数据集，研究人员提出了一种自动方法，利用文本视频描述、字幕和视频帧。他们从公开可用的HD-VILA-100M数据集中筛选出380万个高分辨率视频，并将它们分成语义一致的视频片段，然后利用跨模态教师模型为每个视频获取字幕。接下来，在少量视频子集上微调检索模型，然后在整个数据集上使用该模型选择最佳字幕作为标注。于是得到了7000万个与高质量文本字幕配对的视频数据，命名为Panda-70M数据集。研究人员展示了该数据集在视频字幕生成、视频和文本检索等三项下游任务上的价值，表明基于该数据集训练的模型在大多数指标上得分明显更好。<br /><br />总结: 该研究提出了一种自动方法，利用跨模态输入建立高质量字幕视频数据集Panda-70M，并展示了其在多项下游任务上训练模型得分明显更好的价值。 <div>
The quality of the data and annotation upper-bounds the quality of a
downstream model. While there exist large text corpora and image-text pairs,
high-quality video-text data is much harder to collect. First of all, manual
labeling is more time-consuming, as it requires an annotator to watch an entire
video. Second, videos have a temporal dimension, consisting of several scenes
stacked together, and showing multiple actions. Accordingly, to establish a
video dataset with high-quality captions, we propose an automatic approach
leveraging multimodal inputs, such as textual video description, subtitles, and
individual video frames. Specifically, we curate 3.8M high-resolution videos
from the publicly available HD-VILA-100M dataset. We then split them into
semantically consistent video clips, and apply multiple cross-modality teacher
models to obtain captions for each video. Next, we finetune a retrieval model
on a small subset where the best caption of each video is manually selected and
then employ the model in the whole dataset to select the best caption as the
annotation. In this way, we get 70M videos paired with high-quality text
captions. We dub the dataset as Panda-70M. We show the value of the proposed
dataset on three downstream tasks: video captioning, video and text retrieval,
and text-driven video generation. The models trained on the proposed data score
substantially better on the majority of metrics across all the tasks.
]]></content:encoded>
<pubDate>2024-02-29T18:59:50Z</pubDate>
</item>
<item>
<title>Trajectory Prediction for Autonomous Driving Using a Transformer Network</title>
<link>http://arxiv.org/abs/2402.16501v1</link>
<guid>http://arxiv.org/abs/2402.16501v1</guid>
<content:encoded><![CDATA[
<div> 关键词: autonomous driving, multi-modal trajectory prediction, transformer network, convolutional networks, Lyft l5kit dataset

这篇论文介绍了基于Transformer网络的多模态轨迹预测框架，以及利用每个代理的语义地图作为输入，通过卷积网络自动获取相关的上下文信息。另外，还提出了一种新的辅助损失函数，惩罚不可行的越野预测。实验证明，该模型在Lyft l5kit数据集上取得了最先进的性能，显著提高了预测结果的准确性和可行性。

总结:
1. 论文介绍了自动驾驶中预测周围代理的轨迹是一个具有挑战性的任务。
2. 提出了基于Transformer网络的多模态轨迹预测框架，并利用语义地图作为输入。
3. 使用卷积网络自动获取相关的上下文信息。
4. 提出了一种新的辅助损失函数，用于惩罚不可行的越野预测。
5. 实验证明所提出的模型在Lyft l5kit数据集上取得了最先进的性能，显著提高了预测结果的准确性和可行性。 <div>
Predicting the trajectories of surrounding agents is still considered one of
the most challenging tasks for autonomous driving. In this paper, we introduce
a multi-modal trajectory prediction framework based on the transformer network.
The semantic maps of each agent are used as inputs to convolutional networks to
automatically derive relevant contextual information. A novel auxiliary loss
that penalizes unfeasible off-road predictions is also proposed in this study.
Experiments on the Lyft l5kit dataset show that the proposed model achieves
state-of-the-art performance, substantially improving the accuracy and
feasibility of the prediction outcomes.
]]></content:encoded>
<pubDate>2024-02-26T11:35:23Z</pubDate>
</item>
<item>
<title>LLMArena: Assessing Capabilities of Large Language Models in Dynamic
  Multi-Agent Environments</title>
<link>http://arxiv.org/abs/2402.16499v1</link>
<guid>http://arxiv.org/abs/2402.16499v1</guid>
<content:encoded><![CDATA[
<div> LLMArena, 大型语言模型, 多智能体环境, 评估框架, 实验

LLMArena 是一个新颖且易于扩展的框架，用于评估大型语言模型在多智能体动态环境中的不同能力。该框架包括七个不同的游戏环境，并采用 Trueskill 计分来评估语言模型的关键能力，包括空间推理、战略规划、数值推理、风险评估、沟通、对手建模和团队协作。研究人员通过对不同大小和类型的大型语言模型进行大量实验和人类评估，发现大型语言模型在对手建模和团队协作方面仍有很大的发展空间。作者希望LLMArena可以引导未来的研究，增强大型语言模型的这些能力，最终实现在动态的多智能体环境中更复杂和实用的应用。  <br /><br />总结: <br />LLMArena 是一个新颖且易于扩展的框架，用于评估大型语言模型在多智能体动态环境中的不同能力。该框架包括七个不同的游戏环境，并采用 Trueskill 计分来评估语言模型的关键能力，研究发现大型语言模型在对手建模和团队协作方面仍有很大的发展空间。作者希望LLMArena可以引导未来的研究，增强大型语言模型的这些能力，最终实现在动态的多智能体环境中更复杂和实用的应用。 <div>
Recent advancements in large language models (LLMs) have revealed their
potential for achieving autonomous agents possessing human-level intelligence.
However, existing benchmarks for evaluating LLM Agents either use static
datasets, potentially leading to data leakage or focus only on single-agent
scenarios, overlooking the complexities of multi-agent interactions. There is a
lack of a benchmark that evaluates the diverse capabilities of LLM agents in
multi-agent, dynamic environments. To this end, we introduce LLMArena, a novel
and easily extensible framework for evaluating the diverse capabilities of LLM
in multi-agent dynamic environments. LLMArena encompasses seven distinct gaming
environments, employing Trueskill scoring to assess crucial abilities in LLM
agents, including spatial reasoning, strategic planning, numerical reasoning,
risk assessment, communication, opponent modeling, and team collaboration. We
conduct an extensive experiment and human evaluation among different sizes and
types of LLMs, showing that LLMs still have a significant journey ahead in
their development towards becoming fully autonomous agents, especially in
opponent modeling and team collaboration. We hope LLMArena could guide future
research towards enhancing these capabilities in LLMs, ultimately leading to
more sophisticated and practical applications in dynamic, multi-agent settings.
The code and data will be available.
]]></content:encoded>
<pubDate>2024-02-26T11:31:48Z</pubDate>
</item>
<item>
<title>Q-FOX Learning: Breaking Tradition in Reinforcement Learning</title>
<link>http://arxiv.org/abs/2402.16562v1</link>
<guid>http://arxiv.org/abs/2402.16562v1</guid>
<content:encoded><![CDATA[
<div> 强化学习, 人工智能, 超参数调优, Q-FOX, OpenAI Gym
总结:<br /><br />这篇文章介绍了强化学习中超参数调优的重要性，提出了一种新的自动超参数调优方法 Q-FOX，该方法利用了FOX优化器和Q-learning算法，通过优化新的目标函数，优化了超参数以获得更好的强化学习性能。实验结果表明，Q-FOX相比其他优化器在解决OpenAI Gym环境控制任务时获得了更高的累积奖励。但Q-FOX仍有局限性，不能直接用于真实环境，也需要在模拟环境中进行迭代优化，因此存在一定的时间成本。综合来看，Q-FOX在强化学习的超参数调优方面发挥了重要作用。 <div>
Reinforcement learning (RL) is a subset of artificial intelligence (AI) where
agents learn the best action by interacting with the environment, making it
suitable for tasks that do not require labeled data or direct supervision.
Hyperparameters (HP) tuning refers to choosing the best parameter that leads to
optimal solutions in RL algorithms. Manual or random tuning of the HP may be a
crucial process because variations in this parameter lead to changes in the
overall learning aspects and different rewards. In this paper, a novel and
automatic HP-tuning method called Q-FOX is proposed. This uses both the FOX
optimizer, a new optimization method inspired by nature that mimics red foxes'
hunting behavior, and the commonly used, easy-to-implement RL Q-learning
algorithm to solve the problem of HP tuning. Moreover, a new objective function
is proposed which prioritizes the reward over the mean squared error (MSE) and
learning time (steps). Q-FOX has been evaluated on two OpenAI Gym environment
control tasks: Cart Pole and Frozen Lake. It exposed greater cumulative rewards
than HP tuning with other optimizers, such as PSO, GA, Bee, or randomly
selected HP. The cumulative reward for the Cart Pole task was 32.08, and for
the Frozen Lake task was 0.95. Despite the robustness of Q-FOX, it has
limitations. It cannot be used directly in real-word problems before choosing
the HP in a simulation environment because its processes work iteratively,
making it time-consuming. The results indicate that Q-FOX has played an
essential role in HP tuning for RL algorithms to effectively solve different
control tasks.
]]></content:encoded>
<pubDate>2024-02-26T13:39:04Z</pubDate>
</item>
<item>
<title>Contracts with Inspections</title>
<link>http://arxiv.org/abs/2402.16553v1</link>
<guid>http://arxiv.org/abs/2402.16553v1</guid>
<content:encoded><![CDATA[
<div> hidden-action model, principal-agent, incentive, inspection, deterministic<br />
<br />
本文提出了一个新的模型，放松了隐性行为的假设，允许委托人在一定成本下检查特定的行为。如果委托人发现代理人没有选择协定的行为，他可以扣留支付。这个模型的放松引入了更广泛的策略空间，委托人需要在积极激励（增加支付）和负面激励（增加检查）之间进行权衡。作者展示了如何在所有单调检查成本函数中找到最佳的确定性激励兼容检查方案。然后，作者转向随机检查方案，展示了在检查成本函数为次模的情况下可以有效地找到最佳的随机激励兼容检查方案。作者补充说，对于更一般的XOS检查成本函数，不可能有效地找到最佳的随机检查方案。 <br /><br />总结: 本文提出了一个新的委托人-代理人模型，在此模型下，作者展示了如何找到最佳的确定性和随机激励兼容检查方案。 <div>
In the classical principal-agent hidden-action model, a principal delegates
the execution of a costly task to an agent for which he can choose among
actions with different costs and different success probabilities to accomplish
the task. To incentivize the agent to exert effort, the principal can commit to
a contract, which is the amount of payment based on the task's success. A
crucial assumption of this model is that the principal can only base the
payment on the outcome but not on the agent's chosen action.
  In this work, we relax the hidden-action assumption and introduce a new model
where the principal is allowed to inspect subsets of actions at some cost that
depends on the inspected subset. If the principal discovers that the agent did
not select the agreed-upon action through the inspection, the principal can
withhold payment. This relaxation of the model introduces a broader strategy
space for the principal, who now faces a tradeoff between positive incentives
(increasing payment) and negative incentives (increasing inspection).
  We show how to find the best deterministic incentive-compatible inspection
scheme for all monotone inspection cost functions. We then turn to randomized
inspection schemes and show that one can efficiently find the best randomized
incentive-compatible inspection scheme when the inspection cost function is
submodular. We complement this result by showing that it is impossible to
efficiently find the optimal randomized inspection scheme for the more general
case of XOS inspection cost functions.
]]></content:encoded>
<pubDate>2024-02-26T13:26:34Z</pubDate>
</item>
<item>
<title>AgentOhana: Design Unified Data and Training Pipeline for Effective
  Agent Learning</title>
<link>http://arxiv.org/abs/2402.15506v1</link>
<guid>http://arxiv.org/abs/2402.15506v1</guid>
<content:encoded><![CDATA[
<div> AgentOhana, LLMs, agent trajectories, data loader, xLAM-v0.1
总结:<br /><br />本文介绍了AgentOhana作为解决LLMs在agent-based任务中面临挑战的综合解决方案。AgentOhana聚合了不同环境中的agent轨迹，将它们标准化和统一格式，简化了用于agent训练的数据加载器的创建。利用数据的统一性，我们的训练流程在不同数据源之间保持平衡，并在数据集分区和模型训练过程中保持设备独立的随机性。此外，我们还介绍了xLAM-v0.1，这是一个针对AI Agent的大型动作模型，展现了在各种基准测试中的卓越性能。 <div>
Autonomous agents powered by large language models (LLMs) have garnered
significant research attention. However, fully harnessing the potential of LLMs
for agent-based tasks presents inherent challenges due to the heterogeneous
nature of diverse data sources featuring multi-turn trajectories. In this
paper, we introduce \textbf{AgentOhana} as a comprehensive solution to address
these challenges. \textit{AgentOhana} aggregates agent trajectories from
distinct environments, spanning a wide array of scenarios. It meticulously
standardizes and unifies these trajectories into a consistent format,
streamlining the creation of a generic data loader optimized for agent
training. Leveraging the data unification, our training pipeline maintains
equilibrium across different data sources and preserves independent randomness
across devices during dataset partitioning and model training. Additionally, we
present \textbf{xLAM-v0.1}, a large action model tailored for AI agents, which
demonstrates exceptional performance across various benchmarks.
]]></content:encoded>
<pubDate>2024-02-23T18:56:26Z</pubDate>
</item>
<item>
<title>PALO: A Polyglot Large Multimodal Model for 5B People</title>
<link>http://arxiv.org/abs/2402.14818v1</link>
<guid>http://arxiv.org/abs/2402.14818v1</guid>
<content:encoded><![CDATA[
<div> 多语言模型、视觉推理、Palo、跨语言性能、benchmark<br />
在这项研究中，我们介绍了一种名为Palo的大型多语言多模态模型，该模型涵盖了10种主要语言，包括英语、中文、印地语、西班牙语、法语、阿拉伯语、孟加拉语、俄语、乌尔都语和日语，覆盖了约50亿人口（全球人口的65%）。我们采用半自动化翻译方法，利用经过精细调节的大型语言模型，将多模态指令数据集从英语翻译成目标语言，确保高语言准确性的同时，最大程度地减少了手动工作量。我们的方法提高了跨多种语言的整体性能，特别是对一些较少代表的语言，如印地语、阿拉伯语、孟加拉语和乌尔都语。我们训练了三种规模（1.7B、7B和13B参数）的模型，展示了其泛化性和可扩展性，在与强基线模型相比取得了实质性的改进。此外，我们还提出了首个多语言多模态基准测试，用于评估未来方法在各种语言中的视觉与语言推理能力。 <div>
In pursuit of more inclusive Vision-Language Models (VLMs), this study
introduces a Large Multilingual Multimodal Model called \textsc{Palo}.
\textsc{Palo} offers visual reasoning capabilities in 10 major languages,
including English, Chinese, Hindi, Spanish, French, Arabic, Bengali, Russian,
Urdu, and Japanese, that span a total of $\sim$5B people (65\% of the world
population). Our approach involves a semi-automated translation approach to
adapt the multimodal instruction dataset from English to the target languages
using a fine-tuned Large Language Model, thereby ensuring high linguistic
fidelity while allowing scalability due to minimal manual effort. The
incorporation of diverse instruction sets helps us boost overall performance
across multiple languages especially those that are underrepresented like
Hindi, Arabic, Bengali, and Urdu. The resulting models are trained across three
scales (1.7B, 7B and 13B parameters) to show the generalization and scalability
where we observe substantial improvements compared to strong baselines. We also
propose the first multilingual multimodal benchmark for the forthcoming
approaches to evaluate their vision-language reasoning capabilities across
languages. Code: https://github.com/mbzuai-oryx/PALO.
]]></content:encoded>
<pubDate>2024-02-22T18:59:58Z</pubDate>
</item>
<item>
<title>A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit
  Tasks in Public Health</title>
<link>http://arxiv.org/abs/2402.14807v1</link>
<guid>http://arxiv.org/abs/2402.14807v1</guid>
<content:encoded><![CDATA[
<div> 决策语言模型，RMAB，健康资源分配，政策优先级，模拟研究<br />
决策语言模型（DLM）是一个结合了大型语言模型（LLM）和多臂老虎机算法（RMAB）的工具，旨在通过人类语言指令动态调整公共卫生政策。研究通过与印度的ARMMAN合作，展示了DLM能够通过人类语言指令动态塑造政策结果。文章通过提出DLM这一新方法，希望解决在公共卫生领域面临的挑战，包括资源有限、政策优先级不断变化等问题。这一研究对于提高孕产妇的预防保健水平，降低孕产妇死亡率具有重要意义。 <br /><br />总结: <br />决策语言模型（DLM）结合了大型语言模型和多臂老虎机算法，旨在通过人类语言指令动态调整公共卫生政策。研究展示了DLM能够通过人类语言指令动态塑造政策结果。文章提出了这一新方法，希望解决公共卫生领域面临的挑战，对于提高孕产妇的预防保健水平，降低孕产妇死亡率具有重要意义。 <div>
Efforts to reduce maternal mortality rate, a key UN Sustainable Development
target (SDG Target 3.1), rely largely on preventative care programs to spread
critical health information to high-risk populations. These programs face two
important challenges: efficiently allocating limited health resources to large
beneficiary populations, and adapting to evolving policy priorities. While
prior works in restless multi-armed bandit (RMAB) demonstrated success in
public health allocation tasks, they lack flexibility to adapt to evolving
policy priorities. Concurrently, Large Language Models (LLMs) have emerged as
adept, automated planners in various domains, including robotic control and
navigation. In this paper, we propose DLM: a Decision Language Model for RMABs.
To enable dynamic fine-tuning of RMAB policies for challenging public health
settings using human-language commands, we propose using LLMs as automated
planners to (1) interpret human policy preference prompts, (2) propose code
reward functions for a multi-agent RL environment for RMABs, and (3) iterate on
the generated reward using feedback from RMAB simulations to effectively adapt
policy outcomes. In collaboration with ARMMAN, an India-based public health
organization promoting preventative care for pregnant mothers, we conduct a
simulation study, showing DLM can dynamically shape policy outcomes using only
human language commands as input.
]]></content:encoded>
<pubDate>2024-02-22T18:58:27Z</pubDate>
</item>
<item>
<title>OlympiadBench: A Challenging Benchmark for Promoting AGI with
  Olympiad-Level Bilingual Multimodal Scientific Problems</title>
<link>http://arxiv.org/abs/2402.14008v1</link>
<guid>http://arxiv.org/abs/2402.14008v1</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Multimodal Models, OlympiadBench, GPT-4V, 评估方法

- Large Language Models（LLMs）和 Large Multimodal Models（LMMs）已经超过了一般人类在不同任务上的表现
- 文章介绍了一个新的多语言多模态科学竞赛基准（OlympiadBench），包括数学和物理竞赛问题
- 评估了顶尖模型在OlympiadBench上的表现，并发现最好的模型 GPT-4V 在物理方面仅得到了11.28% 的平均分数
- 评估指出 GPT-4V 模型存在幻觉、知识遗漏和逻辑错误的问题
- 希望这一挑战性的基准可以成为未来通用人工智能研究的宝贵资源

<br /><br />总结:
大型语言模型（LLMs）和多模态模型（LMMs）在多个任务上已经超过了一般人类的能力水平。该文章介绍了OlympiadBench，这是一个面向奥林匹克级别的双语多模态科学基准，包含了来自数学和物理竞赛以及中国高考的 8,952 个问题，并进行了专家级别的详细注释。通过在OlympiadBench上评估顶尖模型，文章实施了全面的评估方法来准确评估模型的响应。然而，最好的表现模型 GPT-4V 在OlympiadBench上的平均得分仅为 17.23%，在物理方面仅有 11.28% 的得分，凸显了基准的严谨性和物理推理的复杂性。评估发现 GPT-4V 模型存在幻觉、知识遗漏和逻辑错误等问题。希望这一具有挑战性的基准可以成为未来通用人工智能研究的宝贵资源。 <div>
Recent advancements have seen Large Language Models (LLMs) and Large
Multimodal Models (LMMs) surpassing general human capabilities in various
tasks, approaching the proficiency level of human experts across multiple
domains. With traditional benchmarks becoming less challenging for these
models, new rigorous challenges are essential to gauge their advanced
abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual
multimodal scientific benchmark, featuring 8,952 problems from Olympiad-level
mathematics and physics competitions, including the Chinese college entrance
exam. Each problem is detailed with expert-level annotations for step-by-step
reasoning. Evaluating top-tier models on OlympiadBench, we implement a
comprehensive assessment methodology to accurately evaluate model responses.
Notably, the best-performing model, GPT-4V, attains an average score of 17.23%
on OlympiadBench, with a mere 11.28% in physics, highlighting the benchmark
rigor and the intricacy of physical reasoning. Our analysis orienting GPT-4V
points out prevalent issues with hallucinations, knowledge omissions, and
logical fallacies. We hope that our challenging benchmark can serve as a
valuable resource for helping future AGI research endeavors.
]]></content:encoded>
<pubDate>2024-02-21T18:49:26Z</pubDate>
</item>
<item>
<title>Information Elicitation in Agency Games</title>
<link>http://arxiv.org/abs/2402.14005v1</link>
<guid>http://arxiv.org/abs/2402.14005v1</guid>
<content:encoded><![CDATA[
<div> 观测性问题，信息披露，代理关系，成本相关变量，市场效率<br />
总结：<br />
本文讨论了数据采集和处理工具的快速发展，提出了决定计算哪些评估指标的挑战。作者通过代理关系模型分析了代理何时有动机向委托人披露成本相关变量的可观测性。结果表明，代理倾向于披露能够揭示高低成本之间明显差异的信息，而在披露信息时倾向于进行信息扭曲。最后，作者分析了总体福利问题，指出信息扭曲可能导致更高的总体福利。 <div>
Rapid progress in scalable, commoditized tools for data collection and data
processing has made it possible for firms and policymakers to employ ever more
complex metrics as guides for decision-making. These developments have
highlighted a prevailing challenge -- deciding *which* metrics to compute. In
particular, a firm's ability to compute a wider range of existing metrics does
not address the problem of *unknown unknowns*, which reflects informational
limitations on the part of the firm. To guide the choice of metrics in the face
of this informational problem, we turn to the evaluated agents themselves, who
may have more information than a principal about how to measure outcomes
effectively. We model this interaction as a simple agency game, where we ask:
*When does an agent have an incentive to reveal the observability of a
cost-correlated variable to the principal?* There are two effects: better
information reduces the agent's information rents but also makes some projects
go forward that otherwise would fail. We show that the agent prefers to reveal
information that exposes a strong enough differentiation between high and low
costs. Expanding the agent's action space to include the ability to *garble*
their information, we show that the agent often prefers to garble over full
revelation. Still, giving the agent the ability to garble can lead to higher
total welfare. Our model has analogies with price discrimination, and we
leverage some of these synergies to analyze total welfare.
]]></content:encoded>
<pubDate>2024-02-21T18:44:38Z</pubDate>
</item>
<item>
<title>CounterCurate: Enhancing Physical and Semantic Visio-Linguistic
  Compositional Reasoning via Counterfactual Examples</title>
<link>http://arxiv.org/abs/2402.13254v1</link>
<guid>http://arxiv.org/abs/2402.13254v1</guid>
<content:encoded><![CDATA[
<div> 对应关键词: CounterCurate, visio-linguistic compositional reasoning, physically grounded reasoning, data augmentation, semantic counterfactuals

本研究提出了CounterCurate框架，旨在全面提高对比和生成式多模态模型的视觉-语言组合推理能力。首先，发现目前模型在与物理相关的组合推理方面表现不佳，然后利用GLIGEN模型进行简单的数据增强，显著提高了模型性能，尤其是在新创建的Flickr30k-Positions基准测试中，CLIP和LLaVA的性能分别提高了33%和37%。其次，利用高性能文本和图像生成模型（GPT-4V和DALLE-3）创建具有挑战性的语义反事实情况，从而进一步提高了组合推理能力，在SugarCrepe基准测试中，CounterCurate的表现超过了GPT-4V。<br /><br />总结: 本研究提出了CounterCurate框架，通过解决当前模型在物理相关推理和反事实情况处理上的不足，提高了多模态模型的组合推理能力。 <div>
We propose CounterCurate, a framework to comprehensively improve the
visio-linguistic compositional reasoning capability for both contrastive and
generative multimodal models. In particular, we identify two under-explored
critical problems: the neglect of the physically grounded reasoning (counting
and position understanding) and the potential of using highly capable text and
image generation models for semantic counterfactual fine-tuning. Our work
pioneers an approach that addresses these gaps. We first spotlight the
near-chance performance of multimodal models like CLIP and LLaVA in physically
grounded compositional reasoning. We then apply simple data augmentation using
a grounded image generation model, GLIGEN, to generate finetuning data,
resulting in significant performance improvements: +33% and +37% for CLIP and
LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark.
Moreover, we exploit the capabilities of high-performing text generation and
image generation models, specifically GPT-4V and DALLE-3, to curate challenging
semantic counterfactuals, thereby further enhancing compositional reasoning
capabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms
GPT-4V.
]]></content:encoded>
<pubDate>2024-02-20T18:59:55Z</pubDate>
</item>
<item>
<title>Fusion of Diffusion Weighted MRI and Clinical Data for Predicting
  Functional Outcome after Acute Ischemic Stroke with Deep Contrastive Learning</title>
<link>http://arxiv.org/abs/2402.10894v1</link>
<guid>http://arxiv.org/abs/2402.10894v1</guid>
<content:encoded><![CDATA[
<div> 病中风，扩散加权MRI，健康结构概要，预测功能结果，深度融合学习网络

总结:<br />
文章探讨了使用扩散加权MRI模态结合健康结构概要对预测中风患者功能结果的有效性，以促进早期干预。提出了两阶段训练的深度融合学习网络，采用监督对比学习来学习区分特征，预测患者在中风发作后3个月是否需要长期护理。研究发现，所提出的融合模型在AUC、F1分数和准确性方面均表现优异，优于现有模型，尤其在医疗领域的结合图像和结构数据模型中。此外，扩散加权MRI可以与其他临床变量结合以获得更好的普适性和准确性，甚至可以替代NIHSS以实现同等水平的准确性。 <div>
Stroke is a common disabling neurological condition that affects about
one-quarter of the adult population over age 25; more than half of patients
still have poor outcomes, such as permanent functional dependence or even
death, after the onset of acute stroke. The aim of this study is to investigate
the efficacy of diffusion-weighted MRI modalities combining with structured
health profile on predicting the functional outcome to facilitate early
intervention. A deep fusion learning network is proposed with two-stage
training: the first stage focuses on cross-modality representation learning and
the second stage on classification. Supervised contrastive learning is
exploited to learn discriminative features that separate the two classes of
patients from embeddings of individual modalities and from the fused multimodal
embedding. The network takes as the input DWI and ADC images, and structured
health profile data. The outcome is the prediction of the patient needing
long-term care at 3 months after the onset of stroke. Trained and evaluated
with a dataset of 3297 patients, our proposed fusion model achieves 0.87, 0.80
and 80.45% for AUC, F1-score and accuracy, respectively, outperforming existing
models that consolidate both imaging and structured data in the medical domain.
If trained with comprehensive clinical variables, including NIHSS and
comorbidities, the gain from images on making accurate prediction is not
considered substantial, but significant. However, diffusion-weighted MRI can
replace NIHSS to achieve comparable level of accuracy combining with other
readily available clinical variables for better generalization.
]]></content:encoded>
<pubDate>2024-02-16T18:51:42Z</pubDate>
</item>
<item>
<title>When is Tree Search Useful for LLM Planning? It Depends on the
  Discriminator</title>
<link>http://arxiv.org/abs/2402.10890v1</link>
<guid>http://arxiv.org/abs/2402.10890v1</guid>
<content:encoded><![CDATA[
<div> 大语言模型，语言代理，规划方法，迭代校正，树搜索<br />
这篇论文研究了大语言模型在多步问题下的解决方法，使用了语言代理框架的三个组件：生成器，鉴别器和规划方法。实验表明，使用迭代校正和树搜索这两种高级规划方法要求鉴别器至少有90%准确率才能取得明显的改进效果，而当前的大语言模型的鉴别能力并未达到这一要求。另外，使用基于大语言模型的鉴别器时，高级规划方法可能未能很好地平衡准确性和效率，例如，相较于其他两种方法，树搜索至少慢了10-20倍，但带来的性能提升微乎其微，这阻碍了其在实际应用中的使用。总结：<br /><br />这篇论文研究了大语言模型在多步问题下的解决方法，使用了语言代理框架的三个组件：生成器，鉴别器和规划方法。实验表明，使用迭代校正和树搜索这两种高级规划方法要求鉴别器至少有90%准确率才能取得明显的改进效果，而当前的大语言模型的鉴别能力并未达到这一要求。另外，使用基于大语言模型的鉴别器时，高级规划方法可能未能很好地平衡准确性和效率，例如，相较于其他两种方法，树搜索至少慢了10-20倍，但带来的性能提升微乎其微，这阻碍了其在实际应用中的使用。 <div>
In this paper, we examine how large language models (LLMs) solve multi-step
problems under a language agent framework with three components: a generator, a
discriminator, and a planning method. We investigate the practical utility of
two advanced planning methods, iterative correction and tree search. We present
a comprehensive analysis of how discrimination accuracy affects the overall
performance of agents when using these two methods or a simpler method,
re-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical
reasoning, show that: (1) advanced planning methods demand discriminators with
at least 90% accuracy to achieve significant improvements over re-ranking; (2)
current LLMs' discrimination abilities have not met the needs of advanced
planning methods to achieve such improvements; (3) with LLM-based
discriminators, advanced planning methods may not adequately balance accuracy
and efficiency. For example, compared to the other two methods, tree search is
at least 10--20 times slower but leads to negligible performance gains, which
hinders its real-world applications. Code and data will be released at
https://github.com/OSU-NLP-Group/llm-planning-eval.
]]></content:encoded>
<pubDate>2024-02-16T18:45:58Z</pubDate>
</item>
<item>
<title>A Trembling House of Cards? Mapping Adversarial Attacks against Language
  Agents</title>
<link>http://arxiv.org/abs/2402.10196v1</link>
<guid>http://arxiv.org/abs/2402.10196v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 语言代理、大型语言模型、自动化技术、安全风险、攻击场景
总结: 
语言代理基于大型语言模型的自动化技术发展迅猛，但其在安全风险方面存在着新的挑战。本文首次系统地探讨了针对语言代理的对抗性攻击，并提出了12种潜在的攻击场景，涵盖了不同的攻击策略。在三个主要组成部分（感知、大脑、行动）的框架下，对攻击场景进行了全面讨论，并与先前应用于大型语言模型的成功攻击策略进行了联系。强调了在广泛部署语言代理之前，我们迫切需要全面了解语言代理的风险。 <br /><br /> <div>
Language agents powered by large language models (LLMs) have seen exploding
development. Their capability of using language as a vehicle for thought and
communication lends an incredible level of flexibility and versatility. People
have quickly capitalized on this capability to connect LLMs to a wide range of
external components and environments: databases, tools, the Internet, robotic
embodiment, etc. Many believe an unprecedentedly powerful automation technology
is emerging. However, new automation technologies come with new safety risks,
especially for intricate systems like language agents. There is a surprisingly
large gap between the speed and scale of their development and deployment and
our understanding of their safety risks. Are we building a house of cards? In
this position paper, we present the first systematic effort in mapping
adversarial attacks against language agents. We first present a unified
conceptual framework for agents with three major components: Perception, Brain,
and Action. Under this framework, we present a comprehensive discussion and
propose 12 potential attack scenarios against different components of an agent,
covering different attack strategies (e.g., input manipulation, adversarial
demonstrations, jailbreaking, backdoors). We also draw connections to
successful attack strategies previously applied to LLMs. We emphasize the
urgency to gain a thorough understanding of language agent risks before their
widespread deployment.
]]></content:encoded>
<pubDate>2024-02-15T18:51:32Z</pubDate>
</item>
<item>
<title>Rec-GPT4V: Multimodal Recommendation with Large Vision-Language Models</title>
<link>http://arxiv.org/abs/2402.08670v1</link>
<guid>http://arxiv.org/abs/2402.08670v1</guid>
<content:encoded><![CDATA[
<div> 关键词: large vision-language models, multimodal recommendations, Rec-GPT4V, user preferences, image sequence dynamics

总结: <br /><br />这篇文章讨论了大型视觉语言模型在多模态推荐中的应用，指出了其存在的挑战和复杂性。为了克服这些问题，提出了一种名为Rec-GPT4V的新颖推理方案：Visual-Summary Thought（VST），并利用用户历史作为上下文用户偏好来解决LVLMs缺乏用户偏好知识的问题。接下来，文章介绍了如何利用LVLMs生成项目图像摘要，并结合项目标题在自然语言空间中查询用户对候选项目的偏好。最后，通过对四个数据集和三种LVLMs进行全面实验，结果表明了VST的有效性。 <div>
The development of large vision-language models (LVLMs) offers the potential
to address challenges faced by traditional multimodal recommendations thanks to
their proficient understanding of static images and textual dynamics. However,
the application of LVLMs in this field is still limited due to the following
complexities: First, LVLMs lack user preference knowledge as they are trained
from vast general datasets. Second, LVLMs suffer setbacks in addressing
multiple image dynamics in scenarios involving discrete, noisy, and redundant
image sequences. To overcome these issues, we propose the novel reasoning
scheme named Rec-GPT4V: Visual-Summary Thought (VST) of leveraging large
vision-language models for multimodal recommendation. We utilize user history
as in-context user preferences to address the first challenge. Next, we prompt
LVLMs to generate item image summaries and utilize image comprehension in
natural language space combined with item titles to query the user preferences
over candidate items. We conduct comprehensive experiments across four datasets
with three LVLMs: GPT4-V, LLaVa-7b, and LLaVa-13b. The numerical results
indicate the efficacy of VST.
]]></content:encoded>
<pubDate>2024-02-13T18:51:18Z</pubDate>
</item>
<item>
<title>MODIPHY: Multimodal Obscured Detection for IoT using PHantom
  Convolution-Enabled Faster YOLO</title>
<link>http://arxiv.org/abs/2402.07894v1</link>
<guid>http://arxiv.org/abs/2402.07894v1</guid>
<content:encoded><![CDATA[
<div> 模型压缩, YOLO Phantom, 低光条件, 多模态数据, 实时性能

总结:<br />
本研究介绍了一种名为"YOLO Phantom"的小型YOLO模型，利用全新的Phantom卷积块实现了可比较的准确性，同时减少了43%的参数和模型大小，减少了19%的GFLOPs。YOLO Phantom利用多模态RGB-红外数据进行迁移学习，解决了低光和遮挡问题，使其在恶劣条件下具有强大的视觉能力。在物联网平台上，与先进的低光和RGB摄像头无缝连接，连接到基于AWS的通知端点，实现了高效的实时目标检测。基准测试显示，与基线YOLOv8n模型相比，热成像和RGB检测的帧率提升了17%和14%。为了贡献于社区，代码和多模态数据集都在GitHub上可用。 <div>
Low-light conditions and occluded scenarios impede object detection in
real-world Internet of Things (IoT) applications like autonomous vehicles and
security systems. While advanced machine learning models strive for accuracy,
their computational demands clash with the limitations of resource-constrained
devices, hampering real-time performance. In our current research, we tackle
this challenge, by introducing "YOLO Phantom", one of the smallest YOLO models
ever conceived. YOLO Phantom utilizes the novel Phantom Convolution block,
achieving comparable accuracy to the latest YOLOv8n model while simultaneously
reducing both parameters and model size by 43%, resulting in a significant 19%
reduction in Giga Floating Point Operations (GFLOPs). YOLO Phantom leverages
transfer learning on our multimodal RGB-infrared dataset to address low-light
and occlusion issues, equipping it with robust vision under adverse conditions.
Its real-world efficacy is demonstrated on an IoT platform with advanced
low-light and RGB cameras, seamlessly connecting to an AWS-based notification
endpoint for efficient real-time object detection. Benchmarks reveal a
substantial boost of 17% and 14% in frames per second (FPS) for thermal and RGB
detection, respectively, compared to the baseline YOLOv8n model. For community
contribution, both the code and the multimodal dataset are available on GitHub.
]]></content:encoded>
<pubDate>2024-02-12T18:56:53Z</pubDate>
</item>
<item>
<title>MAIDCRL: Semi-centralized Multi-Agent Influence Dense-CNN Reinforcement
  Learning</title>
<link>http://arxiv.org/abs/2402.07890v1</link>
<guid>http://arxiv.org/abs/2402.07890v1</guid>
<content:encoded><![CDATA[
<div> Distributed decision-making, multi-agent systems, interactive behavior learning, Dense Reinforcement Learning, agent influence maps<br />
<br />
在多智能体系统中，分布式决策面临着巨大挑战，尤其是在合作和竞争系统中的交互式行为学习方面。为了缓解这种复杂性，本文介绍了一种半集中式的Dense强化学习算法，通过智能体影响图（AIMs）来增强对StarCraft多智能体挑战（SMAC）场景中有效的多智能体控制的学习。我们扩展了MAIDRL中的DenseNet，并引入了半集中式多智能体Dense-CNN强化学习（MAIDCRL），通过将卷积层结合到深度模型架构中，并在同质和异质场景中进行性能评估。结果显示，CNN使得MAIDCRL显著提高了学习性能，并在复杂的异质SMAC场景中取得了更快的学习速度。我们进一步调查了模型的稳定性和鲁棒性。统计数据表明，我们的模型不仅在所有给定场景中实现了更高的胜率，而且提升了智能体的细粒度决策过程的学习。 <br /><br />总结: <br />该研究介绍了一种半集中式Dense-CNN强化学习算法MAIDCRL，该算法通过智能体影响图（AIMs）在StarCraft多智能体挑战（SMAC）场景中实现了有效的多智能体控制。研究结果表明，在异质场景中，CNN-enabled MAIDCRL显著提高了学习性能并实现了更快的学习速度。该模型不仅在所有给定场景中实现了更高的胜率，而且提升了智能体的细粒度决策过程的学习。 <div>
Distributed decision-making in multi-agent systems presents difficult
challenges for interactive behavior learning in both cooperative and
competitive systems. To mitigate this complexity, MAIDRL presents a
semi-centralized Dense Reinforcement Learning algorithm enhanced by agent
influence maps (AIMs), for learning effective multi-agent control on StarCraft
Multi-Agent Challenge (SMAC) scenarios. In this paper, we extend the DenseNet
in MAIDRL and introduce semi-centralized Multi-Agent Dense-CNN Reinforcement
Learning, MAIDCRL, by incorporating convolutional layers into the deep model
architecture, and evaluate the performance on both homogeneous and
heterogeneous scenarios. The results show that the CNN-enabled MAIDCRL
significantly improved the learning performance and achieved a faster learning
rate compared to the existing MAIDRL, especially on more complicated
heterogeneous SMAC scenarios. We further investigate the stability and
robustness of our model. The statistics reflect that our model not only
achieves higher winning rate in all the given scenarios but also boosts the
agent's learning process in fine-grained decision-making.
]]></content:encoded>
<pubDate>2024-02-12T18:53:20Z</pubDate>
</item>
<item>
<title>Feedback Loops With Language Models Drive In-Context Reward Hacking</title>
<link>http://arxiv.org/abs/2402.06627v1</link>
<guid>http://arxiv.org/abs/2402.06627v1</guid>
<content:encoded><![CDATA[
<div> 反馈循环, 语言模型, 奖励欺骗, 输出精化, 策略精化
总结:
反馈循环会导致在上下文环境中发生奖励欺骗，语言模型在测试时会优化一个潜在的目标，但在过程中会产生负面影响。输出精化和策略精化是导致奖励欺骗的两个过程。在静态数据集上的评估不足以捕捉最有害的行为。因此，建议使用三种评估方法来捕捉更多奖励欺骗的情况。随着人工智能的发展加速，反馈循环的影响将会增加，这就增加了理解它们在塑造语言模型行为方面的重要性。 <div>
Language models influence the external world: they query APIs that read and
write to web pages, generate content that shapes human behavior, and run system
commands as autonomous agents. These interactions form feedback loops: LLM
outputs affect the world, which in turn affect subsequent LLM outputs. In this
work, we show that feedback loops can cause in-context reward hacking (ICRH),
where the LLM at test-time optimizes a (potentially implicit) objective but
creates negative side effects in the process. For example, consider an LLM
agent deployed to increase Twitter engagement; the LLM may retrieve its
previous tweets into the context window and make them more controversial,
increasing engagement but also toxicity. We identify and study two processes
that lead to ICRH: output-refinement and policy-refinement. For these
processes, evaluations on static datasets are insufficient -- they miss the
feedback effects and thus cannot capture the most harmful behavior. In
response, we provide three recommendations for evaluation to capture more
instances of ICRH. As AI development accelerates, the effects of feedback loops
will proliferate, increasing the need to understand their role in shaping LLM
behavior.
]]></content:encoded>
<pubDate>2024-02-09T18:59:29Z</pubDate>
</item>
<item>
<title>SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large
  Language Models</title>
<link>http://arxiv.org/abs/2402.05935v1</link>
<guid>http://arxiv.org/abs/2402.05935v1</guid>
<content:encoded><![CDATA[
<div> SPHINX-X, Multimodality, Large Language Model, dataset, training efficiency
<br /><br />
总结:SPHINX-X是在SPHINX基础上开发的一系列大型多模态语言模型（MLLM），通过修改框架并简化多阶段训练，提高了架构和训练效率。他们组建了一个包括语言、视觉和视觉-语言任务的多领域、多模态数据集，并且通过TinyLlama1.1B、InternLM2-7B、LLaMA2-13B和Mixtral8x7B等不同基础LLM的训练，获得了参数大小和多语言能力各异的MLLM。综合基准测试显示了多模态性能与数据和参数规模之间的强相关性。代码和模型已经在https://github.com/Alpha-VLLM/LLaMA2-Accessory上发布。 <div>
We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM)
series developed upon SPHINX. To improve the architecture and training
efficiency, we modify the SPHINX framework by removing redundant visual
encoders, bypassing fully-padded sub-images with skip tokens, and simplifying
multi-stage training into a one-stage all-in-one paradigm. To fully unleash the
potential of MLLMs, we assemble a comprehensive multi-domain and multimodal
dataset covering publicly available resources in language, vision, and
vision-language tasks. We further enrich this collection with our curated OCR
intensive and Set-of-Mark datasets, extending the diversity and generality. By
training over different base LLMs including TinyLlama1.1B, InternLM2-7B,
LLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in
parameter size and multilingual capabilities. Comprehensive benchmarking
reveals a strong correlation between the multi-modal performance with the data
and parameter scales. Code and models are released at
https://github.com/Alpha-VLLM/LLaMA2-Accessory
]]></content:encoded>
<pubDate>2024-02-08T18:59:48Z</pubDate>
</item>
<item>
<title>An Interactive Agent Foundation Model</title>
<link>http://arxiv.org/abs/2402.05929v1</link>
<guid>http://arxiv.org/abs/2402.05929v1</guid>
<content:encoded><![CDATA[
<div> Agent-based systems, multi-task agent training paradigm, versatile AI framework, Robotics, Gaming AI, Healthcare

发展中的人工智能系统正从创建静态的、特定任务的模型转变为能够在广泛应用中表现良好的动态的基于代理的系统。本文提出了一个交互式Agent Foundation模型，采用新颖的多任务代理训练范式，用于跨多个领域、数据集和任务训练AI代理。我们的训练范式统一了各种预训练策略，包括视觉蒙版自动编码器、语言建模和下一个动作预测，实现了多才多艺的、适应性强的AI框架。我们展示了我们的框架在三个不同领域--机器人技术、游戏人工智能和医疗保健领域的性能。我们的模型展示了它在每个领域产生有意义和上下文相关的输出的能力。我们的方法的优势在于其通用性，利用各种数据源，如机器人序列、游戏数据、大规模视频数据集和文字信息，进行有效的多模态和多任务学习。我们的方法为开发通用、行动取向、多模态系统提供了一个有前途的途径。<br /><br />总结: 人工智能系统的发展正在朝着能够适用于多领域的动态代理系统转变，该模型利用多任务训练范式和多种预训练策略，展示了在机器人技术、游戏人工智能和医疗保健领域的良好性能，为开发通用而多才多艺的AI系统提供了有前途的方向。 <div>
The development of artificial intelligence systems is transitioning from
creating static, task-specific models to dynamic, agent-based systems capable
of performing well in a wide range of applications. We propose an Interactive
Agent Foundation Model that uses a novel multi-task agent training paradigm for
training AI agents across a wide range of domains, datasets, and tasks. Our
training paradigm unifies diverse pre-training strategies, including visual
masked auto-encoders, language modeling, and next-action prediction, enabling a
versatile and adaptable AI framework. We demonstrate the performance of our
framework across three separate domains -- Robotics, Gaming AI, and Healthcare.
Our model demonstrates its ability to generate meaningful and contextually
relevant outputs in each area. The strength of our approach lies in its
generality, leveraging a variety of data sources such as robotics sequences,
gameplay data, large-scale video datasets, and textual information for
effective multimodal and multi-task learning. Our approach provides a promising
avenue for developing generalist, action-taking, multimodal systems.
]]></content:encoded>
<pubDate>2024-02-08T18:58:02Z</pubDate>
</item>
<item>
<title>WebLINX: Real-World Website Navigation with Multi-Turn Dialogue</title>
<link>http://arxiv.org/abs/2402.05930v1</link>
<guid>http://arxiv.org/abs/2402.05930v1</guid>
<content:encoded><![CDATA[
We propose the problem of conversational web navigation, where a digital
agent controls a web browser and follows user instructions to solve real-world
tasks in a multi-turn dialogue fashion. To support this problem, we introduce
WEBLINX - a large-scale benchmark of 100K interactions across 2300 expert
demonstrations of conversational web navigation. Our benchmark covers a broad
range of patterns on over 150 real-world websites and can be used to train and
evaluate agents in diverse scenarios. Due to the magnitude of information
present, Large Language Models (LLMs) cannot process entire web pages in
real-time. To solve this bottleneck, we design a retrieval-inspired model that
efficiently prunes HTML pages by ranking relevant elements. We use the selected
elements, along with screenshots and action history, to assess a variety of
models for their ability to replicate human behavior when navigating the web.
Our experiments span from small text-only to proprietary multimodal LLMs. We
find that smaller finetuned decoders surpass the best zero-shot LLMs (including
GPT-4V), but also larger finetuned multimodal models which were explicitly
pretrained on screenshots. However, all finetuned models struggle to generalize
to unseen websites. Our findings highlight the need for large multimodal models
that can generalize to novel settings. Our code, data and models are available
for research: https://mcgill-nlp.github.io/weblinx
]]></content:encoded>
<pubDate>2024-02-08T18:58:02Z</pubDate>
</item>
<item>
<title>Language-Based Augmentation to Address Shortcut Learning in Object Goal
  Navigation</title>
<link>http://arxiv.org/abs/2402.05090v1</link>
<guid>http://arxiv.org/abs/2402.05090v1</guid>
<content:encoded><![CDATA[
<div> DRL, Object-Goal Navigation, Shortcut learning, Language-Based (L-B) augmentation, Vision-Language Model (VLM)
<br /><br />总结:
本文研究了深度强化学习在目标导航中的应用，发现在训练环境中存在快捷学习的问题，即代理程序学习到了针对特定环境细节的策略。作者设计了一个实验，证明了快捷学习的存在，并提出了基于语言的增强方法来解决这一问题。他们发现，通过在视觉-语言模型的多模态特征空间中进行增强，可以降低代理程序在新环境中的成功率下降，从而解决了快捷学习问题。 <div>
Deep Reinforcement Learning (DRL) has shown great potential in enabling
robots to find certain objects (e.g., `find a fridge') in environments like
homes or schools. This task is known as Object-Goal Navigation (ObjectNav). DRL
methods are predominantly trained and evaluated using environment simulators.
Although DRL has shown impressive results, the simulators may be biased or
limited. This creates a risk of shortcut learning, i.e., learning a policy
tailored to specific visual details of training environments. We aim to deepen
our understanding of shortcut learning in ObjectNav, its implications and
propose a solution. We design an experiment for inserting a shortcut bias in
the appearance of training environments. As a proof-of-concept, we associate
room types to specific wall colors (e.g., bedrooms with green walls), and
observe poor generalization of a state-of-the-art (SOTA) ObjectNav method to
environments where this is not the case (e.g., bedrooms with blue walls). We
find that shortcut learning is the root cause: the agent learns to navigate to
target objects, by simply searching for the associated wall color of the target
object's room. To solve this, we propose Language-Based (L-B) augmentation. Our
key insight is that we can leverage the multimodal feature space of a
Vision-Language Model (VLM) to augment visual representations directly at the
feature-level, requiring no changes to the simulator, and only an addition of
one layer to the model. Where the SOTA ObjectNav method's success rate drops
69%, our proposal has only a drop of 23%.
]]></content:encoded>
<pubDate>2024-02-07T18:44:27Z</pubDate>
</item>
<item>
<title>AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls</title>
<link>http://arxiv.org/abs/2402.04253v1</link>
<guid>http://arxiv.org/abs/2402.04253v1</guid>
<content:encoded><![CDATA[
<div> APIs, AnyTool, GPT-4, evaluation protocol, benchmark<br />
<br />总结:
本文介绍了一个名为AnyTool的大型语言模型代理，旨在通过利用大量工具来解决用户查询的方式来彻底改革工具的利用。通过利用超过16,000个来自Rapid API的API，任何工具主要包括三个元素：具有分层结构的API检索器、旨在使用一组选择的API候选解决用户查询的求解器，以及一个自我反思机制，当初始解决方案不可行时重新激活AnyTool。AnyTool由GPT-4的函数调用功能驱动，消除了训练外部模块的需要。作者还重新访问了之前作品引入的评估协议，并确定了这一协议存在的限制，导致人为地高通过率。通过修改评估协议以更好地反映实际应用场景，他们引入了另一个基准，称为AnyToolBench。在各种数据集上进行的实验表明，AnyTool优于强基准，例如ToolLLM和专门用于工具利用的GPT-4变体。例如，在ToolBench的平均通过率上，AnyTool的表现优于ToolLLM 35.4%。代码将在https://github.com/dyabel/AnyTool 上提供。 <div>
We introduce AnyTool, a large language model agent designed to revolutionize
the utilization of a vast array of tools in addressing user queries. We utilize
over 16,000 APIs from Rapid API, operating under the assumption that a subset
of these APIs could potentially resolve the queries. AnyTool primarily
incorporates three elements: an API retriever with a hierarchical structure, a
solver aimed at resolving user queries using a selected set of API candidates,
and a self-reflection mechanism, which re-activates AnyTool if the initial
solution proves impracticable. AnyTool is powered by the function calling
feature of GPT-4, eliminating the need for training external modules. We also
revisit the evaluation protocol introduced by previous works and identify a
limitation in this protocol that leads to an artificially high pass rate. By
revising the evaluation protocol to better reflect practical application
scenarios, we introduce an additional benchmark, termed AnyToolBench.
Experiments across various datasets demonstrate the superiority of our AnyTool
over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool
utilization. For instance, AnyTool outperforms ToolLLM by +35.4% in terms of
average pass rate on ToolBench. Code will be available at
https://github.com/dyabel/AnyTool.
]]></content:encoded>
<pubDate>2024-02-06T18:59:57Z</pubDate>
</item>
<item>
<title>EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters</title>
<link>http://arxiv.org/abs/2402.04252v1</link>
<guid>http://arxiv.org/abs/2402.04252v1</guid>
<content:encoded><![CDATA[
<div> 关键词: CLIP, EVA-CLIP-18B, 18-billion参数, 图像分类基准, EVA-style弱到强视觉模型缩放<br />
<br />
这篇文章介绍了EVA-CLIP-18B，这是迄今为止最大、最强大的开源CLIP模型，拥有180亿个参数。该模型在27个广泛认可的图像分类基准中取得了异常出色的零样本top-1准确率达80.7%，表现优于其前身EVA-CLIP（50亿参数）和其他开源CLIP模型。令人惊讶的是，尽管保持对LAION-2B和COYO-700M的20亿图像文本配对的训练数据集不变，EVA-CLIP模型规模的扩大仍然能够保持一致的性能改进。EVA-CLIP-18B展示了EVA风格的弱到强视觉模型缩放的潜力。通过公开发布模型权重，希望能促进未来在视觉和多模型基础模型上的研究。<br /><br />总结: 本文介绍了EVA-CLIP-18B，这是迄今为止最大、最强大的开源CLIP模型，拥有180亿个参数。该模型表现出色，通过公开发布模型权重，希望促进未来研究。 <div>
Scaling up contrastive language-image pretraining (CLIP) is critical for
empowering both vision and multimodal models. We present EVA-CLIP-18B, the
largest and most powerful open-source CLIP model to date, with 18-billion
parameters. With only 6-billion training samples seen, EVA-CLIP-18B achieves an
exceptional 80.7% zero-shot top-1 accuracy averaged across 27 widely recognized
image classification benchmarks, outperforming its forerunner EVA-CLIP
(5-billion parameters) and other open-source CLIP models by a large margin.
Remarkably, we observe a consistent performance improvement with the model size
scaling of EVA-CLIP, despite maintaining a constant training dataset of
2-billion image-text pairs from LAION-2B and COYO-700M. This dataset is openly
available and much smaller than the in-house datasets (e.g., DFN-5B, WebLI-10B)
employed in other state-of-the-art CLIP models. EVA-CLIP-18B demonstrates the
potential of EVA-style weak-to-strong visual model scaling. With our model
weights made publicly available, we hope to facilitate future research in
vision and multimodal foundation models.
]]></content:encoded>
<pubDate>2024-02-06T18:59:48Z</pubDate>
</item>
<item>
<title>Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science</title>
<link>http://arxiv.org/abs/2402.04247v1</link>
<guid>http://arxiv.org/abs/2402.04247v1</guid>
<content:encoded><![CDATA[
Intelligent agents powered by large language models (LLMs) have demonstrated
substantial promise in autonomously conducting experiments and facilitating
scientific discoveries across various disciplines. While their capabilities are
promising, they also introduce novel vulnerabilities that demand careful
consideration for safety. However, there exists a notable gap in the
literature, as there has been no comprehensive exploration of these
vulnerabilities. This position paper fills this gap by conducting a thorough
examination of vulnerabilities in LLM-based agents within scientific domains,
shedding light on potential risks associated with their misuse and emphasizing
the need for safety measures. We begin by providing a comprehensive overview of
the potential risks inherent to scientific LLM agents, taking into account user
intent, the specific scientific domain, and their potential impact on the
external environment. Then, we delve into the origins of these vulnerabilities
and provide a scoping review of the limited existing works. Based on our
analysis, we propose a triadic framework involving human regulation, agent
alignment, and an understanding of environmental feedback (agent regulation) to
mitigate these identified risks. Furthermore, we highlight the limitations and
challenges associated with safeguarding scientific agents and advocate for the
development of improved models, robust benchmarks, and comprehensive
regulations to address these issues effectively.
]]></content:encoded>
<pubDate>2024-02-06T18:54:07Z</pubDate>
</item>
<item>
<title>V-IRL: Grounding Virtual Intelligence in Real Life</title>
<link>http://arxiv.org/abs/2402.03310v1</link>
<guid>http://arxiv.org/abs/2402.03310v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 感知, AI代理, 环境, 虚拟平台, 互动
总结:<br /><br />这篇文章介绍了一个名为V-IRL的平台，旨在通过虚拟环境让AI代理能够在真实世界中交互。该平台能够帮助开发能够完成各种实际任务的代理，并且可作为一个广阔的测试基地，用于测量在感知、决策和与全球各地真实世界数据交互等能力方面的进展。通过在虚拟而又真实的环境中实现代理的具体体现，可以弥合数字世界和真实世界之间的现实差距。 <div>
There is a sensory gulf between the Earth that humans inhabit and the digital
realms in which modern AI agents are created. To develop AI agents that can
sense, think, and act as flexibly as humans in real-world settings, it is
imperative to bridge the realism gap between the digital and physical worlds.
How can we embody agents in an environment as rich and diverse as the one we
inhabit, without the constraints imposed by real hardware and control? Towards
this end, we introduce V-IRL: a platform that enables agents to scalably
interact with the real world in a virtual yet realistic environment. Our
platform serves as a playground for developing agents that can accomplish
various practical tasks and as a vast testbed for measuring progress in
capabilities spanning perception, decision-making, and interaction with
real-world data across the entire globe.
]]></content:encoded>
<pubDate>2024-02-05T18:59:36Z</pubDate>
</item>
<item>
<title>AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion</title>
<link>http://arxiv.org/abs/2402.03309v1</link>
<guid>http://arxiv.org/abs/2402.03309v1</guid>
<content:encoded><![CDATA[
<div> acoustic-optical neural surface reconstruction, underwater perception, 3D surface reconstruction, baselines, multimodal fusion<br />
<br />
在水下感知和三维表面重建领域存在着诸多挑战，涉及到建筑、安全、海洋考古和环境监测等广泛领域的应用。由于恶劣的操作条件、脆弱的环境和有限的导航控制，潜水器通常需要限制其运动范围，因此也限制了其测量基线。我们的研究开发了一种基于物理的多模式声光神经表面重建框架（AONeuS），能够有效地将高分辨率的RGB测量与低分辨率的深度成像声纳测量相结合。通过融合这些互补的模态，我们的框架可以从受限基线上捕获的测量中重建准确的高分辨率三维表面。通过大量的模拟和实验，我们证明AONeuS显著优于最近的仅RGB和仅声纳的反差分渲染表面重建方法。我们的论文结果可通过以下网址查看：https://aoneus.github.io/ <br /><br />总结: 水下感知和三维表面重建面临诸多挑战，AONeuS框架成功融合了声光神经表面重建，能够在受限的基线下实现准确的高分辨率三维表面重建，并且优于之前的方法。 <div>
Underwater perception and 3D surface reconstruction are challenging problems
with broad applications in construction, security, marine archaeology, and
environmental monitoring. Treacherous operating conditions, fragile
surroundings, and limited navigation control often dictate that submersibles
restrict their range of motion and, thus, the baseline over which they can
capture measurements. In the context of 3D scene reconstruction, it is
well-known that smaller baselines make reconstruction more challenging. Our
work develops a physics-based multimodal acoustic-optical neural surface
reconstruction framework (AONeuS) capable of effectively integrating
high-resolution RGB measurements with low-resolution depth-resolved imaging
sonar measurements. By fusing these complementary modalities, our framework can
reconstruct accurate high-resolution 3D surfaces from measurements captured
over heavily-restricted baselines. Through extensive simulations and in-lab
experiments, we demonstrate that AONeuS dramatically outperforms recent
RGB-only and sonar-only inverse-differentiable-rendering--based surface
reconstruction methods. A website visualizing the results of our paper is
located at this address: https://aoneus.github.io/
]]></content:encoded>
<pubDate>2024-02-05T18:59:31Z</pubDate>
</item>
<item>
<title>Do Diffusion Models Learn Semantically Meaningful and Efficient
  Representations?</title>
<link>http://arxiv.org/abs/2402.03305v1</link>
<guid>http://arxiv.org/abs/2402.03305v1</guid>
<content:encoded><![CDATA[
Diffusion models are capable of impressive feats of image generation with
uncommon juxtapositions such as astronauts riding horses on the moon with
properly placed shadows. These outputs indicate the ability to perform
compositional generalization, but how do the models do so? We perform
controlled experiments on conditional DDPMs learning to generate 2D spherical
Gaussian bumps centered at specified $x$- and $y$-positions. Our results show
that the emergence of semantically meaningful latent representations is key to
achieving high performance. En route to successful performance over learning,
the model traverses three distinct phases of latent representations: (phase A)
no latent structure, (phase B) a 2D manifold of disordered states, and (phase
C) a 2D ordered manifold. Corresponding to each of these phases, we identify
qualitatively different generation behaviors: 1) multiple bumps are generated,
2) one bump is generated but at inaccurate $x$ and $y$ locations, 3) a bump is
generated at the correct $x$ and y location. Furthermore, we show that even
under imbalanced datasets where features ($x$- versus $y$-positions) are
represented with skewed frequencies, the learning process for $x$ and $y$ is
coupled rather than factorized, demonstrating that simple vanilla-flavored
diffusion models cannot learn efficient representations in which localization
in $x$ and $y$ are factorized into separate 1D tasks. These findings suggest
the need for future work to find inductive biases that will push generative
models to discover and exploit factorizable independent structures in their
inputs, which will be required to vault these models into more data-efficient
regimes.
]]></content:encoded>
<pubDate>2024-02-05T18:58:38Z</pubDate>
</item>
<item>
<title>TravelPlanner: A Benchmark for Real-World Planning with Language Agents</title>
<link>http://arxiv.org/abs/2402.01622v1</link>
<guid>http://arxiv.org/abs/2402.01622v1</guid>
<content:encoded><![CDATA[
<div> 语言代理，人工智能，旅行规划，规划基准，挑战性测试<br />
<br />
人工智能自诞生以来一直将规划作为核心追求，但早期的人工智能代理主要专注于受限环境，因为缺乏人类级别规划所需的许多认知基础。然而，最近由大型语言模型（LLM）驱动的语言代理展现出了有趣的能力，如工具使用和推理。但这些语言代理能否在超出以往人工智能代理能力范围的更复杂环境中进行规划呢？为了推动这一调查，提出了“TravelPlanner”规划基准，侧重于旅行规划，这是一个常见的真实世界规划场景。它提供了一个丰富的沙盒环境，各种工具用于访问近400万条数据记录，以及1225个精心策划的规划意图和参考计划。全面的评估显示，当前的语言代理尚不能处理此类复杂规划任务，即使GPT-4的成功率也只有0.6％。语言代理在保持任务、使用正确工具收集信息或跟踪多个约束方面很困难。但我们注意到，语言代理仅仅有可能解决这样一个复杂的问题本身已经是非平凡的进步。TravelPlanner为未来语言代理提供了一个具有挑战性但有意义的试验平台。 <br /><br />总结: <div>
Planning has been part of the core pursuit for artificial intelligence since
its conception, but earlier AI agents mostly focused on constrained settings
because many of the cognitive substrates necessary for human-level planning
have been lacking. Recently, language agents powered by large language models
(LLMs) have shown interesting capabilities such as tool use and reasoning. Are
these language agents capable of planning in more complex settings that are out
of the reach of prior AI agents? To advance this investigation, we propose
TravelPlanner, a new planning benchmark that focuses on travel planning, a
common real-world planning scenario. It provides a rich sandbox environment,
various tools for accessing nearly four million data records, and 1,225
meticulously curated planning intents and reference plans. Comprehensive
evaluations show that the current language agents are not yet capable of
handling such complex planning tasks-even GPT-4 only achieves a success rate of
0.6%. Language agents struggle to stay on task, use the right tools to collect
information, or keep track of multiple constraints. However, we note that the
mere possibility for language agents to tackle such a complex problem is in
itself non-trivial progress. TravelPlanner provides a challenging yet
meaningful testbed for future language agents.
]]></content:encoded>
<pubDate>2024-02-02T18:39:51Z</pubDate>
</item>
<item>
<title>MAGDi: Structured Distillation of Multi-Agent Interaction Graphs
  Improves Reasoning in Smaller Language Models</title>
<link>http://arxiv.org/abs/2402.01620v1</link>
<guid>http://arxiv.org/abs/2402.01620v1</guid>
<content:encoded><![CDATA[
<div> 多智能体交互; 大型语言模型; 知识蒸馏; 推理能力; 效率提升<br />
<br />
多智能体交互的研究展示了大型语言模型在各种推理任务上的重大改进。然而，这些方法需要多个模型进行长时间的生成，并且成本高昂。此外，这些多智能体方法无法提供一个最终的、用于高效推理的单一模型。为了解决这一问题，文章介绍了一种新方法MAGDi，用于将多个大型语言模型之间的推理交互进行结构化蒸馏，以训练较小的模型。MAGDi通过将多智能体交互表示为图形，利用图编码器增强基础学生模型，并使用三种目标函数进行知识蒸馏：下一个标记的预测、正确和错误推理之间的对比损失，以及基于图的目标函数来模拟交互结构。实验证明，MAGDi提高了较小模型的推理能力，在七个广泛使用的常识和数学推理基准测试上表现优异，超过了几种从单个老师和多个老师进行蒸馏的方法。此外，MAGDi的效率也比其老师高一个数量级。作者进行了广泛的分析，表明MAGDi能够增强对跨领域任务的泛化能力，与基础学生模型的大小和强度呈正比，以及在应用自洽性时（一种依赖于模型多样性的推理技术）能够获得更大的改进（通过我们的多老师训练）。<br /><br />总结: <br />多智能体交互的研究展示了大型语言模型在各种推理任务上的重大改进；MAGDi通过结构化蒸馏提高了较小模型的推理能力；MAGDi在常识和数学推理基准测试上表现出色，且效率明显优于其老师；MAGDi增强了对跨领域任务的泛化能力，并且与基础学生模型的大小和强度呈正比；MAGDi在应用自洽性时能够获得更大的改进。 <div>
Multi-agent interactions between Large Language Model (LLM) agents have shown
major improvements on diverse reasoning tasks. However, these involve long
generations from multiple models across several rounds, making them expensive.
Moreover, these multi-agent approaches fail to provide a final, single model
for efficient inference. To address this, we introduce MAGDi, a new method for
structured distillation of the reasoning interactions between multiple LLMs
into smaller LMs. MAGDi teaches smaller models by representing multi-agent
interactions as graphs, augmenting a base student model with a graph encoder,
and distilling knowledge using three objective functions: next-token
prediction, a contrastive loss between correct and incorrect reasoning, and a
graph-based objective to model the interaction structure. Experiments on seven
widely-used commonsense and math reasoning benchmarks show that MAGDi improves
the reasoning capabilities of smaller models, outperforming several methods
that distill from a single teacher and multiple teachers. Moreover, MAGDi also
demonstrates an order of magnitude higher efficiency over its teachers. We
conduct extensive analyses to show that MAGDi (1) enhances the generalizability
to out-of-domain tasks, (2) scales positively with the size and strength of the
base student model, and (3) obtains larger improvements (via our multi-teacher
training) when applying self-consistency - an inference technique that relies
on model diversity.
]]></content:encoded>
<pubDate>2024-02-02T18:35:14Z</pubDate>
</item>
<item>
<title>Binding Touch to Everything: Learning Unified Multimodal Tactile
  Representations</title>
<link>http://arxiv.org/abs/2401.18084v1</link>
<guid>http://arxiv.org/abs/2401.18084v1</guid>
<content:encoded><![CDATA[
<div> tactile, multimodal learning, UniTouch, vision-based touch sensors, zero-shot setting
<br /><br />
1. 该研究介绍了UniTouch，一个统一的触觉模型，用于连接多种传感器从而实现触觉与视觉、语言和声音等多模态学习。
2. 通过将UniTouch嵌入与预训练的图像嵌入相关联，并提出可学习的传感器特定标记，使模型能够同时学习从一组异构触觉传感器中获取信息。
3. UniTouch能够在零-shot设置下进行各种触觉感知任务，从机器人抓取预测到触觉图像问题回答等。
4. 该模型具有巨大的潜在应用价值，能够解决触觉与其他模态之间的关联问题，对人类和计算系统都具有重大意义。
5. UniTouch是首个展示了这些能力的模型，具有重大的研究意义和应用前景。
<br /><br />总结: <br />本研究介绍了UniTouch，一个能够联合不同传感器的触觉模型，实现触觉与其他模态的学习。该模型能够进行多种触觉感知任务，并具有广泛的应用前景。 <div>
The ability to associate touch with other modalities has huge implications
for humans and computational systems. However, multimodal learning with touch
remains challenging due to the expensive data collection process and
non-standardized sensor outputs. We introduce UniTouch, a unified tactile model
for vision-based touch sensors connected to multiple modalities, including
vision, language, and sound. We achieve this by aligning our UniTouch
embeddings to pretrained image embeddings already associated with a variety of
other modalities. We further propose learnable sensor-specific tokens, allowing
the model to learn from a set of heterogeneous tactile sensors, all at the same
time. UniTouch is capable of conducting various touch sensing tasks in the
zero-shot setting, from robot grasping prediction to touch image question
answering. To the best of our knowledge, UniTouch is the first to demonstrate
such capabilities. Project page: https://cfeng16.github.io/UniTouch/
]]></content:encoded>
<pubDate>2024-01-31T18:59:57Z</pubDate>
</item>
<item>
<title>CARFF: Conditional Auto-encoded Radiance Field for 3D Scene Forecasting</title>
<link>http://arxiv.org/abs/2401.18075v1</link>
<guid>http://arxiv.org/abs/2401.18075v1</guid>
<content:encoded><![CDATA[
<div> 关键词: CARFF, Conditional Auto-encoded Radiance Field, 3D Scene Forecasting, Neural Radiance Field, CARLA driving simulator

CARFF是一种用于预测未来3D场景的方法，可以根据过去的观测（如2D自我中心图像）将图像映射到潜在的3D场景配置分布，并预测假设场景随时间的演变。该方法利用概率编码器将潜在场景表示映射到全局神经辐射场（NeRF），以表示3D场景模型，从而实现可解释的预测和简单的下游应用。此方法通过考虑环境状态和动态的不确定性复杂情景，扩展了以往的神经渲染工作。我们使用姿势条件VAE和NeRF的两阶段训练来学习3D表示。此外，我们利用混合密度网络，自回归地预测潜在场景表示作为一种部分可观察的马尔可夫决策过程。我们在CARLA驾驶模拟器中展示了我们方法的实用性，CARFF可用于实现复杂多智能体自动驾驶场景的高效轨迹和应急规划，包括视觉遮挡。<br /><br />总结: CARFF是一种用于预测未来3D场景的方法，利用概率编码器和全局神经辐射场表示潜在的3D场景模型，扩展了以往的神经渲染工作。该方法通过两阶段训练学习3D表示，并利用混合密度网络自回归地预测潜在场景表示。在CARLA驾驶模拟器中展示了方法的实用性，可用于复杂多智能体自动驾驶场景的轨迹和应急规划。 <div>
We propose CARFF: Conditional Auto-encoded Radiance Field for 3D Scene
Forecasting, a method for predicting future 3D scenes given past observations,
such as 2D ego-centric images. Our method maps an image to a distribution over
plausible 3D latent scene configurations using a probabilistic encoder, and
predicts the evolution of the hypothesized scenes through time. Our latent
scene representation conditions a global Neural Radiance Field (NeRF) to
represent a 3D scene model, which enables explainable predictions and
straightforward downstream applications. This approach extends beyond previous
neural rendering work by considering complex scenarios of uncertainty in
environmental states and dynamics. We employ a two-stage training of
Pose-Conditional-VAE and NeRF to learn 3D representations. Additionally, we
auto-regressively predict latent scene representations as a partially
observable Markov decision process, utilizing a mixture density network. We
demonstrate the utility of our method in realistic scenarios using the CARLA
driving simulator, where CARFF can be used to enable efficient trajectory and
contingency planning in complex multi-agent autonomous driving scenarios
involving visual occlusions.
]]></content:encoded>
<pubDate>2024-01-31T18:56:09Z</pubDate>
</item>
<item>
<title>Weaver: Foundation Models for Creative Writing</title>
<link>http://arxiv.org/abs/2401.17268v1</link>
<guid>http://arxiv.org/abs/2401.17268v1</guid>
<content:encoded><![CDATA[
<div> 关键词: Weaver, 大型语言模型, 写作能力, 领域专用, 预训练<br />
总结: <br />
本文介绍了Weaver，它是专门用于内容创作的大型语言模型家族的首个成员。Weaver经过精心筛选的语料库预训练，专注于提高大型语言模型的写作能力。通过新颖的方法进行微调，使Weaver能够生成更接近人类的文本，并能够遵循更多样化的创作指令。Weaver家族包括不同规模的模型，适用于不同的应用场景，并可以根据查询的复杂性进行动态调度。研究表明，Weaver模型在评估语言模型写作能力的基准测试中表现优异，超过了比它们大数倍的通用语言模型。此外，Weaver还原生支持检索增强生成和函数调用，并展示了这些能力在改进AI辅助写作系统中的各种应用案例。最后，文章讨论并总结了预训练和微调领域特定语言模型的指南和最佳实践。 <div>
This work introduces Weaver, our first family of large language models (LLMs)
dedicated to content creation. Weaver is pre-trained on a carefully selected
corpus that focuses on improving the writing capabilities of large language
models. We then fine-tune Weaver for creative and professional writing purposes
and align it to the preference of professional writers using a suit of novel
methods for instruction data synthesis and LLM alignment, making it able to
produce more human-like texts and follow more diverse instructions for content
creation. The Weaver family consists of models of Weaver Mini (1.8B), Weaver
Base (6B), Weaver Pro (14B), and Weaver Ultra (34B) sizes, suitable for
different applications and can be dynamically dispatched by a routing agent
according to query complexity to balance response quality and computation cost.
Evaluation on a carefully curated benchmark for assessing the writing
capabilities of LLMs shows Weaver models of all sizes outperform generalist
LLMs several times larger than them. Notably, our most-capable Weaver Ultra
model surpasses GPT-4, a state-of-the-art generalist LLM, on various writing
scenarios, demonstrating the advantage of training specialized LLMs for writing
purposes. Moreover, Weaver natively supports retrieval-augmented generation
(RAG) and function calling (tool usage). We present various use cases of these
abilities for improving AI-assisted writing systems, including integration of
external knowledge bases, tools, or APIs, and providing personalized writing
assistance. Furthermore, we discuss and summarize a guideline and best
practices for pre-training and fine-tuning domain-specific LLMs.
]]></content:encoded>
<pubDate>2024-01-30T18:58:43Z</pubDate>
</item>
<item>
<title>ReacLLaMA: Merging chemical and textual information in chemical
  reactivity AI models</title>
<link>http://arxiv.org/abs/2401.17267v1</link>
<guid>http://arxiv.org/abs/2401.17267v1</guid>
<content:encoded><![CDATA[
<div> Graphormer, reactivity model, chemical information, procedural text, accuracy

总结:<br />
本文提出了利用过程文本来增强Graphormer反应性模型并提高其准确性的方法。两种主要方法分别是训练一个适配器Graphormer模型，该模型提供了一个由GPT-2衍生的文本过程的潜在表示（ReacLLaMA-Adapter），以及使用LLaMA 2模型对数据集的未标记部分进行标记，然后在扩展数据集上训练Graphormer模型（Zero-Shot Labeling ReacLLaMA）。这两种方法都增强了对不利反应的识别能力，从而提供了更准确的模型，并改善了特异性。 <div>
Chemical reactivity models are developed to predict chemical reaction
outcomes in the form of classification (success/failure) or regression (product
yield) tasks. The vast majority of the reported models are trained solely on
chemical information such as reactants, products, reagents, and solvents, but
not on the details of a synthetic protocol. Herein incorporation of procedural
text with the aim to augment the Graphormer reactivity model and improve its
accuracy is presented. Two major approaches are used: training an adapter
Graphormer model that is provided with a GPT-2-derived latent representation of
the text procedure (ReacLLaMA-Adapter) and labeling an unlabeled part of a
dataset with the LLaMA 2 model followed by training the Graphormer on an
extended dataset (Zero-Shot Labeling ReacLLaMA). Both methodologies enhance the
discernment of unpromising reactions, thereby providing more accurate models
with improved specificity.
]]></content:encoded>
<pubDate>2024-01-30T18:57:08Z</pubDate>
</item>
<item>
<title>InternLM-XComposer2: Mastering Free-form Text-Image Composition and
  Comprehension in Vision-Language Large Model</title>
<link>http://arxiv.org/abs/2401.16420v1</link>
<guid>http://arxiv.org/abs/2401.16420v1</guid>
<content:encoded><![CDATA[
<div> 关键词: InternLM-XComposer2, PLoRA, 多模态理解, 高质量内容, GitHub链接
总结:
InternLM-XComposer2是一种先进的视觉语言模型，擅长自由形式的文本-图像组合和理解。它采用了Partial LoRA (PLoRA)方法，通过额外的LoRA参数对图像标记进行处理，从而在保持预训练语言知识完整性的同时，实现了精确的视觉理解和具有文学才华的文本组成。在实验结果中，InternLM-XComposer2在高质量长文本多模态内容的生成和跨各种基准测试中表现出了优越性能，不仅明显优于现有的多模态模型，而且在某些评估中甚至能够与甚至超过了GPT-4V和Gemini Pro。这突显了它在多模态理解领域的出色能力。InternLM-XComposer2模型系列的7B参数现已在https://github.com/InternLM/InternLM-XComposer公开提供。 <div>
We introduce InternLM-XComposer2, a cutting-edge vision-language model
excelling in free-form text-image composition and comprehension. This model
goes beyond conventional vision-language understanding, adeptly crafting
interleaved text-image content from diverse inputs like outlines, detailed
textual specifications, and reference images, enabling highly customizable
content creation. InternLM-XComposer2 proposes a Partial LoRA (PLoRA) approach
that applies additional LoRA parameters exclusively to image tokens to preserve
the integrity of pre-trained language knowledge, striking a balance between
precise vision understanding and text composition with literary talent.
Experimental results demonstrate the superiority of InternLM-XComposer2 based
on InternLM2-7B in producing high-quality long-text multi-modal content and its
exceptional vision-language understanding performance across various
benchmarks, where it not only significantly outperforms existing multimodal
models but also matches or even surpasses GPT-4V and Gemini Pro in certain
assessments. This highlights its remarkable proficiency in the realm of
multimodal understanding. The InternLM-XComposer2 model series with 7B
parameters are publicly available at
https://github.com/InternLM/InternLM-XComposer.
]]></content:encoded>
<pubDate>2024-01-29T18:59:02Z</pubDate>
</item>
<item>
<title>Annotated Hands for Generative Models</title>
<link>http://arxiv.org/abs/2401.15075v1</link>
<guid>http://arxiv.org/abs/2401.15075v1</guid>
<content:encoded><![CDATA[
<div> 生成模型, 手图像, 训练框架, 语义标注, 图像质量
<br />
生成模型如GANs和扩散模型在图像生成方面取得了令人瞩目的成就，但在生成手部图像方面却表现不佳。本文提出了一种新颖的训练框架，通过在训练图像中增加三个额外的通道，提供手部的标注信息，从而改善生成模型生成手部图像的能力。通过在合成手部图像数据集和真实照片上进行实验，证明了该方法的有效性。最终通过使用现成的手部检测器，测量生成的手部图像的质量得到了提升。 <br /><br />总结: <br />生成模型在图像生成方面表现突出，但在生成手部图像方面表现不佳；本文提出了一种新的训练框架，通过增加手部标注信息改善生成模型的能力；在合成和真实数据集上验证了方法的有效性；最终通过手部检测器测量了生成手部图像的质量。 <div>
Generative models such as GANs and diffusion models have demonstrated
impressive image generation capabilities. Despite these successes, these
systems are surprisingly poor at creating images with hands. We propose a novel
training framework for generative models that substantially improves the
ability of such systems to create hand images. Our approach is to augment the
training images with three additional channels that provide annotations to
hands in the image. These annotations provide additional structure that coax
the generative model to produce higher quality hand images. We demonstrate this
approach on two different generative models: a generative adversarial network
and a diffusion model. We demonstrate our method both on a new synthetic
dataset of hand images and also on real photographs that contain hands. We
measure the improved quality of the generated hands through higher confidence
in finger joint identification using an off-the-shelf hand detector.
]]></content:encoded>
<pubDate>2024-01-26T18:57:54Z</pubDate>
</item>
<item>
<title>Fully Independent Communication in Multi-Agent Reinforcement Learning</title>
<link>http://arxiv.org/abs/2401.15059v1</link>
<guid>http://arxiv.org/abs/2401.15059v1</guid>
<content:encoded><![CDATA[
<div> 多智能体强化学习, 通信方法, 参数共享, 网络容量, 学习效率
<br />
本文研究了多智能体强化学习中独立学习者如何进行通信，提出了一种新的学习方案作为解决方案。研究结果表明，尽管存在挑战，独立智能体仍然可以根据我们的方法学习通信策略。此外，我们利用这种方法研究了MARL中的通信如何受到不同网络容量的影响，无论是共享参数还是不共享参数。我们观察到通信并不总是必要的，并且在使用通信时需要考虑选择的智能体网络大小，以实现高效的学习。
<br /><br />总结: 
本文研究了在多智能体强化学习中独立学习者如何进行通信，并提出了一种新的学习方案作为解决方案。实验结果表明，独立智能体可以学习通信策略，并且证明了在学习效率方面网络容量的选择对通信的影响。 <div>
Multi-Agent Reinforcement Learning (MARL) comprises a broad area of research
within the field of multi-agent systems. Several recent works have focused
specifically on the study of communication approaches in MARL. While multiple
communication methods have been proposed, these might still be too complex and
not easily transferable to more practical contexts. One of the reasons for that
is due to the use of the famous parameter sharing trick. In this paper, we
investigate how independent learners in MARL that do not share parameters can
communicate. We demonstrate that this setting might incur into some problems,
to which we propose a new learning scheme as a solution. Our results show that,
despite the challenges, independent agents can still learn communication
strategies following our method. Additionally, we use this method to
investigate how communication in MARL is affected by different network
capacities, both for sharing and not sharing parameters. We observe that
communication may not always be needed and that the chosen agent network sizes
need to be considered when used together with communication in order to achieve
efficient learning.
]]></content:encoded>
<pubDate>2024-01-26T18:42:01Z</pubDate>
</item>
<item>
<title>LongFin: A Multimodal Document Understanding Model for Long Financial
  Domain Documents</title>
<link>http://arxiv.org/abs/2401.15050v1</link>
<guid>http://arxiv.org/abs/2401.15050v1</guid>
<content:encoded><![CDATA[
Document AI is a growing research field that focuses on the comprehension and
extraction of information from scanned and digital documents to make everyday
business operations more efficient. Numerous downstream tasks and datasets have
been introduced to facilitate the training of AI models capable of parsing and
extracting information from various document types such as receipts and scanned
forms. Despite these advancements, both existing datasets and models fail to
address critical challenges that arise in industrial contexts. Existing
datasets primarily comprise short documents consisting of a single page, while
existing models are constrained by a limited maximum length, often set at 512
tokens. Consequently, the practical application of these methods in financial
services, where documents can span multiple pages, is severely impeded. To
overcome these challenges, we introduce LongFin, a multimodal document AI model
capable of encoding up to 4K tokens. We also propose the LongForms dataset, a
comprehensive financial dataset that encapsulates several industrial challenges
in financial documents. Through an extensive evaluation, we demonstrate the
effectiveness of the LongFin model on the LongForms dataset, surpassing the
performance of existing public models while maintaining comparable results on
existing single-page benchmarks.
]]></content:encoded>
<pubDate>2024-01-26T18:23:45Z</pubDate>
</item>
<item>
<title>Deconstructing Denoising Diffusion Models for Self-Supervised Learning</title>
<link>http://arxiv.org/abs/2401.14404v1</link>
<guid>http://arxiv.org/abs/2401.14404v1</guid>
<content:encoded><![CDATA[
<div> Denoising Diffusion Models, representation learning, self-supervised learning, deconstructive procedure, classical methods
<br />
本研究探讨了最初用于图像生成的去噪扩散模型（DDM）的表示学习能力。我们的方法是逐步解构DDM，逐渐将其转化为经典的去噪自编码器（DAE）。这种解构性过程使我们能够探索现代DDM的各种组件如何影响自监督表示学习。我们观察到，只有很少现代组件对于学习良好的表示是至关重要的，而许多其他组件则是非必要的。我们的研究最终得出了一个高度简化的方法，在很大程度上类似于经典的DAE。我们希望我们的研究能重新引起人们对现代自监督学习领域内一系列经典方法的兴趣。
<br /><br />总结: Denoising Diffusion Models的表示学习能力得到探讨；研究采用逐步解构DDM的方法，发现现代DDMs的很多组件对良好的自监督表示学习并不是必要的；研究最终得出了一个高度简化的方法，类似于经典的DAE；希望研究能重新引起人们对现代自监督学习领域内经典方法的兴趣。 <div>
In this study, we examine the representation learning abilities of Denoising
Diffusion Models (DDM) that were originally purposed for image generation. Our
philosophy is to deconstruct a DDM, gradually transforming it into a classical
Denoising Autoencoder (DAE). This deconstructive procedure allows us to explore
how various components of modern DDMs influence self-supervised representation
learning. We observe that only a very few modern components are critical for
learning good representations, while many others are nonessential. Our study
ultimately arrives at an approach that is highly simplified and to a large
extent resembles a classical DAE. We hope our study will rekindle interest in a
family of classical methods within the realm of modern self-supervised
learning.
]]></content:encoded>
<pubDate>2024-01-25T18:59:57Z</pubDate>
</item>
<item>
<title>VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web
  Tasks</title>
<link>http://arxiv.org/abs/2401.13649v1</link>
<guid>http://arxiv.org/abs/2401.13649v1</guid>
<content:encoded><![CDATA[
<div> benchmark, autonomous agents, multimodal, VisualWebArena, web

在现有的基准测试基本上主要集中在基于文本的代理上，忽略了许多需要视觉信息来有效解决的自然任务。因此，我们引入了VisualWebArena，这是一个旨在评估多模态网络代理在现实中视觉相关任务表现的基准。它包括一系列多样化和复杂的基于网络的任务，评估自主多模态代理的各种能力。通过广泛的定量和定性分析，我们确定了文本-只有LLM代理的几个局限性，并揭示了最先进的多模态语言代理的能力差距。VisualWebArena为评估多模态自主语言代理提供了一个框架，并为构建更强大的网络自主代理提供了见解。 <div>
Autonomous agents capable of planning, reasoning, and executing actions on
the web offer a promising avenue for automating computer tasks. However, the
majority of existing benchmarks primarily focus on text-based agents,
neglecting many natural tasks that require visual information to effectively
solve. Given that most computer interfaces cater to human perception, visual
information often augments textual data in ways that text-only models struggle
to harness effectively. To bridge this gap, we introduce VisualWebArena, a
benchmark designed to assess the performance of multimodal web agents on
realistic \textit{visually grounded tasks}. VisualWebArena comprises of a set
of diverse and complex web-based tasks that evaluate various capabilities of
autonomous multimodal agents. To perform on this benchmark, agents need to
accurately process image-text inputs, interpret natural language instructions,
and execute actions on websites to accomplish user-defined objectives. We
conduct an extensive evaluation of state-of-the-art LLM-based autonomous
agents, including several multimodal models. Through extensive quantitative and
qualitative analysis, we identify several limitations of text-only LLM agents,
and reveal gaps in the capabilities of state-of-the-art multimodal language
agents. VisualWebArena provides a framework for evaluating multimodal
autonomous language agents, and offers insights towards building stronger
autonomous agents for the web. Our code, baseline models, and data is publicly
available at https://jykoh.com/vwa.
]]></content:encoded>
<pubDate>2024-01-24T18:35:21Z</pubDate>
</item>
<item>
<title>HAZARD Challenge: Embodied Decision Making in Dynamically Changing
  Environments</title>
<link>http://arxiv.org/abs/2401.12975v1</link>
<guid>http://arxiv.org/abs/2401.12975v1</guid>
<content:encoded><![CDATA[
<div> 高保真虚拟环境, 智能体, HAZARD, 动态环境, 大语言模型

最近，高保真虚拟环境的技术进步成为促进建立能够感知、推理和与物理世界互动的智能体的主要推动力之一。然而，在真实世界的情景中，智能体可能面临突发事件导致的动态环境变化，需要迅速做出相应的行动。为了弥补这一差距，研究人员提出了一个新的模拟智能体基准测试，名为HAZARD，专门设计用于评估智能体在动态情况下的决策能力。HAZARD包括火灾、洪水和风三种突发灾害场景，并专门支持利用大语言模型（LLMs）来辅助常识推理和决策制定。这一基准测试使我们能够评估自主智能体在动态变化的环境中通过强化学习（RL）、基于规则的方法和搜索方法等各种管道的决策能力。作为利用大语言模型解决这些具有挑战性任务的一个起步，我们进一步开发了一个基于LLM的智能体，并对其解决这些任务的前景和挑战进行了深入分析。HAZARD的详细信息可在https://vis-www.cs.umass.edu/hazard/找到。<br /><br />总结：最近出现了高保真虚拟环境技术，为了评估智能体在动态环境中的决策能力，研究人员提出了一个名为HAZARD的新的模拟智能体基准测试。该基准测试包括三种突发灾害场景，并特别支持利用大语言模型来辅助决策制定。研究人员还开发了基于LLM的智能体，并分析了其解决这些任务的前景和挑战。 <div>
Recent advances in high-fidelity virtual environments serve as one of the
major driving forces for building intelligent embodied agents to perceive,
reason and interact with the physical world. Typically, these environments
remain unchanged unless agents interact with them. However, in real-world
scenarios, agents might also face dynamically changing environments
characterized by unexpected events and need to rapidly take action accordingly.
To remedy this gap, we propose a new simulated embodied benchmark, called
HAZARD, specifically designed to assess the decision-making abilities of
embodied agents in dynamic situations. HAZARD consists of three unexpected
disaster scenarios, including fire, flood, and wind, and specifically supports
the utilization of large language models (LLMs) to assist common sense
reasoning and decision-making. This benchmark enables us to evaluate autonomous
agents' decision-making capabilities across various pipelines, including
reinforcement learning (RL), rule-based, and search-based methods in
dynamically changing environments. As a first step toward addressing this
challenge using large language models, we further develop an LLM-based agent
and perform an in-depth analysis of its promise and challenge of solving these
challenging tasks. HAZARD is available at https://vis-www.cs.umass.edu/hazard/.
]]></content:encoded>
<pubDate>2024-01-23T18:59:43Z</pubDate>
</item>
<item>
<title>CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation</title>
<link>http://arxiv.org/abs/2401.12208v1</link>
<guid>http://arxiv.org/abs/2401.12208v1</guid>
<content:encoded><![CDATA[
<div> CheXinstruct, CheXagent, vision-language模型, CXR解释, CheXbench
<br />
本文介绍了针对胸部X光片（CXR）解释的自动化模型CheXagent的开发。作者首先介绍了CheXinstruct数据集，然后提出了CheXagent模型，该模型包括临床大型语言模型（LLM）、视觉编码器和用于桥接视觉和语言模态的网络。此外，引入了CheXbench评估框架，用于系统评估FMs在8个临床相关的CXR解释任务上的表现。研究结果表明，CheXagent在CheXbench任务上的表现优于先前开发的通用和医学领域的FMs。此外，作者还进行了模型公平性评估，以突出潜在的性别、种族和年龄方面的性能差异。这项工作的详细信息可以在https://stanford-aimi.github.io/chexagent.html找到。
<br /><br />总结: 本文介绍了针对CXR解释的自动化模型CheXagent的开发，包括数据集、模型架构和评估框架。研究结果显示CheXagent在临床相关任务上表现优异，而且还进行了公平性评估。 <div>
Chest X-rays (CXRs) are the most frequently performed imaging test in
clinical practice. Recent advances in the development of vision-language
foundation models (FMs) give rise to the possibility of performing automated
CXR interpretation, which can assist physicians with clinical decision-making
and improve patient outcomes. However, developing FMs that can accurately
interpret CXRs is challenging due to the (1) limited availability of
large-scale vision-language datasets in the medical image domain, (2) lack of
vision and language encoders that can capture the complexities of medical data,
and (3) absence of evaluation frameworks for benchmarking the abilities of FMs
on CXR interpretation. In this work, we address these challenges by first
introducing \emph{CheXinstruct} - a large-scale instruction-tuning dataset
curated from 28 publicly-available datasets. We then present \emph{CheXagent} -
an instruction-tuned FM capable of analyzing and summarizing CXRs. To build
CheXagent, we design a clinical large language model (LLM) for parsing
radiology reports, a vision encoder for representing CXR images, and a network
to bridge the vision and language modalities. Finally, we introduce
\emph{CheXbench} - a novel benchmark designed to systematically evaluate FMs
across 8 clinically-relevant CXR interpretation tasks. Extensive quantitative
evaluations and qualitative reviews with five expert radiologists demonstrate
that CheXagent outperforms previously-developed general- and medical-domain FMs
on CheXbench tasks. Furthermore, in an effort to improve model transparency, we
perform a fairness evaluation across factors of sex, race and age to highlight
potential performance disparities. Our project is at
\url{https://stanford-aimi.github.io/chexagent.html}.
]]></content:encoded>
<pubDate>2024-01-22T18:51:07Z</pubDate>
</item>
<item>
<title>Retrieval-Guided Reinforcement Learning for Boolean Circuit Minimization</title>
<link>http://arxiv.org/abs/2401.12205v1</link>
<guid>http://arxiv.org/abs/2401.12205v1</guid>
<content:encoded><![CDATA[
<div> logic synthesis, chip design, hardware description languages, ABC-RL, quality-of-result

逻辑综合是芯片设计中的关键阶段，涉及将硬件描述语言（如Verilog）中编码的芯片规格优化为使用布尔逻辑门的高效实现。研究发现，预先训练的代理在面对全新设计时可能会偏离轨迹，对搜索轨迹产生不利影响。为解决这一挑战，提出了ABC-RL，一种根据训练数据集中的相似度分数计算出的精心调整建议的技术。实验结果显示，ABC-RL能够显著提高合成电路的结果质量（QoR），与当前最先进的技术相比，提高了最高达24.8%。此外，ABC-RL在运行时间上也取得了惊人的成就，与当前最先进的方法相比，实现了高达9倍的减少（等质量结果的情况下）。<br /><br />总结: 逻辑综合是芯片设计过程中的关键阶段，需要细致调整合成方法以适应各种硬件设计。ABC-RL技术通过精心调整预训练代理的建议，实现了合成电路质量的显著提高（最高24.8%）。与此同时，ABC-RL还取得了显著的运行时间减少（高达9倍）。 <div>
Logic synthesis, a pivotal stage in chip design, entails optimizing chip
specifications encoded in hardware description languages like Verilog into
highly efficient implementations using Boolean logic gates. The process
involves a sequential application of logic minimization heuristics (``synthesis
recipe"), with their arrangement significantly impacting crucial metrics such
as area and delay. Addressing the challenge posed by the broad spectrum of
design complexities - from variations of past designs (e.g., adders and
multipliers) to entirely novel configurations (e.g., innovative processor
instructions) - requires a nuanced `synthesis recipe` guided by human expertise
and intuition. This study conducts a thorough examination of learning and
search techniques for logic synthesis, unearthing a surprising revelation:
pre-trained agents, when confronted with entirely novel designs, may veer off
course, detrimentally affecting the search trajectory. We present ABC-RL, a
meticulously tuned $\alpha$ parameter that adeptly adjusts recommendations from
pre-trained agents during the search process. Computed based on similarity
scores through nearest neighbor retrieval from the training dataset, ABC-RL
yields superior synthesis recipes tailored for a wide array of hardware
designs. Our findings showcase substantial enhancements in the
Quality-of-result (QoR) of synthesized circuits, boasting improvements of up to
24.8% compared to state-of-the-art techniques. Furthermore, ABC-RL achieves an
impressive up to 9x reduction in runtime (iso-QoR) when compared to current
state-of-the-art methodologies.
]]></content:encoded>
<pubDate>2024-01-22T18:46:30Z</pubDate>
</item>
<item>
<title>Applications of flow models to the generation of correlated lattice QCD
  ensembles</title>
<link>http://arxiv.org/abs/2401.10874v1</link>
<guid>http://arxiv.org/abs/2401.10874v1</guid>
<content:encoded><![CDATA[
<div> 关键词: normalizing flows, lattice quantum field theory, variance reduction, gauge theories, QCD observables
总结: 
本研究利用机器学习的正规化流在晶格量子场论的背景下生成不同作用参数下统计相关的晶格规范场集合。这项工作展示了如何利用这些相关性来减少计算观测量时的方差。通过一种新颖的残余流架构，演示了三种概念验证应用：规范理论的连续极限、QCD观测量的质量依赖性，以及基于费曼-赫尔曼方法的强子矩阵元素。在所有三种情况下，实验证明当机器学习流被纳入时，与使用无关集合或直接重加权进行的相同计算相比，统计不确定性显著减少。 <br /><br /> <div>
Machine-learned normalizing flows can be used in the context of lattice
quantum field theory to generate statistically correlated ensembles of lattice
gauge fields at different action parameters. This work demonstrates how these
correlations can be exploited for variance reduction in the computation of
observables. Three different proof-of-concept applications are demonstrated
using a novel residual flow architecture: continuum limits of gauge theories,
the mass dependence of QCD observables, and hadronic matrix elements based on
the Feynman-Hellmann approach. In all three cases, it is shown that statistical
uncertainties are significantly reduced when machine-learned flows are
incorporated as compared with the same calculations performed with uncorrelated
ensembles or direct reweighting.
]]></content:encoded>
<pubDate>2024-01-19T18:33:52Z</pubDate>
</item>
<item>
<title>Vlogger: Make Your Dream A Vlog</title>
<link>http://arxiv.org/abs/2401.09414v1</link>
<guid>http://arxiv.org/abs/2401.09414v1</guid>
<content:encoded><![CDATA[
<div> Vlogger, AI, video blog, Large Language Model, 生成<br />
Script, Actor, ShowMaker, Voicer, 模型合作生成vlog的四个关键角色<br />

总结:<br />
本文介绍了Vlogger，这是一个用于生成用户描述的分钟级视频博客的通用AI系统。与几秒钟的短视频不同，vlog通常包含一个复杂的故事情节和多样化的场景，这对大多数现有的视频生成方法来说是一个挑战。为了突破这一瓶颈，Vlogger巧妙地利用大型语言模型（LLM）作为导演，并将vlog的长视频生成任务分解为四个关键阶段。在这些阶段中，我们调用各种基础模型来扮演vlog专业人员的关键角色，包括（1）脚本，（2）演员，（3）ShowMaker和（4）Voicer。通过这种模仿人类的设计，我们的Vlogger可以通过自上而下的规划和自下而上的拍摄，以可解释的合作方式生成vlog。此外，我们还引入了一个新颖的视频扩散模型ShowMaker，它在我们的Vlogger中充当了一个摄影师的角色，用于生成每个拍摄场景的视频片段。通过巧妙地结合脚本和演员作为文本和视觉提示，它可以有效地增强片段的时空连贯性。此外，我们为ShowMaker设计了一个简洁的混合训练范式，提升了其T2V生成和预测能力。最后，广泛的实验证明，我们的方法在零样本T2V生成和预测任务上取得了最先进的性能。更重要的是，Vlogger可以从开放世界描述中生成超过5分钟的vlog，而且在脚本和演员上不会丢失视频的连贯性。所有代码和模型都可以在https://github.com/zhuangshaobin/Vlogger上找到。 <div>
In this work, we present Vlogger, a generic AI system for generating a
minute-level video blog (i.e., vlog) of user descriptions. Different from short
videos with a few seconds, vlog often contains a complex storyline with
diversified scenes, which is challenging for most existing video generation
approaches. To break through this bottleneck, our Vlogger smartly leverages
Large Language Model (LLM) as Director and decomposes a long video generation
task of vlog into four key stages, where we invoke various foundation models to
play the critical roles of vlog professionals, including (1) Script, (2) Actor,
(3) ShowMaker, and (4) Voicer. With such a design of mimicking human beings,
our Vlogger can generate vlogs through explainable cooperation of top-down
planning and bottom-up shooting. Moreover, we introduce a novel video diffusion
model, ShowMaker, which serves as a videographer in our Vlogger for generating
the video snippet of each shooting scene. By incorporating Script and Actor
attentively as textual and visual prompts, it can effectively enhance
spatial-temporal coherence in the snippet. Besides, we design a concise mixed
training paradigm for ShowMaker, boosting its capacity for both T2V generation
and prediction. Finally, the extensive experiments show that our method
achieves state-of-the-art performance on zero-shot T2V generation and
prediction tasks. More importantly, Vlogger can generate over 5-minute vlogs
from open-world descriptions, without loss of video coherence on script and
actor. The code and model is all available at
https://github.com/zhuangshaobin/Vlogger.
]]></content:encoded>
<pubDate>2024-01-17T18:55:12Z</pubDate>
</item>
<item>
<title>Multimodal assessment of best possible self as a self-regulatory
  activity for the classroom</title>
<link>http://arxiv.org/abs/2401.08424v1</link>
<guid>http://arxiv.org/abs/2401.08424v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 最佳可能自我，积极心理干预，心身效应，自我调节资源，大学生

最佳可能自我（BPS）是一种被证明可以增强幸福感的积极心理干预，包括描述理想未来情景的写作活动。这篇论文比较了为教室环境改编的BPS活动和与之时间匹配的对照活动（NA）对心理生理效应的影响。三十三名本科生参与了这项研究，评估了三个时间段（之前，期间，之后）的状态焦虑（状态-特质焦虑量表，STAI）、情感（情感滑块，AS）和心脏迷走神经活动（心率变异性，HRV）作为自我调节资源使用的指标。结果显示，与NA相比，BPS导致了积极情绪价值（期间）的显著增加，并且整体上更高水平的心脏迷走神经活动（HRV）。这些发现表明，BPS作为一种自我调节技术具有潜在的特性，旨在培养积极情绪，对自我调节资源产生积极影响。由于BPS不需要专业知识或专门技术来进行管理，教育者在教学和学生实践自我调节时可能会选择这种适当的活动。这项研究呈现了对大学生进行的一个简短BPS活动的自我调节效应的可复制的多模态方法的证据。<br /><br />总结:本研究展示了BPS活动对大学生的自我调节效应的证据，表明BPS在提高积极情感和积极影响自我调节资源方面具有潜力。 BPS活动可以在教学中用于教育者教学和学生实践自我调节。 <div>
Best possible self (BPS) is a positive psychological intervention shown to
enhance well-being which involves writing a description of an ideal future
scenario. This paper presents a comparison of psychophysiological effects of a
BPS activity that has been adapted for classroom settings and a time-matched
control activity (NA). Thirty-three undergraduate students participated in the
study that assessed state anxiety (State-Trait Anxiety Inventory, STAI), affect
(Affective Slider, AS), and cardiac vagal activity (heart-rate variability,
HRV) as an indicator of self-regulatory resource usage, at three time periods
(PRE, DURING, POST). Results show that BPS led to a significantly greater
increase in positive valence (DURING) and overall higher levels of cardiac
vagal activity (HRV) compared to NA. These findings suggest that BPS has
promising characteristics as a self-regulatory technique aimed at fostering
positive affect and positively impacting self-regulatory resources. As BPS does
not require expert knowledge nor specialized technology to administer, it may
be a suitable activity for educators to use when teaching and having students
practice self-regulation. This study presents evidence collected in a
replicable multimodal approach of the self-regulatory effects of a brief BPS
activity on undergraduate students.
]]></content:encoded>
<pubDate>2024-01-16T15:11:12Z</pubDate>
</item>
<item>
<title>AGG: Amortized Generative 3D Gaussians for Single Image to 3D</title>
<link>http://arxiv.org/abs/2401.04099v1</link>
<guid>http://arxiv.org/abs/2401.04099v1</guid>
<content:encoded><![CDATA[
<div> 3D content creation, 3D Gaussian splatting, Amortized Generative 3D Gaussian framework, optimization-based, super-resolution

3D内容创建需求增长，研究了各种3D表示以从单个图像生成3D对象。最近，基于3D高斯光斑的模型在3D重建和生成方面取得了优异的渲染效果。然而，基于3D高斯光斑的方法通常是基于优化的，需要许多计算昂贵的分数蒸馏步骤。为了克服这些挑战，引入了一种摊销生成3D高斯框架（AGG），它可以立即从单个图像生成3D高斯，无需每个实例进行优化。AGG利用中间混合表示，将生成3D高斯位置和其他外观属性进行联合优化。此外，还提出了一个级联管道，首先生成3D数据的粗表示，然后利用3D高斯超分辨率模块进行上采样。我们的方法与现有的基于优化的3D高斯框架和利用其他3D表示的基于采样的管线进行了评估，结果表明AGG在生成能力上具有竞争优势，无论是定性还是定量，同时速度快几个数量级。项目页面：https://ir1d.github.io/AGG/ <br /><br />总结: 3D内容创建需求增长，研究了各种3D表示以从单个图像生成3D对象。基于3D高斯光斑的模型在3D重建和生成方面表现出色，但通常是基于优化的，需要许多计算昂贵的分数蒸馏步骤。为了克服这些挑战，引入了一种摊销生成3D高斯框架（AGG），它可以立即从单个图像生成3D高斯，无需每个实例进行优化。除此之外，提出了一个级联管道，首先生成3D数据的粗表示，然后利用3D高斯超分辨率模块进行上采样。我们的方法在生成能力上具有竞争优势，无论是定性还是定量，同时速度快几个数量级。 <div>
Given the growing need for automatic 3D content creation pipelines, various
3D representations have been studied to generate 3D objects from a single
image. Due to its superior rendering efficiency, 3D Gaussian splatting-based
models have recently excelled in both 3D reconstruction and generation. 3D
Gaussian splatting approaches for image to 3D generation are often
optimization-based, requiring many computationally expensive score-distillation
steps. To overcome these challenges, we introduce an Amortized Generative 3D
Gaussian framework (AGG) that instantly produces 3D Gaussians from a single
image, eliminating the need for per-instance optimization. Utilizing an
intermediate hybrid representation, AGG decomposes the generation of 3D
Gaussian locations and other appearance attributes for joint optimization.
Moreover, we propose a cascaded pipeline that first generates a coarse
representation of the 3D data and later upsamples it with a 3D Gaussian
super-resolution module. Our method is evaluated against existing
optimization-based 3D Gaussian frameworks and sampling-based pipelines
utilizing other 3D representations, where AGG showcases competitive generation
abilities both qualitatively and quantitatively while being several orders of
magnitude faster. Project page: https://ir1d.github.io/AGG/
]]></content:encoded>
<pubDate>2024-01-08T18:56:33Z</pubDate>
</item>
<item>
<title>The Tactician's Web of Large-Scale Formal Knowledge</title>
<link>http://arxiv.org/abs/2401.02950v1</link>
<guid>http://arxiv.org/abs/2401.02950v1</guid>
<content:encoded><![CDATA[
<div> Coq proof assistant, formal mathematical knowledge, machine learning, interconnected web, proof engineering
<br /><br />总结:
Tactician's Web是一个基于Coq证明助手构建的平台，提供了一个庞大而强大的机器检查的数学知识网络，方便机器学习、分析和证明工程。该平台导出一个数据集，其中包含广泛的形式化理论，呈现为定义、定理、证明术语、策略和证明状态的网络。紧密集成的Coq提供了使代理商可用于证明工程师作为实用工具的独特可能性。理论被编码为语义图和人类可读的文本，各自具有独特的优点和缺点。证明代理可以通过相同丰富的数据表示与Coq交互，并且可以自动在一组定理上进行基准测试。 <div>
The Tactician's Web is a platform offering a large web of strongly
interconnected, machine-checked, formal mathematical knowledge conveniently
packaged for machine learning, analytics, and proof engineering. Built on top
of the Coq proof assistant, the platform exports a dataset containing a wide
variety of formal theories, presented as a web of definitions, theorems, proof
terms, tactics, and proof states. Theories are encoded both as a semantic graph
(rendered below) and as human-readable text, each with a unique set of
advantages and disadvantages. Proving agents may interact with Coq through the
same rich data representation and can be automatically benchmarked on a set of
theorems. Tight integration with Coq provides the unique possibility to make
agents available to proof engineers as practical tools.
]]></content:encoded>
<pubDate>2024-01-05T18:52:35Z</pubDate>
</item>
<item>
<title>Graph2Tac: Learning Hierarchical Representations of Math Concepts in
  Theorem proving</title>
<link>http://arxiv.org/abs/2401.02949v1</link>
<guid>http://arxiv.org/abs/2401.02949v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 数学, AI代理, Coq证明助手, 图神经网络, 定义嵌入

总结: <br /><br />这篇文章讨论了数学和其应用中的概念，提到了在AI代理进行新定理证明时，需要实时将新信息融入其知识库，特别是在Coq证明助手中。作者使用了基于图的数据集，构建了图神经网络Graph2Tac(G2T)，能够考虑到导致当前目标的整个定义层次。同时，他们还提出了一项新的定义嵌入任务，用于计算训练中未见的数学概念的表示。这些方法使神经网络的性能能够与最先进的k最近邻预测器相媲美。 <div>
Concepts abound in mathematics and its applications. They vary greatly
between subject areas, and new ones are introduced in each mathematical paper
or application. A formal theory builds a hierarchy of definitions, theorems and
proofs that reference each other. When an AI agent is proving a new theorem,
most of the mathematical concepts and lemmas relevant to that theorem may have
never been seen during training. This is especially true in the Coq proof
assistant, which has a diverse library of Coq projects, each with its own
definitions, lemmas, and even custom tactic procedures used to prove those
lemmas. It is essential for agents to incorporate such new information into
their knowledge base on the fly. We work towards this goal by utilizing a new,
large-scale, graph-based dataset for machine learning in Coq. We leverage a
faithful graph-representation of Coq terms that induces a directed graph of
dependencies between definitions to create a novel graph neural network,
Graph2Tac (G2T), that takes into account not only the current goal, but also
the entire hierarchy of definitions that led to the current goal. G2T is an
online model that is deeply integrated into the users' workflow and can adapt
in real time to new Coq projects and their definitions. It complements well
with other online models that learn in real time from new proof scripts. Our
novel definition embedding task, which is trained to compute representations of
mathematical concepts not seen during training, boosts the performance of the
neural network to rival state-of-the-art k-nearest neighbor predictors.
]]></content:encoded>
<pubDate>2024-01-05T18:52:09Z</pubDate>
</item>
<item>
<title>ODIN: A Single Model for 2D and 3D Perception</title>
<link>http://arxiv.org/abs/2401.02416v1</link>
<guid>http://arxiv.org/abs/2401.02416v1</guid>
<content:encoded><![CDATA[
<div> 3D perception benchmarks, ScanNet, point clouds, transformer architecture, ODIN
<br />
这篇论文介绍了一个名为ODIN的模型，它能够同时处理2D RGB图像和3D点云数据，并使用了一种交替融合2D和3D信息的transformer架构。模型通过区分处理的token的位置编码来区分2D和3D特征操作。ODIN在ScanNet200、Matterport3D和AI2THOR 3D实例分割基准测试中实现了最先进的性能，并在ScanNet、S3DIS和COCO基准测试上实现了有竞争力的表现。当使用实际感知的3D点云代替从3D网格采样的点云时，它比所有先前的工作表现出更高的性能。在可指导的具身代理架构中用作3D感知引擎时，在TEACh对话动作基准测试中创造了新的最先进技术。 <div>
State-of-the-art models on contemporary 3D perception benchmarks like ScanNet
consume and label dataset-provided 3D point clouds, obtained through post
processing of sensed multiview RGB-D images. They are typically trained
in-domain, forego large-scale 2D pre-training and outperform alternatives that
featurize the posed RGB-D multiview images instead. The gap in performance
between methods that consume posed images versus post-processed 3D point clouds
has fueled the belief that 2D and 3D perception require distinct model
architectures. In this paper, we challenge this view and propose ODIN
(Omni-Dimensional INstance segmentation), a model that can segment and label
both 2D RGB images and 3D point clouds, using a transformer architecture that
alternates between 2D within-view and 3D cross-view information fusion. Our
model differentiates 2D and 3D feature operations through the positional
encodings of the tokens involved, which capture pixel coordinates for 2D patch
tokens and 3D coordinates for 3D feature tokens. ODIN achieves state-of-the-art
performance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation
benchmarks, and competitive performance on ScanNet, S3DIS and COCO. It
outperforms all previous works by a wide margin when the sensed 3D point cloud
is used in place of the point cloud sampled from 3D mesh. When used as the 3D
perception engine in an instructable embodied agent architecture, it sets a new
state-of-the-art on the TEACh action-from-dialogue benchmark. Our code and
checkpoints can be found at the project website: https://odin-seg.github.io.
]]></content:encoded>
<pubDate>2024-01-04T18:59:25Z</pubDate>
</item>
<item>
<title>LLaMA Pro: Progressive LLaMA with Block Expansion</title>
<link>http://arxiv.org/abs/2401.02415v1</link>
<guid>http://arxiv.org/abs/2401.02415v1</guid>
<content:encoded><![CDATA[
<div> LLaMA, post-pretraining, Transformer blocks, programming, mathematics
<br />
Large Language Models（LLMs）通过扩展Transformer模块的方法进行新的后预训练，从而在编程和数学领域取得了显著的进展。新模型LLaMA Pro-8.3B在通用任务、编程和数学领域表现出色，并在各种基准测试中取得了先进的性能。该研究为整合自然语言和编程语言提供了宝贵的见解，并为在各种环境中有效运行的先进语言代理的发展奠定了坚实的基础。 
<br /><br />总结: 
<br />LLaMA Pro-8.3B是通过扩展Transformer模块的后预训练方法得到的新模型，在通用任务、编程和数学领域获得了出色的性能。该研究为整合自然语言和编程语言提供了宝贵的见解，并为在各种环境中有效运行的先进语言代理的发展奠定了坚实的基础。 <div>
Humans generally acquire new skills without compromising the old; however,
the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to
CodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with
an expansion of Transformer blocks. We tune the expanded blocks using only new
corpus, efficiently and effectively improving the model's knowledge without
catastrophic forgetting. In this paper, we experiment on the corpus of code and
math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from
LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro
and its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced
performance among various benchmarks, demonstrating superiority over existing
open models in the LLaMA family and the immense potential of reasoning and
addressing diverse tasks as an intelligent agent. Our findings provide valuable
insights into integrating natural and programming languages, laying a solid
foundation for developing advanced language agents that operate effectively in
various environments.
]]></content:encoded>
<pubDate>2024-01-04T18:59:12Z</pubDate>
</item>
<item>
<title>TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview</title>
<link>http://arxiv.org/abs/2401.01330v1</link>
<guid>http://arxiv.org/abs/2401.01330v1</guid>
<content:encoded><![CDATA[
<div> TREC iKAT, Conversational Search Agents, personalized context, decisional search tasks, information operators <br />
<br />
总结: 本文介绍了TREC iKAT对话式检索的研究领域，强调了对话搜索代理的个性化适应能力和决策性搜索任务的重要性。文章描述了任务、主题、数据收集和评估框架，并总结了提交的研究成果。文章强调了不同用户角色和其信息需求的多样性，以及对话式检索的个性化上下文对于提高搜索效率的重要性。 <div>
Conversational Information Seeking stands as a pivotal research area with
significant contributions from previous works. The TREC Interactive Knowledge
Assistance Track (iKAT) builds on the foundational work of the TREC
Conversational Assistance Track (CAsT). However, iKAT distinctively emphasizes
the creation and research of conversational search agents that adapt responses
based on user's prior interactions and present context. The challenge lies in
enabling Conversational Search Agents (CSA) to incorporate this personalized
context to efficiency and effectively guide users through the relevant
information to them. iKAT also emphasizes decisional search tasks, where users
sift through data and information to weigh up options in order to reach a
conclusion or perform an action. These tasks, prevalent in everyday
information-seeking decisions -- be it related to travel, health, or shopping
-- often revolve around a subset of high-level information operators where
queries or questions about the information space include: finding options,
comparing options, identifying the pros and cons of options, etc. Given the
different personas and their information need (expressed through the sequence
of questions), diverse conversation trajectories will arise -- because the
answers to these similar queries will be very different. In this paper, we
report on the first year of TREC iKAT, describing the task, topics, data
collection, and evaluation framework. We further review the submissions and
summarize the findings.
]]></content:encoded>
<pubDate>2024-01-02T18:40:03Z</pubDate>
</item>
<item>
<title>K-PERM: Personalized Response Generation Using Dynamic Knowledge
  Retrieval and Persona-Adaptive Queries</title>
<link>http://arxiv.org/abs/2312.17748v1</link>
<guid>http://arxiv.org/abs/2312.17748v1</guid>
<content:encoded><![CDATA[
<div> 个性化、对话代理、知识、K-PERM、FoCus数据集
<br /><br />
对话代理的个性化可以提高对话质量和用户参与度，但缺乏外部知识以适当地满足用户的个性特点。为了增强个性化响应的相关性和全面性，文章提出了一个两步方法，包括选择性地整合用户个性和用补充信息来情景化回应。作者们开发了K-PERM（知识引导的个性化与奖励调节），这是一个动态对话代理，结合了这些元素。K-PERM在流行的FoCus数据集上取得了最先进的性能，该数据集包含有关全球地标的真实个性化对话。作者还表明，使用K-PERM的回应可以提高最先进的LLMs（例如GPT 3.5）的性能10.5％，突显了K-PERM对个性化聊天机器人的影响。 <div>
Personalizing conversational agents can enhance the quality of conversations
and increase user engagement. However, they often lack external knowledge to
appropriately tend to a user's persona. This is particularly crucial for
practical applications like mental health support, nutrition planning,
culturally sensitive conversations, or reducing toxic behavior in
conversational agents. To enhance the relevance and comprehensiveness of
personalized responses, we propose using a two-step approach that involves (1)
selectively integrating user personas and (2) contextualizing the response with
supplementing information from a background knowledge source. We develop K-PERM
(Knowledge-guided PErsonalization with Reward Modulation), a dynamic
conversational agent that combines these elements. K-PERM achieves
state-of-the-art performance on the popular FoCus dataset, containing
real-world personalized conversations concerning global landmarks. We show that
using responses from K-PERM can improve performance in state-of-the-art LLMs
(GPT 3.5) by 10.5%, highlighting the impact of K-PERM for personalizing
chatbots.
]]></content:encoded>
<pubDate>2023-12-29T18:59:58Z</pubDate>
</item>
<item>
<title>MURP: Multi-Agent Ultra-Wideband Relative Pose Estimation with
  Constrained Communications in 3D Environments</title>
<link>http://arxiv.org/abs/2312.17731v1</link>
<guid>http://arxiv.org/abs/2312.17731v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 多机器人系统, 相对定位, 3D姿态估计, 超宽带测距标签, 误差校正
总结:
本文提出了一种新颖的多机器人系统中的相对定位方法，使用超宽带(UWB)测距标签进行3D姿态估计。相比先前的方法，本文的方法通过使用本地收集到的UWB测距数据以及先验状态约束，并在违反约束时进行检测，可以避免通信网络能力不足和团队规模增加时可能出现的问题。通过利用已学习的平均测距偏差校正，我们实现了19%的定位误差改进，实验结果显示平均绝对位置误差为0.24米，航向误差为9.5度。与其他最新方法相比，本文的方法表现得更好，同时在通信成本显著较高的方法中也具有竞争力。此外，我们还提供了数据集供他人使用。 <br /><br />总结: 本文提出了一种新的多机器人系统的相对定位方法，通过使用UWB测距标签和误差校正来提高定位精度，并在保持竞争力的同时降低了通信成本。 <div>
Inter-agent relative localization is critical for many multi-robot systems
operating in the absence of external positioning infrastructure or prior
environmental knowledge. We propose a novel inter-agent relative 3D pose
estimation system where each participating agent is equipped with several
ultra-wideband (UWB) ranging tags. Prior work typically supplements noisy UWB
range measurements with additional continuously transmitted data, such as
odometry, leading to potential scaling issues with increased team size and/or
decreased communication network capability. By equipping each agent with
multiple UWB antennas, our approach addresses these concerns by using only
locally collected UWB range measurements, a priori state constraints, and
detections of when said constraints are violated. Leveraging our learned mean
ranging bias correction, we gain a 19% positional error improvement giving us
experimental mean absolute position and heading errors of 0.24m and 9.5 degrees
respectively. When compared to other state-of-the-art approaches, our work
demonstrates improved performance over similar systems, while remaining
competitive with methods that have significantly higher communication costs.
Additionally, we make our datasets available.
]]></content:encoded>
<pubDate>2023-12-29T18:40:05Z</pubDate>
</item>
<item>
<title>EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards
  Embodied AI</title>
<link>http://arxiv.org/abs/2312.16170v1</link>
<guid>http://arxiv.org/abs/2312.16170v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 计算机视觉, 机器人, 3D场景理解, 多模态感知数据集, Embodied Perceptron

总结:<br /><br />本文介绍了一种新的多模态、自我中心的3D感知数据集和基准测试，名为EmbodiedScan，以及一个基于此数据集的基线框架Embodied Perceptron。该数据集包含了超过5k个扫描，包括100万个自我中心的RGB-D视图，160k个涵盖760个类别的3D定向框和80个常见类别的密集语义占用。Embodied Perceptron能够处理任意数量的多模态输入，并展示出卓越的3D感知能力，不仅在基本3D感知任务和语言相关任务方面，在实际环境中也能取得显著成果。该研究填补了传统研究在3D场景理解方面的空白，为计算机视觉和机器人领域的进一步研究提供了有益的参考。GitHub链接提供了代码、数据集和基准测试。 <div>
In the realm of computer vision and robotics, embodied agents are expected to
explore their environment and carry out human instructions. This necessitates
the ability to fully understand 3D scenes given their first-person observations
and contextualize them into language for interaction. However, traditional
research focuses more on scene-level input and output setups from a global
view. To address the gap, we introduce EmbodiedScan, a multi-modal, ego-centric
3D perception dataset and benchmark for holistic 3D scene understanding. It
encompasses over 5k scans encapsulating 1M ego-centric RGB-D views, 1M language
prompts, 160k 3D-oriented boxes spanning over 760 categories, some of which
partially align with LVIS, and dense semantic occupancy with 80 common
categories. Building upon this database, we introduce a baseline framework
named Embodied Perceptron. It is capable of processing an arbitrary number of
multi-modal inputs and demonstrates remarkable 3D perception capabilities, both
within the two series of benchmarks we set up, i.e., fundamental 3D perception
tasks and language-grounded tasks, and in the wild. Codes, datasets, and
benchmarks will be available at https://github.com/OpenRobotLab/EmbodiedScan.
]]></content:encoded>
<pubDate>2023-12-26T18:59:11Z</pubDate>
</item>
<item>
<title>From Text to Multimodal: A Comprehensive Survey of Adversarial Example
  Generation in Question Answering Systems</title>
<link>http://arxiv.org/abs/2312.16156v1</link>
<guid>http://arxiv.org/abs/2312.16156v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 对抗性机器学习, 问答系统, 文本, 多模态, 模型漏洞

总结: 
本文综合评述了在问答系统领域中对抗性示例生成技术，包括文本和多模态情景。首先概述了传统问答模型，然后通过对基于规则的扰动和先进的生成模型的探讨，研究了对抗性示例的生成技术。之后，扩展到多模态问答系统，分析了各种方法，并对生成模型、seq2seq架构和混合方法进行了研究。研究覆盖了不同的防御策略、对抗性数据集和评估指标，并展示了对抗性问答方面的全面文献。最后，考虑了对抗性问题生成的未来发展方向，突出了可以推进文本和多模态问答系统在对抗性挑战方面的潜在研究方向。 <div>
Integrating adversarial machine learning with Question Answering (QA) systems
has emerged as a critical area for understanding the vulnerabilities and
robustness of these systems. This article aims to comprehensively review
adversarial example-generation techniques in the QA field, including textual
and multimodal contexts. We examine the techniques employed through systematic
categorization, providing a comprehensive, structured review. Beginning with an
overview of traditional QA models, we traverse the adversarial example
generation by exploring rule-based perturbations and advanced generative
models. We then extend our research to include multimodal QA systems, analyze
them across various methods, and examine generative models, seq2seq
architectures, and hybrid methodologies. Our research grows to different
defense strategies, adversarial datasets, and evaluation metrics and
illustrates the comprehensive literature on adversarial QA. Finally, the paper
considers the future landscape of adversarial question generation, highlighting
potential research directions that can advance textual and multimodal QA
systems in the context of adversarial challenges.
]]></content:encoded>
<pubDate>2023-12-26T18:30:29Z</pubDate>
</item>
</channel>
</rss>