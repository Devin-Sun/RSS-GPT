<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>ArXiv Query: search_query=cat:cs.*&amp;amp;id_list=&amp;amp;start=0&amp;amp;max_results=10</title>
<link>http://arxiv.org/api//Sr0Ktnalppie/A9qvr4BIZuOfQ</link>


<item>
<title>Text-Animator: Controllable Visual Text Video Generation</title>
<link>http://arxiv.org/abs/2406.17777v1</link>
<guid>http://arxiv.org/abs/2406.17777v1</guid>
<content:encoded><![CDATA[
<div> Text-to-Video, Visualization, Text-Animator, Video generation, Visual text
<br /><br />本研究提出了一种名为Text-Animator的创新方法，用于视觉文本视频生成。该方法通过文本嵌入注入模块准确地描述生成视频中的视觉文本结构。此外，研究团队还开发了一个摄像头控制模块和一个文本精化模块，以改善所生成的视觉文本的稳定性。实验结果定量和定性地展示了该方法相比于现有视频生成方法在生成视觉文本准确性方面的优越性。 Text-Animator对于文本在视频中的可视化具有重要意义，尤其是在游戏、电子商务和广告等行业中。虽然现有的文本到视频生成方法取得了进展，但仍然面临着许多问题。本研究提出的Text-Animator方法填补了这一空白，并展现了出色的表现。总结: <br />Text-Animator方法针对文本到视频生成问题提出了创新的解决方案，实现了在生成视频中准确地展现视觉文本的结构。 <div>
Video generation is a challenging yet pivotal task in various industries,
such as gaming, e-commerce, and advertising. One significant unresolved aspect
within T2V is the effective visualization of text within generated videos.
Despite the progress achieved in Text-to-Video~(T2V) generation, current
methods still cannot effectively visualize texts in videos directly, as they
mainly focus on summarizing semantic scene information, understanding, and
depicting actions. While recent advances in image-level visual text generation
show promise, transitioning these techniques into the video domain faces
problems, notably in preserving textual fidelity and motion coherence. In this
paper, we propose an innovative approach termed Text-Animator for visual text
video generation. Text-Animator contains a text embedding injection module to
precisely depict the structures of visual text in generated videos. Besides, we
develop a camera control module and a text refinement module to improve the
stability of generated visual text by controlling the camera movement as well
as the motion of visualized text. Quantitative and qualitative experimental
results demonstrate the superiority of our approach to the accuracy of
generated visual text over state-of-the-art video generation methods. The
project page can be found at https://laulampaul.github.io/text-animator.html.
]]></content:encoded>
<pubDate>2024-06-25T17:59:41Z</pubDate>
</item>
<item>
<title>MG-LLaVA: Towards Multi-Granularity Visual Instruction Tuning</title>
<link>http://arxiv.org/abs/2406.17770v1</link>
<guid>http://arxiv.org/abs/2406.17770v1</guid>
<content:encoded><![CDATA[
<div> 多模态大型语言模型、MG-LLaVA、视觉处理、细节信息、性能评估
<br />
多模态大型语言模型MG-LLaVA结合了多个视觉流程，包括低分辨率、高分辨率和物体中心特征，以增强模型的视觉处理能力。该模型引入了额外的高分辨率视觉编码器和经过Conv-Gate融合网络与基础视觉特征相结合，以捕捉细节信息。此外，还通过离线检测器识别的边界框来整合物体级特征以进一步提高模型的对象识别能力。经过指导调整，MG-LLaVA仅在公开可用的多模态数据上训练，展现出了卓越的感知能力。经过广泛的评估，MG-LLaVA在多个基准测试中优于相似参数大小的现有多模态大型语言模型，展现出了卓越的效能。<br /><br />总结: 多模态大型语言模型MG-LLaVA结合了多个视觉流程，包括低分辨率、高分辨率和物体中心特征，以增强模型的视觉处理能力。与现有模型相比，MG-LLaVA表现出卓越的性能。 <div>
Multi-modal large language models (MLLMs) have made significant strides in
various visual understanding tasks. However, the majority of these models are
constrained to process low-resolution images, which limits their effectiveness
in perception tasks that necessitate detailed visual information. In our study,
we present MG-LLaVA, an innovative MLLM that enhances the model's visual
processing capabilities by incorporating a multi-granularity vision flow, which
includes low-resolution, high-resolution, and object-centric features. We
propose the integration of an additional high-resolution visual encoder to
capture fine-grained details, which are then fused with base visual features
through a Conv-Gate fusion network. To further refine the model's object
recognition abilities, we incorporate object-level features derived from
bounding boxes identified by offline detectors. Being trained solely on
publicly available multimodal data through instruction tuning, MG-LLaVA
demonstrates exceptional perception skills. We instantiate MG-LLaVA with a wide
variety of language encoders, ranging from 3.8B to 34B, to evaluate the model's
performance comprehensively. Extensive evaluations across multiple benchmarks
demonstrate that MG-LLaVA outperforms existing MLLMs of comparable parameter
sizes, showcasing its remarkable efficacy. The code will be available at
https://github.com/PhoenixZ810/MG-LLaVA.
]]></content:encoded>
<pubDate>2024-06-25T17:55:11Z</pubDate>
</item>
<item>
<title>EXTRACT: Efficient Policy Learning by Extracting Transferrable Robot
  Skills from Offline Data</title>
<link>http://arxiv.org/abs/2406.17768v1</link>
<guid>http://arxiv.org/abs/2406.17768v1</guid>
<content:encoded><![CDATA[
Most reinforcement learning (RL) methods focus on learning optimal policies
over low-level action spaces. While these methods can perform well in their
training environments, they lack the flexibility to transfer to new tasks.
Instead, RL agents that can act over useful, temporally extended skills rather
than low-level actions can learn new tasks more easily. Prior work in
skill-based RL either requires expert supervision to define useful skills,
which is hard to scale, or learns a skill-space from offline data with
heuristics that limit the adaptability of the skills, making them difficult to
transfer during downstream RL. Our approach, EXTRACT, instead utilizes
pre-trained vision language models to extract a discrete set of semantically
meaningful skills from offline data, each of which is parameterized by
continuous arguments, without human supervision. This skill parameterization
allows robots to learn new tasks by only needing to learn when to select a
specific skill and how to modify its arguments for the specific task. We
demonstrate through experiments in sparse-reward, image-based, robot
manipulation environments that EXTRACT can more quickly learn new tasks than
prior works, with major gains in sample efficiency and performance over prior
skill-based RL. Website at https://www.jessezhang.net/projects/extract/.
]]></content:encoded>
<pubDate>2024-06-25T17:50:03Z</pubDate>
</item>
<item>
<title>Revisiting Referring Expression Comprehension Evaluation in the Era of
  Large Multimodal Models</title>
<link>http://arxiv.org/abs/2406.16866v1</link>
<guid>http://arxiv.org/abs/2406.16866v1</guid>
<content:encoded><![CDATA[
<div> 关键词: Referring expression comprehension, large multimodal models, benchmarks, labeling error rates, Ref-L4

总结:<br /><br />这篇文章主要介绍了针对文本描述定位目标实例的Referring expression comprehension (REC)技术。最近，大型多模态模型（LMMs）如CogVLM的出现推动了REC技术的进展，取得了92.44%的准确率。然而，研究质疑现有的基准数据集如RefCOCO，RefCOCO+和RefCOCOg是否能充分评估LMMs的能力。作者通过对这些基准数据集进行手动检查，发现了高达14%到24%的标注错误率，这削弱了评估的真实性。为了解决这一问题，他们排除了问题实例，并重新评估了多个能够处理REC任务的LMMs，显示出显著的准确率提高，突显了基准数据集噪音的影响。作为回应，他们推出了Ref-L4，这是一个专门设计用于评估现代REC模型的全面基准数据集，具有大样本量、多样的物体类别、长的指代表达和丰富的词汇量等特点。他们在Ref-L4上评估了总共24个大型模型，并提供了有价值的见解。原始的RefCOCO，RefCOCO+和RefCOCOg数据集的清理版本，以及他们的Ref-L4基准数据集和评估代码，均可在https://github.com/JierunChen/Ref-L4上获得。 <div>
Referring expression comprehension (REC) involves localizing a target
instance based on a textual description. Recent advancements in REC have been
driven by large multimodal models (LMMs) like CogVLM, which achieved 92.44%
accuracy on RefCOCO. However, this study questions whether existing benchmarks
such as RefCOCO, RefCOCO+, and RefCOCOg, capture LMMs' comprehensive
capabilities. We begin with a manual examination of these benchmarks, revealing
high labeling error rates: 14% in RefCOCO, 24% in RefCOCO+, and 5% in RefCOCOg,
which undermines the authenticity of evaluations. We address this by excluding
problematic instances and reevaluating several LMMs capable of handling the REC
task, showing significant accuracy improvements, thus highlighting the impact
of benchmark noise. In response, we introduce Ref-L4, a comprehensive REC
benchmark, specifically designed to evaluate modern REC models. Ref-L4 is
distinguished by four key features: 1) a substantial sample size with 45,341
annotations; 2) a diverse range of object categories with 365 distinct types
and varying instance scales from 30 to 3,767; 3) lengthy referring expressions
averaging 24.2 words; and 4) an extensive vocabulary comprising 22,813 unique
words. We evaluate a total of 24 large models on Ref-L4 and provide valuable
insights. The cleaned versions of RefCOCO, RefCOCO+, and RefCOCOg, as well as
our Ref-L4 benchmark and evaluation code, are available at
https://github.com/JierunChen/Ref-L4.
]]></content:encoded>
<pubDate>2024-06-24T17:59:58Z</pubDate>
</item>
<item>
<title>FreeTraj: Tuning-Free Trajectory Control in Video Diffusion Models</title>
<link>http://arxiv.org/abs/2406.16863v1</link>
<guid>http://arxiv.org/abs/2406.16863v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 扩散模型, 视频生成, 轨迹控制, 噪音构造, 注意力计算

总结:<br /><br />
这篇文章介绍了一个新的方法，不需经过训练就能实现视频生成过程的轨迹控制。首先分析了初始噪音对生成内容运动轨迹的影响，并提出了一个名为 FreeTraj 的方法，通过修改噪音采样和注意力机制来实现轨迹控制。接着，扩展了 FreeTraj 方法，实现了更长、更大的视频生成，并可以手动或自动生成轨迹。实验结果表明，该方法在增强视频扩散模型轨迹可控性方面取得了很好的效果。 <div>
Diffusion model has demonstrated remarkable capability in video generation,
which further sparks interest in introducing trajectory control into the
generation process. While existing works mainly focus on training-based methods
(e.g., conditional adapter), we argue that diffusion model itself allows decent
control over the generated content without requiring any training. In this
study, we introduce a tuning-free framework to achieve trajectory-controllable
video generation, by imposing guidance on both noise construction and attention
computation. Specifically, 1) we first show several instructive phenomenons and
analyze how initial noises influence the motion trajectory of generated
content. 2) Subsequently, we propose FreeTraj, a tuning-free approach that
enables trajectory control by modifying noise sampling and attention
mechanisms. 3) Furthermore, we extend FreeTraj to facilitate longer and larger
video generation with controllable trajectories. Equipped with these designs,
users have the flexibility to provide trajectories manually or opt for
trajectories automatically generated by the LLM trajectory planner. Extensive
experiments validate the efficacy of our approach in enhancing the trajectory
controllability of video diffusion models.
]]></content:encoded>
<pubDate>2024-06-24T17:59:56Z</pubDate>
</item>
<item>
<title>Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs</title>
<link>http://arxiv.org/abs/2406.16860v1</link>
<guid>http://arxiv.org/abs/2406.16860v1</guid>
<content:encoded><![CDATA[
We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a
vision-centric approach. While stronger language models can enhance multimodal
capabilities, the design choices for vision components are often insufficiently
explored and disconnected from visual representation learning research. This
gap hinders accurate sensory grounding in real-world scenarios. Our study uses
LLMs and visual instruction tuning as an interface to evaluate various visual
representations, offering new insights into different models and architectures
-- self-supervised, strongly supervised, or combinations thereof -- based on
experiments with over 20 vision encoders. We critically examine existing MLLM
benchmarks, addressing the difficulties involved in consolidating and
interpreting results from various tasks, and introduce a new vision-centric
benchmark, CV-Bench. To further improve visual grounding, we propose the
Spatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that
integrates high-resolution vision features with LLMs while reducing the number
of tokens. Additionally, we discuss the curation of high-quality visual
instruction-tuning data from publicly available sources, emphasizing the
importance of data source balancing and distribution ratio. Collectively,
Cambrian-1 not only achieves state-of-the-art performance but also serves as a
comprehensive, open cookbook for instruction-tuned MLLMs. We provide model
weights, code, supporting tools, datasets, and detailed instruction-tuning and
evaluation recipes. We hope our release will inspire and accelerate
advancements in multimodal systems and visual representation learning.
]]></content:encoded>
<pubDate>2024-06-24T17:59:42Z</pubDate>
</item>
<item>
<title>DreamBench++: A Human-Aligned Benchmark for Personalized Image
  Generation</title>
<link>http://arxiv.org/abs/2406.16855v1</link>
<guid>http://arxiv.org/abs/2406.16855v1</guid>
<content:encoded><![CDATA[
Personalized image generation holds great promise in assisting humans in
everyday work and life due to its impressive function in creatively generating
personalized content. However, current evaluations either are automated but
misalign with humans or require human evaluations that are time-consuming and
expensive. In this work, we present DreamBench++, a human-aligned benchmark
automated by advanced multimodal GPT models. Specifically, we systematically
design the prompts to let GPT be both human-aligned and self-aligned, empowered
with task reinforcement. Further, we construct a comprehensive dataset
comprising diverse images and prompts. By benchmarking 7 modern generative
models, we demonstrate that DreamBench++ results in significantly more
human-aligned evaluation, helping boost the community with innovative findings.
]]></content:encoded>
<pubDate>2024-06-24T17:58:47Z</pubDate>
</item>
<item>
<title>Long Context Transfer from Language to Vision</title>
<link>http://arxiv.org/abs/2406.16852v1</link>
<guid>http://arxiv.org/abs/2406.16852v1</guid>
<content:encoded><![CDATA[
Video sequences offer valuable temporal information, but existing large
multimodal models (LMMs) fall short in understanding extremely long videos.
Many works address this by reducing the number of visual tokens using visual
resamplers. Alternatively, in this paper, we approach this problem from the
perspective of the language model. By simply extrapolating the context length
of the language backbone, we enable LMMs to comprehend orders of magnitude more
visual tokens without any video training. We call this phenomenon long context
transfer and carefully ablate its properties. To effectively measure LMMs'
ability to generalize to long contexts in the vision modality, we develop
V-NIAH (Visual Needle-In-A-Haystack), a purely synthetic long vision benchmark
inspired by the language model's NIAH test. Our proposed Long Video Assistant
(LongVA) can process 2000 frames or over 200K visual tokens without additional
complexities. With its extended context length, LongVA achieves
state-of-the-art performance on Video-MME among 7B-scale models by densely
sampling more input frames. Our work is open-sourced at
https://github.com/EvolvingLMMs-Lab/LongVA.
]]></content:encoded>
<pubDate>2024-06-24T17:58:06Z</pubDate>
</item>
<item>
<title>GenoTEX: A Benchmark for Evaluating LLM-Based Exploration of Gene
  Expression Data in Alignment with Bioinformaticians</title>
<link>http://arxiv.org/abs/2406.15341v1</link>
<guid>http://arxiv.org/abs/2406.15341v1</guid>
<content:encoded><![CDATA[
<div> 基因表达数据，机器学习，大型语言模型，基准数据集，基因组数据分析
<br /><br />
研究介绍了一个名为GenoTEX的基准数据集，用于自动探索基因表达数据，包括数据集选择、预处理和统计分析等任务。GenoTEX由人类生物信息学家精心筛选和注释，提供了解决各种基因识别问题的代码和结果。同时，研究还提出了基于大型语言模型的GenoAgents团队，他们使用上下文感知规划、迭代校正和领域专家协商的方式共同探索基因数据集。实验表明了基于大型语言模型的方法在基因组数据分析中的潜力，同时也指出了存在的挑战和改进的方向。研究者们将GenoTEX公开发布在GitHub上，作为基于人工智能的基因组数据分析方法的基准和改进的新资源。 <div>
Recent advancements in machine learning have significantly improved the
identification of disease-associated genes from gene expression datasets.
However, these processes often require extensive expertise and manual effort,
limiting their scalability. Large Language Model (LLM)-based agents have shown
promise in automating these tasks due to their increasing problem-solving
abilities. To support the evaluation and development of such methods, we
introduce GenoTEX, a benchmark dataset for the automatic exploration of gene
expression data, involving the tasks of dataset selection, preprocessing, and
statistical analysis. GenoTEX provides annotated code and results for solving a
wide range of gene identification problems, in a full analysis pipeline that
follows the standard of computational genomics. These annotations are curated
by human bioinformaticians who carefully analyze the datasets to ensure
accuracy and reliability. To provide baselines for these tasks, we present
GenoAgents, a team of LLM-based agents designed with context-aware planning,
iterative correction, and domain expert consultation to collaboratively explore
gene datasets. Our experiments with GenoAgents demonstrate the potential of
LLM-based approaches in genomics data analysis, while error analysis highlights
the challenges and areas for future improvement. We propose GenoTEX as a
promising resource for benchmarking and enhancing AI-driven methods for
genomics data analysis. We make our benchmark publicly available at
\url{https://github.com/Liu-Hy/GenoTex}.
]]></content:encoded>
<pubDate>2024-06-21T17:55:24Z</pubDate>
</item>
<item>
<title>Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning</title>
<link>http://arxiv.org/abs/2406.15334v1</link>
<guid>http://arxiv.org/abs/2406.15334v1</guid>
<content:encoded><![CDATA[
<div> 多模态学习, 多示例学习, 在上下文学习, 注意力头, 多模态任务向量

本文研究了多模态学习中的多示例学习问题，指出了现有模型在预训练阶段设定的上下文长度限制了多示例学习的效果。为了解决这一问题，研究者们引入了多模态任务向量（MTV），并利用这些提取出的MTV来实现多示例学习。实验证明，MTV可以随着压缩示例的数量而提高性能，并且在推理阶段可以推广到类似的跨领域任务。通过这种方法，LMMs可以实现多模态、多示例的上下文学习，为视觉与语言任务提供了新的可能性。<br /><br />总结: 多模态学习中存在多示例学习问题，研究者引入了多模态任务向量（MTV）来解决这一问题。实验证明MTV可以提高性能并推广到类似的跨领域任务。 <div>
The recent success of interleaved Large Multimodal Models (LMMs) in few-shot
learning suggests that in-context learning (ICL) with many examples can be
promising for learning new tasks. However, this many-shot multimodal ICL
setting has one crucial problem: it is fundamentally limited by the model's
context length set at pretraining. The problem is especially prominent in the
multimodal domain, which processes both text and images, requiring additional
tokens. This motivates the need for a multimodal method to compress many shots
into fewer tokens without finetuning. In this work, we enable LMMs to perform
multimodal, many-shot in-context learning by leveraging Multimodal Task Vectors
(MTV)--compact implicit representations of in-context examples compressed in
the model's attention heads. Specifically, we first demonstrate the existence
of such MTV in LMMs and then leverage these extracted MTV to enable many-shot
in-context learning for various vision-and-language tasks. Our experiments
suggest that MTV can scale in performance with the number of compressed shots
and generalize to similar out-of-domain tasks without additional context length
for inference.
]]></content:encoded>
<pubDate>2024-06-21T17:50:02Z</pubDate>
</item>
<item>
<title>GeoLRM: Geometry-Aware Large Reconstruction Model for High-Quality 3D
  Gaussian Generation</title>
<link>http://arxiv.org/abs/2406.15333v1</link>
<guid>http://arxiv.org/abs/2406.15333v1</guid>
<content:encoded><![CDATA[
In this work, we introduce the Geometry-Aware Large Reconstruction Model
(GeoLRM), an approach which can predict high-quality assets with 512k Gaussians
and 21 input images in only 11 GB GPU memory. Previous works neglect the
inherent sparsity of 3D structure and do not utilize explicit geometric
relationships between 3D and 2D images. This limits these methods to a
low-resolution representation and makes it difficult to scale up to the dense
views for better quality. GeoLRM tackles these issues by incorporating a novel
3D-aware transformer structure that directly processes 3D points and uses
deformable cross-attention mechanisms to effectively integrate image features
into 3D representations. We implement this solution through a two-stage
pipeline: initially, a lightweight proposal network generates a sparse set of
3D anchor points from the posed image inputs; subsequently, a specialized
reconstruction transformer refines the geometry and retrieves textural details.
Extensive experimental results demonstrate that GeoLRM significantly
outperforms existing models, especially for dense view inputs. We also
demonstrate the practical applicability of our model with 3D generation tasks,
showcasing its versatility and potential for broader adoption in real-world
applications.
]]></content:encoded>
<pubDate>2024-06-21T17:49:31Z</pubDate>
</item>
<item>
<title>Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities</title>
<link>http://arxiv.org/abs/2406.14562v1</link>
<guid>http://arxiv.org/abs/2406.14562v1</guid>
<content:encoded><![CDATA[
<div> 多模态大型语言模型、视觉推理、白板思维提示、结果、分析<br />
总结:<br />
这篇论文介绍了一种简单的方法，即白板思维提示，用于在多模态大型语言模型中解锁视觉推理能力。通过为模型提供一个“白板”，让其将推理步骤以图像的形式展示，然后将这些图像返回给模型进行进一步处理。该方法在四项涉及视觉和空间推理的自然语言任务中取得了最先进的结果。论文还探讨了该技术成功的领域以及错误的来源。白板思维提示能够显著提高模型的准确性，而传统的“链式思维”方法在某些情况下表现不佳。  <div>
When presented with questions involving visual thinking, humans naturally
switch reasoning modalities, often forming mental images or drawing visual
aids. Large language models have shown promising results in arithmetic and
symbolic reasoning by expressing intermediate reasoning in text as a chain of
thought, yet struggle to extend this capability to answer text queries that are
easily solved by visual reasoning, even with extensive multimodal pretraining.
We introduce a simple method, whiteboard-of-thought prompting, to unlock the
visual reasoning capabilities of multimodal large language models across
modalities. Whiteboard-of-thought prompting provides multimodal large language
models with a metaphorical `whiteboard' to draw out reasoning steps as images,
then returns these images back to the model for further processing. We find
this can be accomplished with no demonstrations or specialized modules, instead
leveraging models' existing ability to write code with libraries such as
Matplotlib and Turtle. This simple approach shows state-of-the-art results on
four difficult natural language tasks that involve visual and spatial
reasoning. We identify multiple settings where GPT-4o using chain-of-thought
fails dramatically, including more than one where it achieves $0\%$ accuracy,
while whiteboard-of-thought enables up to $92\%$ accuracy in these same
settings. We present a detailed exploration of where the technique succeeds as
well as its sources of error.
]]></content:encoded>
<pubDate>2024-06-20T17:59:45Z</pubDate>
</item>
<item>
<title>CooHOI: Learning Cooperative Human-Object Interaction with Manipulated
  Object Dynamics</title>
<link>http://arxiv.org/abs/2406.14558v1</link>
<guid>http://arxiv.org/abs/2406.14558v1</guid>
<content:encoded><![CDATA[
<div> 多人合作，物体运输，学习范式，CooHOI框架，AMP框架

多年来，由于大规模动作捕捉数据的可用性和强化学习方法的应用，人形控制取得了显著进展。然而，许多现实世界的任务，如移动大型和沉重的家具，需要多角色协作。由于多角色协作数据的稀缺性和多代理学习所带来的效率挑战，这些任务不能简单地使用为单一代理场景设计的训练范式来解决。在本文中，我们介绍了合作人物物体交互（CooHOI）框架，这是一个新颖的框架，通过两阶段学习范式来解决多人物体运输问题：个体技能获取和随后的转移。最初，一个单一代理使用对抗性运动先验（AMP）框架学习执行任务。随后，代理通过考虑并行训练中操纵对象的共享动力学来学习与其他人合作，使用多代理近端策略优化（MAPPO）方法。当一个代理与对象互动，导致特定物体动力学变化时，其他代理学习做出适当的响应，从而实现了队友之间的隐式沟通和协调。与先前依赖基于跟踪的方法进行多字符HOI的方法不同，CooHOI本质上是高效的，不依赖于多角色交互的动作捕捉数据，并且可以无缝地扩展到包括更多参与者和广泛的对象类型。<br /><br />总结: 该研究介绍了CoоHOI框架，通过AMP框架进行个体技能获取，随后使用MAPPO方法进行多角色协作学习。该框架不依赖于动作捕捉数据，能够实现隐式沟通和协调。 <div>
Recent years have seen significant advancements in humanoid control, largely
due to the availability of large-scale motion capture data and the application
of reinforcement learning methodologies. However, many real-world tasks, such
as moving large and heavy furniture, require multi-character collaboration.
Given the scarcity of data on multi-character collaboration and the efficiency
challenges associated with multi-agent learning, these tasks cannot be
straightforwardly addressed using training paradigms designed for single-agent
scenarios. In this paper, we introduce Cooperative Human-Object Interaction
(CooHOI), a novel framework that addresses multi-character objects transporting
through a two-phase learning paradigm: individual skill acquisition and
subsequent transfer. Initially, a single agent learns to perform tasks using
the Adversarial Motion Priors (AMP) framework. Following this, the agent learns
to collaborate with others by considering the shared dynamics of the
manipulated object during parallel training using Multi Agent Proximal Policy
Optimization (MAPPO). When one agent interacts with the object, resulting in
specific object dynamics changes, the other agents learn to respond
appropriately, thereby achieving implicit communication and coordination
between teammates. Unlike previous approaches that relied on tracking-based
methods for multi-character HOI, CooHOI is inherently efficient, does not
depend on motion capture data of multi-character interactions, and can be
seamlessly extended to include more participants and a wide range of object
types
]]></content:encoded>
<pubDate>2024-06-20T17:59:22Z</pubDate>
</item>
<item>
<title>A Survey of Multimodal-Guided Image Editing with Text-to-Image Diffusion
  Models</title>
<link>http://arxiv.org/abs/2406.14555v1</link>
<guid>http://arxiv.org/abs/2406.14555v1</guid>
<content:encoded><![CDATA[
Image editing aims to edit the given synthetic or real image to meet the
specific requirements from users. It is widely studied in recent years as a
promising and challenging field of Artificial Intelligence Generative Content
(AIGC). Recent significant advancement in this field is based on the
development of text-to-image (T2I) diffusion models, which generate images
according to text prompts. These models demonstrate remarkable generative
capabilities and have become widely used tools for image editing. T2I-based
image editing methods significantly enhance editing performance and offer a
user-friendly interface for modifying content guided by multimodal inputs. In
this survey, we provide a comprehensive review of multimodal-guided image
editing techniques that leverage T2I diffusion models. First, we define the
scope of image editing from a holistic perspective and detail various control
signals and editing scenarios. We then propose a unified framework to formalize
the editing process, categorizing it into two primary algorithm families. This
framework offers a design space for users to achieve specific goals.
Subsequently, we present an in-depth analysis of each component within this
framework, examining the characteristics and applicable scenarios of different
combinations. Given that training-based methods learn to directly map the
source image to target one under user guidance, we discuss them separately, and
introduce injection schemes of source image in different scenarios.
Additionally, we review the application of 2D techniques to video editing,
highlighting solutions for inter-frame inconsistency. Finally, we discuss open
challenges in the field and suggest potential future research directions. We
keep tracing related works at
https://github.com/xinchengshuai/Awesome-Image-Editing.
]]></content:encoded>
<pubDate>2024-06-20T17:58:52Z</pubDate>
</item>
<item>
<title>DrVideo: Document Retrieval Based Long Video Understanding</title>
<link>http://arxiv.org/abs/2406.12846v1</link>
<guid>http://arxiv.org/abs/2406.12846v1</guid>
<content:encoded><![CDATA[
<div> 长视频理解，document-retrieval-based system, large language models, agent-based iterative loop, 实验证实方法有效。<br /><br />长视频理解在现有方法中存在挑战，难以定位关键信息和进行长距离推理。为了解决这一问题，研究人员提出了DrVideo，这是一个基于文档检索的系统，旨在利用大型语言模型的力量。具体来说，DrVideo将长视频转换为基于文本的长文档，以便最初检索关键帧并增加这些帧的信息。随后，它采用基于代理的迭代循环不断搜索缺失信息，增加相关数据，并一旦收集到足够的与问题相关的信息，就以链式思维的方式提供最终预测。对长视频基准进行的实验证实了该方法的有效性。DrVideo在EgoSchema基准（3分钟）上的准确率高出3.8，MovieChat-1K break模式上高出17.9，在MovieChat-1K全局模式（10分钟）上高出38.0，在LLama-Vid QA数据集（超过60分钟）上高出30.2。 <br /><br />总结: 该研究提出了DrVideo系统，利用大型语言模型将长视频转换为文本以进行理解，证实其在长视频理解任务中的有效性。 <div>
Existing methods for long video understanding primarily focus on videos only
lasting tens of seconds, with limited exploration of techniques for handling
longer videos. The increased number of frames in longer videos presents two
main challenges: difficulty in locating key information and performing
long-range reasoning. Thus, we propose DrVideo, a document-retrieval-based
system designed for long video understanding. Our key idea is to convert the
long-video understanding problem into a long-document understanding task so as
to effectively leverage the power of large language models. Specifically,
DrVideo transforms a long video into a text-based long document to initially
retrieve key frames and augment the information of these frames, which is used
this as the system's starting point. It then employs an agent-based iterative
loop to continuously search for missing information, augment relevant data, and
provide final predictions in a chain-of-thought manner once sufficient
question-related information is gathered. Extensive experiments on long video
benchmarks confirm the effectiveness of our method. DrVideo outperforms
existing state-of-the-art methods with +3.8 accuracy on EgoSchema benchmark (3
minutes), +17.9 in MovieChat-1K break mode, +38.0 in MovieChat-1K global mode
(10 minutes), and +30.2 on the LLama-Vid QA dataset (over 60 minutes).
]]></content:encoded>
<pubDate>2024-06-18T17:59:03Z</pubDate>
</item>
<item>
<title>Synergizing Foundation Models and Federated Learning: A Survey</title>
<link>http://arxiv.org/abs/2406.12844v1</link>
<guid>http://arxiv.org/abs/2406.12844v1</guid>
<content:encoded><![CDATA[
<div> 关键词: Foundation Models, Federated Learning, 数据隐私, 协作学习, 应用领域
<br />
总结:<br />
本文主要讨论了基金会模型（FMs）和联邦学习（FL）的潜力和挑战。FMs在预训练阶段需要大量的高质量数据，而FL是一种协作学习范式，可以突破数据可用性的障碍。通过FL，FMs可以定制和适应各种领域特定任务，同时保护数据隐私。文章还总结了核心技术、未来方向和应用，并提供了一个关于FM-FL的定期更新的文集。 <div>
The recent development of Foundation Models (FMs), represented by large
language models, vision transformers, and multimodal models, has been making a
significant impact on both academia and industry. Compared with small-scale
models, FMs have a much stronger demand for high-volume data during the
pre-training phase. Although general FMs can be pre-trained on data collected
from open sources such as the Internet, domain-specific FMs need proprietary
data, posing a practical challenge regarding the amount of data available due
to privacy concerns. Federated Learning (FL) is a collaborative learning
paradigm that breaks the barrier of data availability from different
participants. Therefore, it provides a promising solution to customize and
adapt FMs to a wide range of domain-specific tasks using distributed datasets
whilst preserving privacy. This survey paper discusses the potentials and
challenges of synergizing FL and FMs and summarizes core techniques, future
directions, and applications. A periodically updated paper collection on FM-FL
is available at https://github.com/lishenghui/awesome-fm-fl.
]]></content:encoded>
<pubDate>2024-06-18T17:58:09Z</pubDate>
</item>
<item>
<title>Can Go AIs be adversarially robust?</title>
<link>http://arxiv.org/abs/2406.12843v1</link>
<guid>http://arxiv.org/abs/2406.12843v1</guid>
<content:encoded><![CDATA[
Prior work found that superhuman Go AIs like KataGo can be defeated by simple
adversarial strategies. In this paper, we study if simple defenses can improve
KataGo's worst-case performance. We test three natural defenses: adversarial
training on hand-constructed positions, iterated adversarial training, and
changing the network architecture. We find that some of these defenses are able
to protect against previously discovered attacks. Unfortunately, we also find
that none of these defenses are able to withstand adaptive attacks. In
particular, we are able to train new adversaries that reliably defeat our
defended agents by causing them to blunder in ways humans would not. Our
results suggest that building robust AI systems is challenging even in narrow
domains such as Go. For interactive examples of attacks and a link to our
codebase, see https://goattack.far.ai.
]]></content:encoded>
<pubDate>2024-06-18T17:57:49Z</pubDate>
</item>
<item>
<title>Autoregressive Image Generation without Vector Quantization</title>
<link>http://arxiv.org/abs/2406.11838v1</link>
<guid>http://arxiv.org/abs/2406.11838v1</guid>
<content:encoded><![CDATA[
<div> Diffusion, Autoregressive modeling, Continuous-valued space, Diffusion Loss function, Image generation
总结: 本文提出了一种在连续数值空间中使用扩散过程来建模每个标记概率分布的方法，从而消除了离散值标记器的需求。这种方法在标准自回归模型和广义掩模自回归（MAR）变体中取得了良好的效果，并且在序列建模方面具有速度优势。作者希望这项工作能够激发在其他连续数值领域和应用中使用自回归生成的动力。 <div>
Conventional wisdom holds that autoregressive models for image generation are
typically accompanied by vector-quantized tokens. We observe that while a
discrete-valued space can facilitate representing a categorical distribution,
it is not a necessity for autoregressive modeling. In this work, we propose to
model the per-token probability distribution using a diffusion procedure, which
allows us to apply autoregressive models in a continuous-valued space. Rather
than using categorical cross-entropy loss, we define a Diffusion Loss function
to model the per-token probability. This approach eliminates the need for
discrete-valued tokenizers. We evaluate its effectiveness across a wide range
of cases, including standard autoregressive models and generalized masked
autoregressive (MAR) variants. By removing vector quantization, our image
generator achieves strong results while enjoying the speed advantage of
sequence modeling. We hope this work will motivate the use of autoregressive
generation in other continuous-valued domains and applications.
]]></content:encoded>
<pubDate>2024-06-17T17:59:58Z</pubDate>
</item>
<item>
<title>mDPO: Conditional Preference Optimization for Multimodal Large Language
  Models</title>
<link>http://arxiv.org/abs/2406.11839v1</link>
<guid>http://arxiv.org/abs/2406.11839v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 多模态，偏好优化，大语言模型，mDPO，奖励锚定<br />
<br />
在多模态情景下，直接偏好优化（DPO）已被证明是对齐大语言模型（LLM）的一个有效方法。最近的研究尝试将DPO应用于多模态场景，但发现难以实现一致的改进。通过一项比较实验，我们确定了多模态偏好优化中的无条件偏好问题，即模型忽视了图像条件。为了解决这个问题，我们提出了mDPO，这是一个多模态DPO目标，通过优化图像偏好来防止语言偏好的过度优先。此外，我们引入了奖励锚定，强制奖励对于所选的响应保持为正值，从而避免相对偏好优化中降低其可能性的问题。在两个不同大小的多模态LLM和三个广泛使用的基准测试上的实验证明，mDPO有效解决了多模态偏好优化中的无条件偏好问题，并显著提升了模型性能，特别是减少了臆想现象。<br /><br />总结:多模态直接偏好优化（DPO）有效解决了无条件偏好问题，通过优化图像偏好防止语言偏好的过度优先。引入奖励锚定避免了相对偏好优化中降低其可能性的问题。实验表明mDPO显著提升了模型性能，特别是在减少臆想现象方面。 <div>
Direct preference optimization (DPO) has shown to be an effective method for
large language model (LLM) alignment. Recent works have attempted to apply DPO
to multimodal scenarios but have found it challenging to achieve consistent
improvement. Through a comparative experiment, we identify the unconditional
preference problem in multimodal preference optimization, where the model
overlooks the image condition. To address this problem, we propose mDPO, a
multimodal DPO objective that prevents the over-prioritization of language-only
preferences by also optimizing image preference. Moreover, we introduce a
reward anchor that forces the reward to be positive for chosen responses,
thereby avoiding the decrease in their likelihood -- an intrinsic problem of
relative preference optimization. Experiments on two multimodal LLMs of
different sizes and three widely used benchmarks demonstrate that mDPO
effectively addresses the unconditional preference problem in multimodal
preference optimization and significantly improves model performance,
particularly in reducing hallucination.
]]></content:encoded>
<pubDate>2024-06-17T17:59:58Z</pubDate>
</item>
<item>
<title>Scaling the Codebook Size of VQGAN to 100,000 with a Utilization Rate of
  99%</title>
<link>http://arxiv.org/abs/2406.11837v1</link>
<guid>http://arxiv.org/abs/2406.11837v1</guid>
<content:encoded><![CDATA[
In the realm of image quantization exemplified by VQGAN, the process encodes
images into discrete tokens drawn from a codebook with a predefined size.
Recent advancements, particularly with LLAMA 3, reveal that enlarging the
codebook significantly enhances model performance. However, VQGAN and its
derivatives, such as VQGAN-FC (Factorized Codes) and VQGAN-EMA, continue to
grapple with challenges related to expanding the codebook size and enhancing
codebook utilization. For instance, VQGAN-FC is restricted to learning a
codebook with a maximum size of 16,384, maintaining a typically low utilization
rate of less than 12% on ImageNet. In this work, we propose a novel image
quantization model named VQGAN-LC (Large Codebook), which extends the codebook
size to 100,000, achieving an utilization rate exceeding 99%. Unlike previous
methods that optimize each codebook entry, our approach begins with a codebook
initialized with 100,000 features extracted by a pre-trained vision encoder.
Optimization then focuses on training a projector that aligns the entire
codebook with the feature distributions of the encoder in VQGAN-LC. We
demonstrate the superior performance of our model over its counterparts across
a variety of tasks, including image reconstruction, image classification,
auto-regressive image generation using GPT, and image creation with diffusion-
and flow-based generative models. Code and models are available at
https://github.com/zh460045050/VQGAN-LC.
]]></content:encoded>
<pubDate>2024-06-17T17:59:57Z</pubDate>
</item>
<item>
<title>Exploring the Role of Large Language Models in Prompt Encoding for
  Diffusion Models</title>
<link>http://arxiv.org/abs/2406.11831v1</link>
<guid>http://arxiv.org/abs/2406.11831v1</guid>
<content:encoded><![CDATA[
Large language models (LLMs) based on decoder-only transformers have
demonstrated superior text understanding capabilities compared to CLIP and
T5-series models. However, the paradigm for utilizing current advanced LLMs in
text-to-image diffusion models remains to be explored. We observed an unusual
phenomenon: directly using a large language model as the prompt encoder
significantly degrades the prompt-following ability in image generation. We
identified two main obstacles behind this issue. One is the misalignment
between the next token prediction training in LLM and the requirement for
discriminative prompt features in diffusion models. The other is the intrinsic
positional bias introduced by the decoder-only architecture. To deal with this
issue, we propose a novel framework to fully harness the capabilities of LLMs.
Through the carefully designed usage guidance, we effectively enhance the text
representation capability for prompt encoding and eliminate its inherent
positional bias. This allows us to integrate state-of-the-art LLMs into the
text-to-image generation model flexibly. Furthermore, we also provide an
effective manner to fuse multiple LLMs into our framework. Considering the
excellent performance and scaling capabilities demonstrated by the transformer
architecture, we further design an LLM-Infused Diffusion Transformer (LI-DiT)
based on the framework. We conduct extensive experiments to validate LI-DiT
across model size and data size. Benefiting from the inherent ability of the
LLMs and our innovative designs, the prompt understanding performance of LI-DiT
easily surpasses state-of-the-art open-source models as well as mainstream
closed-source commercial models including Stable Diffusion 3, DALL-E 3, and
Midjourney V6. The powerful LI-DiT-10B will be available after further
optimization and security checks.
]]></content:encoded>
<pubDate>2024-06-17T17:59:43Z</pubDate>
</item>
<item>
<title>VideoGUI: A Benchmark for GUI Automation from Instructional Videos</title>
<link>http://arxiv.org/abs/2406.10227v1</link>
<guid>http://arxiv.org/abs/2406.10227v1</guid>
<content:encoded><![CDATA[
<div> GUI automation, VideoGUI, multi-modal benchmark, visual-centric tasks, evaluation metrics
<br /><br />本研究引入了VideoGUI，这是一个新颖的多模态基准，旨在评估GUI助手在以视觉为中心的GUI任务上的表现。该基准源自高质量的网络教学视频，重点关注涉及专业和新颖软件（例如Adobe Photoshop或Stable Diffusion WebUI）以及复杂活动（例如视频编辑）的任务。VideoGUI通过分层过程评估GUI助手，允许识别它们可能失败的具体级别：（i）高级规划：从视觉条件重构程序子任务，而无需语言描述；（ii）中级规划：根据视觉状态（即屏幕截图）和目标生成精确操作序列；（iii）原子操作执行：执行诸如准确点击指定元素之类的特定操作。对于每个级别，我们设计了跨个体维度的评估指标，以提供清晰的信号，例如在原子操作执行中点击、拖动、输入和滚动的个体表现。我们在VideoGUI上的评估显示，即使是当前最先进的大型多模态模型GPT4o在视觉为中心的GUI任务上表现不佳，特别是在高级规划方面。
<br /><br />总结: 本研究介绍了VideoGUI基准，其通过多层次过程评估GUI助手的性能，特别关注视觉为中心的GUI任务。该基准涵盖了复杂软件和活动，为评估提供了详细的指标，揭示了当前先进模型在面对视觉为中心的GUI任务时的不足之处。 <div>
Graphical User Interface (GUI) automation holds significant promise for
enhancing human productivity by assisting with computer tasks. Existing task
formulations primarily focus on simple tasks that can be specified by a single,
language-only instruction, such as "Insert a new slide." In this work, we
introduce VideoGUI, a novel multi-modal benchmark designed to evaluate GUI
assistants on visual-centric GUI tasks. Sourced from high-quality web
instructional videos, our benchmark focuses on tasks involving professional and
novel software (e.g., Adobe Photoshop or Stable Diffusion WebUI) and complex
activities (e.g., video editing). VideoGUI evaluates GUI assistants through a
hierarchical process, allowing for identification of the specific levels at
which they may fail: (i) high-level planning: reconstruct procedural subtasks
from visual conditions without language descriptions; (ii) middle-level
planning: generate sequences of precise action narrations based on visual state
(i.e., screenshot) and goals; (iii) atomic action execution: perform specific
actions such as accurately clicking designated elements. For each level, we
design evaluation metrics across individual dimensions to provide clear
signals, such as individual performance in clicking, dragging, typing, and
scrolling for atomic action execution. Our evaluation on VideoGUI reveals that
even the SoTA large multimodal model GPT4o performs poorly on visual-centric
GUI tasks, especially for high-level planning.
]]></content:encoded>
<pubDate>2024-06-14T17:59:08Z</pubDate>
</item>
<item>
<title>Alleviating Distortion in Image Generation via Multi-Resolution
  Diffusion Models</title>
<link>http://arxiv.org/abs/2406.09416v1</link>
<guid>http://arxiv.org/abs/2406.09416v1</guid>
<content:encoded><![CDATA[
<div> 高清图像生成, 扩散模型, 多分辨率网络, 时间相关层归一化, Transformer架构<br />
<br />
扩散模型在高保真图像生成方面非常有效。传统的方法使用卷积U-Net架构，而最近基于Transformer的设计表现出卓越的性能和可扩展性。然而，Transformer架构通过“分块化”输入数据（通过“分块化”）所面临的问题是在于输入标记长度对自注意力操作的二次复杂性方面存在着视觉保真度和计算复杂度之间的权衡。为了解决这一挑战，作者提出了使用DiMR多分辨率网络和TD-LN时间相关层归一化增强扩散模型。他们的方法在类相关的ImageNet生成基准测试中取得了良好的效果，超越了先前的扩散模型，并在ImageNet 256x256和512x512上取得了新的FID分数。 <br /><br />总结: <br />本文提出了一种结合了DiMR多分辨率网络和TD-LN时间相关层归一化的增强扩散模型，以应对Transformer架构在保真度和计算复杂度之间的权衡问题。作者的方法在ImageNet生成基准测试中取得了良好的效果，并取得了新的FID分数。 <div>
This paper presents innovative enhancements to diffusion models by
integrating a novel multi-resolution network and time-dependent layer
normalization. Diffusion models have gained prominence for their effectiveness
in high-fidelity image generation. While conventional approaches rely on
convolutional U-Net architectures, recent Transformer-based designs have
demonstrated superior performance and scalability. However, Transformer
architectures, which tokenize input data (via "patchification"), face a
trade-off between visual fidelity and computational complexity due to the
quadratic nature of self-attention operations concerning token length. While
larger patch sizes enable attention computation efficiency, they struggle to
capture fine-grained visual details, leading to image distortions. To address
this challenge, we propose augmenting the Diffusion model with the
Multi-Resolution network (DiMR), a framework that refines features across
multiple resolutions, progressively enhancing detail from low to high
resolution. Additionally, we introduce Time-Dependent Layer Normalization
(TD-LN), a parameter-efficient approach that incorporates time-dependent
parameters into layer normalization to inject time information and achieve
superior performance. Our method's efficacy is demonstrated on the
class-conditional ImageNet generation benchmark, where DiMR-XL variants
outperform prior diffusion models, setting new state-of-the-art FID scores of
1.70 on ImageNet 256 x 256 and 2.89 on ImageNet 512 x 512. Project page:
https://qihao067.github.io/projects/DiMR
]]></content:encoded>
<pubDate>2024-06-13T17:59:58Z</pubDate>
</item>
<item>
<title>Explore the Limits of Omni-modal Pretraining at Scale</title>
<link>http://arxiv.org/abs/2406.09412v1</link>
<guid>http://arxiv.org/abs/2406.09412v1</guid>
<content:encoded><![CDATA[
<div> 多模态智能、预训练、MiCo、性能记录、GitHub
<br /><br />
总结:
本研究提出了一种名为MiCo的可扩展预训练范式，旨在构建多模态智能。该方法能够在不同的模态和数据量下进行预训练，并在各种任务中展现出显著的能力，包括对10种不同模态的单模态感知基准任务、25种跨模态理解任务以及18种多模态大型语言模型基准任务。我们的模型在性能上建立了37项最新记录。希望本研究能够为全模态智能的发展做出贡献。可以在https://github.com/invictus717/MiCo找到代码和模型。 <div>
We propose to build omni-modal intelligence, which is capable of
understanding any modality and learning universal representations. In specific,
we propose a scalable pretraining paradigm, named Multimodal Context (MiCo),
which can scale up the numbers of modalities and amount of data, together with
the model parameters, in the pretraining process. With MiCo, the pretrained
models show significant emergent abilities in multimodal learning, which are
evaluated on the following tasks: i) single-modality perception benchmarks of
10 different modalities, ii) 25 cross-modality understanding tasks of
retrieval, question-answering, captioning, and iii) 18 multimodal large
language model benchmarks. Our models establish 37 new records for
state-of-the-art performance. We hope that our research could contribute to the
development of omni-modal intelligence. Code and Models are at
https://github.com/invictus717/MiCo
]]></content:encoded>
<pubDate>2024-06-13T17:59:53Z</pubDate>
</item>
<item>
<title>MuirBench: A Comprehensive Benchmark for Robust Multi-image
  Understanding</title>
<link>http://arxiv.org/abs/2406.09411v1</link>
<guid>http://arxiv.org/abs/2406.09411v1</guid>
<content:encoded><![CDATA[
We introduce MuirBench, a comprehensive benchmark that focuses on robust
multi-image understanding capabilities of multimodal LLMs. MuirBench consists
of 12 diverse multi-image tasks (e.g., scene understanding, ordering) that
involve 10 categories of multi-image relations (e.g., multiview, temporal
relations). Comprising 11,264 images and 2,600 multiple-choice questions,
MuirBench is created in a pairwise manner, where each standard instance is
paired with an unanswerable variant that has minimal semantic differences, in
order for a reliable assessment. Evaluated upon 20 recent multi-modal LLMs, our
results reveal that even the best-performing models like GPT-4o and Gemini Pro
find it challenging to solve MuirBench, achieving 68.0% and 49.3% in accuracy.
Open-source multimodal LLMs trained on single images can hardly generalize to
multi-image questions, hovering below 33.3% in accuracy. These results
highlight the importance of MuirBench in encouraging the community to develop
multimodal LLMs that can look beyond a single image, suggesting potential
pathways for future improvements.
]]></content:encoded>
<pubDate>2024-06-13T17:59:52Z</pubDate>
</item>
<item>
<title>Words Worth a Thousand Pictures: Measuring and Understanding Perceptual
  Variability in Text-to-Image Generation</title>
<link>http://arxiv.org/abs/2406.08482v1</link>
<guid>http://arxiv.org/abs/2406.08482v1</guid>
<content:encoded><![CDATA[
<div> 黑盒扩散模型、图像可变性、W1KP、提示重用、语言特征分析<br />
扩散模型是文本到图像生成的最新技术，但其感知可变性尚未得到充分研究。本文研究了提示如何影响基于黑盒扩散模型的图像可变性。我们提出了W1KP，这是一种经过人类校准的图像可变性度量，可以从现有的图像对感知距离中提取。由于当前数据集不包括最新的扩散模型，因此我们为评估而策划了三个测试集。我们的最佳感知距离在准确性方面比九个基线模型提高了高达18个百分点，我们的校准与人类的判断相匹配的概率达到了78%。利用W1KP，我们研究了提示的重复使用，并表明Imagen提示可以在生成的图像变得与已生成的图像太相似之前重复使用10-50次，而Stable Diffusion XL和DALL-E 3可以重复使用50-200次。最后，我们分析了真实提示的56个语言特征，发现提示的长度、CLIP嵌入范数、具体性和词义对可变性影响最大。据我们所知，我们是首批从视觉语言学角度分析扩散可变性的研究者。我们的项目页面位于http://w1kp.com。<br /><br />总结: 本文研究了扩散模型的图像可变性，提出了W1KP度量及其重要性，以及对提示重复使用和语言特征分析的结果进行了详细研究，为扩散模型的进一步发展提出了有益的见解。 <div>
Diffusion models are the state of the art in text-to-image generation, but
their perceptual variability remains understudied. In this paper, we examine
how prompts affect image variability in black-box diffusion-based models. We
propose W1KP, a human-calibrated measure of variability in a set of images,
bootstrapped from existing image-pair perceptual distances. Current datasets do
not cover recent diffusion models, thus we curate three test sets for
evaluation. Our best perceptual distance outperforms nine baselines by up to 18
points in accuracy, and our calibration matches graded human judgements 78% of
the time. Using W1KP, we study prompt reusability and show that Imagen prompts
can be reused for 10-50 random seeds before new images become too similar to
already generated images, while Stable Diffusion XL and DALL-E 3 can be reused
50-200 times. Lastly, we analyze 56 linguistic features of real prompts,
finding that the prompt's length, CLIP embedding norm, concreteness, and word
senses influence variability most. As far as we are aware, we are the first to
analyze diffusion variability from a visuolinguistic perspective. Our project
page is at http://w1kp.com
]]></content:encoded>
<pubDate>2024-06-12T17:59:27Z</pubDate>
</item>
<item>
<title>What If We Recaption Billions of Web Images with LLaMA-3?</title>
<link>http://arxiv.org/abs/2406.08478v1</link>
<guid>http://arxiv.org/abs/2406.08478v1</guid>
<content:encoded><![CDATA[
<div> LLaMA-3, 图像-文本对, 数据增强, 生成模型, 判别模型
<br /> 
本文介绍了如何利用开源的LLaMA-3进行图像-文本对的数据增强，并且在数据集Recap-DataComp-1B上进行了实验。实验证明，经过增强的数据集可以显著提升训练先进的视觉-语言模型，在判别模型（如CLIP）中表现出增强的零-shot性能，在生成模型（如文本到图像Diffusion Transformers）中生成的图像与用户的文本指令更加吻合，特别是在处理复杂查询时。 通过这篇文章，我们可以了解到数据增强对于提升视觉-语言模型的作用，以及LLaMA-3的重要性和应用价值。
<br /><br />总结: 
数据增强的重要性和作用、LLaMA-3的应用、Recap-DataComp-1B数据集的实验效果、判别模型和生成模型的表现提升、复杂查询的处理。 <div>
Web-crawled image-text pairs are inherently noisy. Prior studies demonstrate
that semantically aligning and enriching textual descriptions of these pairs
can significantly enhance model training across various vision-language tasks,
particularly text-to-image generation. However, large-scale investigations in
this area remain predominantly closed-source. Our paper aims to bridge this
community effort, leveraging the powerful and \textit{open-sourced} LLaMA-3, a
GPT-4 level LLM. Our recaptioning pipeline is simple: first, we fine-tune a
LLaMA-3-8B powered LLaVA-1.5 and then employ it to recaption 1.3 billion images
from the DataComp-1B dataset. Our empirical results confirm that this enhanced
dataset, Recap-DataComp-1B, offers substantial benefits in training advanced
vision-language models. For discriminative models like CLIP, we observe
enhanced zero-shot performance in cross-modal retrieval tasks. For generative
models like text-to-image Diffusion Transformers, the generated images exhibit
a significant improvement in alignment with users' text instructions,
especially in following complex queries. Our project page is
https://www.haqtu.me/Recap-Datacomp-1B/
]]></content:encoded>
<pubDate>2024-06-12T17:59:07Z</pubDate>
</item>
<item>
<title>Commonsense-T2I Challenge: Can Text-to-Image Generation Models
  Understand Commonsense?</title>
<link>http://arxiv.org/abs/2406.07546v1</link>
<guid>http://arxiv.org/abs/2406.07546v1</guid>
<content:encoded><![CDATA[
<div> 文本-图像生成；常识推理；评估基准；挑战性数据集；模型分析<br />
<br />
挑战性的Commonsense-T2I数据集是一个新颖的任务和基准，用于评估文本-图像生成模型产生符合现实生活常识的能力。数据集由专家手动筛选和标注，提供对模型行为的细粒度分析。研究发现，即使是DALL-E 3模型在Commonsense-T2I上也只能达到48.92%的准确率，而稳定的Diffusion XL模型仅实现24.92%的准确率。实验结果表明，GPT丰富的提示无法解决这一挑战，并进行了详细的分析。Commonsense-T2I旨在成为T2I常识检查的高质量评估基准，促进现实生活图像生成的发展。<br /><br />总结: 挑战性的Commonsense-T2I数据集是一个评估文本-图像生成模型的新基准，旨在促进现实生活图像生成的进步。研究发现目前的模型在该数据集上仍存在较大差距，即使是最先进的模型也只能实现有限的准确率。对于GPT-enriched提示也无法解决这一挑战，需要进行更深入的分析和改进。 <div>
We present a novel task and benchmark for evaluating the ability of
text-to-image(T2I) generation models to produce images that fit commonsense in
real life, which we call Commonsense-T2I. Given two adversarial text prompts
containing an identical set of action words with minor differences, such as "a
lightbulb without electricity" v.s. "a lightbulb with electricity", we evaluate
whether T2I models can conduct visual-commonsense reasoning, e.g. produce
images that fit "the lightbulb is unlit" vs. "the lightbulb is lit"
correspondingly. Commonsense-T2I presents an adversarial challenge, providing
pairwise text prompts along with expected outputs. The dataset is carefully
hand-curated by experts and annotated with fine-grained labels, such as
commonsense type and likelihood of the expected outputs, to assist analyzing
model behavior. We benchmark a variety of state-of-the-art (sota) T2I models
and surprisingly find that, there is still a large gap between image synthesis
and real life photos--even the DALL-E 3 model could only achieve 48.92% on
Commonsense-T2I, and the stable diffusion XL model only achieves 24.92%
accuracy. Our experiments show that GPT-enriched prompts cannot solve this
challenge, and we include a detailed analysis about possible reasons for such
deficiency. We aim for Commonsense-T2I to serve as a high-quality evaluation
benchmark for T2I commonsense checking, fostering advancements in real life
image generation.
]]></content:encoded>
<pubDate>2024-06-11T17:59:48Z</pubDate>
</item>
<item>
<title>Situational Awareness Matters in 3D Vision Language Reasoning</title>
<link>http://arxiv.org/abs/2406.07544v1</link>
<guid>http://arxiv.org/abs/2406.07544v1</guid>
<content:encoded><![CDATA[
<div> 3D vision language reasoning, household robots, embodied AI, SIG3D, situational awareness<br />
<br />
该研究展示了在3D空间内进行复杂的视觉语言推理任务对于开发家庭机器人和以人为中心的智能AI具有重要意义。作者提出了SIG3D模型，该模型能够自主地在基于语言提示的情境中确定自身位置，并以此为基础回答开放性问题。通过在SQA3D和ScanQA数据集上的实验，发现SIG3D在情境估计和问题回答方面优于现有模型，情境估计准确率提升超过30%。进一步分析验证了模型设计的合理性，探究了视觉和文本标记的不同功能，并突出了在3D问题回答领域中情境意识的重要性。<br /><br />总结: 该研究围绕着3D视觉语言推理展开，提出了SIG3D模型以解决情境感知和问题回答的挑战。实验证明SIG3D在这两方面表现优异，突显了情境意识在3D问题回答中的重要性。 <div>
Being able to carry out complicated vision language reasoning tasks in 3D
space represents a significant milestone in developing household robots and
human-centered embodied AI. In this work, we demonstrate that a critical and
distinct challenge in 3D vision language reasoning is situational awareness,
which incorporates two key components: (1) The autonomous agent grounds its
self-location based on a language prompt. (2) The agent answers open-ended
questions from the perspective of its calculated position. To address this
challenge, we introduce SIG3D, an end-to-end Situation-Grounded model for 3D
vision language reasoning. We tokenize the 3D scene into sparse voxel
representation and propose a language-grounded situation estimator, followed by
a situated question answering module. Experiments on the SQA3D and ScanQA
datasets show that SIG3D outperforms state-of-the-art models in situation
estimation and question answering by a large margin (e.g., an enhancement of
over 30% on situation estimation accuracy). Subsequent analysis corroborates
our architectural design choices, explores the distinct functions of visual and
textual tokens, and highlights the importance of situational awareness in the
domain of 3D question answering.
]]></content:encoded>
<pubDate>2024-06-11T17:59:45Z</pubDate>
</item>
<item>
<title>Cognitive Insights Across Languages: Enhancing Multimodal Interview
  Analysis</title>
<link>http://arxiv.org/abs/2406.07542v1</link>
<guid>http://arxiv.org/abs/2406.07542v1</guid>
<content:encoded><![CDATA[
Cognitive decline is a natural process that occurs as individuals age. Early
diagnosis of anomalous decline is crucial for initiating professional treatment
that can enhance the quality of life of those affected. To address this issue,
we propose a multimodal model capable of predicting Mild Cognitive Impairment
and cognitive scores. The TAUKADIAL dataset is used to conduct the evaluation,
which comprises audio recordings of clinical interviews. The proposed model
demonstrates the ability to transcribe and differentiate between languages used
in the interviews. Subsequently, the model extracts audio and text features,
combining them into a multimodal architecture to achieve robust and generalized
results. Our approach involves in-depth research to implement various features
obtained from the proposed modalities.
]]></content:encoded>
<pubDate>2024-06-11T17:59:31Z</pubDate>
</item>
<item>
<title>GaussianCity: Generative Gaussian Splatting for Unbounded 3D City
  Generation</title>
<link>http://arxiv.org/abs/2406.06526v1</link>
<guid>http://arxiv.org/abs/2406.06526v1</guid>
<content:encoded><![CDATA[
<div> GaussianCity, 3D city generation, NeRF, 3D-GS, unbounded 3D scenes

GaussianCity是一种用于高效合成无限规模3D城市的生成高斯分布点框架。文章提出了两个关键见解：1）紧凑的3D场景表示：引入BEV-Point作为高度紧凑的中间表示，确保对于无限场景的VRAM使用量保持恒定，从而实现无限城市的生成。2）空间感知的高斯属性解码器：提出了空间感知的BEV-Point解码器，以产生3D高斯属性，利用Point Serializer集成BEV点的结构和上下文特征。大量实验证明，GaussianCity在俯视和街景3D城市生成方面取得了最先进的结果。值得注意的是，与CityDreamer相比，GaussianCity表现出60倍的速度提升（10.72 FPS对0.18 FPS）。<br /><br />总结: GaussianCity 提出了一种用于高效生成无限规模3D城市的高斯分布点框架，具有紧凑的3D场景表示和空间感知的高斯属性解码器。在实验中表现出了优越的性能。 <div>
3D city generation with NeRF-based methods shows promising generation results
but is computationally inefficient. Recently 3D Gaussian Splatting (3D-GS) has
emerged as a highly efficient alternative for object-level 3D generation.
However, adapting 3D-GS from finite-scale 3D objects and humans to
infinite-scale 3D cities is non-trivial. Unbounded 3D city generation entails
significant storage overhead (out-of-memory issues), arising from the need to
expand points to billions, often demanding hundreds of Gigabytes of VRAM for a
city scene spanning 10km^2. In this paper, we propose GaussianCity, a
generative Gaussian Splatting framework dedicated to efficiently synthesizing
unbounded 3D cities with a single feed-forward pass. Our key insights are
two-fold: 1) Compact 3D Scene Representation: We introduce BEV-Point as a
highly compact intermediate representation, ensuring that the growth in VRAM
usage for unbounded scenes remains constant, thus enabling unbounded city
generation. 2) Spatial-aware Gaussian Attribute Decoder: We present
spatial-aware BEV-Point decoder to produce 3D Gaussian attributes, which
leverages Point Serializer to integrate the structural and contextual
characteristics of BEV points. Extensive experiments demonstrate that
GaussianCity achieves state-of-the-art results in both drone-view and
street-view 3D city generation. Notably, compared to CityDreamer, GaussianCity
exhibits superior performance with a speedup of 60 times (10.72 FPS v.s. 0.18
FPS).
]]></content:encoded>
<pubDate>2024-06-10T17:59:55Z</pubDate>
</item>
<item>
<title>Autoregressive Model Beats Diffusion: Llama for Scalable Image
  Generation</title>
<link>http://arxiv.org/abs/2406.06525v1</link>
<guid>http://arxiv.org/abs/2406.06525v1</guid>
<content:encoded><![CDATA[
<div> 关键词: LlamaGen, 图像生成模型, 自回归模型, 缩放, 训练数据<br />
总结: <br />本文介绍了LlamaGen，这是一种新的图像生成模型，将大型语言模型的原始“下一个标记预测”范式应用于视觉生成领域。作者发现，即使没有对视觉信号进行归纳偏差，例如Llama这样的纯自回归模型，只要适当地进行缩放，也可以实现最先进的图像生成性能。他们重新审视了图像标记器的设计空间、图像生成模型的可扩展性属性以及它们的训练数据质量。在这次探索中，他们取得了一系列成果：（1）一种图像标记器，其下采样比为16，重构质量为0.94 rFID，ImageNet基准上的代码本使用率为97%；（2）一系列有条件类别的图像生成模型，参数范围从1.11亿到31亿，在ImageNet 256x256基准测试上实现了2.18 FID，优于流行的扩散模型，如LDM、DiT；（3）一种文本有条件的图像生成模型，参数为7.75亿，经过LAION-COCO和高美学质量图像的两阶段训练，展现出了与文本对齐的竞争性视觉质量。他们验证了LLM服务框架在优化图像生成模型推理速度方面的有效性，并实现了326% - 414%的加速。他们发布了所有模型和代码，以促进视觉生成和多模式基础模型的开源社区。 <div>
We introduce LlamaGen, a new family of image generation models that apply
original ``next-token prediction'' paradigm of large language models to visual
generation domain. It is an affirmative answer to whether vanilla
autoregressive models, e.g., Llama, without inductive biases on visual signals
can achieve state-of-the-art image generation performance if scaling properly.
We reexamine design spaces of image tokenizers, scalability properties of image
generation models, and their training data quality. The outcome of this
exploration consists of: (1) An image tokenizer with downsample ratio of 16,
reconstruction quality of 0.94 rFID and codebook usage of 97% on ImageNet
benchmark. (2) A series of class-conditional image generation models ranging
from 111M to 3.1B parameters, achieving 2.18 FID on ImageNet 256x256
benchmarks, outperforming the popular diffusion models such as LDM, DiT. (3) A
text-conditional image generation model with 775M parameters, from two-stage
training on LAION-COCO and high aesthetics quality images, demonstrating
competitive performance of visual quality and text alignment. (4) We verify the
effectiveness of LLM serving frameworks in optimizing the inference speed of
image generation models and achieve 326% - 414% speedup. We release all models
and codes to facilitate open-source community of visual generation and
multimodal foundation models.
]]></content:encoded>
<pubDate>2024-06-10T17:59:52Z</pubDate>
</item>
<item>
<title>3D-GRAND: Towards Better Grounding and Less Hallucination for 3D-LLMs</title>
<link>http://arxiv.org/abs/2406.05132v1</link>
<guid>http://arxiv.org/abs/2406.05132v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 语言理解, 3D感知, 大规模数据集, 3D-LLMs, 综合评估<br />
总结:<br />
这篇论文讨论了语言和3D感知的整合对于发展具有身体感知能力并能理解和互动物理世界的智能体和机器人的重要性。作者介绍了一个包含大规模家庭场景和密集语言-场景指令的数据集3D-GRAND，结果显示使用这个数据集可以显著提高3D-LLMs的语境能力，并减少幻觉。作者还提出了一个综合评估标准3D-POPE来系统评估3D-LLMs的幻觉情况，结果表明数据集大小和3D-LLM性能之间存在着扩展效应。最后，作者还展示了大规模合成数据训练的模型在真实3D扫描中表现良好的初步信号。通过3D-GRAND和3D-POPE，作者旨在为身体感知人工智能社区提供必要的资源和见解，为更可靠、更基础的3D-LLMs铺平道路。 <div>
The integration of language and 3D perception is crucial for developing
embodied agents and robots that comprehend and interact with the physical
world. While large language models (LLMs) have demonstrated impressive language
understanding and generation capabilities, their adaptation to 3D environments
(3D-LLMs) remains in its early stages. A primary challenge is the absence of
large-scale datasets that provide dense grounding between language and 3D
scenes. In this paper, we introduce 3D-GRAND, a pioneering large-scale dataset
comprising 40,087 household scenes paired with 6.2 million densely-grounded
scene-language instructions. Our results show that instruction tuning with
3D-GRAND significantly enhances grounding capabilities and reduces
hallucinations in 3D-LLMs. As part of our contributions, we propose a
comprehensive benchmark 3D-POPE to systematically evaluate hallucination in
3D-LLMs, enabling fair comparisons among future models. Our experiments
highlight a scaling effect between dataset size and 3D-LLM performance,
emphasizing the critical role of large-scale 3D-text datasets in advancing
embodied AI research. Notably, our results demonstrate early signals for
effective sim-to-real transfer, indicating that models trained on large
synthetic data can perform well on real-world 3D scans. Through 3D-GRAND and
3D-POPE, we aim to equip the embodied AI community with essential resources and
insights, setting the stage for more reliable and better-grounded 3D-LLMs.
Project website: https://3d-grand.github.io
]]></content:encoded>
<pubDate>2024-06-07T17:59:59Z</pubDate>
</item>
<item>
<title>An Empirical Study on Parameter-Efficient Fine-Tuning for MultiModal
  Large Language Models</title>
<link>http://arxiv.org/abs/2406.05130v1</link>
<guid>http://arxiv.org/abs/2406.05130v1</guid>
<content:encoded><![CDATA[
<div> 参数-efficient fine-tuning, MLLM, PEFT methods, multimodal instruction datasets, 模型性能

参数-efficient fine-tuning（PEFT）方法是为了解决大型语言模型（MLLMs）参数过多的问题，通过对开源MLLMs的LLM组件进行实证研究，比较了四种流行的PEFT方法在多个方面的影响：对各种模型的影响、参数和PEFT模块位置的影响、微调数据的规模对模型性能的影响、PEFT方法对模型稳定性、MLLM的泛化能力以及错觉现象。研究发现，在各种实验中，adapter是表现最佳的PEFT方法，同时微调连接器层可以提高大多数MLLM的性能。总结：PEFT方法对MLLM的性能影响显著，adapter方法表现最佳，微调连接器层也能改善模型性能。 <div>
Multimodal large language models (MLLMs) fine-tuned with multimodal
instruction datasets have demonstrated remarkable capabilities in multimodal
tasks. However, fine-tuning all parameters of MLLMs has become challenging as
they usually contain billions of parameters. To address this issue, we study
parameter-efficient fine-tuning (PEFT) methods for MLLMs. We aim to identify
effective methods for enhancing the performance of MLLMs in scenarios where
only a limited number of parameters are trained. This paper conducts empirical
studies using four popular PEFT methods to fine-tune the LLM component of
open-source MLLMs. We present a comprehensive analysis that encompasses various
aspects, including the impact of PEFT methods on various models, parameters and
location of the PEFT module, size of fine-tuning data, model stability based on
PEFT methods, MLLM's generalization, and hallucination. We evaluated four PEFT
methods on seven datasets from two different categories: unseen and seen
datasets. Across all experiments, we show that the adapter is the
best-performing PEFT method. At the same time, fine-tuning the connector layers
leads to improved performance in most MLLMs. Code and data are available at
https://github.com/alenai97/PEFT-MLLM.git.
]]></content:encoded>
<pubDate>2024-06-07T17:58:11Z</pubDate>
</item>
<item>
<title>Physics3D: Learning Physical Properties of 3D Gaussians via Video
  Diffusion</title>
<link>http://arxiv.org/abs/2406.04338v2</link>
<guid>http://arxiv.org/abs/2406.04338v2</guid>
<content:encoded><![CDATA[
<div> 物理3D，生成模型，物理特性，视频扩散模型，材料模拟

<br /><br />总结: 
近年来，3D生成模型得到了快速发展，为模拟3D物体的动态运动和自定义行为等应用打开了新的可能性。然而，当前的3D生成模型往往只关注颜色和形状等表面特征，忽略了真实世界中影响物体行为的固有物理特性。为了准确模拟与物理对齐的动力学特性，预测物体的物理特性并将其纳入行为预测过程是至关重要的。本文提出了一种名为Physics3D的新方法，通过视频扩散模型学习3D物体的各种物理特性。我们的方法涉及设计一个基于粘弹性材料模型的高度通用的物理模拟系统，使我们能够以高保真度模拟各种材料。此外，我们从视频扩散模型中提取了物理先验知识，更加了解真实物体材料的特性。大量实验证明了我们的方法在弹性和塑性材料方面的有效性。Physics3D显示出了在虚拟神经空间和物理世界之间弥合差距的巨大潜力，提供了在虚拟环境中更好地整合和应用真实物理原理的可能性。 <div>
In recent years, there has been rapid development in 3D generation models,
opening up new possibilities for applications such as simulating the dynamic
movements of 3D objects and customizing their behaviors. However, current 3D
generative models tend to focus only on surface features such as color and
shape, neglecting the inherent physical properties that govern the behavior of
objects in the real world. To accurately simulate physics-aligned dynamics, it
is essential to predict the physical properties of materials and incorporate
them into the behavior prediction process. Nonetheless, predicting the diverse
materials of real-world objects is still challenging due to the complex nature
of their physical attributes. In this paper, we propose \textbf{Physics3D}, a
novel method for learning various physical properties of 3D objects through a
video diffusion model. Our approach involves designing a highly generalizable
physical simulation system based on a viscoelastic material model, which
enables us to simulate a wide range of materials with high-fidelity
capabilities. Moreover, we distill the physical priors from a video diffusion
model that contains more understanding of realistic object materials. Extensive
experiments demonstrate the effectiveness of our method with both elastic and
plastic materials. Physics3D shows great potential for bridging the gap between
the physical world and virtual neural space, providing a better integration and
application of realistic physical principles in virtual environments. Project
page: https://liuff19.github.io/Physics3D.
]]></content:encoded>
<pubDate>2024-06-07T01:30:11Z</pubDate>
</item>
<item>
<title>Physics3D: Learning Physical Properties of 3D Gaussians via Video
  Diffusion</title>
<link>http://arxiv.org/abs/2406.04338v1</link>
<guid>http://arxiv.org/abs/2406.04338v1</guid>
<content:encoded><![CDATA[
<div> 物理属性，3D对象，视频扩散模型，物理仿真，材料模型<br />
总结:<br />
近年来，3D生成模型发展迅速，但目前的模型偏重于表面特征，忽视了真实世界中物体行为的内在物理特性。为了准确模拟物理对齐的动态行为，有必要预测材料的物理属性并将其纳入行为预测过程。本文提出了一种通过视频扩散模型学习3D对象各种物理属性的新方法，该方法基于粘弹性材料模型设计了高度通用的物理仿真系统，能够模拟广泛的材料，并利用视频扩散模型中的物理先验知识。大量实验证明了我们方法的有效性，展示了Physics3D在弹性和塑性材料方面的巨大潜力，有助于弥合物理世界与虚拟神经空间之间的差距，提供更好的虚拟环境中真实物理原则的整合和应用。 <div>
In recent years, there has been rapid development in 3D generation models,
opening up new possibilities for applications such as simulating the dynamic
movements of 3D objects and customizing their behaviors. However, current 3D
generative models tend to focus only on surface features such as color and
shape, neglecting the inherent physical properties that govern the behavior of
objects in the real world. To accurately simulate physics-aligned dynamics, it
is essential to predict the physical properties of materials and incorporate
them into the behavior prediction process. Nonetheless, predicting the diverse
materials of real-world objects is still challenging due to the complex nature
of their physical attributes. In this paper, we propose \textbf{Physics3D}, a
novel method for learning various physical properties of 3D objects through a
video diffusion model. Our approach involves designing a highly generalizable
physical simulation system based on a viscoelastic material model, which
enables us to simulate a wide range of materials with high-fidelity
capabilities. Moreover, we distill the physical priors from a video diffusion
model that contains more understanding of realistic object materials. Extensive
experiments demonstrate the effectiveness of our method with both elastic and
plastic materials. Physics3D shows great potential for bridging the gap between
the physical world and virtual neural space, providing a better integration and
application of realistic physical principles in virtual environments. Project
page: https://liuff19.github.io/Physics3D.
]]></content:encoded>
<pubDate>2024-06-06T17:59:47Z</pubDate>
</item>
<item>
<title>Coherent Zero-Shot Visual Instruction Generation</title>
<link>http://arxiv.org/abs/2406.04337v1</link>
<guid>http://arxiv.org/abs/2406.04337v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 文本到图像合成, 扩散模型, 图像生成, 文本理解, 大型语言模型

扩散模型和大型语言模型的进展使得文本到图像合成取得了巨大进展，但生成需要保持一致性的视觉说明仍然是一个巨大挑战。本文引入了一个简单的、无需训练的框架来解决这些问题，利用了扩散模型和大型语言模型的进步。我们的方法系统地整合了文本理解和图像生成，以确保视觉说明在整个指导序列中既具有视觉吸引力，又保持一致性和准确性。我们通过测试多步说明并与几个基准进行文本对齐和一致性比较来验证其有效性。实验结果表明，我们的方法能够可视化一致和视觉上令人愉悦的说明。<br /><br />总结: 本文介绍了一个简单的、无需训练的框架，利用了扩散模型和大型语言模型的进步，系统地整合了文本理解和图像生成，解决了文本到图像合成中的一致性和准确性问题。 <div>
Despite the advances in text-to-image synthesis, particularly with diffusion
models, generating visual instructions that require consistent representation
and smooth state transitions of objects across sequential steps remains a
formidable challenge. This paper introduces a simple, training-free framework
to tackle the issues, capitalizing on the advancements in diffusion models and
large language models (LLMs). Our approach systematically integrates text
comprehension and image generation to ensure visual instructions are visually
appealing and maintain consistency and accuracy throughout the instruction
sequence. We validate the effectiveness by testing multi-step instructions and
comparing the text alignment and consistency with several baselines. Our
experiments show that our approach can visualize coherent and visually pleasing
instructions
]]></content:encoded>
<pubDate>2024-06-06T17:59:44Z</pubDate>
</item>
<item>
<title>Wings: Learning Multimodal LLMs without Text-only Forgetting</title>
<link>http://arxiv.org/abs/2406.03496v1</link>
<guid>http://arxiv.org/abs/2406.03496v1</guid>
<content:encoded><![CDATA[
<div> Wings, MLLMs, attention, text-only forgetting, multimodal comprehension<br />
<br />
Wings是一种新型的MLLM，它在文本对话和多模态理解方面表现出色。通过分析MLLM在多模态指令中的注意力，我们发现文本遗忘与注意力从图像前到图像后的转移有关。因此，我们设计了额外的模块作为增强学习器，以补偿注意力转移。视觉和文本学习者在每一层的注意力模块内并行连接，以平衡对视觉元素的关注，并在后续阶段通过基于注意力的路由与文本学习者合作整合输出。我们设计了低秩残差注意力（LoRRA）来保证学习效率。实验证明，Wings在纯文本和视觉问答任务中均优于同等规模的MLLM。在新建的交错式图像文本（IIT）基准测试上，Wings在纯文本丰富和多模态丰富问答任务中表现出卓越的性能。<br /><br />总结:Wings是一种新型的MLLM，它在文本对话和多模态理解方面表现出色。同时，通过对MLLM在多模态指令中的注意力分析，发现了文本遗忘与注意力转移的关联，从而设计了补偿模块。Wings在实验中表现出比同等规模的MLLM更优异的性能，在纯文本丰富和多模态丰富问答任务中均表现出色。 <div>
Multimodal large language models (MLLMs), initiated with a trained LLM, first
align images with text and then fine-tune on multimodal mixed inputs. However,
the MLLM catastrophically forgets the text-only instructions, which do not
include images and can be addressed within the initial LLM. In this paper, we
present Wings, a novel MLLM that excels in both text-only dialogues and
multimodal comprehension. Analyzing MLLM attention in multimodal instructions
reveals that text-only forgetting is related to the attention shifts from
pre-image to post-image text. From that, we construct extra modules that act as
the boosted learner to compensate for the attention shift. The complementary
visual and textual learners, like "wings" on either side, are connected in
parallel within each layer's attention block. Initially, image and text inputs
are aligned with visual learners operating alongside the main attention,
balancing focus on visual elements. Textual learners are later collaboratively
integrated with attention-based routing to blend the outputs of the visual and
textual learners. We design the Low-Rank Residual Attention (LoRRA) to
guarantee high efficiency for learners. Our experimental results demonstrate
that Wings outperforms equally-scaled MLLMs in both text-only and visual
question-answering tasks. On a newly constructed Interleaved Image-Text (IIT)
benchmark, Wings exhibits superior performance from text-only-rich to
multimodal-rich question-answering tasks.
]]></content:encoded>
<pubDate>2024-06-05T17:59:40Z</pubDate>
</item>
<item>
<title>Leveraging Visual Tokens for Extended Text Contexts in Multi-Modal
  Learning</title>
<link>http://arxiv.org/abs/2406.02547v1</link>
<guid>http://arxiv.org/abs/2406.02547v1</guid>
<content:encoded><![CDATA[
<div> 多模态模型，训练，长上下文长度，GPU内存，计算成本<br />
探索性研究，视觉化上下文文本处理，长文本处理，FLOPs，下游基准性能<br />
VisInContext方法显著减少了GPU内存使用和FLOPs<br />
研究表明，VisInContext训练的模型在下游基准测试中表现优异<br />
VisInContext方法对增加上下文文本长度和增强文档理解能力有潜力

<br /><br />总结:
本研究探索了一种新的多模态大语言模型中增加长上下文文本长度的方法——视觉化上下文文本处理（VisInContext）。该方法显著降低了GPU内存使用和浮点运算（FLOPs），使得在训练和推断阶段，可以将预训练的上下文文本长度从256扩展到2048个标记，而几乎保持相同的FLOPs。实验结果表明，采用VisInContext训练的模型在常见的下游基准测试中表现优异，还显示出在增加上下文文本长度和增强文档理解能力方面有巨大潜力。 <div>
Training models with longer in-context lengths is a significant challenge for
multimodal model due to substantial GPU memory and computational costs. This
exploratory study does not present state-of-the-art models; rather, it
introduces an innovative method designed to increase in-context text length in
multi-modality large language models (MLLMs) efficiently. We present Visualized
In-Context Text Processing (VisInContext), which processes long in-context text
using visual tokens. This technique significantly reduces GPU memory usage and
floating point operations (FLOPs) for both training and inferenceing stage. For
instance, our method expands the pre-training in-context text length from 256
to 2048 tokens with nearly same FLOPs for a 56 billion parameter MOE model.
Experimental results demonstrate that model trained with VisInContext delivers
superior performance on common downstream benchmarks for in-context few-shot
evaluation. Additionally, VisInContext is complementary to existing methods for
increasing in-context text length and enhances document understanding
capabilities, showing great potential in document QA tasks and sequential
document retrieval.
]]></content:encoded>
<pubDate>2024-06-04T17:59:25Z</pubDate>
</item>
<item>
<title>Robust and highly scalable estimation of directional couplings from
  time-shifted signals</title>
<link>http://arxiv.org/abs/2406.02545v1</link>
<guid>http://arxiv.org/abs/2406.02545v1</guid>
<content:encoded><![CDATA[
<div> 网络、方向耦合、延迟、变分贝叶斯、测量参数

这篇论文介绍了如何通过变分贝叶斯框架来估计网络节点之间的方向耦合，解决了因测量中可能存在未知延迟而导致问题不适定的情况。通过边际化延迟的不确定性，得到了保守的耦合估计。为了克服传统变分方法的过于自信的问题，采用了混合变分推断方案，其中测量参数的后验分布使用前向KL loss进行估计，而耦合的条件后验分布则使用高度可扩展的基于梯度的变分推断方法进行估计。在对实际数据进行的实验证实中，表明该网络能够提供可靠且保守的耦合估计，远远优于回归DCM等类似方法。 <br /><br />总结: 网络节点间的方向耦合估计在存在未知延迟的情况下是一个挑战，本文提出了使用变分贝叶斯框架来边际化延迟的不确定性以获得保守的耦合估计。通过混合变分推断方案克服了传统方法的问题，在实验中展示了该方法的可靠性和优越性。 <div>
The estimation of directed couplings between the nodes of a network from
indirect measurements is a central methodological challenge in scientific
fields such as neuroscience, systems biology and economics. Unfortunately, the
problem is generally ill-posed due to the possible presence of unknown delays
in the measurements. In this paper, we offer a solution of this problem by
using a variational Bayes framework, where the uncertainty over the delays is
marginalized in order to obtain conservative coupling estimates. To overcome
the well-known overconfidence of classical variational methods, we use a
hybrid-VI scheme where the (possibly flat or multimodal) posterior over the
measurement parameters is estimated using a forward KL loss while the (nearly
convex) conditional posterior over the couplings is estimated using the highly
scalable gradient-based VI. In our ground-truth experiments, we show that the
network provides reliable and conservative estimates of the couplings, greatly
outperforming similar methods such as regression DCM.
]]></content:encoded>
<pubDate>2024-06-04T17:58:33Z</pubDate>
</item>
<item>
<title>ViDiT-Q: Efficient and Accurate Quantization of Diffusion Transformers
  for Image and Video Generation</title>
<link>http://arxiv.org/abs/2406.02540v1</link>
<guid>http://arxiv.org/abs/2406.02540v1</guid>
<content:encoded><![CDATA[
Diffusion transformers (DiTs) have exhibited remarkable performance in visual
generation tasks, such as generating realistic images or videos based on
textual instructions. However, larger model sizes and multi-frame processing
for video generation lead to increased computational and memory costs, posing
challenges for practical deployment on edge devices. Post-Training Quantization
(PTQ) is an effective method for reducing memory costs and computational
complexity. When quantizing diffusion transformers, we find that applying
existing diffusion quantization methods designed for U-Net faces challenges in
preserving quality. After analyzing the major challenges for quantizing
diffusion transformers, we design an improved quantization scheme: "ViDiT-Q":
Video and Image Diffusion Transformer Quantization) to address these issues.
Furthermore, we identify highly sensitive layers and timesteps hinder
quantization for lower bit-widths. To tackle this, we improve ViDiT-Q with a
novel metric-decoupled mixed-precision quantization method (ViDiT-Q-MP). We
validate the effectiveness of ViDiT-Q across a variety of text-to-image and
video models. While baseline quantization methods fail at W8A8 and produce
unreadable content at W4A8, ViDiT-Q achieves lossless W8A8 quantization.
ViDiTQ-MP achieves W4A8 with negligible visual quality degradation, resulting
in a 2.5x memory optimization and a 1.5x latency speedup.
]]></content:encoded>
<pubDate>2024-06-04T17:57:10Z</pubDate>
</item>
<item>
<title>RapVerse: Coherent Vocals and Whole-Body Motions Generations from Text</title>
<link>http://arxiv.org/abs/2405.20336v1</link>
<guid>http://arxiv.org/abs/2405.20336v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 3D holistic body motions, singing vocals, RapVerse dataset, autoregressive multimodal transformers, vector-quantized variational autoencoder

生成音频和人体运动的统一模型，使用RapVerse数据集进行研究，在语言、音频和动作之间进行变换。通过变换编码整体运动序列和音频，确保音频和人体运动的无缝和真实混合。实验证明，该统一生成框架不仅可以直接从文本输入中产生连贯和逼真的歌唱声音和人体运动，而且可以与专门的单模态生成系统相匹敌，为联合声音-运动生成建立了新的基准。 项目页面可用于研究目的，网址为https://vis-www.cs.umass.edu/RapVerse。<br /><br />总结: 通过使用RapVerse数据集，这项工作介绍了一个新的挑战性任务，即从文本歌词直接生成3D整体人体动作和歌唱声音。研究人员使用变换编码模型统一处理语言、音频和动作，并通过实验证明了该框架的有效性和性能。 <div>
In this work, we introduce a challenging task for simultaneously generating
3D holistic body motions and singing vocals directly from textual lyrics
inputs, advancing beyond existing works that typically address these two
modalities in isolation. To facilitate this, we first collect the RapVerse
dataset, a large dataset containing synchronous rapping vocals, lyrics, and
high-quality 3D holistic body meshes. With the RapVerse dataset, we investigate
the extent to which scaling autoregressive multimodal transformers across
language, audio, and motion can enhance the coherent and realistic generation
of vocals and whole-body human motions. For modality unification, a
vector-quantized variational autoencoder is employed to encode whole-body
motion sequences into discrete motion tokens, while a vocal-to-unit model is
leveraged to obtain quantized audio tokens preserving content, prosodic
information, and singer identity. By jointly performing transformer modeling on
these three modalities in a unified way, our framework ensures a seamless and
realistic blend of vocals and human motions. Extensive experiments demonstrate
that our unified generation framework not only produces coherent and realistic
singing vocals alongside human motions directly from textual inputs but also
rivals the performance of specialized single-modality generation systems,
establishing new benchmarks for joint vocal-motion generation. The project page
is available for research purposes at https://vis-www.cs.umass.edu/RapVerse.
]]></content:encoded>
<pubDate>2024-05-30T17:59:39Z</pubDate>
</item>
<item>
<title>LLMs Meet Multimodal Generation and Editing: A Survey</title>
<link>http://arxiv.org/abs/2405.19334v1</link>
<guid>http://arxiv.org/abs/2405.19334v1</guid>
<content:encoded><![CDATA[
<div> 大型语言模型，多模态学习，生成模型，技术组件，人机交互<br />
本篇文章对多模态生成进行了系统性综述，重点在不同领域（包括图像、视频、3D和音频）的多模态生成上进行了详细阐述，突出了这些领域的里程碑式进展。具体来说，文章详尽调查了这些研究中使用的方法和多模态数据集的关键技术组成部分。此外，文章还深入探讨了工具增强的多模态代理，这些代理可以利用现有的生成模型进行人机交互。最后，文章还全面讨论了AI安全性的进展，研究了新兴应用以及未来展望。我们的工作为多模态生成提供了系统性和富有洞察力的概述，预计将推动生成内容人工智能（AIGC）和世界模型的发展。 <div>
With the recent advancement in large language models (LLMs), there is a
growing interest in combining LLMs with multimodal learning. Previous surveys
of multimodal large language models (MLLMs) mainly focus on understanding. This
survey elaborates on multimodal generation across different domains, including
image, video, 3D, and audio, where we highlight the notable advancements with
milestone works in these fields. Specifically, we exhaustively investigate the
key technical components behind methods and multimodal datasets utilized in
these studies. Moreover, we dig into tool-augmented multimodal agents that can
use existing generative models for human-computer interaction. Lastly, we also
comprehensively discuss the advancement in AI safety and investigate emerging
applications as well as future prospects. Our work provides a systematic and
insightful overview of multimodal generation, which is expected to advance the
development of Artificial Intelligence for Generative Content (AIGC) and world
models. A curated list of all related papers can be found at
https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation
]]></content:encoded>
<pubDate>2024-05-29T17:59:20Z</pubDate>
</item>
<item>
<title>Multi-Modal Generative Embedding Model</title>
<link>http://arxiv.org/abs/2405.19333v1</link>
<guid>http://arxiv.org/abs/2405.19333v1</guid>
<content:encoded><![CDATA[
<div> 生成，嵌入，Multi-Modal Generative Embedding Model (MM-GEM)，PoolAggregator，ViT-Large

该研究提出了一种多模态生成嵌入模型(MM-GEM)，将生成和嵌入两个目标整合到一个大型语言模型中。他们还提出了一种PoolAggregator来提高效率，并实现细粒度的嵌入和生成能力。研究发现，这两个目标并不会显著冲突。MM-GEM在跨模态检索和零样本分类等多模态嵌入模型基准测试上表现出色，同时具有良好的图像字幕生成能力。此外，MM-GEM可以无缝执行区域级图像字幕生成和检索任务。MM-GEM中的高级文本模型还提高了长文本和图像检索的召回率超过5%。 <br /><br />总结: 该研究提出了一种多模态生成嵌入模型(MM-GEM)，实现了在一个大型语言模型中整合生成和嵌入两个目标，并通过PoolAggregator实现了高效率和细粒度的嵌入和生成能力。研究发现，这种模型在多种基准测试上表现出色，并且在图像字幕生成和检索任务中也能够达到良好的效果。 <div>
Most multi-modal tasks can be formulated into problems of either generation
or embedding. Existing models usually tackle these two types of problems by
decoupling language modules into a text decoder for generation, and a text
encoder for embedding. To explore the minimalism of multi-modal paradigms, we
attempt to achieve only one model per modality in this work. We propose a
Multi-Modal Generative Embedding Model (MM-GEM), whereby the generative and
embedding objectives are encapsulated in one Large Language Model. We also
propose a PoolAggregator to boost efficiency and enable the ability of
fine-grained embedding and generation. A surprising finding is that these two
objectives do not significantly conflict with each other. For example, MM-GEM
instantiated from ViT-Large and TinyLlama shows competitive performance on
benchmarks for multimodal embedding models such as cross-modal retrieval and
zero-shot classification, while has good ability of image captioning.
Additionally, MM-GEM can seamlessly execute region-level image caption
generation and retrieval tasks. Besides, the advanced text model in MM-GEM
brings over 5% improvement in Recall@1 for long text and image retrieval.
]]></content:encoded>
<pubDate>2024-05-29T17:59:10Z</pubDate>
</item>
<item>
<title>Normative Modules: A Generative Agent Architecture for Learning Norms
  that Supports Multi-Agent Cooperation</title>
<link>http://arxiv.org/abs/2405.19328v1</link>
<guid>http://arxiv.org/abs/2405.19328v1</guid>
<content:encoded><![CDATA[
Generative agents, which implement behaviors using a large language model
(LLM) to interpret and evaluate an environment, has demonstrated the capacity
to solve complex tasks across many social and technological domains. However,
when these agents interact with other agents and humans in presence of social
structures such as existing norms, fostering cooperation between them is a
fundamental challenge. In this paper, we develop the framework of a 'Normative
Module': an architecture designed to enhance cooperation by enabling agents to
recognize and adapt to the normative infrastructure of a given environment. We
focus on the equilibrium selection aspect of the cooperation problem and inform
our agent design based on the existence of classification institutions that
implement correlated equilibrium to provide effective resolution of the
equilibrium selection problem. Specifically, the normative module enables
agents to learn through peer interactions which of multiple candidate
institutions in the environment, does a group treat as authoritative. By
enabling normative competence in this sense, agents gain ability to coordinate
their sanctioning behaviour; coordinated sanctioning behaviour in turn shapes
primary behaviour within a social environment, leading to higher average
welfare. We design a new environment that supports institutions and evaluate
the proposed framework based on two key criteria derived from agent
interactions with peers and institutions: (i) the agent's ability to disregard
non-authoritative institutions and (ii) the agent's ability to identify
authoritative institutions among several options. We show that these
capabilities allow the agent to achieve more stable cooperative outcomes
compared to baseline agents without the normative module, paving the way for
research in a new avenue of designing environments and agents that account for
normative infrastructure.
]]></content:encoded>
<pubDate>2024-05-29T17:57:30Z</pubDate>
</item>
<item>
<title>Hierarchical World Models as Visual Whole-Body Humanoid Controllers</title>
<link>http://arxiv.org/abs/2405.18418v1</link>
<guid>http://arxiv.org/abs/2405.18418v1</guid>
<content:encoded><![CDATA[
<div> 强化学习，全身控制，视觉观察，层次世界模型，仿真人形机器人<br />本文探讨了基于强化学习的高度数据驱动的视觉全身人形机器人控制方法。研究采用分层世界模型，由高级代理根据视觉观察生成指令，低级代理执行这些指令，通过奖励进行训练。该方法在模拟环境下成功实现了8项任务的控制，并生成了广受人类喜爱的动作。详细信息可在https://nicklashansen.com/rlpuppeteer 网站找到。<br /><br />总结: 本文使用强化学习方法，提出了一种基于视觉观察的全身人形机器人控制方法，通过分层世界模型进行训练，取得了良好效果。 <div>
Whole-body control for humanoids is challenging due to the high-dimensional
nature of the problem, coupled with the inherent instability of a bipedal
morphology. Learning from visual observations further exacerbates this
difficulty. In this work, we explore highly data-driven approaches to visual
whole-body humanoid control based on reinforcement learning, without any
simplifying assumptions, reward design, or skill primitives. Specifically, we
propose a hierarchical world model in which a high-level agent generates
commands based on visual observations for a low-level agent to execute, both of
which are trained with rewards. Our approach produces highly performant control
policies in 8 tasks with a simulated 56-DoF humanoid, while synthesizing
motions that are broadly preferred by humans. Code and videos:
https://nicklashansen.com/rlpuppeteer
]]></content:encoded>
<pubDate>2024-05-28T17:57:23Z</pubDate>
</item>
<item>
<title>Reason3D: Searching and Reasoning 3D Segmentation via Large Language
  Model</title>
<link>http://arxiv.org/abs/2405.17427v1</link>
<guid>http://arxiv.org/abs/2405.17427v1</guid>
<content:encoded><![CDATA[
<div> 大型语言模型、3D理解、Reason3D、分割、层次掩模<br />
本文介绍了一种名为Reason3D的新型大型语言模型，用于全面理解3D环境。Reason3D接受点云数据和文本提示作为输入，生成文本响应和分割掩模，实现了高级任务，如3D推理分割、层次搜索、表达引用和问题回答。文章提出了一种分层掩模解码器，用于定位广阔场景中的小物体。实验验证了Reason3D在大规模ScanNet和Matterport3D数据集上在3D表达引用、3D问题回答和3D推理分割任务上取得了显著成果。可从https://github.com/KuanchihHuang/Reason3D获取代码和模型。<br /><br />总结: 本文介绍了一种新型大型语言模型Reason3D，用于理解3D环境。Reason3D能够处理点云数据和文本提示，实现了3D推理分割、层次搜索、表达引用和问题回答等任务。文章还提出了分层掩模解码器，用于在广阔场景中定位小物体。实验证实了Reason3D在大规模数据集上取得了显著的成果。 <div>
Recent advancements in multimodal large language models (LLMs) have shown
their potential in various domains, especially concept reasoning. Despite these
developments, applications in understanding 3D environments remain limited.
This paper introduces Reason3D, a novel LLM designed for comprehensive 3D
understanding. Reason3D takes point cloud data and text prompts as input to
produce textual responses and segmentation masks, facilitating advanced tasks
like 3D reasoning segmentation, hierarchical searching, express referring, and
question answering with detailed mask outputs. Specifically, we propose a
hierarchical mask decoder to locate small objects within expansive scenes. This
decoder initially generates a coarse location estimate covering the object's
general area. This foundational estimation facilitates a detailed,
coarse-to-fine segmentation strategy that significantly enhances the precision
of object identification and segmentation. Experiments validate that Reason3D
achieves remarkable results on large-scale ScanNet and Matterport3D datasets
for 3D express referring, 3D question answering, and 3D reasoning segmentation
tasks. Code and models are available at:
https://github.com/KuanchihHuang/Reason3D.
]]></content:encoded>
<pubDate>2024-05-27T17:59:41Z</pubDate>
</item>
<item>
<title>LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence</title>
<link>http://arxiv.org/abs/2405.17424v1</link>
<guid>http://arxiv.org/abs/2405.17424v1</guid>
<content:encoded><![CDATA[
<div> 大型自回归模型、综合先验知识、长程规划、决策链、Minecraft。

总结:<br />
本文介绍了一种名为大型自回归模型（LARM）的新型代理技术。该技术结合了文本和多视角图像输入，并能够以自回归方式预测后续行动。为了训练LARM，研究人员开发了一种名为自回归节点传输结构的新型数据格式，并组建了相应的数据集。通过采用两阶段训练方法，LARM成功收获了Minecraft游戏中的强化装备，这需要比以往最佳方法更复杂的决策链。此外，LARM的速度是以前方法的6.8倍。这表明LARM在处理现实世界互动时具有很大的潜力。 <div>
Due to the need to interact with the real world, embodied agents are required
to possess comprehensive prior knowledge, long-horizon planning capability, and
a swift response speed. Despite recent large language model (LLM) based agents
achieving promising performance, they still exhibit several limitations. For
instance, the output of LLMs is a descriptive sentence, which is ambiguous when
determining specific actions. To address these limitations, we introduce the
large auto-regressive model (LARM). LARM leverages both text and multi-view
images as input and predicts subsequent actions in an auto-regressive manner.
To train LARM, we develop a novel data format named auto-regressive node
transmission structure and assemble a corresponding dataset. Adopting a
two-phase training regimen, LARM successfully harvests enchanted equipment in
Minecraft, which demands significantly more complex decision-making chains than
the highest achievements of prior best methods. Besides, the speed of LARM is
6.8x faster.
]]></content:encoded>
<pubDate>2024-05-27T17:59:32Z</pubDate>
</item>
<item>
<title>Improved Distribution Matching Distillation for Fast Image Synthesis</title>
<link>http://arxiv.org/abs/2405.14867v1</link>
<guid>http://arxiv.org/abs/2405.14867v1</guid>
<content:encoded><![CDATA[
<div> Distribution Matching Distillation, DMD, regression loss, GAN loss, multi-step sampling <br />
总结: 
本文介绍了DMD2技术，旨在提高分布匹配蒸馏（DMD）训练的质量和效率。首先，通过消除回归损失以及昂贵的数据集构建需求，解决了DMD的稳定性问题。接着，引入了GAN损失来使学生模型在真实数据上进行训练，提升了生成图片的质量。最后，修改了训练程序以实现多步采样，并解决了训练-推断输入不匹配的问题。这些改进使得该方法在一步图片生成领域取得了新的成果，且在推断成本减少500倍的情况下，FID分数在ImageNet-64x64达到了1.28，在零样例COCO 2014上达到了8.35，超过了原始教师模型。此外，通过对SDXL进行蒸馏，展示了优异的视觉质量，尤其在少步方法中能够生成百万像素的大型图片。 <div>
Recent approaches have shown promises distilling diffusion models into
efficient one-step generators. Among them, Distribution Matching Distillation
(DMD) produces one-step generators that match their teacher in distribution,
without enforcing a one-to-one correspondence with the sampling trajectories of
their teachers. However, to ensure stable training, DMD requires an additional
regression loss computed using a large set of noise-image pairs generated by
the teacher with many steps of a deterministic sampler. This is costly for
large-scale text-to-image synthesis and limits the student's quality, tying it
too closely to the teacher's original sampling paths. We introduce DMD2, a set
of techniques that lift this limitation and improve DMD training. First, we
eliminate the regression loss and the need for expensive dataset construction.
We show that the resulting instability is due to the fake critic not estimating
the distribution of generated samples accurately and propose a two time-scale
update rule as a remedy. Second, we integrate a GAN loss into the distillation
procedure, discriminating between generated samples and real images. This lets
us train the student model on real data, mitigating the imperfect real score
estimation from the teacher model, and enhancing quality. Lastly, we modify the
training procedure to enable multi-step sampling. We identify and address the
training-inference input mismatch problem in this setting, by simulating
inference-time generator samples during training time. Taken together, our
improvements set new benchmarks in one-step image generation, with FID scores
of 1.28 on ImageNet-64x64 and 8.35 on zero-shot COCO 2014, surpassing the
original teacher despite a 500X reduction in inference cost. Further, we show
our approach can generate megapixel images by distilling SDXL, demonstrating
exceptional visual quality among few-step methods.
]]></content:encoded>
<pubDate>2024-05-23T17:59:49Z</pubDate>
</item>
<item>
<title>Comprehensive Multimodal Deep Learning Survival Prediction Enabled by a
  Transformer Architecture: A Multicenter Study in Glioblastoma</title>
<link>http://arxiv.org/abs/2405.12963v1</link>
<guid>http://arxiv.org/abs/2405.12963v1</guid>
<content:encoded><![CDATA[
<div> 深度学习模型, 胶质母细胞瘤, 图像数据, 临床数据, 生存预测<br />
<br />
胶质母细胞瘤是一种致命的脑肿瘤，预测其生存率对治疗和临床决策至关重要。本研究提出了一种基于transformer的深度学习模型，结合了核磁共振图像、临床和分子病理数据，以改善胶质母细胞瘤的生存预测。该模型利用自监督学习技术对高维MRI输入进行编码，并通过交叉注意力有效地整合了图像和非图像数据。实验结果表明，该模型在多个独立测试集上表现稳定，且显著优于目前的基于3D-CNN的模型。结论指出，该transformer模型整合了多种输入模态的信息，为改善胶质母细胞瘤生存预测提供了重要贡献，并具有较强的泛化能力。 <br /><br />总结: <br />胶质母细胞瘤是一种致命的脑肿瘤，预测其生存率对治疗和临床决策至关重要。<br />本研究提出了一种基于transformer的深度学习模型，结合了核磁共振图像、临床和分子病理数据，以改善胶质母细胞瘤的生存预测。<br />实验结果表明，该模型在多个独立测试集上表现稳定，且显著优于目前的基于3D-CNN的模型。<br />结论指出，该transformer模型整合了多种输入模态的信息，为改善胶质母细胞瘤生存预测提供了重要贡献，并具有较强的泛化能力。 <div>
Background: This research aims to improve glioblastoma survival prediction by
integrating MR images, clinical and molecular-pathologic data in a
transformer-based deep learning model, addressing data heterogeneity and
performance generalizability. Method: We propose and evaluate a
transformer-based non-linear and non-proportional survival prediction model.
The model employs self-supervised learning techniques to effectively encode the
high-dimensional MRI input for integration with non-imaging data using
cross-attention. To demonstrate model generalizability, the model is assessed
with the time-dependent concordance index (Cdt) in two training setups using
three independent public test sets: UPenn-GBM, UCSF-PDGM, and RHUH-GBM, each
comprising 378, 366, and 36 cases, respectively. Results: The proposed
transformer model achieved promising performance for imaging as well as
non-imaging data, effectively integrating both modalities for enhanced
performance (UPenn-GBM test-set, imaging Cdt 0.645, multimodal Cdt 0.707) while
outperforming state-of-the-art late-fusion 3D-CNN-based models. Consistent
performance was observed across the three independent multicenter test sets
with Cdt values of 0.707 (UPenn-GBM, internal test set), 0.672 (UCSF-PDGM,
first external test set) and 0.618 (RHUH-GBM, second external test set). The
model achieved significant discrimination between patients with favorable and
unfavorable survival for all three datasets (logrank p 1.9\times{10}^{-8},
9.7\times{10}^{-3}, and 1.2\times{10}^{-2}). Conclusions: The proposed
transformer-based survival prediction model integrates complementary
information from diverse input modalities, contributing to improved
glioblastoma survival prediction compared to state-of-the-art methods.
Consistent performance was observed across institutions supporting model
generalizability.
]]></content:encoded>
<pubDate>2024-05-21T17:44:48Z</pubDate>
</item>
<item>
<title>Adapting Large Multimodal Models to Distribution Shifts: The Role of
  In-Context Learning</title>
<link>http://arxiv.org/abs/2405.12217v1</link>
<guid>http://arxiv.org/abs/2405.12217v1</guid>
<content:encoded><![CDATA[
<div> LMMs, adaptability, in-context learning, TopKNearestPR, InvariantSelectPR
LMM大型多模型，具有高鲁棒性，适应性强，但在特定领域需要领域特定的适应性。提出了在上下文中学习(ICL)作为增强LMM适应性的有效替代方法，以及选择示例的困难和方法。通过评估无监督ICL方法TopKNearestPR，发现其效果受制于预训练视觉编码器的不足。因此提出了一种新方法InvariantSelectPR，利用Class-conditioned Contrastive Invariance（CCI）来增强预训练视觉编码器，提高其鲁棒性。实验证明，InvariantSelectPR显著提高了LMM的适应性，在基准数据集上取得了显著的性能提高。总结: LMM具有高鲁棒性但需要特定领域的适应性；ICL是一种有效的增强LMM适应性的替代方法；TopKNearestPR方法的效果受限于预训练视觉编码器的不足；InvariantSelectPR利用CCI提高了LMM的适应性。 <div>
Recent studies indicate that large multimodal models (LMMs) are highly robust
against natural distribution shifts, often surpassing previous baselines.
Despite this, domain-specific adaptation is still necessary, particularly in
specialized areas like healthcare. Due to the impracticality of fine-tuning
LMMs given their vast parameter space, this work investigates in-context
learning (ICL) as an effective alternative for enhancing LMMs' adaptability. We
find that the success of ICL heavily relies on the choice of demonstration,
mirroring challenges seen in large language models but introducing unique
complexities for LMMs facing distribution shifts. Our study addresses this by
evaluating an unsupervised ICL method, TopKNearestPR, which selects in-context
examples through a nearest example search based on feature similarity. We
uncover that its effectiveness is limited by the deficiencies of pre-trained
vision encoders under distribution shift scenarios. To address these
challenges, we propose InvariantSelectPR, a novel method leveraging
Class-conditioned Contrastive Invariance (CCI) for more robust demonstration
selection. Specifically, CCI enhances pre-trained vision encoders by improving
their discriminative capabilities across different classes and ensuring
invariance to domain-specific variations. This enhancement allows the encoders
to effectively identify and retrieve the most informative examples, which are
then used to guide LMMs in adapting to new query samples under varying
distributions. Our experiments show that InvariantSelectPR substantially
improves the adaptability of LMMs, achieving significant performance gains on
benchmark datasets, with a 34.2%$\uparrow$ accuracy increase in 7-shot on
Camelyon17 and 16.9%$\uparrow$ increase in 7-shot on HAM10000 compared to the
baseline zero-shot performance.
]]></content:encoded>
<pubDate>2024-05-20T17:59:21Z</pubDate>
</item>
<item>
<title>Observational Scaling Laws and the Predictability of Language Model
  Performance</title>
<link>http://arxiv.org/abs/2405.10938v1</link>
<guid>http://arxiv.org/abs/2405.10938v1</guid>
<content:encoded><![CDATA[
<div> scaling laws, language model performance, observational approach, training compute efficiencies, predictability<br />
<br />总结:<br />文章主要讨论了语言模型性能随规模变化的规律性以及如何利用观测方法构建缩放定律。作者提出了通过观察已有的约80个公开模型来构建缩放定律的替代方法，并指出了模型家族在训练计算效率和能力方面的差异。作者还展示了复杂的缩放现象是可以预测的，包括一些新现象的平滑、S形行为以及如何预测后训练干预对语言模型能力的影响。 <div>
Understanding how language model performance varies with scale is critical to
benchmark and algorithm development. Scaling laws are one approach to building
this understanding, but the requirement of training models across many
different scales has limited their use. We propose an alternative,
observational approach that bypasses model training and instead builds scaling
laws from ~80 publically available models. Building a single scaling law from
multiple model families is challenging due to large variations in their
training compute efficiencies and capabilities. However, we show that these
variations are consistent with a simple, generalized scaling law where language
model performance is a function of a low-dimensional capability space, and
model families only vary in their efficiency in converting training compute to
capabilities. Using this approach, we show the surprising predictability of
complex scaling phenomena: we show that several emergent phenomena follow a
smooth, sigmoidal behavior and are predictable from small models; we show that
the agent performance of models such as GPT-4 can be precisely predicted from
simpler non-agentic benchmarks; and we show how to predict the impact of
post-training interventions like Chain-of-Thought and Self-Consistency as
language model capabilities continue to improve.
]]></content:encoded>
<pubDate>2024-05-17T17:49:44Z</pubDate>
</item>
<item>
<title>CinePile: A Long Video Question Answering Dataset and Benchmark</title>
<link>http://arxiv.org/abs/2405.08813v1</link>
<guid>http://arxiv.org/abs/2405.08813v1</guid>
<content:encoded><![CDATA[
<div> 长视频理解，数据集，CinePile，问题-答案数据集，LLMs，视频理解模型
<br /><br />
本文介绍了针对真实长视频理解的创新数据集和基准测试CinePile。该数据集包含了30.5万道多项选择题，涵盖了视觉和多模态方面的各种内容，包括时间理解，人物与物体的互动，以及场景中事件或动作的推理。作者利用先进的LLMs与人类相结合的方法构建了这个问题-答案数据集。此外，他们还评估了最近的视频中心化LLMs在数据集的测试部分上的表现，结果显示即使最先进的视频中心化LLMs在这些任务上的表现也明显落后于人类表现，突显了视频理解中的复杂性和挑战。数据集可以在链接https://hf.co/datasets/tomg-group-umd/cinepile上获取。
<br /><br />总结: 本文介绍了CinePile数据集，包括多项选择题和LLMs评估结果，突出了视频理解的复杂性和挑战。 <div>
Current datasets for long-form video understanding often fall short of
providing genuine long-form comprehension challenges, as many tasks derived
from these datasets can be successfully tackled by analyzing just one or a few
random frames from a video. To address this issue, we present a novel dataset
and benchmark, CinePile, specifically designed for authentic long-form video
understanding. This paper details our innovative approach for creating a
question-answer dataset, utilizing advanced LLMs with human-in-the-loop and
building upon human-generated raw data. Our comprehensive dataset comprises
305,000 multiple-choice questions (MCQs), covering various visual and
multimodal aspects, including temporal comprehension, understanding
human-object interactions, and reasoning about events or actions within a
scene. Additionally, we evaluate recent video-centric LLMs, both open-source
and proprietary, on the test split of our dataset. The findings reveal that
even state-of-the-art video-centric LLMs significantly lag behind human
performance in these tasks, highlighting the complexity and challenge inherent
in video understanding. The dataset is available at
https://hf.co/datasets/tomg-group-umd/cinepile
]]></content:encoded>
<pubDate>2024-05-14T17:59:02Z</pubDate>
</item>
<item>
<title>SciFIBench: Benchmarking Large Multimodal Models for Scientific Figure
  Interpretation</title>
<link>http://arxiv.org/abs/2405.08807v1</link>
<guid>http://arxiv.org/abs/2405.08807v1</guid>
<content:encoded><![CDATA[
<div> 科学研究, 图形解释, 多模态模型, 基准测试, CS arXiv paper

多模态模型在科学研究中的应用潜力巨大，但在图形解释方面的能力尚未得到很好的表征。本文介绍了SciFIBench，这是一个科学图形解释基准测试，包括1000个多项选择题和12个类别的两项任务。题目经过人工筛选和质量控制，对26个多模态模型进行了评估，结果显示这是一个具有挑战性的基准测试。最后，研究了多模态模型在扩展问题集上的表现。SciFIBench的发布旨在推动该领域的进展。 <br /><br />总结: <div>
Large multimodal models (LMMs) have proven flexible and generalisable across
many tasks and fields. Although they have strong potential to aid scientific
research, their capabilities in this domain are not well characterised. A key
aspect of scientific research is the ability to understand and interpret
figures, which serve as a rich, compressed source of complex information. In
this work, we present SciFIBench, a scientific figure interpretation benchmark.
Our main benchmark consists of a 1000-question gold set of multiple-choice
questions split between two tasks across 12 categories. The questions are
curated from CS arXiv paper figures and captions, using adversarial filtering
to find hard negatives and human verification for quality control. We evaluate
26 LMMs on SciFIBench, finding it to be a challenging benchmark. Finally, we
investigate the alignment and reasoning faithfulness of the LMMs on augmented
question sets from our benchmark. We release SciFIBench to encourage progress
in this domain.
]]></content:encoded>
<pubDate>2024-05-14T17:54:17Z</pubDate>
</item>
<item>
<title>SPIN: Simultaneous Perception, Interaction and Navigation</title>
<link>http://arxiv.org/abs/2405.07991v1</link>
<guid>http://arxiv.org/abs/2405.07991v1</guid>
<content:encoded><![CDATA[
<div> 移动操作，机器人，视觉，整体协调，感知
<br />
该文章介绍了移动操作系统的挑战以及提出的解决方案。移动操作相比于静态操作或者运动操作来说更加具有挑战性，需要在无序动态环境中完成长期任务。文章提出了一种活动视觉系统来感知和反应环境，类似于人类利用整体协调和手眼协调来完成任务。这种系统不仅可以在复杂的环境中移动，还可以通过活动视觉系统选择何时感知何物。通过这种方法，机器人学会了在复杂杂乱的环境中移动，并展示了灵活的整体协调能力，而无需创建环境地图。文章还提供了结果可视化和视频。 <div>
While there has been remarkable progress recently in the fields of
manipulation and locomotion, mobile manipulation remains a long-standing
challenge. Compared to locomotion or static manipulation, a mobile system must
make a diverse range of long-horizon tasks feasible in unstructured and dynamic
environments. While the applications are broad and interesting, there are a
plethora of challenges in developing these systems such as coordination between
the base and arm, reliance on onboard perception for perceiving and interacting
with the environment, and most importantly, simultaneously integrating all
these parts together. Prior works approach the problem using disentangled
modular skills for mobility and manipulation that are trivially tied together.
This causes several limitations such as compounding errors, delays in
decision-making, and no whole-body coordination. In this work, we present a
reactive mobile manipulation framework that uses an active visual system to
consciously perceive and react to its environment. Similar to how humans
leverage whole-body and hand-eye coordination, we develop a mobile manipulator
that exploits its ability to move and see, more specifically -- to move in
order to see and to see in order to move. This allows it to not only move
around and interact with its environment but also, choose "when" to perceive
"what" using an active visual system. We observe that such an agent learns to
navigate around complex cluttered scenarios while displaying agile whole-body
coordination using only ego-vision without needing to create environment maps.
Results visualizations and videos at https://spin-robot.github.io/
]]></content:encoded>
<pubDate>2024-05-13T17:59:36Z</pubDate>
</item>
<item>
<title>A Generalist Learner for Multifaceted Medical Image Interpretation</title>
<link>http://arxiv.org/abs/2405.07988v1</link>
<guid>http://arxiv.org/abs/2405.07988v1</guid>
<content:encoded><![CDATA[
<div> 医疗人工智能，MedVersa，多模态学习，医学图像解释，数据集
<br /><br />
提出了MedVersa，一个通用的学习系统，可以学习和处理医学图像的多个任务。它利用大型语言模型作为可学习的协调器，允许从视觉和语言监督中学习，支持多模态输入，并且可以实时进行任务规范。通过引入MedInterp数据集，包括超过1300万个注释实例，涵盖了11个任务跨越3种模态，来支持MedVersa的开发。实验表明，MedVersa在9个任务中实现了最先进的性能，有时优于专家对手超过10％。这种通用方法为医学图像解释打开了新的可能性，为更适应和高效的人工智能辅助临床决策铺平了道路。 
<br />MedVersa是第一个展示多模态生成医学人工智能在实现多模态输出、输入和动态任务规范方面的可行性，突显了它作为全面医学图像分析多功能系统的潜力。 <div>
Current medical artificial intelligence systems are often limited to narrow
applications, hindering their widespread adoption in clinical practice. To
address this limitation, we propose MedVersa, a generalist learner that enables
flexible learning and tasking for medical image interpretation. By leveraging a
large language model as a learnable orchestrator, MedVersa can learn from both
visual and linguistic supervision, support multimodal inputs, and perform
real-time task specification. This versatility allows MedVersa to adapt to
various clinical scenarios and perform multifaceted medical image analysis. We
introduce MedInterp, the largest multimodal dataset to date for medical image
interpretation, consisting of over 13 million annotated instances spanning 11
tasks across 3 modalities, to support the development of MedVersa. Our
experiments demonstrate that MedVersa achieves state-of-the-art performance in
9 tasks, sometimes outperforming specialist counterparts by over 10%. MedVersa
is the first to showcase the viability of multimodal generative medical AI in
implementing multimodal outputs, inputs, and dynamic task specification,
highlighting its potential as a multifunctional system for comprehensive
medical image analysis. This generalist approach to medical image
interpretation paves the way for more adaptable and efficient AI-assisted
clinical decision-making.
]]></content:encoded>
<pubDate>2024-05-13T17:58:51Z</pubDate>
</item>
<item>
<title>Conformal Validity Guarantees Exist for Any Data Distribution</title>
<link>http://arxiv.org/abs/2405.06627v1</link>
<guid>http://arxiv.org/abs/2405.06627v1</guid>
<content:encoded><![CDATA[
<div> 黑盒优化, 主动学习, 数据分布, 不确定性, 风险量化
总结:
本文讨论了随着机器学习的普及，从业者日益寻求量化和控制这些系统所承担风险的手段。特别是在黑盒优化和主动学习中，机器学习系统具有自主收集数据的能力，其行为引发了数据分布的顺序反馈循环转变，这一挑战尤为突出。虽然符合预测已成为一种有前途的不确定性和风险量化方法，但现有的变种要么无法适应数据依赖性转变的序列，要么没有充分利用由我们控制的代理引发的转变事实。本文证明了符合预测在理论上可以扩展到\textit{任何}联合数据分布，而不仅仅是可交换或准可交换的数据分布，尽管在最一般的情况下计算极为不切实际。对于实际应用，我们概述了一种推导任何数据分布的具体符合算法的程序，并利用这一程序导出了一系列关于代理引发的协变量转变的易处理的算法。我们在合成黑盒优化和主动学习任务上对所提出的算法进行了实证评估。 <div>
As machine learning (ML) gains widespread adoption, practitioners are
increasingly seeking means to quantify and control the risk these systems
incur. This challenge is especially salient when ML systems have autonomy to
collect their own data, such as in black-box optimization and active learning,
where their actions induce sequential feedback-loop shifts in the data
distribution. Conformal prediction has emerged as a promising approach to
uncertainty and risk quantification, but existing variants either fail to
accommodate sequences of data-dependent shifts, or do not fully exploit the
fact that agent-induced shift is under our control. In this work we prove that
conformal prediction can theoretically be extended to \textit{any} joint data
distribution, not just exchangeable or quasi-exchangeable ones, although it is
exceedingly impractical to compute in the most general case. For practical
applications, we outline a procedure for deriving specific conformal algorithms
for any data distribution, and we use this procedure to derive tractable
algorithms for a series of agent-induced covariate shifts. We evaluate the
proposed algorithms empirically on synthetic black-box optimization and active
learning tasks.
]]></content:encoded>
<pubDate>2024-05-10T17:40:24Z</pubDate>
</item>
<item>
<title>Characterizing the Accuracy - Efficiency Trade-off of Low-rank
  Decomposition in Language Models</title>
<link>http://arxiv.org/abs/2405.06626v1</link>
<guid>http://arxiv.org/abs/2405.06626v1</guid>
<content:encoded><![CDATA[
<div> 低秩分解、大型语言模型、内存优化、模型压缩、精度-效率权衡
<br /><br />
在大型语言模型中，模型大小的激增使得计算与模型大小的比率大幅降低，从而将大型语言模型推向内存受限的领域。为了优化内存占用和流量，研究者们积极探索模型压缩方法，如量化和参数修剪。然而，针对低秩分解在大型语言模型中的精度与效率权衡尚未得到充分理解。因此，本文对低秩分解方法，特别是Tucker分解，在最近的语言模型上进行了精度与效率权衡的研究。通过对BERT和Llama 2模型在六个广泛使用的语言模型基准上进行案例研究，结果显示，我们可以在几乎不影响精度的情况下实现9%的模型大小减少。这表明低秩分解可能是大型语言模型应用的一个有前景的方向，尤其是对于需要实时服务并且延迟和模型精度同等重要的应用场景，比如人工智能辅助和实时编程助手。 
<br />
总结: <br />
1. 模型压缩是为了优化大型语言模型的内存占用和流量。
2. 低秩分解方法有望在不影响精度的情况下减小模型大小。
3. 通过对Tucker分解在BERT和Llama 2模型上进行研究，证明了低秩分解方法的潜力。 <div>
Large language models (LLMs) have emerged and presented their general
problem-solving capabilities with one model. However, the model size has
increased dramatically with billions of parameters to enable such broad
problem-solving capabilities. In addition, due to the dominance of
matrix-matrix and matrix-vector multiplications in LLMs, the compute-to-model
size ratio is significantly lower than that of CNNs. This shift pushes LLMs
from a computation-bound regime to a memory-bound regime. Therefore, optimizing
the memory footprint and traffic is an important optimization direction for
LLMs today.
  Model compression methods such as quantization and parameter pruning have
been actively explored for achieving the memory footprint and traffic
optimization. However, the accuracy-efficiency trade-off of rank pruning for
LLMs is not well-understood yet. Therefore, we characterize the
accuracy-efficiency trade-off of a low-rank decomposition method, specifically
Tucker decomposition, on recent language models, including an open-source LLM,
Llama 2.
  We formalize the low-rank decomposition design space and show that the
decomposition design space is enormous (e.g., O($2^{37}$) for Llama2-7B). To
navigate such a vast design space, we formulate the design space and perform
thorough case studies of accuracy-efficiency trade-offs using six widely used
LLM benchmarks on BERT and Llama 2 models. Our results show that we can achieve
a 9\% model size reduction with minimal accuracy drops, which range from 4\%p
to 10\%p, depending on the difficulty of the benchmark, without any retraining
to recover accuracy after decomposition. The results show that low-rank
decomposition can be a promising direction for LLM-based applications that
require real-time service in scale (e.g., AI agent assist and real-time coding
assistant), where the latency is as important as the model accuracy.
]]></content:encoded>
<pubDate>2024-05-10T17:40:02Z</pubDate>
</item>
<item>
<title>Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency
  for Tool Planning</title>
<link>http://arxiv.org/abs/2405.05955v1</link>
<guid>http://arxiv.org/abs/2405.05955v1</guid>
<content:encoded><![CDATA[
<div> 模型、Smurfs、多智能体框架、任务分解、工具利用<br />
在本文中，介绍了一个名为"Smurfs"的多智能体框架，旨在将传统的大型语言模型转化为协同作用的多智能体集合，以提高任务分解和执行效率。该框架通过创新的提示策略，在模型内分配不同的角色，从而促进专业智能体之间的合作。实证研究以mistral-7b-instruct模型为案例，展示了Smurfs在复杂工具利用场景中的卓越能力。值得注意的是，在ToolBench I2和I3基准测试中，Smurfs以惊人的84.4%胜率超过了ChatGPT-ReACT和GPT-4模型的最高记录。此外，通过全面的割除研究，验证了多智能体框架核心组件对其整体有效性的贡献，并为未来探索多智能体大型语言模型系统铺平了道路。 <br /><br />总结: <br />模型的介绍，Smurfs框架的设计目的和主要功能<br />基于mistral-7b-instruct模型的应用实证研究，展示了Smurfs框架在复杂工具利用场景中的优越能力和表现<br />通过全面的割除研究，验证了多智能体框架核心组件对其整体有效性的贡献<br />对比了Smurfs和其他模型在ToolBench I2和I3基准测试中的表现，突出了Smurfs的优越性能<br />为未来探索多智能体大型语言模型系统提供了新的思路和方向 <div>
The emergence of large language models (LLMs) has opened up unprecedented
possibilities for automating complex tasks that are often comparable to human
performance. Despite their capabilities, LLMs still encounter difficulties in
completing tasks that require high levels of accuracy and complexity due to
their inherent limitations in handling multifaceted problems single-handedly.
This paper introduces "Smurfs", a cutting-edge multi-agent framework designed
to revolutionize the application of LLMs. By transforming a conventional LLM
into a synergistic multi-agent ensemble, Smurfs enhances task decomposition and
execution without necessitating extra training. This is achieved through
innovative prompting strategies that allocate distinct roles within the model,
thereby facilitating collaboration among specialized agents. The framework
gives access to external tools to efficiently solve complex tasks. Our
empirical investigation, featuring the mistral-7b-instruct model as a case
study, showcases Smurfs' superior capability in intricate tool utilization
scenarios. Notably, Smurfs outmatches the ChatGPT-ReACT in the ToolBench I2 and
I3 benchmark with a remarkable 84.4% win rate, surpassing the highest recorded
performance of a GPT-4 model at 73.5%. Furthermore, through comprehensive
ablation studies, we dissect the contribution of the core components of the
multi-agent framework to its overall efficacy. This not only verifies the
effectiveness of the framework, but also sets a route for future exploration of
multi-agent LLM systems.
]]></content:encoded>
<pubDate>2024-05-09T17:49:04Z</pubDate>
</item>
<item>
<title>Frame Interpolation with Consecutive Brownian Bridge Diffusion</title>
<link>http://arxiv.org/abs/2405.05953v1</link>
<guid>http://arxiv.org/abs/2405.05953v1</guid>
<content:encoded><![CDATA[
<div> Latent Diffusion Models, Video Frame Interpolation, Conditional Image Generation, Random Generation, Deterministic Output
总结: 最近视频帧插值的研究采用了基于扩散的条件图像生成方法，利用潜在扩散模型作为条件生成模型。然而，潜在扩散模型会产生多样化的输出，导致输出不确定性。为了解决这个问题，研究提出了连续布朗桥扩散方法，能够减小生成的累积方差，从而改善视频帧插值的性能。 <div>
Recent work in Video Frame Interpolation (VFI) tries to formulate VFI as a
diffusion-based conditional image generation problem, synthesizing the
intermediate frame given a random noise and neighboring frames. Due to the
relatively high resolution of videos, Latent Diffusion Models (LDMs) are
employed as the conditional generation model, where the autoencoder compresses
images into latent representations for diffusion and then reconstructs images
from these latent representations. Such a formulation poses a crucial
challenge: VFI expects that the output is deterministically equal to the ground
truth intermediate frame, but LDMs randomly generate a diverse set of different
images when the model runs multiple times. The reason for the diverse
generation is that the cumulative variance (variance accumulated at each step
of generation) of generated latent representations in LDMs is large. This makes
the sampling trajectory random, resulting in diverse rather than deterministic
generations. To address this problem, we propose our unique solution: Frame
Interpolation with Consecutive Brownian Bridge Diffusion. Specifically, we
propose consecutive Brownian Bridge diffusion that takes a deterministic
initial value as input, resulting in a much smaller cumulative variance of
generated latent representations. Our experiments suggest that our method can
improve together with the improvement of the autoencoder and achieve
state-of-the-art performance in VFI, leaving strong potential for further
enhancement.
]]></content:encoded>
<pubDate>2024-05-09T17:46:22Z</pubDate>
</item>
<item>
<title>LLMs with Personalities in Multi-issue Negotiation Games</title>
<link>http://arxiv.org/abs/2405.05248v2</link>
<guid>http://arxiv.org/abs/2405.05248v2</guid>
<content:encoded><![CDATA[
<div> 人工智能, 大语言模型, 谈判, 个性特质, 游戏理论
<br />
这篇文章使用大型语言模型（LLMs）来调查人工智能代理人在谈判中的表现，测量他们对游戏理论中的谈判能力以及衡量公平和风险概念的方法论挑战。研究使用模拟方法进行了1500次单问题和多问题谈判的实验，结果显示对称性和不对称性的议题估值增加了领域复杂性，提高了协议达成率，但减少了侵略性谈判的剩余价值。通过梯度提升回归和Shapley解释器，研究发现高度的开放性、尽责性和神经质与公平倾向相关，而低宜人性和低开放性与理性倾向相关。低尽责性与高毒性相关。这些结果表明LLMs可能具有内置的防护措施，以默认公平行为，但可以"越狱"以利用易于相处的对手。此外，研究还提出了关于如何设计谈判机器人以及基于博弈论和计算社会科学的谈判行为评估框架的实用见解。
<br /><br />总结: 本研究使用大型语言模型检验了人工智能代理人在谈判中的表现，并得出了一些重要结果。通过实验发现了LLMs在谈判中的行为特质，以及如何设计谈判机器人和评估谈判行为的框架。 <div>
Powered by large language models (LLMs), AI agents have become capable of
many human tasks. Using the most canonical definitions of the Big Five
personality, we measure the ability of LLMs to negotiate within a
game-theoretical framework, as well as methodological challenges to measuring
notions of fairness and risk. Simulations (n=1,500) for both single-issue and
multi-issue negotiation reveal increase in domain complexity with asymmetric
issue valuations improve agreement rates but decrease surplus from aggressive
negotiation. Through gradient-boosted regression and Shapley explainers, we
find high openness, conscientiousness, and neuroticism are associated with fair
tendencies; low agreeableness and low openness are associated with rational
tendencies. Low conscientiousness is associated with high toxicity. These
results indicate that LLMs may have built-in guardrails that default to fair
behavior, but can be "jail broken" to exploit agreeable opponents. We also
offer pragmatic insight in how negotiation bots can be designed, and a
framework of assessing negotiation behavior based on game theory and
computational social science.
]]></content:encoded>
<pubDate>2024-05-09T01:09:09Z</pubDate>
</item>
<item>
<title>Diffusion-HMC: Parameter Inference with Diffusion Model driven
  Hamiltonian Monte Carlo</title>
<link>http://arxiv.org/abs/2405.05255v1</link>
<guid>http://arxiv.org/abs/2405.05255v1</guid>
<content:encoded><![CDATA[
<div> 生成模型，宇宙学，参数推断，暗物质密度场，哈密顿蒙特卡洛<br />
<br />
本研究利用扩散生成模型作为宇宙学参数推断模型和冷暗物质密度场的仿真模型。该模型能够仿真与目标分布一致的总结统计信息的场景。进一步利用扩散生成模型的近似似然性质，通过哈密顿蒙特卡洛方法对给定的测试图像进行宇宙学参数的后验采样，从而获得对宇宙学的紧密限制。最后，本研究证明了与基线参数推断网络相比，这种参数推断方法对噪声的影响更为稳健。 <br /><br />总结: <br />生成模型在宇宙学中的应用，模拟暗物质密度场，参数推断，哈密顿蒙特卡洛方法，对噪声的鲁棒性。 <div>
Diffusion generative models have excelled at diverse image generation and
reconstruction tasks across fields. A less explored avenue is their application
to discriminative tasks involving regression or classification problems. The
cornerstone of modern cosmology is the ability to generate predictions for
observed astrophysical fields from theory and constrain physical models from
observations using these predictions. This work uses a single diffusion
generative model to address these interlinked objectives -- as a surrogate
model or emulator for cold dark matter density fields conditional on input
cosmological parameters, and as a parameter inference model that solves the
inverse problem of constraining the cosmological parameters of an input field.
The model is able to emulate fields with summary statistics consistent with
those of the simulated target distribution. We then leverage the approximate
likelihood of the diffusion generative model to derive tight constraints on
cosmology by using the Hamiltonian Monte Carlo method to sample the posterior
on cosmological parameters for a given test image. Finally, we demonstrate that
this parameter inference approach is more robust to the addition of noise than
baseline parameter inference networks.
]]></content:encoded>
<pubDate>2024-05-08T17:59:03Z</pubDate>
</item>
<item>
<title>LLMs with Personalities in Multi-issue Negotiation Games</title>
<link>http://arxiv.org/abs/2405.05248v1</link>
<guid>http://arxiv.org/abs/2405.05248v1</guid>
<content:encoded><![CDATA[
<div> Negotiation, Large Language Models, Personality, Fairness, Risk

AI代理人通过大型语言模型（LLM）已经可以完成很多人类任务。本研究使用大五人格的经典定义，衡量了LLM在博弈理论框架内进行谈判的能力，以及衡量公平和风险概念的方法论挑战。针对单一议题和多议题谈判的1500次模拟显示，领域复杂性的增加以及对称议题价值的不平衡会提高协议率，但会降低激进谈判的剩余价值。通过梯度提升回归和Shapley解释器，我们发现高的开放性、责任心和神经质与公平倾向相关；低的宜人性和低的开放性与理性倾向相关。低的责任心与高的毒性相关。这些结果表明，LLM可能具有默认公平行为的内置防护，但可以被"越狱"以利用易被说服的对手。我们还提供了关于如何设计谈判机器人以及基于博弈论和计算社会科学的评估谈判行为的实用见解。 <br /><br />总结: 本研究通过大型语言模型对谈判能力进行了量化分析，发现LLM可能内置了默认公平行为的保障，但也可以被“越狱”以利用易被说服的对手。同时，提出了设计谈判机器人和评估谈判行为的实用见解。 <div>
Powered by large language models (LLMs), AI agents have become capable of
many human tasks. Using the most canonical definitions of the Big Five
personality, we measure the ability of LLMs to negotiate within a
game-theoretical framework, as well as methodological challenges to measuring
notions of fairness and risk. Simulations (n=1,500) for both single-issue and
multi-issue negotiation reveal increase in domain complexity with asymmetric
issue valuations improve agreement rates but decrease surplus from aggressive
negotiation. Through gradient-boosted regression and Shapley explainers, we
find high openness, conscientiousness, and neuroticism are associated with fair
tendencies; low agreeableness and low openness are associated with rational
tendencies. Low conscientiousness is associated with high toxicity. These
results indicate that LLMs may have built-in guardrails that default to fair
behavior, but can be "jail broken" to exploit agreeable opponents. We also
offer pragmatic insight in how negotiation bots can be designed, and a
framework of assessing negotiation behavior based on game theory and
computational social science.
]]></content:encoded>
<pubDate>2024-05-08T17:51:53Z</pubDate>
</item>
<item>
<title>Pose Priors from Language Models</title>
<link>http://arxiv.org/abs/2405.03689v1</link>
<guid>http://arxiv.org/abs/2405.03689v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 零-shot姿势优化, 3D姿势估计, 文本模型, 物理接触约束, 社交互动

总结: 
零-shot姿势优化是一种利用大型预训练文本模型的先验知识来改进3D姿势估计的方法。通过将自然语言描述转换为可控制的损失，可以约束3D姿势优化，从而正确捕捉人们在近距离互动时的语义。该方法与更复杂的最先进方法相媲美，而且不需要昂贵的人工标注接触点和训练专门模型。此外，与以往的方法不同，该方法提供了一个统一框架来解决自身接触和人与人之间的接触。 <div>
We present a zero-shot pose optimization method that enforces accurate
physical contact constraints when estimating the 3D pose of humans. Our central
insight is that since language is often used to describe physical interaction,
large pretrained text-based models can act as priors on pose estimation.
  We can thus leverage this insight to improve pose estimation by converting
natural language descriptors, generated by a large multimodal model (LMM), into
tractable losses to constrain the 3D pose optimization. Despite its simplicity,
our method produces surprisingly compelling pose reconstructions of people in
close contact, correctly capturing the semantics of the social and physical
interactions. We demonstrate that our method rivals more complex
state-of-the-art approaches that require expensive human annotation of contact
points and training specialized models. Moreover, unlike previous approaches,
our method provides a unified framework for resolving self-contact and
person-to-person contact.
]]></content:encoded>
<pubDate>2024-05-06T17:59:36Z</pubDate>
</item>
<item>
<title>Vibe-Eval: A hard evaluation suite for measuring progress of multimodal
  language models</title>
<link>http://arxiv.org/abs/2405.02287v1</link>
<guid>http://arxiv.org/abs/2405.02287v1</guid>
<content:encoded><![CDATA[
<div> 评估框架, 多模态对话模型, 困难难度, 人工评估, 自动评估

评估框架Vibe-Eval是一个新的开放式基准测试和评估多模态对话模型的框架。它包括269个视觉理解提示，其中包括100个高难度提示，并配有专家撰写的标准答案。该框架旨在进行日常任务的多模态对话模型的评估，并严格测试和探索当前前沿模型的能力。特别地，高难度提示集中包含了超过50%的问题，所有前沿模型都回答错误。文章探讨了设计、评估和排名模型在极具挑战性提示上的微妙之处。同时讨论了人工评估和自动评估之间的权衡，并显示了使用Reka Core进行自动模型评估与人类判断之间的大致相关性。为了轻量级评估目的，他们提供了免费的API访问，并计划对在Vibe-Eval的自动评分上表现良好的公共模型进行正式的人工评估。他们还发布了评估代码和数据，网址为https://github.com/reka-ai/reka-vibe-eval。<br /><br />总结: 评估框架Vibe-Eval旨在评估多模态对话模型的日常任务能力，包含了大量高难度提示，并探讨了人工评估和自动评估之间的相互关系。他们提供了免费的API访问，并计划对在自动评分上表现良好的公共模型进行正式的人工评估。 <div>
We introduce Vibe-Eval: a new open benchmark and framework for evaluating
multimodal chat models. Vibe-Eval consists of 269 visual understanding prompts,
including 100 of hard difficulty, complete with gold-standard responses
authored by experts. Vibe-Eval is open-ended and challenging with dual
objectives: (i) vibe checking multimodal chat models for day-to-day tasks and
(ii) rigorously testing and probing the capabilities of present frontier
models. Notably, our hard set contains >50% questions that all frontier models
answer incorrectly. We explore the nuances of designing, evaluating, and
ranking models on ultra challenging prompts. We also discuss trade-offs between
human and automatic evaluation, and show that automatic model evaluation using
Reka Core roughly correlates to human judgment. We offer free API access for
the purpose of lightweight evaluation and plan to conduct formal human
evaluations for public models that perform well on the Vibe-Eval's automatic
scores. We release the evaluation code and data, see
https://github.com/reka-ai/reka-vibe-eval
]]></content:encoded>
<pubDate>2024-05-03T17:59:55Z</pubDate>
</item>
<item>
<title>Geometric Fabrics: a Safe Guiding Medium for Policy Learning</title>
<link>http://arxiv.org/abs/2405.02250v1</link>
<guid>http://arxiv.org/abs/2405.02250v1</guid>
<content:encoded><![CDATA[
<div> 强化学习, 机器人政策, 控制器, 二阶动力学, 几何结构

强化学习中的机器人政策必须解密复杂的交互作用，学习如何完成任务。传统控制器只能实现直线运动，无法捕捉机器人需要展现的非线性行为，因此引入了几何结构来实现更丰富和期望的行为。通过人工的二阶动力学来控制机器人的未受控动力学，形成行为动力学，从而解锁新的行为空间并帮助训练强化学习政策。这种行为动力学使得强化学习政策能够安全地执行动作，简化奖励设计，并帮助组合高性能政策。文章将这个框架具体应用到一个问题上，即通过高度驱动的机器人手对一个立方体进行灵巧的手部重新定位。 <div>
Robotics policies are always subjected to complex, second order dynamics that
entangle their actions with resulting states. In reinforcement learning (RL)
contexts, policies have the burden of deciphering these complicated
interactions over massive amounts of experience and complex reward functions to
learn how to accomplish tasks. Moreover, policies typically issue actions
directly to controllers like Operational Space Control (OSC) or joint PD
control, which induces straightline motion towards these action targets in task
or joint space. However, straightline motion in these spaces for the most part
do not capture the rich, nonlinear behavior our robots need to exhibit,
shifting the burden of discovering these behaviors more completely to the
agent. Unlike these simpler controllers, geometric fabrics capture a much
richer and desirable set of behaviors via artificial, second order dynamics
grounded in nonlinear geometry. These artificial dynamics shift the
uncontrolled dynamics of a robot via an appropriate control law to form
behavioral dynamics. Behavioral dynamics unlock a new action space and safe,
guiding behavior over which RL policies are trained. Behavioral dynamics enable
bang-bang-like RL policy actions that are still safe for real robots, simplify
reward engineering, and help sequence real-world, high-performance policies. We
describe the framework more generally and create a specific instantiation for
the problem of dexterous, in-hand reorientation of a cube by a highly actuated
robot hand.
]]></content:encoded>
<pubDate>2024-05-03T17:07:45Z</pubDate>
</item>
<item>
<title>Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon
  Robotics Tasks</title>
<link>http://arxiv.org/abs/2405.01534v1</link>
<guid>http://arxiv.org/abs/2405.01534v1</guid>
<content:encoded><![CDATA[
<div> 高级语言模型，机器人任务规划，技能库，机器人控制任务，Plan-Seq-Learn。

高级语言模型（LLMs）已经表明能够执行长期规划的机器人任务，然而现有方法需要访问预定义的技能库（例如拾取、放置、拉动、推动、导航等）。然而，LLM规划并不解决如何设计或学习这些行为，尤其是在长期规划环境中仍然具有挑战性。此外，对于许多感兴趣的任务，机器人需要能够以细粒度的方式调整其行为，这要求代理能够修改低级控制动作。我们是否可以利用LLMs的互联网规模知识来指导强化学习（RL）策略，从而在不需要预先确定的技能集的情况下在线有效解决机器人控制任务？在本文中，我们提出了Plan-Seq-Learn（PSL）：一种模块化方法，利用运动规划来弥合摘要语言与学习低级控制之间的鸿沟，从头开始解决长期规划的机器人任务。我们展示了PSL在超过25项具有10个阶段的挑战性机器人任务上实现了最先进的成果。PSL利用原始视觉输入解决长期规划任务，覆盖了四个基准任务，在成功率超过85%，优于基于语言、经典和端到端方法。视频结果和代码可在https://mihdalal.github.io/planseqlearn/找到。<br /><br />总结: 高级语言模型在机器人领域具有潜在应用，但现有方法存在一些挑战，Plan-Seq-Learn提出了一种新的模块化方法，有效地解决了长期规划的机器人任务，取得了很好的结果。 <div>
Large Language Models (LLMs) have been shown to be capable of performing
high-level planning for long-horizon robotics tasks, yet existing methods
require access to a pre-defined skill library (e.g. picking, placing, pulling,
pushing, navigating). However, LLM planning does not address how to design or
learn those behaviors, which remains challenging particularly in long-horizon
settings. Furthermore, for many tasks of interest, the robot needs to be able
to adjust its behavior in a fine-grained manner, requiring the agent to be
capable of modifying low-level control actions. Can we instead use the
internet-scale knowledge from LLMs for high-level policies, guiding
reinforcement learning (RL) policies to efficiently solve robotic control tasks
online without requiring a pre-determined set of skills? In this paper, we
propose Plan-Seq-Learn (PSL): a modular approach that uses motion planning to
bridge the gap between abstract language and learned low-level control for
solving long-horizon robotics tasks from scratch. We demonstrate that PSL
achieves state-of-the-art results on over 25 challenging robotics tasks with up
to 10 stages. PSL solves long-horizon tasks from raw visual input spanning four
benchmarks at success rates of over 85%, out-performing language-based,
classical, and end-to-end approaches. Video results and code at
https://mihdalal.github.io/planseqlearn/
]]></content:encoded>
<pubDate>2024-05-02T17:59:31Z</pubDate>
</item>
<item>
<title>OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D
  Perception, Reasoning and Planning</title>
<link>http://arxiv.org/abs/2405.01533v1</link>
<guid>http://arxiv.org/abs/2405.01533v1</guid>
<content:encoded><![CDATA[
<div> 3D MLLM，autonomous driving agents，OmniDrive-nuScenes，visual question-answering，3D situational awareness<br />
<br />
提出了一个整体框架，通过使用稀疏查询将视觉表示转换成3D形式，结合动态对象和静态地图元素的信息，为3D世界模型提供压缩表示，以实现3D感知-行动对齐。还提出了OmniDrive-nuScenes数据集，通过视觉问答等多种任务评估模型的真实3D情境认知能力。研究表明，所提出的架构有效性强，同时视觉问答任务对复杂3D场景中的推理和规划至关重要。<br /><br />总结: <div>
The advances in multimodal large language models (MLLMs) have led to growing
interests in LLM-based autonomous driving agents to leverage their strong
reasoning capabilities. However, capitalizing on MLLMs' strong reasoning
capabilities for improved planning behavior is challenging since planning
requires full 3D situational awareness beyond 2D reasoning. To address this
challenge, our work proposes a holistic framework for strong alignment between
agent models and 3D driving tasks. Our framework starts with a novel 3D MLLM
architecture that uses sparse queries to lift and compress visual
representations into 3D before feeding them into an LLM. This query-based
representation allows us to jointly encode dynamic objects and static map
elements (e.g., traffic lanes), providing a condensed world model for
perception-action alignment in 3D. We further propose OmniDrive-nuScenes, a new
visual question-answering dataset challenging the true 3D situational awareness
of a model with comprehensive visual question-answering (VQA) tasks, including
scene description, traffic regulation, 3D grounding, counterfactual reasoning,
decision making and planning. Extensive studies show the effectiveness of the
proposed architecture as well as the importance of the VQA tasks for reasoning
and planning in complex 3D scenes.
]]></content:encoded>
<pubDate>2024-05-02T17:59:24Z</pubDate>
</item>
<item>
<title>No Representation, No Trust: Connecting Representation, Collapse, and
  Trust Issues in PPO</title>
<link>http://arxiv.org/abs/2405.00662v1</link>
<guid>http://arxiv.org/abs/2405.00662v1</guid>
<content:encoded><![CDATA[
<div> 非稳态、深度强化学习、表示动态、Proximal Policy Optimization、性能崩溃<br />
在这项工作中，研究人员对Proximal Policy Optimization（PPO）在Atari和MuJoCo环境中的表示动态进行了实证研究，发现PPO代理也受到特征排名恶化和可塑性丧失的影响。他们展示了这种情况在非稳态性较强时会加剧，最终导致actor的性能崩溃，而与评论家的性能无关。作者还指出了表示崩溃、性能崩溃和PPO中的信任区域问题之间的联系，并提出了一种新的辅助损失Proximal Feature Optimization（PFO），通过其他干预措施表明，规范化表示动态可以改善PPO代理的性能。<br /><br />总结: 非稳态环境中，深度强化学习中的表示动态对性能具有重要影响，特别是在Proximal Policy Optimization中，这种影响会导致性能崩溃。通过实证研究，研究人员发现了这种影响，并提出了Proximal Feature Optimization作为解决方案，通过规范化表示动态来改善代理的性能。 <div>
Reinforcement learning (RL) is inherently rife with non-stationarity since
the states and rewards the agent observes during training depend on its
changing policy. Therefore, networks in deep RL must be capable of adapting to
new observations and fitting new targets. However, previous works have observed
that networks in off-policy deep value-based methods exhibit a decrease in
representation rank, often correlated with an inability to continue learning or
a collapse in performance. Although this phenomenon has generally been
attributed to neural network learning under non-stationarity, it has been
overlooked in on-policy policy optimization methods which are often thought
capable of training indefinitely. In this work, we empirically study
representation dynamics in Proximal Policy Optimization (PPO) on the Atari and
MuJoCo environments, revealing that PPO agents are also affected by feature
rank deterioration and loss of plasticity. We show that this is aggravated with
stronger non-stationarity, ultimately driving the actor's performance to
collapse, regardless of the performance of the critic. We draw connections
between representation collapse, performance collapse, and trust region issues
in PPO, and present Proximal Feature Optimization (PFO), a novel auxiliary
loss, that along with other interventions shows that regularizing the
representation dynamics improves the performance of PPO agents.
]]></content:encoded>
<pubDate>2024-05-01T17:50:16Z</pubDate>
</item>
<item>
<title>DOCCI: Descriptions of Connected and Contrasting Images</title>
<link>http://arxiv.org/abs/2404.19753v1</link>
<guid>http://arxiv.org/abs/2404.19753v1</guid>
<content:encoded><![CDATA[
<div> 视觉-语言数据集；描述；DOCCI；图像到文本生成；文本到图像生成

<br /><br />总结:
视觉-语言数据集对于文本到图像（T2I）和图像到文本（I2T）研究至关重要。然而，目前的数据集缺乏细致详细的描述，这些描述可以让模型学习到更丰富的关联信息。为了填补这一空白，我们介绍了DOCCI（Descriptions of Connected and Contrasting Images），这是一个包含长篇人工注释英文描述的数据集，涵盖了1.5万张图像。我们指示人工注释者为每个图像创建全面的描述，这些描述平均有136个词，并且被精心设计，以清晰地区分每个图像与相关或相似的其他图像。通过定量和定性分析，我们证明DOCCI是一个有效的图像到文本生成训练资源。此外，我们还展示了DOCCI对于文本到图像生成的有用性，并凸显了当前文本到图像模型在捕捉长篇描述和细节方面的局限性。 <div>
Vision-language datasets are vital for both text-to-image (T2I) and
image-to-text (I2T) research. However, current datasets lack descriptions with
fine-grained detail that would allow for richer associations to be learned by
models. To fill the gap, we introduce Descriptions of Connected and Contrasting
Images (DOCCI), a dataset with long, human-annotated English descriptions for
15k images that were taken, curated and donated by a single researcher intent
on capturing key challenges such as spatial relations, counting, text
rendering, world knowledge, and more. We instruct human annotators to create
comprehensive descriptions for each image; these average 136 words in length
and are crafted to clearly distinguish each image from those that are related
or similar. Each description is highly compositional and typically encompasses
multiple challenges. Through both quantitative and qualitative analyses, we
demonstrate that DOCCI serves as an effective training resource for
image-to-text generation -- a PaLI 5B model finetuned on DOCCI shows equal or
superior results compared to highly-performant larger models like LLaVA-1.5 7B
and InstructBLIP 7B. Furthermore, we show that DOCCI is a useful testbed for
text-to-image generation, highlighting the limitations of current text-to-image
models in capturing long descriptions and fine details.
]]></content:encoded>
<pubDate>2024-04-30T17:56:24Z</pubDate>
</item>
<item>
<title>Hallucination of Multimodal Large Language Models: A Survey</title>
<link>http://arxiv.org/abs/2404.18930v1</link>
<guid>http://arxiv.org/abs/2404.18930v1</guid>
<content:encoded><![CDATA[
<div> 语言模型；视觉-语言模型；幻觉；评估方法；挑战

幻觉在多模态大语言模型（MLLMs）中是一个严重的问题，也被称为大视觉-语言模型（LVLMs）。文章综述了在MLLMs中出现幻觉的原因、评估方法以及缓解策略。同时指出了当前挑战和限制，并提出了未来研究的方向。通过深入的分析和细致的分类，为进一步提升MLLMs的健壮性和可靠性提供了宝贵的见解和资源。 <div>
This survey presents a comprehensive analysis of the phenomenon of
hallucination in multimodal large language models (MLLMs), also known as Large
Vision-Language Models (LVLMs), which have demonstrated significant
advancements and remarkable abilities in multimodal tasks. Despite these
promising developments, MLLMs often generate outputs that are inconsistent with
the visual content, a challenge known as hallucination, which poses substantial
obstacles to their practical deployment and raises concerns regarding their
reliability in real-world applications. This problem has attracted increasing
attention, prompting efforts to detect and mitigate such inaccuracies. We
review recent advances in identifying, evaluating, and mitigating these
hallucinations, offering a detailed overview of the underlying causes,
evaluation benchmarks, metrics, and strategies developed to address this issue.
Additionally, we analyze the current challenges and limitations, formulating
open questions that delineate potential pathways for future research. By
drawing the granular classification and landscapes of hallucination causes,
evaluation benchmarks, and mitigation methods, this survey aims to deepen the
understanding of hallucinations in MLLMs and inspire further advancements in
the field. Through our thorough and in-depth review, we contribute to the
ongoing dialogue on enhancing the robustness and reliability of MLLMs,
providing valuable insights and resources for researchers and practitioners
alike. Resources are available at:
https://github.com/showlab/Awesome-MLLM-Hallucination.
]]></content:encoded>
<pubDate>2024-04-29T17:59:41Z</pubDate>
</item>
<item>
<title>Stylus: Automatic Adapter Selection for Diffusion Models</title>
<link>http://arxiv.org/abs/2404.18928v1</link>
<guid>http://arxiv.org/abs/2404.18928v1</guid>
<content:encoded><![CDATA[
<div> Stylus, adapters, fine-tuned, matching, prompt

总结: 本文介绍了使用适配器来生成高保真度定制图像的替代方法，以降低成本。文章探讨了将适配器与提示匹配的问题，并提出了一种名为Stylus的解决方案。Stylus通过三个阶段的方法来高效地选择和自动组合任务特定的适配器，从而提高了模型的性能，并在评估中取得了良好的效果。作者还开发了一个包含75K适配器的数据集StylusDocs，并在常见的Stable Diffusion检查点上进行了评估，结果表明Stylus在CLIP-FID Pareto效率方面表现更好，并且被人类和多模型评估者比基础模型更受欢迎。 <div>
Beyond scaling base models with more data or parameters, fine-tuned adapters
provide an alternative way to generate high fidelity, custom images at reduced
costs. As such, adapters have been widely adopted by open-source communities,
accumulating a database of over 100K adapters-most of which are highly
customized with insufficient descriptions. This paper explores the problem of
matching the prompt to a set of relevant adapters, built on recent work that
highlight the performance gains of composing adapters. We introduce Stylus,
which efficiently selects and automatically composes task-specific adapters
based on a prompt's keywords. Stylus outlines a three-stage approach that first
summarizes adapters with improved descriptions and embeddings, retrieves
relevant adapters, and then further assembles adapters based on prompts'
keywords by checking how well they fit the prompt. To evaluate Stylus, we
developed StylusDocs, a curated dataset featuring 75K adapters with
pre-computed adapter embeddings. In our evaluation on popular Stable Diffusion
checkpoints, Stylus achieves greater CLIP-FID Pareto efficiency and is twice as
preferred, with humans and multimodal models as evaluators, over the base
model. See stylus-diffusion.github.io for more.
]]></content:encoded>
<pubDate>2024-04-29T17:59:16Z</pubDate>
</item>
<item>
<title>TheaterGen: Character Management with LLM for Consistent Multi-turn
  Image Generation</title>
<link>http://arxiv.org/abs/2404.18919v1</link>
<guid>http://arxiv.org/abs/2404.18919v1</guid>
<content:encoded><![CDATA[
Recent advances in diffusion models can generate high-quality and stunning
images from text. However, multi-turn image generation, which is of high demand
in real-world scenarios, still faces challenges in maintaining semantic
consistency between images and texts, as well as contextual consistency of the
same subject across multiple interactive turns. To address this issue, we
introduce TheaterGen, a training-free framework that integrates large language
models (LLMs) and text-to-image (T2I) models to provide the capability of
multi-turn image generation. Within this framework, LLMs, acting as a
"Screenwriter", engage in multi-turn interaction, generating and managing a
standardized prompt book that encompasses prompts and layout designs for each
character in the target image. Based on these, Theatergen generate a list of
character images and extract guidance information, akin to the "Rehearsal".
Subsequently, through incorporating the prompt book and guidance information
into the reverse denoising process of T2I diffusion models, Theatergen generate
the final image, as conducting the "Final Performance". With the effective
management of prompt books and character images, TheaterGen significantly
improves semantic and contextual consistency in synthesized images.
Furthermore, we introduce a dedicated benchmark, CMIGBench (Consistent
Multi-turn Image Generation Benchmark) with 8000 multi-turn instructions.
Different from previous multi-turn benchmarks, CMIGBench does not define
characters in advance. Both the tasks of story generation and multi-turn
editing are included on CMIGBench for comprehensive evaluation. Extensive
experimental results show that TheaterGen outperforms state-of-the-art methods
significantly. It raises the performance bar of the cutting-edge Mini DALLE 3
model by 21% in average character-character similarity and 19% in average
text-image similarity.
]]></content:encoded>
<pubDate>2024-04-29T17:58:14Z</pubDate>
</item>
<item>
<title>Sample-Efficient Robust Multi-Agent Reinforcement Learning in the Face
  of Environmental Uncertainty</title>
<link>http://arxiv.org/abs/2404.18909v1</link>
<guid>http://arxiv.org/abs/2404.18909v1</guid>
<content:encoded><![CDATA[
To overcome the sim-to-real gap in reinforcement learning (RL), learned
policies must maintain robustness against environmental uncertainties. While
robust RL has been widely studied in single-agent regimes, in multi-agent
environments, the problem remains understudied -- despite the fact that the
problems posed by environmental uncertainties are often exacerbated by
strategic interactions. This work focuses on learning in distributionally
robust Markov games (RMGs), a robust variant of standard Markov games, wherein
each agent aims to learn a policy that maximizes its own worst-case performance
when the deployed environment deviates within its own prescribed uncertainty
set. This results in a set of robust equilibrium strategies for all agents that
align with classic notions of game-theoretic equilibria. Assuming a
non-adaptive sampling mechanism from a generative model, we propose a
sample-efficient model-based algorithm (DRNVI) with finite-sample complexity
guarantees for learning robust variants of various notions of game-theoretic
equilibria. We also establish an information-theoretic lower bound for solving
RMGs, which confirms the near-optimal sample complexity of DRNVI with respect
to problem-dependent factors such as the size of the state space, the target
accuracy, and the horizon length.
]]></content:encoded>
<pubDate>2024-04-29T17:51:47Z</pubDate>
</item>
<item>
<title>Learning Visuotactile Skills with Two Multifingered Hands</title>
<link>http://arxiv.org/abs/2404.16823v1</link>
<guid>http://arxiv.org/abs/2404.16823v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 人体灵巧性，双手系统，触觉数据，数据收集，机械手
总结:
人们致力于通过双手系统和视触觉数据来模拟人类的灵巧性、感知体验和运动模式。他们面临两个挑战：缺乏适用于多指手的双臂设置的可负担和可访问的远程操作系统，以及缺乏配备触觉传感器的多指手硬件。为了解决这些问题，他们开发了一种低成本的双手-臂远程操作系统HATO，同时引入了一种新的硬件适应方法，利用触觉传感器装备了两个假肢手来进行研究。利用他们系统收集的视触觉数据，他们学习完成长时间、高精度任务的技能，进一步实证研究了数据集大小、感知模式和视觉输入预处理对策略学习的影响。他们的研究结果标志着从视触觉数据中实现双手多指操作迈出了一大步。 <div>
Aiming to replicate human-like dexterity, perceptual experiences, and motion
patterns, we explore learning from human demonstrations using a bimanual system
with multifingered hands and visuotactile data. Two significant challenges
exist: the lack of an affordable and accessible teleoperation system suitable
for a dual-arm setup with multifingered hands, and the scarcity of
multifingered hand hardware equipped with touch sensing. To tackle the first
challenge, we develop HATO, a low-cost hands-arms teleoperation system that
leverages off-the-shelf electronics, complemented with a software suite that
enables efficient data collection; the comprehensive software suite also
supports multimodal data processing, scalable policy learning, and smooth
policy deployment. To tackle the latter challenge, we introduce a novel
hardware adaptation by repurposing two prosthetic hands equipped with touch
sensors for research. Using visuotactile data collected from our system, we
learn skills to complete long-horizon, high-precision tasks which are difficult
to achieve without multifingered dexterity and touch feedback. Furthermore, we
empirically investigate the effects of dataset size, sensing modality, and
visual input preprocessing on policy learning. Our results mark a promising
step forward in bimanual multifingered manipulation from visuotactile data.
Videos, code, and datasets can be found at https://toruowo.github.io/hato/ .
]]></content:encoded>
<pubDate>2024-04-25T17:59:41Z</pubDate>
</item>
<item>
<title>How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal
  Models with Open-Source Suites</title>
<link>http://arxiv.org/abs/2404.16821v1</link>
<guid>http://arxiv.org/abs/2404.16821v1</guid>
<content:encoded><![CDATA[
<div> 关键词: InternVL 1.5, 多模态大语言模型, 视觉编码器, 高分辨率, 双语数据集

总结:
InternVL 1.5是一个开源的多模态大语言模型，通过引入三项简单的改进，架起了开源和专有商业模型在多模态理解方面的巨大差距。首先是强大的视觉编码器，通过不断学习的策略提升了视觉基础模型InternViT-6B的能力，使其可以在不同的LLM中转移和重复使用。其次是动态高分辨率，将图像根据输入图像的长宽比和分辨率划分为1到40个448×448像素的图块，支持高达4K分辨率的输入。第三是高质量的双语数据集，精心收集了涵盖常见场景、文档图像的双语数据集，并用英文和中文问答对进行注释，显著提升了OCR和中文相关任务的性能。通过一系列基准测试和比较研究，InternVL 1.5表现出了与开源和专有模型的竞争性能，成绩在18项基准测试中有8项达到了最先进的水平。Code已发布在https://github.com/OpenGVLab/InternVL。<br /><br /> <div>
In this report, we introduce InternVL 1.5, an open-source multimodal large
language model (MLLM) to bridge the capability gap between open-source and
proprietary commercial models in multimodal understanding. We introduce three
simple improvements: (1) Strong Vision Encoder: we explored a continuous
learning strategy for the large-scale vision foundation model -- InternViT-6B,
boosting its visual understanding capabilities, and making it can be
transferred and reused in different LLMs. (2) Dynamic High-Resolution: we
divide images into tiles ranging from 1 to 40 of 448$\times$448 pixels
according to the aspect ratio and resolution of the input images, which
supports up to 4K resolution input. (3) High-Quality Bilingual Dataset: we
carefully collected a high-quality bilingual dataset that covers common scenes,
document images, and annotated them with English and Chinese question-answer
pairs, significantly enhancing performance in OCR- and Chinese-related tasks.
We evaluate InternVL 1.5 through a series of benchmarks and comparative
studies. Compared to both open-source and proprietary models, InternVL 1.5
shows competitive performance, achieving state-of-the-art results in 8 of 18
benchmarks. Code has been released at https://github.com/OpenGVLab/InternVL.
]]></content:encoded>
<pubDate>2024-04-25T17:59:19Z</pubDate>
</item>
<item>
<title>DPO: Differential reinforcement learning with application to optimal
  configuration search</title>
<link>http://arxiv.org/abs/2404.15617v1</link>
<guid>http://arxiv.org/abs/2404.15617v1</guid>
<content:encoded><![CDATA[
<div> 强化学习、连续状态空间、连续动作空间、Differential Policy Optimization (DPO)、遗憾上界
<br />
本文提出了第一个可以处理训练样本有限且具有短长度回合的差分强化学习框架DPO。该方法引入了差分策略优化（DPO），这是一种逐点和逐阶段迭代方法，可以优化由局部移动算子编码的策略。研究证明了DPO的逐点收敛估计，并提供了与当前理论研究相当的遗憾上界。此类逐点估计确保了学习的策略在不同步骤间能够均匀匹配最优路径。然后，DPO被应用于一类具有Lagrangian奖励的实际强化学习问题。DPO易于实现，可扩展，并在对比基准实验中表现出与几种流行的强化学习方法相媲美的结果。
<br /><br />总结: <br />本文介绍了一种新的差分强化学习框架DPO，能够处理具有有限训练样本和短长度回合的情况。该方法通过优化局部移动算子编码的策略，实现了逐点收敛和与当前理论研究相当的遗憾上界。在实际应用中，DPO表现出了可扩展性和竞争力。 <div>
Reinforcement learning (RL) with continuous state and action spaces remains
one of the most challenging problems within the field. Most current learning
methods focus on integral identities such as value functions to derive an
optimal strategy for the learning agent. In this paper, we instead study the
dual form of the original RL formulation to propose the first differential RL
framework that can handle settings with limited training samples and
short-length episodes. Our approach introduces Differential Policy Optimization
(DPO), a pointwise and stage-wise iteration method that optimizes policies
encoded by local-movement operators. We prove a pointwise convergence estimate
for DPO and provide a regret bound comparable with current theoretical works.
Such pointwise estimate ensures that the learned policy matches the optimal
path uniformly across different steps. We then apply DPO to a class of
practical RL problems which search for optimal configurations with Lagrangian
rewards. DPO is easy to implement, scalable, and shows competitive results on
benchmarking experiments against several popular RL methods.
]]></content:encoded>
<pubDate>2024-04-24T03:11:12Z</pubDate>
</item>
<item>
<title>ID-Animator: Zero-Shot Identity-Preserving Human Video Generation</title>
<link>http://arxiv.org/abs/2404.15275v1</link>
<guid>http://arxiv.org/abs/2404.15275v1</guid>
<content:encoded><![CDATA[
<div> 视频生成、人脸识别、数据集构建、模型优势、代码开源
<br /><br />
总结:
本研究提出了一种零样本人体视频生成方法ID-Animator，能在没有额外训练的情况下根据单个面部参考图像执行个性化视频生成。该方法利用面部适配器从可学习的面部潜在查询中编码身份相关嵌入，以促进视频生成过程中的身份信息提取。此外，研究还介绍了一个面向身份的数据集构建流水线，利用构建的面部图像池的解耦人类属性和行为字幕技术。在此基础上，进一步设计了随机面部参考训练方法，以精确捕获参考图像中的身份相关嵌入，从而提高模型对个性化视频生成的保真度和泛化能力。大量实验证明，ID-Animator相对于以前的模型在生成个性化人体视频方面具有显著优势。此外，我们的方法与流行的预训练T2V模型（如animatediff）和各种社区骨干模型高度兼容，在视频生成的实际应用中显示出很高的可扩展性，这在需要高度保留身份的视频生成中非常理想。我们的代码和检查点将在https://github.com/ID-Animator/ID-Animator上发布。 <div>
Generating high fidelity human video with specified identities has attracted
significant attention in the content generation community. However, existing
techniques struggle to strike a balance between training efficiency and
identity preservation, either requiring tedious case-by-case finetuning or
usually missing the identity details in video generation process. In this
study, we present ID-Animator, a zero-shot human-video generation approach that
can perform personalized video generation given single reference facial image
without further training. ID-Animator inherits existing diffusion-based video
generation backbones with a face adapter to encode the ID-relevant embeddings
from learnable facial latent queries. To facilitate the extraction of identity
information in video generation, we introduce an ID-oriented dataset
construction pipeline, which incorporates decoupled human attribute and action
captioning technique from a constructed facial image pool. Based on this
pipeline, a random face reference training method is further devised to
precisely capture the ID-relevant embeddings from reference images, thus
improving the fidelity and generalization capacity of our model for ID-specific
video generation. Extensive experiments demonstrate the superiority of
ID-Animator to generate personalized human videos over previous models.
Moreover, our method is highly compatible with popular pre-trained T2V models
like animatediff and various community backbone models, showing high
extendability in real-world applications for video generation where identity
preservation is highly desired. Our codes and checkpoints will be released at
https://github.com/ID-Animator/ID-Animator.
]]></content:encoded>
<pubDate>2024-04-23T17:59:43Z</pubDate>
</item>
<item>
<title>Estimation Network Design framework for efficient distributed
  optimization</title>
<link>http://arxiv.org/abs/2404.15273v1</link>
<guid>http://arxiv.org/abs/2404.15273v1</guid>
<content:encoded><![CDATA[
<div> 分布式决策问题, 代理, 估计网络设计, 稀疏性, 优化

分布式决策问题涉及一组只能通过点对点网络进行通信的代理，没有中央内存。在应用中，如网络控制和数据排名，每个代理只受决策向量的一小部分影响：这种稀疏性通常在分布式算法中被忽略，而它可以提高效率和可扩展性。为了解决这个问题，我们最近的论文引入了估计网络设计(END)，这是一个用于分析和设计分布式迭代的图论语言。END算法可以调整以利用特定问题实例的稀疏性，减少通信开销，最小化冗余，而不需要逐个案例的收敛分析。在本文中，我们展示了END在分布式优化领域的灵活性。特别是，我们研究了许多已有方法的稀疏感知版本，包括ADMM、AugDGM和Push-Sum DGD。对传感器网络中的估计问题进行的模拟表明，END算法可以提高收敛速度，并大大减少通信和内存成本。

<br /><br />总结: 本文介绍了一种新的图论语言END，用于分析和设计分布式迭代算法。这种算法可以根据具体问题实例的稀疏性进行调整，减少通信开销和冗余，不需要逐个案例的收敛分析。通过模拟传感器网络中的估计问题，展示了END算法可以提高收敛速度并大大降低通信和内存成本。 <div>
Distributed decision problems features a group of agents that can only
communicate over a peer-to-peer network, without a central memory. In
applications such as network control and data ranking, each agent is only
affected by a small portion of the decision vector: this sparsity is typically
ignored in distributed algorithms, while it could be leveraged to improve
efficiency and scalability. To address this issue, our recent paper introduces
Estimation Network Design (END), a graph theoretical language for the analysis
and design of distributed iterations. END algorithms can be tuned to exploit
the sparsity of specific problem instances, reducing communication overhead and
minimizing redundancy, yet without requiring case-by-case convergence analysis.
In this paper, we showcase the flexility of END in the context of distributed
optimization. In particular, we study the sparsity-aware version of many
established methods, including ADMM, AugDGM and Push-Sum DGD. Simulations on an
estimation problem in sensor networks demonstrate that END algorithms can boost
convergence speed and greatly reduce the communication and memory cost.
]]></content:encoded>
<pubDate>2024-04-23T17:59:09Z</pubDate>
</item>
<item>
<title>CT-GLIP: 3D Grounded Language-Image Pretraining with CT Scans and
  Radiology Reports for Full-Body Scenarios</title>
<link>http://arxiv.org/abs/2404.15272v1</link>
<guid>http://arxiv.org/abs/2404.15272v1</guid>
<content:encoded><![CDATA[
Medical Vision-Language Pretraining (Med-VLP) establishes a connection
between visual content from medical images and the relevant textual
descriptions. Existing Med-VLP methods primarily focus on 2D images depicting a
single body part, notably chest X-rays. In this paper, we extend the scope of
Med-VLP to encompass 3D images, specifically targeting full-body scenarios, by
using a multimodal dataset of CT images and reports. Compared with the 2D
counterpart, 3D VLP is required to effectively capture essential semantics from
significantly sparser representation in 3D imaging. In this paper, we introduce
CT-GLIP (Grounded Language-Image Pretraining with CT scans), a novel method
that constructs organ-level image-text pairs to enhance multimodal contrastive
learning, aligning grounded visual features with precise diagnostic text.
Additionally, we developed an abnormality dictionary to augment contrastive
learning with diverse negative samples. Our method, trained on a multimodal CT
dataset comprising 44,011 organ-level vision-text pairs from 17,702 patients
across 104 organs, demonstrates it can identify organs and abnormalities in a
zero-shot manner using natural languages. The performance of CT-GLIP is
validated on a separate test set of 1,130 patients, focusing on the 16 most
frequent abnormalities across 7 organs. The experimental results show our
model's superior performance over the standard CLIP framework across zero-shot
and fine-tuning scenarios, using both CNN and ViT architectures.
]]></content:encoded>
<pubDate>2024-04-23T17:59:01Z</pubDate>
</item>
<item>
<title>Automatic Layout Planning for Visually-Rich Documents with
  Instruction-Following Models</title>
<link>http://arxiv.org/abs/2404.15271v1</link>
<guid>http://arxiv.org/abs/2404.15271v1</guid>
<content:encoded><![CDATA[
Recent advancements in instruction-following models have made user
interactions with models more user-friendly and efficient, broadening their
applicability. In graphic design, non-professional users often struggle to
create visually appealing layouts due to limited skills and resources. In this
work, we introduce a novel multimodal instruction-following framework for
layout planning, allowing users to easily arrange visual elements into tailored
layouts by specifying canvas size and design purpose, such as for book covers,
posters, brochures, or menus. We developed three layout reasoning tasks to
train the model in understanding and executing layout instructions. Experiments
on two benchmarks show that our method not only simplifies the design process
for non-professionals but also surpasses the performance of few-shot GPT-4V
models, with mIoU higher by 12% on Crello. This progress highlights the
potential of multimodal instruction-following models to automate and simplify
the design process, providing an approachable solution for a wide range of
design tasks on visually-rich documents.
]]></content:encoded>
<pubDate>2024-04-23T17:58:33Z</pubDate>
</item>
<item>
<title>Aligning LLM Agents by Learning Latent Preference from User Edits</title>
<link>http://arxiv.org/abs/2404.15269v1</link>
<guid>http://arxiv.org/abs/2404.15269v1</guid>
<content:encoded><![CDATA[
We study interactive learning of language agents based on user edits made to
the agent's output. In a typical setting such as writing assistants, the user
interacts with a language agent to generate a response given a context, and may
optionally edit the agent response to personalize it based on their latent
preference, in addition to improving the correctness. The edit feedback is
naturally generated, making it a suitable candidate for improving the agent's
alignment with the user's preference, and for reducing the cost of user edits
over time. We propose a learning framework, PRELUDE that infers a description
of the user's latent preference based on historic edit data and using it to
define a prompt policy that drives future response generation. This avoids
fine-tuning the agent, which is costly, challenging to scale with the number of
users, and may even degrade its performance on other tasks. Furthermore,
learning descriptive preference improves interpretability, allowing the user to
view and modify the learned preference. However, user preference can be complex
and vary based on context, making it challenging to learn. To address this, we
propose a simple yet effective algorithm named CIPHER that leverages a large
language model (LLM) to infer the user preference for a given context based on
user edits. In the future, CIPHER retrieves inferred preferences from the
k-closest contexts in the history, and forms an aggregate preference for
response generation. We introduce two interactive environments -- summarization
and email writing, for evaluation using a GPT-4 simulated user. We compare with
algorithms that directly retrieve user edits but do not learn descriptive
preference, and algorithms that learn context-agnostic preference. On both
tasks, CIPHER achieves the lowest edit distance cost and learns preferences
that show significant similarity to the ground truth preferences
]]></content:encoded>
<pubDate>2024-04-23T17:57:47Z</pubDate>
</item>
<item>
<title>From Parts to Whole: A Unified Reference Framework for Controllable
  Human Image Generation</title>
<link>http://arxiv.org/abs/2404.15267v1</link>
<guid>http://arxiv.org/abs/2404.15267v1</guid>
<content:encoded><![CDATA[
Recent advancements in controllable human image generation have led to
zero-shot generation using structural signals (e.g., pose, depth) or facial
appearance. Yet, generating human images conditioned on multiple parts of human
appearance remains challenging. Addressing this, we introduce Parts2Whole, a
novel framework designed for generating customized portraits from multiple
reference images, including pose images and various aspects of human
appearance. To achieve this, we first develop a semantic-aware appearance
encoder to retain details of different human parts, which processes each image
based on its textual label to a series of multi-scale feature maps rather than
one image token, preserving the image dimension. Second, our framework supports
multi-image conditioned generation through a shared self-attention mechanism
that operates across reference and target features during the diffusion
process. We enhance the vanilla attention mechanism by incorporating mask
information from the reference human images, allowing for the precise selection
of any part. Extensive experiments demonstrate the superiority of our approach
over existing alternatives, offering advanced capabilities for multi-part
controllable human image customization. See our project page at
https://huanngzh.github.io/Parts2Whole/.
]]></content:encoded>
<pubDate>2024-04-23T17:56:08Z</pubDate>
</item>
<item>
<title>MoVA: Adapting Mixture of Vision Experts to Multimodal Context</title>
<link>http://arxiv.org/abs/2404.13046v1</link>
<guid>http://arxiv.org/abs/2404.13046v1</guid>
<content:encoded><![CDATA[
<div> 大语言模型、视觉编码器、MoVA、多模态理解、图像内容理解<br />
本文介绍了MoVA，这是一种新型的多模态大语言模型，通过采用粗到细的机制，自适应地路由和融合任务特定的视觉专家。在粗粒度阶段，使用上下文感知的专家路由策略动态选择最合适的视觉专家，利用大语言模型具有的专家路由低秩自适应（LoRA）能力。在细粒度阶段，使用混合视觉专家适配器（MoV-Adapter）从各种专家中提取和融合特定任务的知识。这种粗到细的范式有效地利用了基于多模态上下文和模型专业知识的专家表示，进一步增强了泛化能力。在广泛的实验中，MoVA在各种具有挑战性的多模态基准测试中取得了显著的性能提升，而无需任何额外复杂的设置。MoVA的代码和模型可以在https://github.com/TempleX98/MoVA上找到。 <br /><br />总结: 大语言模型在多模态理解中发挥重要作用，MoVA通过粗到细的机制，自适应地路由和融合任务特定的视觉专家，取得了显著的性能提升。 <div>
As the key component in multimodal large language models (MLLMs), the ability
of the visual encoder greatly affects MLLM's understanding on diverse image
content. Although some large-scale pretrained vision encoders such as vision
encoders in CLIP and DINOv2 have brought promising performance, we found that
there is still no single vision encoder that can dominate various image content
understanding, e.g., the CLIP vision encoder leads to outstanding results on
general image understanding but poor performance on document or chart content.
To alleviate the bias of CLIP vision encoder, we first delve into the inherent
behavior of different pre-trained vision encoders and then propose the MoVA, a
powerful and novel MLLM, adaptively routing and fusing task-specific vision
experts with a coarse-to-fine mechanism. In the coarse-grained stage, we design
a context-aware expert routing strategy to dynamically select the most suitable
vision experts according to the user instruction, input image, and expertise of
vision experts. This benefits from the powerful model function understanding
ability of the large language model (LLM) equipped with expert-routing low-rank
adaptation (LoRA). In the fine-grained stage, we elaborately conduct the
mixture-of-vision-expert adapter (MoV-Adapter) to extract and fuse
task-specific knowledge from various experts. This coarse-to-fine paradigm
effectively leverages representations from experts based on multimodal context
and model expertise, further enhancing the generalization ability. We conduct
extensive experiments to evaluate the effectiveness of the proposed approach.
Without any bells and whistles, MoVA can achieve significant performance gains
over current state-of-the-art methods in a wide range of challenging multimodal
benchmarks. Codes and models will be available at
https://github.com/TempleX98/MoVA.
]]></content:encoded>
<pubDate>2024-04-19T17:59:48Z</pubDate>
</item>
<item>
<title>BLINK: Multimodal Large Language Models Can See but Not Perceive</title>
<link>http://arxiv.org/abs/2404.12390v1</link>
<guid>http://arxiv.org/abs/2404.12390v1</guid>
<content:encoded><![CDATA[
<div> benchmark, multimodal language models, visual perception, challenges, improvement
<br /><br />总结:
研究介绍了Blink，一个针对多模态语言模型(LLMs)的新基准，专注于核心视觉感知能力，这在其他评估中找不到。Blink的任务大多可以在人类“眨眼”之内解决，例如相对深度估计、视觉对应、取证检测和多视角推理。然而，研究发现这些对感知能力要求很高的任务对当前多模态LLMs构成重大挑战，因为它们无法通过自然语言进行调解。Blink将14个经典计算机视觉任务重新格式化为3,807个多项选择题，配备单个或多个图像和视觉提示。人类平均准确率达到95.70%，但对现有多模态LLMs来说却具有挑战性。即使是性能最好的GPT-4V和Gemini的准确率也分别只有51.26%和45.72%，仅比随机猜测高出13.17%和7.63%，这表明这些感知能力在最近的多模态LLMs中尚未“出现”。研究还强调，专业的CV模型可以更好地解决这些问题，为未来的改进提供了潜在途径。我们相信Blink将激发社区帮助多模态LLMs赶上人类级别的视觉感知水平。 <div>
We introduce Blink, a new benchmark for multimodal language models (LLMs)
that focuses on core visual perception abilities not found in other
evaluations. Most of the Blink tasks can be solved by humans "within a blink"
(e.g., relative depth estimation, visual correspondence, forensics detection,
and multi-view reasoning). However, we find these perception-demanding tasks
cast significant challenges for current multimodal LLMs because they resist
mediation through natural language. Blink reformats 14 classic computer vision
tasks into 3,807 multiple-choice questions, paired with single or multiple
images and visual prompting. While humans get 95.70% accuracy on average, Blink
is surprisingly challenging for existing multimodal LLMs: even the
best-performing GPT-4V and Gemini achieve accuracies of 51.26% and 45.72%, only
13.17% and 7.63% higher than random guessing, indicating that such perception
abilities have not "emerged" yet in recent multimodal LLMs. Our analysis also
highlights that specialist CV models could solve these problems much better,
suggesting potential pathways for future improvements. We believe Blink will
stimulate the community to help multimodal LLMs catch up with human-level
visual perception.
]]></content:encoded>
<pubDate>2024-04-18T17:59:54Z</pubDate>
</item>
<item>
<title>Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language
  Models</title>
<link>http://arxiv.org/abs/2404.12387v1</link>
<guid>http://arxiv.org/abs/2404.12387v1</guid>
<content:encoded><![CDATA[
<div> 模型, 训练, 评估, 多模态, Reka
<br /><br />
总结: 
这篇技术报告介绍了Reka Core, Flash和Edge，它们是强大的多模态语言模型，能够处理和推理文本、图像、视频和音频输入。报告讨论了其中一些模型的训练细节，并提供了全面的评估结果。Reka Edge和Reka Flash不仅达到了行业最新水平，而且在性能上超过了许多更大的模型，在其计算类别上获得了巨大的价值。与此同时，Reka最强大和最大的模型Reka Core在自动评估和盲目人类评估方面接近最先进的模型。在图像问答基准测试（如MMMU、VQAv2）上，Core的表现与GPT4-V相媲美。在多模态聊天方面，Core在盲目第三方人类评估设置下排名第二，超越了其他模型，如Claude 3 Opus。在文本基准测试上，Core不仅在一系列公认的基准测试（如MMLU、GSM8K）上表现出色，而且在人类评估上超越了GPT4-0613。在视频问答（Perception-Test）方面，Core的表现超过了Gemini Ultra。这些模型已经在http://chat.reka.ai 上投入生产。您还可以在http://showcase.reka.ai 上找到一些高质量的定性示例。 <div>
We introduce Reka Core, Flash, and Edge, a series of powerful multimodal
language models trained from scratch by Reka. Reka models are able to process
and reason with text, images, video, and audio inputs. This technical report
discusses details of training some of these models and provides comprehensive
evaluation results. We show that Reka Edge and Reka Flash are not only
state-of-the-art but also outperform many much larger models, delivering
outsized values for their respective compute class. Meanwhile, our most capable
and largest model, Reka Core, approaches the best frontier models on both
automatic evaluations and blind human evaluations. On image question answering
benchmarks (e.g. MMMU, VQAv2), Core performs competitively to GPT4-V.
Meanwhile, on multimodal chat, Core ranks as the second most preferred model
under a blind third-party human evaluation setup, outperforming other models
such as Claude 3 Opus. On text benchmarks, Core not only performs competitively
to other frontier models on a set of well-established benchmarks (e.g. MMLU,
GSM8K) but also outperforms GPT4-0613 on human evaluation. On video question
answering (Perception-Test), Core outperforms Gemini Ultra. Models are shipped
in production at http://chat.reka.ai . A showcase of non cherry picked
qualitative examples can also be found at http://showcase.reka.ai .
]]></content:encoded>
<pubDate>2024-04-18T17:59:48Z</pubDate>
</item>
<item>
<title>MeshLRM: Large Reconstruction Model for High-Quality Mesh</title>
<link>http://arxiv.org/abs/2404.12385v1</link>
<guid>http://arxiv.org/abs/2404.12385v1</guid>
<content:encoded><![CDATA[
We propose MeshLRM, a novel LRM-based approach that can reconstruct a
high-quality mesh from merely four input images in less than one second.
Different from previous large reconstruction models (LRMs) that focus on
NeRF-based reconstruction, MeshLRM incorporates differentiable mesh extraction
and rendering within the LRM framework. This allows for end-to-end mesh
reconstruction by fine-tuning a pre-trained NeRF LRM with mesh rendering.
Moreover, we improve the LRM architecture by simplifying several complex
designs in previous LRMs. MeshLRM's NeRF initialization is sequentially trained
with low- and high-resolution images; this new LRM training strategy enables
significantly faster convergence and thereby leads to better quality with less
compute. Our approach achieves state-of-the-art mesh reconstruction from
sparse-view inputs and also allows for many downstream applications, including
text-to-3D and single-image-to-3D generation. Project page:
https://sarahweiii.github.io/meshlrm/
]]></content:encoded>
<pubDate>2024-04-18T17:59:41Z</pubDate>
</item>
<item>
<title>COMBO: Compositional World Models for Embodied Multi-Agent Cooperation</title>
<link>http://arxiv.org/abs/2404.10775v1</link>
<guid>http://arxiv.org/abs/2404.10775v1</guid>
<content:encoded><![CDATA[
<div> 多智能体协作, 分布式智能, 生成模型, 视觉观测, 规划
<br />
这篇论文研究了多智能体协作的问题，这些智能体在只有部分视角观测到世界的情况下必须进行合作。为了有效地在这种设置中进行规划，我们首先训练生成模型来估计部分视角观测的整体世界状态。然后，我们提出学习一个组合式世界模型，通过对多个智能体的联合动作进行分解，并组合生成视频，以精确模拟这个世界状态上的多组动作。最后，结合视觉语言模型推断其他智能体的动作，我们可以使用树搜索过程集成这些模块，促进在线协作规划。在ThreeDWorld模拟器上创建了两个具有挑战性的多智能体长时间跨度协作任务，并进行了2-4个智能体的实验。结果表明我们的组合式世界模型是有效的，该框架使得多智能体能够在各种任务和任意数量的其他智能体之间高效协作，展现了我们提出框架的未来前景。 <div>
In this paper, we investigate the problem of embodied multi-agent
cooperation, where decentralized agents must cooperate given only partial
egocentric views of the world. To effectively plan in this setting, in contrast
to learning world dynamics in a single-agent scenario, we must simulate world
dynamics conditioned on an arbitrary number of agents' actions given only
partial egocentric visual observations of the world. To address this issue of
partial observability, we first train generative models to estimate the overall
world state given partial egocentric observations. To enable accurate
simulation of multiple sets of actions on this world state, we then propose to
learn a compositional world model for multi-agent cooperation by factorizing
the naturally composable joint actions of multiple agents and compositionally
generating the video. By leveraging this compositional world model, in
combination with Vision Language Models to infer the actions of other agents,
we can use a tree search procedure to integrate these modules and facilitate
online cooperative planning. To evaluate the efficacy of our methods, we create
two challenging embodied multi-agent long-horizon cooperation tasks using the
ThreeDWorld simulator and conduct experiments with 2-4 agents. The results show
our compositional world model is effective and the framework enables the
embodied agents to cooperate efficiently with different agents across various
tasks and an arbitrary number of agents, showing the promising future of our
proposed framework. More videos can be found at
https://vis-www.cs.umass.edu/combo/.
]]></content:encoded>
<pubDate>2024-04-16T17:59:11Z</pubDate>
</item>
<item>
<title>A Conceptual Framework for Conversational Search and Recommendation:
  Conceptualizing Agent-Human Interactions During the Conversational Search
  Process</title>
<link>http://arxiv.org/abs/2404.08630v1</link>
<guid>http://arxiv.org/abs/2404.08630v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 对话式搜索, 用户意图, 搜索空间, 决策点, 代理人<br /> 
总结: 
本文旨在发展用户和代理人的行为和意图的概念框架，解释这些行为如何使用户能够探索搜索空间并满足他们的信息需求。我们概述了不同的行为和意图，然后讨论了在对话过程中代理人需要决策如何引导对话搜索过程以达到成功和/或令人满意的结论的关键决策点。本文实质上提供了代理人和用户之间对话搜索过程的概念化，为对话搜索代理人的研究、开发和评估提供了框架和起点。<br /> <div>
The conversational search task aims to enable a user to resolve information
needs via natural language dialogue with an agent. In this paper, we aim to
develop a conceptual framework of the actions and intents of users and agents
explaining how these actions enable the user to explore the search space and
resolve their information need. We outline the different actions and intents,
before discussing key decision points in the conversation where the agent needs
to decide how to steer the conversational search process to a successful and/or
satisfactory conclusion. Essentially, this paper provides a conceptualization
of the conversational search process between an agent and user, which provides
a framework and a starting point for research, development and evaluation of
conversational search agents.
]]></content:encoded>
<pubDate>2024-04-12T17:48:18Z</pubDate>
</item>
<item>
<title>Connecting NeRFs, Images, and Text</title>
<link>http://arxiv.org/abs/2404.07993v1</link>
<guid>http://arxiv.org/abs/2404.07993v1</guid>
<content:encoded><![CDATA[
<div> Neural Radiance Fields, 3D scenes, multimodal representation learning, NeRF embeddings, bidirectional mapping<br />
<br />
本文探讨了将NeRF模态与其他模态相连的新研究方向，类似于图像和文本的建立方法。作者提出了一个简单的框架，利用预训练模型用于NeRF表示，同时结合文本和图像处理的多模态模型。这个框架学习了NeRF嵌入和与之对应的图像和文本嵌入之间的双向映射。这种映射解锁了几种新颖且有用的应用，包括NeRF零样本分类以及从图像或文本中检索NeRF。 <br /><br />总结: <br />NeRF模态与其他模态相连的新研究方向；提出了简单的框架，利用预训练模型用于NeRF表示，同时结合文本和图像处理的多模态模型；学习了NeRF嵌入和与之对应的图像和文本嵌入之间的双向映射；解锁了几种新颖且有用的应用，包括NeRF零样本分类以及从图像或文本中检索NeRF。 <div>
Neural Radiance Fields (NeRFs) have emerged as a standard framework for
representing 3D scenes and objects, introducing a novel data type for
information exchange and storage. Concurrently, significant progress has been
made in multimodal representation learning for text and image data. This paper
explores a novel research direction that aims to connect the NeRF modality with
other modalities, similar to established methodologies for images and text. To
this end, we propose a simple framework that exploits pre-trained models for
NeRF representations alongside multimodal models for text and image processing.
Our framework learns a bidirectional mapping between NeRF embeddings and those
obtained from corresponding images and text. This mapping unlocks several novel
and useful applications, including NeRF zero-shot classification and NeRF
retrieval from images or text.
]]></content:encoded>
<pubDate>2024-04-11T17:59:59Z</pubDate>
</item>
<item>
<title>UMBRAE: Unified Multimodal Decoding of Brain Signals</title>
<link>http://arxiv.org/abs/2404.07202v1</link>
<guid>http://arxiv.org/abs/2404.07202v1</guid>
<content:encoded><![CDATA[
<div> 多模态解码、脑信号、空间信息、跨学科训练、UMBRAE
总结:
多模态解码UMBRACE旨在解决大脑研究中的挑战，提取脑信号的概念和空间细节，并引入跨学科训练策略，使模型能够在多个学科上进行训练，并在新任务中取得优越结果。实验证明，UMBRAE不仅在新任务中取得了优异的成绩，并且胜过了现有方法，在综合大脑理解基准BrainHub上也展现出了良好的表现。 <div>
We address prevailing challenges of the brain-powered research, departing
from the observation that the literature hardly recover accurate spatial
information and require subject-specific models. To address these challenges,
we propose UMBRAE, a unified multimodal decoding of brain signals. First, to
extract instance-level conceptual and spatial details from neural signals, we
introduce an efficient universal brain encoder for multimodal-brain alignment
and recover object descriptions at multiple levels of granularity from
subsequent multimodal large language model (MLLM). Second, we introduce a
cross-subject training strategy mapping subject-specific features to a common
feature space. This allows a model to be trained on multiple subjects without
extra resources, even yielding superior results compared to subject-specific
models. Further, we demonstrate this supports weakly-supervised adaptation to
new subjects, with only a fraction of the total training data. Experiments
demonstrate that UMBRAE not only achieves superior results in the newly
introduced tasks but also outperforms methods in well established tasks. To
assess our method, we construct and share with the community a comprehensive
brain understanding benchmark BrainHub. Our code and benchmark are available at
https://weihaox.github.io/UMBRAE.
]]></content:encoded>
<pubDate>2024-04-10T17:59:20Z</pubDate>
</item>
<item>
<title>MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video
  Understanding</title>
<link>http://arxiv.org/abs/2404.05726v1</link>
<guid>http://arxiv.org/abs/2404.05726v1</guid>
<content:encoded><![CDATA[
<div> 长视频理解 模型设计 在线处理 内存存储 多模态模型

该研究提出了一种针对长视频理解的高效有效模型设计。与现有工作不同的是，该模型采用在线处理视频的方式，并将过去的视频信息存储在内存银行中。这使得模型能够在不超过LLMs上下文长度限制或GPU内存限制的情况下参考历史视频内容进行长期分析。我们的内存银行可以轻松集成到当前多模态LLMs中。我们在各种视频理解任务上进行了大量实验，如长视频理解、视频问题回答和视频字幕生成，我们的模型在多个数据集上均取得了最先进的性能。具体代码可在https://boheumd.github.io/MA-LMM/上找到。 <br /><br />总结: <br />长视频理解的挑战，提出了一种针对长视频理解的高效有效模型设计，通过在线处理视频并将信息存储在内存中，实现长期视频分析。在多个数据集上取得了最先进的性能。 <div>
With the success of large language models (LLMs), integrating the vision
model into LLMs to build vision-language foundation models has gained much more
interest recently. However, existing LLM-based large multimodal models (e.g.,
Video-LLaMA, VideoChat) can only take in a limited number of frames for short
video understanding. In this study, we mainly focus on designing an efficient
and effective model for long-term video understanding. Instead of trying to
process more frames simultaneously like most existing work, we propose to
process videos in an online manner and store past video information in a memory
bank. This allows our model to reference historical video content for long-term
analysis without exceeding LLMs' context length constraints or GPU memory
limits. Our memory bank can be seamlessly integrated into current multimodal
LLMs in an off-the-shelf manner. We conduct extensive experiments on various
video understanding tasks, such as long-video understanding, video question
answering, and video captioning, and our model can achieve state-of-the-art
performances across multiple datasets. Code available at
https://boheumd.github.io/MA-LMM/.
]]></content:encoded>
<pubDate>2024-04-08T17:59:24Z</pubDate>
</item>
<item>
<title>Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs</title>
<link>http://arxiv.org/abs/2404.05719v1</link>
<guid>http://arxiv.org/abs/2404.05719v1</guid>
<content:encoded><![CDATA[
<div> 关键词: Multimodal large language models, Ferret-UI, user interface, training samples, model evaluation

Ferret-UI是一种新型的Multimodal large language model，专门用于增强对移动用户界面的理解能力。它通过引入了"any resolution"技术来放大细节，提供了更好的视觉特性。该模型通过收集大量基本UI任务的训练样本，并对其进行格式化处理，以便进行精确的指代和基础。此外，还编制了一个包括详细描述、感知/交互对话和功能推断在内的数据集，以增强模型的推理能力。经过训练后，Ferret-UI在UI界面的理解和执行开放式指令方面表现出色。针对模型评估，作者建立了一个全面的基准，涵盖了所有前述任务。Ferret-UI不仅在大部分开源UI MLLMs方面表现出色，而且在所有基本UI任务上都超越了GPT-4V。

<br /><br />总结:
Ferret-UI是一款专门针对移动用户界面的Multimodal large language model，具有增强理解能力。作者收集了大量UI任务的训练样本，并编制了用于增强模型推理能力的数据集。经过训练后，Ferret-UI在理解和执行UI界面任务方面表现出色，超越了开源UI MLLMs和GPT-4V。 <div>
Recent advancements in multimodal large language models (MLLMs) have been
noteworthy, yet, these general-domain MLLMs often fall short in their ability
to comprehend and interact effectively with user interface (UI) screens. In
this paper, we present Ferret-UI, a new MLLM tailored for enhanced
understanding of mobile UI screens, equipped with referring, grounding, and
reasoning capabilities. Given that UI screens typically exhibit a more
elongated aspect ratio and contain smaller objects of interest (e.g., icons,
texts) than natural images, we incorporate "any resolution" on top of Ferret to
magnify details and leverage enhanced visual features. Specifically, each
screen is divided into 2 sub-images based on the original aspect ratio (i.e.,
horizontal division for portrait screens and vertical division for landscape
screens). Both sub-images are encoded separately before being sent to LLMs. We
meticulously gather training samples from an extensive range of elementary UI
tasks, such as icon recognition, find text, and widget listing. These samples
are formatted for instruction-following with region annotations to facilitate
precise referring and grounding. To augment the model's reasoning ability, we
further compile a dataset for advanced tasks, including detailed description,
perception/interaction conversations, and function inference. After training on
the curated datasets, Ferret-UI exhibits outstanding comprehension of UI
screens and the capability to execute open-ended instructions. For model
evaluation, we establish a comprehensive benchmark encompassing all the
aforementioned tasks. Ferret-UI excels not only beyond most open-source UI
MLLMs, but also surpasses GPT-4V on all the elementary UI tasks.
]]></content:encoded>
<pubDate>2024-04-08T17:55:44Z</pubDate>
</item>
<item>
<title>SwapAnything: Enabling Arbitrary Object Swapping in Personalized Visual
  Editing</title>
<link>http://arxiv.org/abs/2404.05717v1</link>
<guid>http://arxiv.org/abs/2404.05717v1</guid>
<content:encoded><![CDATA[
Effective editing of personal content holds a pivotal role in enabling
individuals to express their creativity, weaving captivating narratives within
their visual stories, and elevate the overall quality and impact of their
visual content. Therefore, in this work, we introduce SwapAnything, a novel
framework that can swap any objects in an image with personalized concepts
given by the reference, while keeping the context unchanged. Compared with
existing methods for personalized subject swapping, SwapAnything has three
unique advantages: (1) precise control of arbitrary objects and parts rather
than the main subject, (2) more faithful preservation of context pixels, (3)
better adaptation of the personalized concept to the image. First, we propose
targeted variable swapping to apply region control over latent feature maps and
swap masked variables for faithful context preservation and initial semantic
concept swapping. Then, we introduce appearance adaptation, to seamlessly adapt
the semantic concept into the original image in terms of target location,
shape, style, and content during the image generation process. Extensive
results on both human and automatic evaluation demonstrate significant
improvements of our approach over baseline methods on personalized swapping.
Furthermore, SwapAnything shows its precise and faithful swapping abilities
across single object, multiple objects, partial object, and cross-domain
swapping tasks. SwapAnything also achieves great performance on text-based
swapping and tasks beyond swapping such as object insertion.
]]></content:encoded>
<pubDate>2024-04-08T17:52:29Z</pubDate>
</item>
<item>
<title>Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation</title>
<link>http://arxiv.org/abs/2404.04256v1</link>
<guid>http://arxiv.org/abs/2404.04256v1</guid>
<content:encoded><![CDATA[
<div> 多模态语义分割, AI代理, 低光环境, 过曝环境, Sigma, Siamese Mamba网络<br />
<br />
多模态语义分割方法Sigma利用Selective Structured State Space Model, Mamba, 实现了全局感知域与线性复杂度的结合。通过Siamese编码器和创新的Mamba融合机制有效地选择不同模态的关键信息。再结合解码器增强模型的通道建模能力。该方法在RGB-热像和RGB-深度语义分割任务上得到了有效评估，并展示了其优越性，标志着State Space Models (SSMs)在多模态感知任务中的首次成功应用。 代码可在https://github.com/zifuwan/Sigma找到。<br />
<br />总结: 多模态语义分割方法Sigma利用了新颖的Siamese Mamba网络和State Space Models (SSMs)，在低光和过曝环境下实现了更可靠的场景理解和全局感知，且具有线性复杂度，有效结合了不同模态的关键信息，在RGB-热像和RGB-深度语义分割任务上得到了有效验证。 <div>
Multi-modal semantic segmentation significantly enhances AI agents'
perception and scene understanding, especially under adverse conditions like
low-light or overexposed environments. Leveraging additional modalities
(X-modality) like thermal and depth alongside traditional RGB provides
complementary information, enabling more robust and reliable segmentation. In
this work, we introduce Sigma, a Siamese Mamba network for multi-modal semantic
segmentation, utilizing the Selective Structured State Space Model, Mamba.
Unlike conventional methods that rely on CNNs, with their limited local
receptive fields, or Vision Transformers (ViTs), which offer global receptive
fields at the cost of quadratic complexity, our model achieves global receptive
fields coverage with linear complexity. By employing a Siamese encoder and
innovating a Mamba fusion mechanism, we effectively select essential
information from different modalities. A decoder is then developed to enhance
the channel-wise modeling ability of the model. Our method, Sigma, is
rigorously evaluated on both RGB-Thermal and RGB-Depth segmentation tasks,
demonstrating its superiority and marking the first successful application of
State Space Models (SSMs) in multi-modal perception tasks. Code is available at
https://github.com/zifuwan/Sigma.
]]></content:encoded>
<pubDate>2024-04-05T17:59:44Z</pubDate>
</item>
<item>
<title>CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept
  Matching</title>
<link>http://arxiv.org/abs/2404.03653v1</link>
<guid>http://arxiv.org/abs/2404.03653v1</guid>
<content:encoded><![CDATA[
<div> 提取关键词: Diffusion models, text-to-image generation, CoMat, fine-tuning strategy, image captioning

总结:
Diffusion模型在文本到图像生成领域取得了巨大成功，但是文本提示和图像之间的不匹配仍然是一个挑战。主要原因是不足的标记注意激活导致的。研究人员提出了CoMat，这是一个端到端的扩散模型微调策略，具有图像到文本概念匹配机制。他们利用图像字幕模型来测量图像到文本的对齐并引导扩散模型重新审视被忽略的标记。另外，他们还提出了一种新颖的属性集中模块来解决属性绑定问题。通过对SDXL进行微调，他们得到了CoMat-SDXL模型，在两个文本到图像对齐基准测试中明显优于基线模型SDXL，并取得了领先的性能。<br /><br />总结: 该研究提出了一种新的文本到图像生成模型微调策略，通过引入图像到文本对齐机制和属性集中模块，显著改善了现有模型中存在的文本提示和图像不匹配的问题，取得了领先的性能。 <div>
Diffusion models have demonstrated great success in the field of
text-to-image generation. However, alleviating the misalignment between the
text prompts and images is still challenging. The root reason behind the
misalignment has not been extensively investigated. We observe that the
misalignment is caused by inadequate token attention activation. We further
attribute this phenomenon to the diffusion model's insufficient condition
utilization, which is caused by its training paradigm. To address the issue, we
propose CoMat, an end-to-end diffusion model fine-tuning strategy with an
image-to-text concept matching mechanism. We leverage an image captioning model
to measure image-to-text alignment and guide the diffusion model to revisit
ignored tokens. A novel attribute concentration module is also proposed to
address the attribute binding problem. Without any image or human preference
data, we use only 20K text prompts to fine-tune SDXL to obtain CoMat-SDXL.
Extensive experiments show that CoMat-SDXL significantly outperforms the
baseline model SDXL in two text-to-image alignment benchmarks and achieves
start-of-the-art performance.
]]></content:encoded>
<pubDate>2024-04-04T17:59:46Z</pubDate>
</item>
<item>
<title>AutoWebGLM: Bootstrap And Reinforce A Large Language Model-based Web
  Navigating Agent</title>
<link>http://arxiv.org/abs/2404.03648v1</link>
<guid>http://arxiv.org/abs/2404.03648v1</guid>
<content:encoded><![CDATA[
<div> GPT-4,Automated Web Navigation, AutoWebGLM, ChatGLM3-6B, Reinforcement Learning
<br /><br />总结:
本文介绍了基于大语言模型的自动网页导航代理的开发。现有的代理在处理真实世界的网页时存在三个问题：网页操作的多样性、HTML文本超出模型处理能力以及决策复杂性。为了解决这一挑战，研究人员开发了AutoWebGLM，这是一种建立在ChatGLM3-6B基础上的GPT-4智能代理，用于自动网页导航。他们设计了一种HTML简化算法，以保留重要信息的同时简洁地表示网页，并采用混合人工智能方法构建网页浏览数据进行课程培训。随后，他们通过强化学习和拒绝抽样来进一步促进模型对网页的理解、浏览器操作和高效任务分解。最后，他们建立了一个双语基准测试集AutoWebBench，用于进行真实世界的网页浏览任务。他们评估了AutoWebGLM在各种网页导航基准测试中的表现，并揭示了改进之处以及需要解决的问题。 <div>
Large language models (LLMs) have fueled many intelligent agent tasks, such
as web navigation -- but most existing agents perform far from satisfying in
real-world webpages due to three factors: (1) the versatility of actions on
webpages, (2) HTML text exceeding model processing capacity, and (3) the
complexity of decision-making due to the open-domain nature of web. In light of
the challenge, we develop AutoWebGLM, a GPT-4-outperforming automated web
navigation agent built upon ChatGLM3-6B. Inspired by human browsing patterns,
we design an HTML simplification algorithm to represent webpages, preserving
vital information succinctly. We employ a hybrid human-AI method to build web
browsing data for curriculum training. Then, we bootstrap the model by
reinforcement learning and rejection sampling to further facilitate webpage
comprehension, browser operations, and efficient task decomposition by itself.
For testing, we establish a bilingual benchmark -- AutoWebBench -- for
real-world web browsing tasks. We evaluate AutoWebGLM across diverse web
navigation benchmarks, revealing its improvements but also underlying
challenges to tackle real environments. Related code, model, and data will be
released at \url{https://github.com/THUDM/AutoWebGLM}.
]]></content:encoded>
<pubDate>2024-04-04T17:58:40Z</pubDate>
</item>
<item>
<title>Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale
  Prediction</title>
<link>http://arxiv.org/abs/2404.02905v1</link>
<guid>http://arxiv.org/abs/2404.02905v1</guid>
<content:encoded><![CDATA[
<div> 关键词: Visual AutoRegressive modeling, next-scale prediction, image generation, scalability, zero-shot generalization<br />
<br />
这篇文章介绍了Visual AutoRegressive modeling (VAR)作为一种新的图像自回归学习范式，重新定义了图像上的自回归学习，将其视为粗到细的“下一尺度预测”或“下一分辨率预测”，而不是标准的光栅扫描“下一个标记预测”。这种简单直观的方法使得自回归(AR)变换器可以快速学习视觉分布并具有良好的泛化能力：VAR首次使得AR模型在图像生成方面超过了扩散变压器。在ImageNet 256x256基准测试中，VAR通过将Frechet inception距离(FID)从18.65提高到1.80，Inception分数(IS)从80.4提高到356.4，且推理速度约快20倍，显著改善了AR基线。实验证实，VAR在多个维度上均优于扩散变压器(DiT)，包括图像质量、推理速度、数据效率和可扩展性。扩大VAR模型呈现出与LLMs中观察到的类似的幂律缩放规律，相关系数接近-0.998，为其提供了坚实的证据。VAR在下游任务中展示了零样本泛化能力，包括图像修补、外扩和编辑。这些结果表明VAR已经模拟出了LLMs的两个重要特性：缩放规律和零样本任务泛化。我们已发布所有模型和代码，以促进对AR/VAR模型在视觉生成和统一学习方面的探索。<br /><br />总结:Visual AutoRegressive modeling (VAR)重新定义了图像上的自回归学习范式，实现了对视觉分布的快速学习和良好的泛化能力。在图像生成方面，VAR优于扩散变压器，并呈现出幂律缩放规律。此外，VAR还展示了在下游任务中的零样本泛化能力。我们已发布相关模型和代码，以推动AR/VAR模型在视觉生成和统一学习中的进一步探索。 <div>
We present Visual AutoRegressive modeling (VAR), a new generation paradigm
that redefines the autoregressive learning on images as coarse-to-fine
"next-scale prediction" or "next-resolution prediction", diverging from the
standard raster-scan "next-token prediction". This simple, intuitive
methodology allows autoregressive (AR) transformers to learn visual
distributions fast and generalize well: VAR, for the first time, makes AR
models surpass diffusion transformers in image generation. On ImageNet 256x256
benchmark, VAR significantly improve AR baseline by improving Frechet inception
distance (FID) from 18.65 to 1.80, inception score (IS) from 80.4 to 356.4,
with around 20x faster inference speed. It is also empirically verified that
VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions
including image quality, inference speed, data efficiency, and scalability.
Scaling up VAR models exhibits clear power-law scaling laws similar to those
observed in LLMs, with linear correlation coefficients near -0.998 as solid
evidence. VAR further showcases zero-shot generalization ability in downstream
tasks including image in-painting, out-painting, and editing. These results
suggest VAR has initially emulated the two important properties of LLMs:
Scaling Laws and zero-shot task generalization. We have released all models and
codes to promote the exploration of AR/VAR models for visual generation and
unified learning.
]]></content:encoded>
<pubDate>2024-04-03T17:59:53Z</pubDate>
</item>
<item>
<title>ALOHa: A New Measure for Hallucination in Captioning Models</title>
<link>http://arxiv.org/abs/2404.02904v1</link>
<guid>http://arxiv.org/abs/2404.02904v1</guid>
<content:encoded><![CDATA[
<div> 提取关键词：multimodal pre-training, visual description, object hallucination, ALOHa, large language models（LLMs）

在这项工作中，作者提出了一种现代化的开放词汇量度量标准ALOHa，用于测量物体的幻觉。他们使用大型语言模型来从候选字幕中提取可接地的物体，并测量它们与参考物体的语义相似性，最终使用匈牙利匹配生成最终的幻觉分数。 ALoha在HAT和nocaps数据集上的表现优于现有的CHIAR度量标准。总的来说，这项工作提出了一个有效的新方法来评估图像描述中的对象幻觉，并且在两个不同的数据集上都取得了令人满意的结果。

<br /><br />总结: 本文提出了一种现代化的开放词汇量度量标准ALOHa，用于衡量图像描述中的对象幻觉。该方法利用大型语言模型来提取候选字幕中的可接地物体，并测量它们与参考物体的语义相似性，最终使用匈牙利匹配生成最终的幻觉分数。ALOHa在HAT和nocaps数据集上的表现优于现有的CHIAR度量标准。这项工作提出了一个有效的新方法来评估图像描述中的对象幻觉，并且在两个不同的数据集上都取得了令人满意的结果。 <div>
Despite recent advances in multimodal pre-training for visual description,
state-of-the-art models still produce captions containing errors, such as
hallucinating objects not present in a scene. The existing prominent metric for
object hallucination, CHAIR, is limited to a fixed set of MS COCO objects and
synonyms. In this work, we propose a modernized open-vocabulary metric, ALOHa,
which leverages large language models (LLMs) to measure object hallucinations.
Specifically, we use an LLM to extract groundable objects from a candidate
caption, measure their semantic similarity to reference objects from captions
and object detections, and use Hungarian matching to produce a final
hallucination score. We show that ALOHa correctly identifies 13.6% more
hallucinated objects than CHAIR on HAT, a new gold-standard subset of MS COCO
Captions annotated for hallucinations, and 30.8% more on nocaps, where objects
extend beyond MS COCO categories. Our code is available at
https://davidmchan.github.io/aloha/.
]]></content:encoded>
<pubDate>2024-04-03T17:59:36Z</pubDate>
</item>
<item>
<title>MatAtlas: Text-driven Consistent Geometry Texturing and Material
  Assignment</title>
<link>http://arxiv.org/abs/2404.02899v1</link>
<guid>http://arxiv.org/abs/2404.02899v1</guid>
<content:encoded><![CDATA[
We present MatAtlas, a method for consistent text-guided 3D model texturing.
Following recent progress we leverage a large scale text-to-image generation
model (e.g., Stable Diffusion) as a prior to texture a 3D model. We carefully
design an RGB texturing pipeline that leverages a grid pattern diffusion,
driven by depth and edges. By proposing a multi-step texture refinement
process, we significantly improve the quality and 3D consistency of the
texturing output. To further address the problem of baked-in lighting, we move
beyond RGB colors and pursue assigning parametric materials to the assets.
Given the high-quality initial RGB texture, we propose a novel material
retrieval method capitalized on Large Language Models (LLM), enabling
editabiliy and relightability. We evaluate our method on a wide variety of
geometries and show that our method significantly outperform prior arts. We
also analyze the role of each component through a detailed ablation study.
]]></content:encoded>
<pubDate>2024-04-03T17:57:15Z</pubDate>
</item>
<item>
<title>A Mean Field Game Model for Timely Computation in Edge Computing Systems</title>
<link>http://arxiv.org/abs/2404.02898v1</link>
<guid>http://arxiv.org/abs/2404.02898v1</guid>
<content:encoded><![CDATA[
We consider the problem of task offloading in multi-access edge computing
(MEC) systems constituting $N$ devices assisted by an edge server (ES), where
the devices can split task execution between a local processor and the ES.
Since the local task execution and communication with the ES both consume
power, each device must judiciously choose between the two. We model the
problem as a large population non-cooperative game among the $N$ devices. Since
computation of an equilibrium in this scenario is difficult due to the presence
of a large number of devices, we employ the mean-field game framework to reduce
the finite-agent game problem to a generic user's multi-objective optimization
problem, with a coupled consistency condition. By leveraging the novel age of
information (AoI) metric, we invoke techniques from stochastic hybrid systems
(SHS) theory and study the tradeoffs between increasing information freshness
and reducing power consumption. In numerical simulations, we validate that a
higher load at the ES may lead devices to upload their task to the ES less
often.
]]></content:encoded>
<pubDate>2024-04-03T17:55:20Z</pubDate>
</item>
<item>
<title>Segment Any 3D Object with Language</title>
<link>http://arxiv.org/abs/2404.02157v1</link>
<guid>http://arxiv.org/abs/2404.02157v1</guid>
<content:encoded><![CDATA[
<div> Open-Vocabulary 3D Instance Segmentation, free-form language instructions, semantic-aware, geometric-aware, multimodal fusion network<br />
<br />
本文研究了使用自由形式语言指令的开放词汇3D实例分割（OV-3DIS）。先前的研究依赖于只使用标注的基本类别进行训练，但容易在未知的新领域中受限。最近的研究通过生成类别不可知的掩模或从2D到3D的投影泛化掩模，但忽略了语义或几何信息，导致性能不佳。相反，直接从3D点云中生成具有语义相关性的可泛化掩模会产生更好的结果。本文引入了Segment any 3D Object with LanguagE（SOLE），它是一个具有强大泛化能力的语义和几何感知的视觉-语言学习框架，通过直接从3D点云生成与语义相关的掩模。我们提出一个多模态融合网络来在骨干和解码器中融合多模态语义。另外，为了与各种语言指令对齐3D分割模型并增强掩模质量，我们引入了三种类型的多模态关联作为监督。我们的SOLE在ScanNetv2、ScanNet200和Replica基准测试上的表现大大超过先前方法，即使在训练中缺乏类别注释，结果甚至接近于完全监督的对照组。此外，大量的定性结果证明了我们的SOLE对语言指令的多功能性。<br /><br />总结: 本文研究了开放词汇3D实例分割（OV-3DIS）和自由形式语言指令。作者提出了SOLE框架，它是一个具有强大泛化能力的语义和几何感知的视觉-语言学习框架。SOLE在基准测试上表现出色，并且在缺乏类别注释的训练中也取得了良好的效果。最终的结果证明了SOLE对语言指令的多功能性。 <div>
In this paper, we investigate Open-Vocabulary 3D Instance Segmentation
(OV-3DIS) with free-form language instructions. Earlier works that rely on only
annotated base categories for training suffer from limited generalization to
unseen novel categories. Recent works mitigate poor generalizability to novel
categories by generating class-agnostic masks or projecting generalized masks
from 2D to 3D, but disregard semantic or geometry information, leading to
sub-optimal performance. Instead, generating generalizable but semantic-related
masks directly from 3D point clouds would result in superior outcomes. In this
paper, we introduce Segment any 3D Object with LanguagE (SOLE), which is a
semantic and geometric-aware visual-language learning framework with strong
generalizability by generating semantic-related masks directly from 3D point
clouds. Specifically, we propose a multimodal fusion network to incorporate
multimodal semantics in both backbone and decoder. In addition, to align the 3D
segmentation model with various language instructions and enhance the mask
quality, we introduce three types of multimodal associations as supervision.
Our SOLE outperforms previous methods by a large margin on ScanNetv2,
ScanNet200, and Replica benchmarks, and the results are even close to the
fully-supervised counterpart despite the absence of class annotations in the
training. Furthermore, extensive qualitative results demonstrate the
versatility of our SOLE to language instructions.
]]></content:encoded>
<pubDate>2024-04-02T17:59:10Z</pubDate>
</item>
<item>
<title>Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks</title>
<link>http://arxiv.org/abs/2404.02151v1</link>
<guid>http://arxiv.org/abs/2404.02151v1</guid>
<content:encoded><![CDATA[
<div> 关键词: LLMs, adaptive attacks, jailbreaking, logprobs, vulnerabilities

总结: 
本文研究表明，即使是最新的安全对齐的LLMs也无法抵御简单的自适应越狱攻击。首先，作者展示了成功利用对logprobs的访问进行越狱的方法，然后介绍了对不公开logprobs的Claude模型进行迁移或预填攻击的方法。另外，还展示了如何在受毒害的模型中使用有限的令牌搜索来查找特洛伊木马字符串。攻击的共同主题是适应性至关重要：不同的模型对不同的提示模板敏感，某些模型则基于其API具有独特的弱点，有时候根据先前的知识来限制令牌搜索空间也是至关重要的。详细攻击代码、提示和日志可在https://github.com/tml-epfl/llm-adaptive-attacks找到。<br /><br /> <div>
We show that even the most recent safety-aligned LLMs are not robust to
simple adaptive jailbreaking attacks. First, we demonstrate how to successfully
leverage access to logprobs for jailbreaking: we initially design an
adversarial prompt template (sometimes adapted to the target LLM), and then we
apply random search on a suffix to maximize the target logprob (e.g., of the
token "Sure"), potentially with multiple restarts. In this way, we achieve
nearly 100\% attack success rate -- according to GPT-4 as a judge -- on
GPT-3.5/4, Llama-2-Chat-7B/13B/70B, Gemma-7B, and R2D2 from HarmBench that was
adversarially trained against the GCG attack. We also show how to jailbreak all
Claude models -- that do not expose logprobs -- via either a transfer or
prefilling attack with 100\% success rate. In addition, we show how to use
random search on a restricted set of tokens for finding trojan strings in
poisoned models -- a task that shares many similarities with jailbreaking --
which is the algorithm that brought us the first place in the SaTML'24 Trojan
Detection Competition. The common theme behind these attacks is that adaptivity
is crucial: different models are vulnerable to different prompting templates
(e.g., R2D2 is very sensitive to in-context learning prompts), some models have
unique vulnerabilities based on their APIs (e.g., prefilling for Claude), and
in some settings it is crucial to restrict the token search space based on
prior knowledge (e.g., for trojan detection). We provide the code, prompts, and
logs of the attacks at https://github.com/tml-epfl/llm-adaptive-attacks.
]]></content:encoded>
<pubDate>2024-04-02T17:58:27Z</pubDate>
</item>
<item>
<title>Diffusion$^2$: Dynamic 3D Content Generation via Score Composition of
  Orthogonal Diffusion Models</title>
<link>http://arxiv.org/abs/2404.02148v1</link>
<guid>http://arxiv.org/abs/2404.02148v1</guid>
<content:encoded><![CDATA[
Recent advancements in 3D generation are predominantly propelled by
improvements in 3D-aware image diffusion models which are pretrained on
Internet-scale image data and fine-tuned on massive 3D data, offering the
capability of producing highly consistent multi-view images. However, due to
the scarcity of synchronized multi-view video data, it is impractical to adapt
this paradigm to 4D generation directly. Despite that, the available video and
3D data are adequate for training video and multi-view diffusion models that
can provide satisfactory dynamic and geometric priors respectively. In this
paper, we present Diffusion$^2$, a novel framework for dynamic 3D content
creation that leverages the knowledge about geometric consistency and temporal
smoothness from these models to directly sample dense multi-view and
multi-frame images which can be employed to optimize continuous 4D
representation. Specifically, we design a simple yet effective denoising
strategy via score composition of video and multi-view diffusion models based
on the probability structure of the images to be generated. Owing to the high
parallelism of the image generation and the efficiency of the modern 4D
reconstruction pipeline, our framework can generate 4D content within few
minutes. Furthermore, our method circumvents the reliance on 4D data, thereby
having the potential to benefit from the scalability of the foundation video
and multi-view diffusion models. Extensive experiments demonstrate the efficacy
of our proposed framework and its capability to flexibly adapt to various types
of prompts.
]]></content:encoded>
<pubDate>2024-04-02T17:58:03Z</pubDate>
</item>
<item>
<title>Iterated Learning Improves Compositionality in Large Vision-Language
  Models</title>
<link>http://arxiv.org/abs/2404.02145v1</link>
<guid>http://arxiv.org/abs/2404.02145v1</guid>
<content:encoded><![CDATA[
A fundamental characteristic common to both human vision and natural language
is their compositional nature. Yet, despite the performance gains contributed
by large vision and language pretraining, recent investigations find that
most-if not all-our state-of-the-art vision-language models struggle at
compositionality. They are unable to distinguish between images of " a girl in
white facing a man in black" and "a girl in black facing a man in white".
Moreover, prior work suggests that compositionality doesn't arise with scale:
larger model sizes or training data don't help. This paper develops a new
iterated training algorithm that incentivizes compositionality. We draw on
decades of cognitive science research that identifies cultural transmission-the
need to teach a new generation-as a necessary inductive prior that incentivizes
humans to develop compositional languages. Specifically, we reframe
vision-language contrastive learning as the Lewis Signaling Game between a
vision agent and a language agent, and operationalize cultural transmission by
iteratively resetting one of the agent's weights during training. After every
iteration, this training paradigm induces representations that become "easier
to learn", a property of compositional languages: e.g. our model trained on
CC3M and CC12M improves standard CLIP by 4.7%, 4.0% respectfully in the
SugarCrepe benchmark.
]]></content:encoded>
<pubDate>2024-04-02T17:57:31Z</pubDate>
</item>
<item>
<title>Detecting Image Attribution for Text-to-Image Diffusion Models in RGB
  and Beyond</title>
<link>http://arxiv.org/abs/2403.19653v1</link>
<guid>http://arxiv.org/abs/2403.19653v1</guid>
<content:encoded><![CDATA[
<div> 文本到图像，T2I模型，真实主义，图像生成，图像检测，可追溯性

T2I模型在生成图像方面取得了巨大进步，具有惊人的真实主义和创造力。这种进展引发了对假图像检测和归因的研究，但先前的研究并未完全探讨这一任务的实际和科学维度。除了将图像归因给12种最先进的T2I生成器外，我们还对推断阶段的超参数和图像修改进行了广泛的分析。我们的实验揭示了初始化种子是高度可检测的，以及图像生成过程中的其他细微变化在一定程度上是可辨识的。我们进一步调查了对图像归因所利用的视觉痕迹，通过扰动高频细节和利用图像风格和结构的中等级表示。值得注意的是，改变高频信息只会稍微降低准确性，并且在风格表示上训练的归因器表现优于在RGB图像上进行训练。我们的分析强调了与先前探索的各种视觉粒度级别相比，假图像在不同级别上是可检测和可归因的。 <br /><br />总结: 文本到图像（T2I）模型取得了巨大进步，可以生成具有惊人真实主义和创造力的图像。研究人员对假图像检测和归因进行了深入探讨，发现初始化种子和图像生成过程中的微小变化都是可辨识的。此外，通过扰动高频细节和利用图像风格和结构的中等级表示，也可以追踪图像的可信度。这些分析结果强调了实验发现的各种视觉粒度级别，进一步证明了假图像在不同级别上是可检测和可归因的。 <div>
Modern text-to-image (T2I) diffusion models can generate images with
remarkable realism and creativity. These advancements have sparked research in
fake image detection and attribution, yet prior studies have not fully explored
the practical and scientific dimensions of this task. In addition to
attributing images to 12 state-of-the-art T2I generators, we provide extensive
analyses on what inference stage hyperparameters and image modifications are
discernible. Our experiments reveal that initialization seeds are highly
detectable, along with other subtle variations in the image generation process
to some extent. We further investigate what visual traces are leveraged in
image attribution by perturbing high-frequency details and employing mid-level
representations of image style and structure. Notably, altering high-frequency
information causes only slight reductions in accuracy, and training an
attributor on style representations outperforms training on RGB images. Our
analyses underscore that fake images are detectable and attributable at various
levels of visual granularity than previously explored.
]]></content:encoded>
<pubDate>2024-03-28T17:59:42Z</pubDate>
</item>
<item>
<title>MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions</title>
<link>http://arxiv.org/abs/2403.19651v1</link>
<guid>http://arxiv.org/abs/2403.19651v1</guid>
<content:encoded><![CDATA[
<div> 自监督图像检索模型，文本指令，MagicLens，多模态模型，语言模型
<br /><br />总结:
本文介绍了一种名为MagicLens的自监督图像检索模型，利用文本指令来实现更丰富的图像检索。通过在同一网页上自然出现的图像对中挖掘丰富的语义关系，并利用大型多模态模型和大型语言模型来合成指令，使得模型能够支持开放式指令的图像检索。MagicLens在3600万个（查询图像，指令，目标图像）三元组上进行训练，并在八个不同的图像检索任务基准上取得可比或更好的结果，超过之前最先进的方法，同时模型规模减小了50倍。另外，对140万个未见过的图像进行人类分析，进一步展示了MagicLens支持的多样化搜索意图。 <div>
Image retrieval, i.e., finding desired images given a reference image,
inherently encompasses rich, multi-faceted search intents that are difficult to
capture solely using image-based measures. Recent work leverages text
instructions to allow users to more freely express their search intents.
However, existing work primarily focuses on image pairs that are visually
similar and/or can be characterized by a small set of pre-defined relations.
The core thesis of this paper is that text instructions can enable retrieving
images with richer relations beyond visual similarity. To show this, we
introduce MagicLens, a series of self-supervised image retrieval models that
support open-ended instructions. MagicLens is built on a key novel insight:
image pairs that naturally occur on the same web pages contain a wide range of
implicit relations (e.g., inside view of), and we can bring those implicit
relations explicit by synthesizing instructions via large multimodal models
(LMMs) and large language models (LLMs). Trained on 36.7M (query image,
instruction, target image) triplets with rich semantic relations mined from the
web, MagicLens achieves comparable or better results on eight benchmarks of
various image retrieval tasks than prior state-of-the-art (SOTA) methods.
Remarkably, it outperforms previous SOTA but with a 50X smaller model size on
multiple benchmarks. Additional human analyses on a 1.4M-image unseen corpus
further demonstrate the diversity of search intents supported by MagicLens.
]]></content:encoded>
<pubDate>2024-03-28T17:59:20Z</pubDate>
</item>
<item>
<title>Human-compatible driving partners through data-regularized self-play
  reinforcement learning</title>
<link>http://arxiv.org/abs/2403.19648v1</link>
<guid>http://arxiv.org/abs/2403.19648v1</guid>
<content:encoded><![CDATA[
A central challenge for autonomous vehicles is coordinating with humans.
Therefore, incorporating realistic human agents is essential for scalable
training and evaluation of autonomous driving systems in simulation. Simulation
agents are typically developed by imitating large-scale, high-quality datasets
of human driving. However, pure imitation learning agents empirically have high
collision rates when executed in a multi-agent closed-loop setting. To build
agents that are realistic and effective in closed-loop settings, we propose
Human-Regularized PPO (HR-PPO), a multi-agent algorithm where agents are
trained through self-play with a small penalty for deviating from a human
reference policy. In contrast to prior work, our approach is RL-first and only
uses 30 minutes of imperfect human demonstrations. We evaluate agents in a
large set of multi-agent traffic scenes. Results show our HR-PPO agents are
highly effective in achieving goals, with a success rate of 93%, an off-road
rate of 3.5%, and a collision rate of 3%. At the same time, the agents drive in
a human-like manner, as measured by their similarity to existing human driving
logs. We also find that HR-PPO agents show considerable improvements on proxy
measures for coordination with human driving, particularly in highly
interactive scenarios. We open-source our code and trained agents at
https://github.com/Emerge-Lab/nocturne_lab and provide demonstrations of agent
behaviors at https://sites.google.com/view/driving-partners.
]]></content:encoded>
<pubDate>2024-03-28T17:56:56Z</pubDate>
</item>
<item>
<title>GANTASTIC: GAN-based Transfer of Interpretable Directions for
  Disentangled Image Editing in Text-to-Image Diffusion Models</title>
<link>http://arxiv.org/abs/2403.19645v1</link>
<guid>http://arxiv.org/abs/2403.19645v1</guid>
<content:encoded><![CDATA[
The rapid advancement in image generation models has predominantly been
driven by diffusion models, which have demonstrated unparalleled success in
generating high-fidelity, diverse images from textual prompts. Despite their
success, diffusion models encounter substantial challenges in the domain of
image editing, particularly in executing disentangled edits-changes that target
specific attributes of an image while leaving irrelevant parts untouched. In
contrast, Generative Adversarial Networks (GANs) have been recognized for their
success in disentangled edits through their interpretable latent spaces. We
introduce GANTASTIC, a novel framework that takes existing directions from
pre-trained GAN models-representative of specific, controllable attributes-and
transfers these directions into diffusion-based models. This novel approach not
only maintains the generative quality and diversity that diffusion models are
known for but also significantly enhances their capability to perform precise,
targeted image edits, thereby leveraging the best of both worlds.
]]></content:encoded>
<pubDate>2024-03-28T17:55:16Z</pubDate>
</item>
<item>
<title>SLEDGE: Synthesizing Simulation Environments for Driving Agents with
  Generative Models</title>
<link>http://arxiv.org/abs/2403.17933v1</link>
<guid>http://arxiv.org/abs/2403.17933v1</guid>
<content:encoded><![CDATA[
<div> 训练、模拟、车辆、生成模型、交通仿真
总结：<br /><br />这篇文章介绍了SLEDGE，它是第一个在真实驾驶记录上训练的车辆运动规划生成器。其核心组件是一个学习模型，能够生成车辆包围框和车道图。通过对现有车道图表示的系统研究，引入了一种新颖的栅格到矢量自编码器(RVAE)。RVAE能够将代理和车道图编码成栅格化潜在地图的不同通道，这有助于车道条件下的代理生成，以及使用扩散变压器同时生成车道和代理。SLEDGE的生成实体能够更好地控制仿真，例如增加转弯或提高交通密度。与nuPlan相比，SLEDGE设置存储量减少了500倍(<4GB)，更易于获取，有助于民主化这一领域的未来研究。在测试SLEDGE生成的困难路线和密集交通时，nuPlan比赛获胜者PDM的失败率超过40%，这为规划算法带来了新的挑战。 <div>
SLEDGE is the first generative simulator for vehicle motion planning trained
on real-world driving logs. Its core component is a learned model that is able
to generate agent bounding boxes and lane graphs. The model's outputs serve as
an initial state for traffic simulation. The unique properties of the entities
to be generated for SLEDGE, such as their connectivity and variable count per
scene, render the naive application of most modern generative models to this
task non-trivial. Therefore, together with a systematic study of existing lane
graph representations, we introduce a novel raster-to-vector autoencoder
(RVAE). It encodes agents and the lane graph into distinct channels in a
rasterized latent map. This facilitates both lane-conditioned agent generation
and combined generation of lanes and agents with a Diffusion Transformer. Using
generated entities in SLEDGE enables greater control over the simulation, e.g.
upsampling turns or increasing traffic density. Further, SLEDGE can support
500m long routes, a capability not found in existing data-driven simulators
like nuPlan. It presents new challenges for planning algorithms, evidenced by
failure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge,
when tested on hard routes and dense traffic generated by our model. Compared
to nuPlan, SLEDGE requires 500$\times$ less storage to set up (<4GB), making it
a more accessible option and helping with democratizing future research in this
field.
]]></content:encoded>
<pubDate>2024-03-26T17:58:29Z</pubDate>
</item>
<item>
<title>MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution</title>
<link>http://arxiv.org/abs/2403.17927v1</link>
<guid>http://arxiv.org/abs/2403.17927v1</guid>
<content:encoded><![CDATA[
<div> GitHub, LLMs, MAGIS, software evolution, agents
总结:<br />
这篇文章介绍了在GitHub存储库中解决出现的问题是一个复杂的挑战，需要不断地更新代码并维护现有功能。大型语言模型（LLMs）在代码生成和理解方面表现出了潜力，但在存储库级别的代码更改上遇到了困难。为了克服这些挑战，作者通过实证研究了LLMs通常无法解决GitHub问题的原因，并分析了一些影响因素。在此基础上，他们提出了基于LLM的多Agent框架MAGIS，该框架包括四种个性化的代理：经理、存储库管理员、开发人员和质量保证工程师代理。在实验中，他们使用SWE-bench基准来比较MAGIS与流行的LLMs，包括GPT-3.5，GPT-4和Claude-2。结果显示，MAGIS能够解决13.94%的GitHub问题，明显优于基线。具体来说，MAGIS在解决比率上比直接应用GPT-4提高了八倍。文章还分析了提高GitHub问题解决率的因素，如代码行位置、任务分配等。 <div>
In software evolution, resolving the emergent issues within GitHub
repositories is a complex challenge that involves not only the incorporation of
new code but also the maintenance of existing functionalities. Large Language
Models (LLMs) have shown promise in code generation and understanding but face
difficulties in code change, particularly at the repository level. To overcome
these challenges, we empirically study the reason why LLMs mostly fail to
resolve GitHub issues and analyze some impact factors. Motivated by the
empirical findings, we propose a novel LLM-based Multi-Agent framework for
GitHub Issue reSolution, MAGIS, consisting of four kinds of agents customized
for the software evolution: Manager, Repository Custodian, Developer, and
Quality Assurance Engineer agents. This framework leverages the collaboration
of various agents in the planning and coding process to unlock the potential of
LLMs to resolve GitHub issues. In experiments, we employ the SWE-bench
benchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4, and
Claude-2. MAGIS can resolve 13.94% GitHub issues, which significantly
outperforms the baselines. Specifically, MAGIS achieves an eight-fold increase
in resolved ratio over the direct application of GPT-4, the based LLM of our
method. We also analyze the factors for improving GitHub issue resolution
rates, such as line location, task allocation, etc.
]]></content:encoded>
<pubDate>2024-03-26T17:57:57Z</pubDate>
</item>
<item>
<title>An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems</title>
<link>http://arxiv.org/abs/2403.16809v1</link>
<guid>http://arxiv.org/abs/2403.16809v1</guid>
<content:encoded><![CDATA[
<div> 关键词: CPS-IoT, 大型语言模型(LLMs), 控制环境, 感知优化, 强化学习<br />
<br />
本文介绍了大型语言模型(LLMs)在Cyber-Physical Systems and the Internet of Things (CPS-IoT) 应用中的作用。文章提出了利用LLMs模拟人群的行为和热舒适度偏好，并将其集成到基于代理的强化学习算法中，以实现能源节约和居住者舒适度的平衡。研究结果表明，LLMs能够模拟大型开放空间中复杂的人群运动，而基于代理的强化学习算法相对于现有的设定点控制策略表现更优，显示出在CPS-IoT应用中，自适应和个性化的决策是至关重要的。通过这个案例研究，文章展示了将先进的Foundation Models如LLMs集成到CPS-IoT中以增强系统适应性和效率的潜力。 <div>
The increasing prevalence of Cyber-Physical Systems and the Internet of
Things (CPS-IoT) applications and Foundation Models are enabling new
applications that leverage real-time control of the environment. For example,
real-time control of Heating, Ventilation and Air-Conditioning (HVAC) systems
can reduce its usage when not needed for the comfort of human occupants, hence
reducing energy consumption. Collecting real-time feedback on human preferences
in such human-in-the-loop (HITL) systems, however, is difficult in practice. We
propose the use of large language models (LLMs) to deal with the challenges of
dynamic environments and difficult-to-obtain data in CPS optimization. In this
paper, we present a case study that employs LLM agents to mimic the behaviors
and thermal preferences of various population groups (e.g. young families, the
elderly) in a shopping mall. The aggregated thermal preferences are integrated
into an agent-in-the-loop based reinforcement learning algorithm AitL-RL, which
employs the LLM as a dynamic simulation of the physical environment to learn
how to balance between energy savings and occupant comfort. Our results show
that LLMs are capable of simulating complex population movements within large
open spaces. Besides, AitL-RL demonstrates superior performance compared to the
popular existing policy of set point control, suggesting that adaptive and
personalized decision-making is critical for efficient optimization in CPS-IoT
applications. Through this case study, we demonstrate the potential of
integrating advanced Foundation Models like LLMs into CPS-IoT to enhance system
adaptability and efficiency. The project's code can be found on our GitHub
repository.
]]></content:encoded>
<pubDate>2024-03-25T14:32:28Z</pubDate>
</item>
<item>
<title>Exploiting Priors from 3D Diffusion Models for RGB-Based One-Shot View
  Planning</title>
<link>http://arxiv.org/abs/2403.16803v1</link>
<guid>http://arxiv.org/abs/2403.16803v1</guid>
<content:encoded><![CDATA[
<div> 一次性视角规划, 机器人, 环境交互, 3D生成模型, 几何先验

利用扩散模型的3D生成能力，我们提出了一种新颖的一次性视角规划方法。这种方法利用几何先验，能够有效地规划出全局最短路径，从而实现仅凭物体的单个RGB图像即可进行高效的一次性数据收集和物体重建。我们在模拟和真实环境中进行了规划实验，结果表明我们的方法在物体重建质量和移动成本之间取得了很好的平衡。<br /><br />总结: 本研究提出了一种利用3D生成模型的新一次性视角规划方法，通过几何先验实现了高效的物体重建，同时在实验中取得了良好的效果。 <div>
Object reconstruction is relevant for many autonomous robotic tasks that
require interaction with the environment. A key challenge in such scenarios is
planning view configurations to collect informative measurements for
reconstructing an initially unknown object. One-shot view planning enables
efficient data collection by predicting view configurations and planning the
globally shortest path connecting all views at once. However, geometric priors
about the object are required to conduct one-shot view planning. In this work,
we propose a novel one-shot view planning approach that utilizes the powerful
3D generation capabilities of diffusion models as priors. By incorporating such
geometric priors into our pipeline, we achieve effective one-shot view planning
starting with only a single RGB image of the object to be reconstructed. Our
planning experiments in simulation and real-world setups indicate that our
approach balances well between object reconstruction quality and movement cost.
]]></content:encoded>
<pubDate>2024-03-25T14:21:49Z</pubDate>
</item>
<item>
<title>LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis</title>
<link>http://arxiv.org/abs/2403.15385v1</link>
<guid>http://arxiv.org/abs/2403.15385v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 文本到3D生成, LATTE3D, 优化, 3D数据, 高效性

总结:<br /><br />
LATTE3D是一个新的文本到3D生成方法，通过建立可伸缩的架构和利用3D数据进行优化，实现了快速高质量的生成。与以往方法相比，LATTE3D能够在单次前向传递中产生高度详细的纹理网格，生成速度为400毫秒。该方法进一步可以通过快速的测试时间优化来提升效果。LATTE3D克服了以往方法的缺点，能够在更大的输入数据集上表现良好。 <div>
Recent text-to-3D generation approaches produce impressive 3D results but
require time-consuming optimization that can take up to an hour per prompt.
Amortized methods like ATT3D optimize multiple prompts simultaneously to
improve efficiency, enabling fast text-to-3D synthesis. However, they cannot
capture high-frequency geometry and texture details and struggle to scale to
large prompt sets, so they generalize poorly. We introduce LATTE3D, addressing
these limitations to achieve fast, high-quality generation on a significantly
larger prompt set. Key to our method is 1) building a scalable architecture and
2) leveraging 3D data during optimization through 3D-aware diffusion priors,
shape regularization, and model initialization to achieve robustness to diverse
and complex training prompts. LATTE3D amortizes both neural field and textured
surface generation to produce highly detailed textured meshes in a single
forward pass. LATTE3D generates 3D objects in 400ms, and can be further
enhanced with fast test-time optimization.
]]></content:encoded>
<pubDate>2024-03-22T17:59:37Z</pubDate>
</item>
<item>
<title>ThemeStation: Generating Theme-Aware 3D Assets from Few Exemplars</title>
<link>http://arxiv.org/abs/2403.15383v1</link>
<guid>http://arxiv.org/abs/2403.15383v1</guid>
<content:encoded><![CDATA[
<div> 3D assets, ThemeStation, generation, diversity, quality
<br /><br />
本文介绍了一种名为ThemeStation的方法，用于主题感知的3D到3D生成。该方法通过两个阶段的框架设计来实现自定义3D资产的合成，首先生成概念图像，然后进行参考信息辅助的3D建模。文章提出了一种新颖的双重分数蒸馏（DSD）损失，以共同利用来自输入示例和合成概念图像的先验知识。大量实验和用户研究证实，ThemeStation在生成多样化的主题感知3D模型方面优于先前的工作，并且质量令人印象深刻。ThemeStation还能够实现各种应用，比如可控的3D到3D生成。
<br /> 
总结:本文介绍了一种新颖的方法ThemeStation用于主题感知的3D到3D生成，通过两个阶段的框架设计实现自定义3D资产的合成。提出了一种新颖的双重分数蒸馏（DSD）损失以共同利用先验知识。实验证实，ThemeStation在生成多样化的主题感知3D模型方面优于先前的工作，并且质量令人印象深刻。 <div>
Real-world applications often require a large gallery of 3D assets that share
a consistent theme. While remarkable advances have been made in general 3D
content creation from text or image, synthesizing customized 3D assets
following the shared theme of input 3D exemplars remains an open and
challenging problem. In this work, we present ThemeStation, a novel approach
for theme-aware 3D-to-3D generation. ThemeStation synthesizes customized 3D
assets based on given few exemplars with two goals: 1) unity for generating 3D
assets that thematically align with the given exemplars and 2) diversity for
generating 3D assets with a high degree of variations. To this end, we design a
two-stage framework that draws a concept image first, followed by a
reference-informed 3D modeling stage. We propose a novel dual score
distillation (DSD) loss to jointly leverage priors from both the input
exemplars and the synthesized concept image. Extensive experiments and user
studies confirm that ThemeStation surpasses prior works in producing diverse
theme-aware 3D models with impressive quality. ThemeStation also enables
various applications such as controllable 3D-to-3D generation.
]]></content:encoded>
<pubDate>2024-03-22T17:59:01Z</pubDate>
</item>
<item>
<title>Long-CLIP: Unlocking the Long-Text Capability of CLIP</title>
<link>http://arxiv.org/abs/2403.15378v1</link>
<guid>http://arxiv.org/abs/2403.15378v1</guid>
<content:encoded><![CDATA[
Contrastive Language-Image Pre-training (CLIP) has been the cornerstone for
zero-shot classification, text-image retrieval, and text-image generation by
aligning image and text modalities. Despite its widespread adoption, a
significant limitation of CLIP lies in the inadequate length of text input. The
length of the text token is restricted to 77, and an empirical study shows the
actual effective length is even less than 20. This prevents CLIP from handling
detailed descriptions, limiting its applications for image retrieval and
text-to-image generation with extensive prerequisites. To this end, we propose
Long-CLIP as a plug-and-play alternative to CLIP that supports long-text input,
retains or even surpasses its zero-shot generalizability, and aligns the CLIP
latent space, making it readily replace CLIP without any further adaptation in
downstream frameworks. Nevertheless, achieving this goal is far from
straightforward, as simplistic fine-tuning can result in a significant
degradation of CLIP's performance. Moreover, substituting the text encoder with
a language model supporting longer contexts necessitates pretraining with vast
amounts of data, incurring significant expenses. Accordingly, Long-CLIP
introduces an efficient fine-tuning solution on CLIP with two novel strategies
designed to maintain the original capabilities, including (1) a
knowledge-preserved stretching of positional embedding and (2) a primary
component matching of CLIP features. With leveraging just one million extra
long text-image pairs, Long-CLIP has shown the superiority to CLIP for about
20% in long caption text-image retrieval and 6% in traditional text-image
retrieval tasks, e.g., COCO and Flickr30k. Furthermore, Long-CLIP offers
enhanced capabilities for generating images from detailed text descriptions by
replacing CLIP in a plug-and-play manner.
]]></content:encoded>
<pubDate>2024-03-22T17:58:16Z</pubDate>
</item>
<item>
<title>Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language
  Models</title>
<link>http://arxiv.org/abs/2403.12966v1</link>
<guid>http://arxiv.org/abs/2403.12966v1</guid>
<content:encoded><![CDATA[
<div> 关键词：视觉-语言理解、大型视觉-语言模型、交互式推理、细节信息、多模态数据集

总结：<br /><br />本文介绍了大型视觉-语言模型中视觉编码器在提取有针对性的特征方面所面临的挑战，以及现有模型普遍采用低分辨率图像限制了视觉识别能力的问题。作者提出了Chain-of-Spot (CoS)方法，即交互式推理，通过关注图像中与问题或指令相对应的关键区域，从而增强特征提取，实现了多粒度图像特征的获取。将Chain-of-Spot与LLaVA-1.5模型相结合，显著提升了模型在多模态数据集和基准测试中的性能，取得了新的最先进结果。作者的实证研究表明，该方法显著改善了大型视觉-语言模型理解和推理视觉内容的能力，为更复杂的视觉指令跟随应用铺平了道路。 <div>
In the realm of vision-language understanding, the proficiency of models in
interpreting and reasoning over visual content has become a cornerstone for
numerous applications. However, it is challenging for the visual encoder in
Large Vision-Language Models (LVLMs) to extract useful features tailored to
questions that aid the language model's response. Furthermore, a common
practice among existing LVLMs is to utilize lower-resolution images, which
restricts the ability for visual recognition. Our work introduces the
Chain-of-Spot (CoS) method, which we describe as Interactive Reasoning, a novel
approach that enhances feature extraction by focusing on key regions of
interest (ROI) within the image, corresponding to the posed questions or
instructions. This technique allows LVLMs to access more detailed visual
information without altering the original image resolution, thereby offering
multi-granularity image features. By integrating Chain-of-Spot with
instruct-following LLaVA-1.5 models, the process of image reasoning
consistently improves performance across a wide range of multimodal datasets
and benchmarks without bells and whistles and achieves new state-of-the-art
results. Our empirical findings demonstrate a significant improvement in LVLMs'
ability to understand and reason about visual content, paving the way for more
sophisticated visual instruction-following applications. Code and models are
available at https://github.com/dongyh20/Chain-of-Spot
]]></content:encoded>
<pubDate>2024-03-19T17:59:52Z</pubDate>
</item>
<item>
<title>LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images</title>
<link>http://arxiv.org/abs/2403.11703v1</link>
<guid>http://arxiv.org/abs/2403.11703v1</guid>
<content:encoded><![CDATA[
<div> 关键词: Visual encoding, Large multimodal model (LMM), LLaVA-UHD, 图像模块化策略, 压缩模块

LLaVA-UHD是一个能够高效地感知任何宽高比和高分辨率图像的大型多模式模型。它包括图像模块化策略、压缩模块和空间模式等关键组件，能够在9个基准测试中胜过已有LMM模型。LLaVA-UHD在仅使用94%的推理计算的情况下，支持比LLaVA-1.5 336x336大6倍的分辨率图像，并在TextVQA上取得了6.4%的准确性提升。此外，该模型能够在学术环境下高效训练，仅需在8个A100 GPU上花费23小时，相比LLaVA-1.5的26小时。具体数据和代码可在https://github.com/thunlp/LLaVA-UHD上公开获取。<br /><br />总结: 本文介绍了LLaVA-UHD，一个针对视觉编码策略的大型多模式模型。通过图像模块化策略、压缩模块和空间模式等关键组件，LLaVA-UHD能够在高分辨率和任何宽高比的图像上表现出色，在多个基准测试中超越已有的LMM模型。 <div>
Visual encoding constitutes the basis of large multimodal models (LMMs) in
understanding the visual world. Conventional LMMs process images in fixed sizes
and limited resolutions, while recent explorations in this direction are
limited in adaptivity, efficiency, and even correctness. In this work, we first
take GPT-4V and LLaVA-1.5 as representative examples and expose systematic
flaws rooted in their visual encoding strategy. To address the challenges, we
present LLaVA-UHD, a large multimodal model that can efficiently perceive
images in any aspect ratio and high resolution. LLaVA-UHD includes three key
components: (1) An image modularization strategy that divides native-resolution
images into smaller variable-sized slices for efficient and extensible
encoding, (2) a compression module that further condenses image tokens from
visual encoders, and (3) a spatial schema to organize slice tokens for LLMs.
Comprehensive experiments show that LLaVA-UHD outperforms established LMMs
trained with 2-3 orders of magnitude more data on 9 benchmarks. Notably, our
model built on LLaVA-1.5 336x336 supports 6 times larger (i.e., 672x1088)
resolution images using only 94% inference computation, and achieves 6.4
accuracy improvement on TextVQA. Moreover, the model can be efficiently trained
in academic settings, within 23 hours on 8 A100 GPUs (vs. 26 hours of
LLaVA-1.5). We make the data and code publicly available at
https://github.com/thunlp/LLaVA-UHD.
]]></content:encoded>
<pubDate>2024-03-18T12:04:11Z</pubDate>
</item>
<item>
<title>Virbo: Multimodal Multilingual Avatar Video Generation in Digital
  Marketing</title>
<link>http://arxiv.org/abs/2403.11700v1</link>
<guid>http://arxiv.org/abs/2403.11700v1</guid>
<content:encoded><![CDATA[
<div> 智能系统、短视频制作、多语言定制、视频营销、成本降低
<br /><br />总结:
本文介绍了一个名为Virbo的智能系统，支持自动生成讲话头像视频。用户只需提供指定的脚本，Virbo便可以使用深度生成模型生成目标讲话视频。同时，该系统还支持多模态输入，可定制具有指定面部、指定声音和特效的视频。此外，系统还集成了一个多语言定制模块，支持批量生成数百种精美模板和创意特效的多语言讲话头像视频。通过一系列用户研究和演示测试，我们发现Virbo能够生成保持高质量的视频，而整个制作成本大大降低。这一智能系统将有效推动视频制作行业的发展，解决语言障碍和成本挑战，促进互联网营销。 <div>
With the widespread popularity of internet celebrity marketing all over the
world, short video production has gradually become a popular way of presenting
products information. However, the traditional video production industry
usually includes series of procedures as script writing, video filming in a
professional studio, video clipping, special effects rendering, customized
post-processing, and so forth. Not to mention that multilingual videos is not
accessible for those who could not speak multilingual languages. These
complicated procedures usually needs a professional team to complete, and this
made short video production costly in both time and money. This paper presents
an intelligent system that supports the automatic generation of talking avatar
videos, namely Virbo. With simply a user-specified script, Virbo could use a
deep generative model to generate a target talking videos. Meanwhile, the
system also supports multimodal inputs to customize the video with specified
face, specified voice and special effects. This system also integrated a
multilingual customization module that supports generate multilingual talking
avatar videos in a batch with hundreds of delicate templates and creative
special effects. Through a series of user studies and demo tests, we found that
Virbo can generate talking avatar videos that maintained a high quality of
videos as those from a professional team while reducing the entire production
costs significantly. This intelligent system will effectively promote the video
production industry and facilitate the internet marketing neglecting of
language barriers and cost challenges.
]]></content:encoded>
<pubDate>2024-03-18T11:56:35Z</pubDate>
</item>
<item>
<title>VideoAgent: Long-form Video Understanding with Large Language Model as
  Agent</title>
<link>http://arxiv.org/abs/2403.10517v1</link>
<guid>http://arxiv.org/abs/2403.10517v1</guid>
<content:encoded><![CDATA[
<div> 长形视频理解，计算机视觉，VideoAgent，互动推理和规划，EgoSchema和NExT-QA benchmarks<br />
总结:<br />
这篇文章介绍了长形视频理解方面的挑战，提出了一种名为VideoAgent的代理系统，它采用大型语言模型作为中央代理来迭代地识别和编译关键信息以回答问题，并利用视觉-语言基础模型来翻译和检索视觉信息。经过在具有挑战性的EgoSchema和NExT-QA基准测试上的评估，VideoAgent取得了优秀的成绩，证明了该方法的有效性和效率，突显了代理式方法在推进长形视频理解方面的潜力。 <div>
Long-form video understanding represents a significant challenge within
computer vision, demanding a model capable of reasoning over long multi-modal
sequences. Motivated by the human cognitive process for long-form video
understanding, we emphasize interactive reasoning and planning over the ability
to process lengthy visual inputs. We introduce a novel agent-based system,
VideoAgent, that employs a large language model as a central agent to
iteratively identify and compile crucial information to answer a question, with
vision-language foundation models serving as tools to translate and retrieve
visual information. Evaluated on the challenging EgoSchema and NExT-QA
benchmarks, VideoAgent achieves 54.1% and 71.3% zero-shot accuracy with only
8.4 and 8.2 frames used on average. These results demonstrate superior
effectiveness and efficiency of our method over the current state-of-the-art
methods, highlighting the potential of agent-based approaches in advancing
long-form video understanding.
]]></content:encoded>
<pubDate>2024-03-15T17:57:52Z</pubDate>
</item>
<item>
<title>SCP-Diff: Photo-Realistic Semantic Image Synthesis with
  Spatial-Categorical Joint Prior</title>
<link>http://arxiv.org/abs/2403.09638v1</link>
<guid>http://arxiv.org/abs/2403.09638v1</guid>
<content:encoded><![CDATA[
<div> 关键词: Semantic image synthesis, ControlNet, latent diffusion models, noise priors, SCP-Diff

总结:
语义图像合成是传感器模拟领域的一个有希望的技术，但目前基于GAN的最佳实践在质量上还未达到期望的水平。然而，随着潜在扩散模型在图像生成方面取得了重大进展，有必要评估ControlNet这一密集控制能力显著的方法。我们的研究发现了ControlNet结果的两个主要问题：大语义区域内存在奇怪的子结构，以及内容与语义掩模不匹配。通过实证研究，我们找到了这些问题的原因，即训练数据分布与推断阶段应用的标准正态先验之间的不匹配。为了解决这一挑战，我们为SIS开发了特定的噪声先验，包括空间、分类和新颖的空间-分类联合先验。这种方法被我们称为SCP-Diff，并取得了卓越的成果，在Cityscapes上达到了10.53的FID，在ADE20K上达到了12.66的FID。项目页面上可以访问到代码和模型。 <br /><br /> <div>
Semantic image synthesis (SIS) shows good promises for sensor simulation.
However, current best practices in this field, based on GANs, have not yet
reached the desired level of quality. As latent diffusion models make
significant strides in image generation, we are prompted to evaluate
ControlNet, a notable method for its dense control capabilities. Our
investigation uncovered two primary issues with its results: the presence of
weird sub-structures within large semantic areas and the misalignment of
content with the semantic mask. Through empirical study, we pinpointed the
cause of these problems as a mismatch between the noised training data
distribution and the standard normal prior applied at the inference stage. To
address this challenge, we developed specific noise priors for SIS,
encompassing spatial, categorical, and a novel spatial-categorical joint prior
for inference. This approach, which we have named SCP-Diff, has yielded
exceptional results, achieving an FID of 10.53 on Cityscapes and 12.66 on
ADE20K.The code and models can be accessed via the project page.
]]></content:encoded>
<pubDate>2024-03-14T17:59:55Z</pubDate>
</item>
<item>
<title>3D-VLA: A 3D Vision-Language-Action Generative World Model</title>
<link>http://arxiv.org/abs/2403.09631v1</link>
<guid>http://arxiv.org/abs/2403.09631v1</guid>
<content:encoded><![CDATA[
<div> 3D-VLA, perception, action, world model, embodied foundation model

3D-VLA是一种新型的视觉-语言-行动模型，与以往的2D输入不同，它能够无缝地连接3D感知、推理和行动。该模型引入了一系列交互令牌，以便在具体环境中进行交互，并通过训练一系列体现扩散模型，使模型具备生成能力。此外，模型还建立了一个大规模的3D具体指令数据集，通过提取现有机器人数据集中的大量3D相关信息进行训练。实验表明，3D-VLA显著改善了在具体环境中的推理、多模态生成和规划能力，展现出其在真实世界应用中的潜力。<br /><br />总结: <div>
Recent vision-language-action (VLA) models rely on 2D inputs, lacking
integration with the broader realm of the 3D physical world. Furthermore, they
perform action prediction by learning a direct mapping from perception to
action, neglecting the vast dynamics of the world and the relations between
actions and dynamics. In contrast, human beings are endowed with world models
that depict imagination about future scenarios to plan actions accordingly. To
this end, we propose 3D-VLA by introducing a new family of embodied foundation
models that seamlessly link 3D perception, reasoning, and action through a
generative world model. Specifically, 3D-VLA is built on top of a 3D-based
large language model (LLM), and a set of interaction tokens is introduced to
engage with the embodied environment. Furthermore, to inject generation
abilities into the model, we train a series of embodied diffusion models and
align them into the LLM for predicting the goal images and point clouds. To
train our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by
extracting vast 3D-related information from existing robotics datasets. Our
experiments on held-in datasets demonstrate that 3D-VLA significantly improves
the reasoning, multimodal generation, and planning capabilities in embodied
environments, showcasing its potential in real-world applications.
]]></content:encoded>
<pubDate>2024-03-14T17:58:41Z</pubDate>
</item>
<item>
<title>Quiet-STaR: Language Models Can Teach Themselves to Think Before
  Speaking</title>
<link>http://arxiv.org/abs/2403.09629v1</link>
<guid>http://arxiv.org/abs/2403.09629v1</guid>
<content:encoded><![CDATA[
When writing and talking, people sometimes pause to think. Although
reasoning-focused works have often framed reasoning as a method of answering
questions or completing agentic tasks, reasoning is implicit in almost all
written text. For example, this applies to the steps not stated between the
lines of a proof or to the theory of mind underlying a conversation. In the
Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned
by inferring rationales from few-shot examples in question-answering and
learning from those that lead to a correct answer. This is a highly constrained
setting -- ideally, a language model could instead learn to infer unstated
rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR
in which LMs learn to generate rationales at each token to explain future text,
improving their predictions. We address key challenges, including 1) the
computational cost of generating continuations, 2) the fact that the LM does
not initially know how to generate or use internal thoughts, and 3) the need to
predict beyond individual next tokens. To resolve these, we propose a tokenwise
parallel sampling algorithm, using learnable tokens indicating a thought's
start and end, and an extended teacher-forcing technique. Encouragingly,
generated rationales disproportionately help model difficult-to-predict tokens
and improve the LM's ability to directly answer difficult questions. In
particular, after continued pretraining of an LM on a corpus of internet text
with Quiet-STaR, we find zero-shot improvements on GSM8K
(5.9%$\rightarrow$10.9%) and CommonsenseQA (36.3%$\rightarrow$47.2%) and
observe a perplexity improvement of difficult tokens in natural text.
Crucially, these improvements require no fine-tuning on these tasks. Quiet-STaR
marks a step towards LMs that can learn to reason in a more general and
scalable way.
]]></content:encoded>
<pubDate>2024-03-14T17:58:16Z</pubDate>
</item>
<item>
<title>VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis</title>
<link>http://arxiv.org/abs/2403.08764v1</link>
<guid>http://arxiv.org/abs/2403.08764v1</guid>
<content:encoded><![CDATA[
<div> VLOGGER, audio-driven, human video generation, diffusion model, MENTOR

VLOGGER提出了一种基于音频驱动的单张人物图像生成视频的方法，借鉴了最近生成扩散模型的成功。该方法包括1）随机的人体到三维运动扩散模型，2）一种新颖的基于扩散的架构，将文本到图像模型与空间和时间控制相结合。这支持生成可变长度的高质量视频，通过人脸和身体的高级表示轻松控制。与先前的工作不同，该方法不需要为每个人进行训练，也不依赖于人脸检测和裁剪，并且生成完整的图像（不仅仅是脸部或嘴唇），考虑到关键的场景（例如可见的躯干或多样化的主体身份），以正确合成交流的人类。他们还提出了MENTOR，这是一个新的多样化数据集，具有3D姿势和表情注释，比以前的数据集大一个数量级（800,000个身份），并具有动态手势，他们在这个数据集上训练并消融了他们的主要技术贡献。VLOGGER在三个公开基准测试中表现优越，考虑到图像质量，身份保留和时间一致性，同时生成上半身手势。他们分析了VLOGGER的性能，关于多个多样性指标，显示出他们的架构选择和MENTOR的使用有利于以规模训练公正和无偏差的模型。最后，他们展示了视频编辑和个性化的应用。

<br /><br />总结: VLOGGER是一种基于音频驱动的人类视频生成方法，利用扩散模型和大规模数据集MENTOR，能够生成高质量、可变长度的视频，并具有上半身手势。它不需要为每个人进行训练，能够生成完整的人物图像，并考虑到不同的场景和主体身份。该方法在公开基准测试中表现优越，同时能够训练公正和无偏差的模型。 <div>
We propose VLOGGER, a method for audio-driven human video generation from a
single input image of a person, which builds on the success of recent
generative diffusion models. Our method consists of 1) a stochastic
human-to-3d-motion diffusion model, and 2) a novel diffusion-based architecture
that augments text-to-image models with both spatial and temporal controls.
This supports the generation of high quality video of variable length, easily
controllable through high-level representations of human faces and bodies. In
contrast to previous work, our method does not require training for each
person, does not rely on face detection and cropping, generates the complete
image (not just the face or the lips), and considers a broad spectrum of
scenarios (e.g. visible torso or diverse subject identities) that are critical
to correctly synthesize humans who communicate. We also curate MENTOR, a new
and diverse dataset with 3d pose and expression annotations, one order of
magnitude larger than previous ones (800,000 identities) and with dynamic
gestures, on which we train and ablate our main technical contributions.
  VLOGGER outperforms state-of-the-art methods in three public benchmarks,
considering image quality, identity preservation and temporal consistency while
also generating upper-body gestures. We analyze the performance of VLOGGER with
respect to multiple diversity metrics, showing that our architectural choices
and the use of MENTOR benefit training a fair and unbiased model at scale.
Finally we show applications in video editing and personalization.
]]></content:encoded>
<pubDate>2024-03-13T17:59:02Z</pubDate>
</item>
<item>
<title>Bridging Different Language Models and Generative Vision Models for
  Text-to-Image Generation</title>
<link>http://arxiv.org/abs/2403.07860v1</link>
<guid>http://arxiv.org/abs/2403.07860v1</guid>
<content:encoded><![CDATA[
<div> text-to-image generation, language model, vision model, integration, LaVi-Bridge

总结: 
本文介绍了文本到图像生成的最新发展，即文本到图像扩散模型。这些模型通常包括一个语言模型，用于解释用户提示，以及一个视觉模型，用于生成相应的图像。研究的目标是探索如何整合任意两个不相关的语言和生成视觉模型，以提高文本到图像生成的质量。文中提出了LaVi-Bridge，这是一个能够整合各种预训练语言模型和生成视觉模型的流程。通过利用LoRA和适配器，LaVi-Bridge提供了一种灵活的插拔式方法，无需修改语言和视觉模型的原始权重。研究表明，整合更先进的模块，如更先进的语言模型或生成视觉模型，可以显着提高文本对齐和图像质量等能力。通过广泛的评估验证了LaVi-Bridge的有效性。 该项目的代码可在https://github.com/ShihaoZhaoZSH/LaVi-Bridge 上找到。 <div>
Text-to-image generation has made significant advancements with the
introduction of text-to-image diffusion models. These models typically consist
of a language model that interprets user prompts and a vision model that
generates corresponding images. As language and vision models continue to
progress in their respective domains, there is a great potential in exploring
the replacement of components in text-to-image diffusion models with more
advanced counterparts. A broader research objective would therefore be to
investigate the integration of any two unrelated language and generative vision
models for text-to-image generation. In this paper, we explore this objective
and propose LaVi-Bridge, a pipeline that enables the integration of diverse
pre-trained language models and generative vision models for text-to-image
generation. By leveraging LoRA and adapters, LaVi-Bridge offers a flexible and
plug-and-play approach without requiring modifications to the original weights
of the language and vision models. Our pipeline is compatible with various
language models and generative vision models, accommodating different
structures. Within this framework, we demonstrate that incorporating superior
modules, such as more advanced language models or generative vision models,
results in notable improvements in capabilities like text alignment or image
quality. Extensive evaluations have been conducted to verify the effectiveness
of LaVi-Bridge. Code is available at
https://github.com/ShihaoZhaoZSH/LaVi-Bridge.
]]></content:encoded>
<pubDate>2024-03-12T17:50:11Z</pubDate>
</item>
<item>
<title>Gemini 1.5: Unlocking multimodal understanding across millions of tokens
  of context</title>
<link>http://arxiv.org/abs/2403.05530v1</link>
<guid>http://arxiv.org/abs/2403.05530v1</guid>
<content:encoded><![CDATA[
<div> Gemini 1.5 Pro, multimodal, long-context, recall, state-of-the-art

Gemini 1.5 Pro是Gemini家族的最新模型，是一种高效的多模态专家混合模型，能够在包括多个长文档和数小时的视频和音频在内的上下文中召回和推理细粒度信息。在长上下文检索任务中，Gemini 1.5 Pro实现了接近完美的召回率，提高了长文档QA、长视频QA和长上下文ASR的最新水平，并在各种基准测试中达到或超过了Gemini 1.0 Ultra的最新表现。研究了Gemini 1.5 Pro在长上下文能力方面的极限，发现了在接下来的标记预测和近乎完美的（>99%）检索方面的持续改进，可达到至少10M个标记，比现有模型如Claude 2.1（20万）和GPT-4 Turbo（12.8万）有了一代的飞跃。最后，我们突出介绍了大型语言模型在前沿的惊人新能力；当给定一个Kalamang语法手册时，这个模型学会了将英语翻译成Kalamang，其水平与从相同内容学习的人相似。 <br /><br />总结: Gemimi 1.5 Pro是一款高效的多模态专家混合模型，能够处理长上下文任务，并在长文档QA、长视频QA和长上下文ASR方面取得了最新的突破。此外，该模型还在标记预测和检索方面表现出非常优越的能力，超越了现有的一些模型。最后，该模型还展示了在小语种翻译方面的惊人能力。 <div>
In this report, we present the latest model of the Gemini family, Gemini 1.5
Pro, a highly compute-efficient multimodal mixture-of-experts model capable of
recalling and reasoning over fine-grained information from millions of tokens
of context, including multiple long documents and hours of video and audio.
Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks
across modalities, improves the state-of-the-art in long-document QA,
long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's
state-of-the-art performance across a broad set of benchmarks. Studying the
limits of Gemini 1.5 Pro's long-context ability, we find continued improvement
in next-token prediction and near-perfect retrieval (>99%) up to at least 10M
tokens, a generational leap over existing models such as Claude 2.1 (200k) and
GPT-4 Turbo (128k). Finally, we highlight surprising new capabilities of large
language models at the frontier; when given a grammar manual for Kalamang, a
language with fewer than 200 speakers worldwide, the model learns to translate
English to Kalamang at a similar level to a person who learned from the same
content.
]]></content:encoded>
<pubDate>2024-03-08T18:54:20Z</pubDate>
</item>
<item>
<title>Mechanism for Decision-aware Collaborative Federated Learning: A Pitfall
  of Shapley Values</title>
<link>http://arxiv.org/abs/2403.04753v1</link>
<guid>http://arxiv.org/abs/2403.04753v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 机制设计, 协作学习, 联邦学习平台, 决策感知, Shapley value

总结: 本论文研究了通过联邦学习平台实现决策感知协作学习的机制设计。文章介绍了一个由数字平台和多个决策感知代理组成的框架，平台提供基础设施，使代理可以访问数据，为协作学习创造激励，进行联邦学习以避免直接共享原始数据。文章引入了多行动协作联邦学习（MCFL）框架，分析了著名的Shapley value机制的均衡问题。研究发现，虽然Shapley value通过鼓励充分参与有效地最大化了联合创造的剩余价值，但不慎促进了虚假身份操纵，进一步增加了平台进行联邦学习时的通信成本。因此，Shapley value机制有一个严重的缺陷，即暗示着数据分割和身份复制，最终损害了联邦学习系统的整体效率。<br /><br />总结: 本论文研究了决策感知协作学习的机制设计，介绍了联邦学习平台的框架，分析了Shapley value机制的均衡问题，并发现其存在的缺陷。 <div>
This paper investigates mechanism design for decision-aware collaboration via
federated learning (FL) platforms. Our framework consists of a digital platform
and multiple decision-aware agents, each endowed with proprietary data sets.
The platform offers an infrastructure that enables access to the data, creates
incentives for collaborative learning aimed at operational decision-making, and
conducts FL to avoid direct raw data sharing. The computation and communication
efficiency of the FL process is inherently influenced by the agent
participation equilibrium induced by the mechanism. Therefore, assessing the
system's efficiency involves two critical factors: the surplus created by
coalition formation and the communication costs incurred across the coalition
during FL. To evaluate the system efficiency under the intricate interplay
between mechanism design, agent participation, operational decision-making, and
the performance of FL algorithms, we introduce a multi-action collaborative
federated learning (MCFL) framework for decision-aware agents. Under this
framework, we further analyze the equilibrium for the renowned Shapley value
based mechanisms. Specifically, we examine the issue of false-name
manipulation, a form of dishonest behavior where participating agents create
duplicate fake identities to split their original data among these identities.
By solving the agent participation equilibrium, we demonstrate that while
Shapley value effectively maximizes coalition-generated surplus by encouraging
full participation, it inadvertently promotes false-name manipulation. This
further significantly increases the communication costs when the platform
conducts FL. Thus, we highlight a significant pitfall of Shapley value based
mechanisms, which implicitly incentivizes data splitting and identity
duplication, ultimately impairing the overall efficiency in FL systems.
]]></content:encoded>
<pubDate>2024-03-07T18:54:59Z</pubDate>
</item>
<item>
<title>Stop Regressing: Training Value Functions via Classification for
  Scalable Deep RL</title>
<link>http://arxiv.org/abs/2403.03950v1</link>
<guid>http://arxiv.org/abs/2403.03950v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 深度强化学习, 值函数, 分类交叉熵, 可扩展性, 改进性能

深度强化学习中的值函数是一个核心组件，通过神经网络参数化，采用均方误差回归目标进行训练以匹配自举目标值。然而，将使用回归的基于值的强化学习方法扩展到大型网络（如高容量Transformer）的难度已被证明是具有挑战性的。观察到这一差异，本文研究了是否通过在训练值函数时仅使用分类代替回归可以改进深度强化学习的可扩展性。我们证明，使用分类交叉熵训练的值函数显著改善了多个领域的性能和可扩展性。其中包括：在Atari 2600游戏中使用SoftMoEs进行单任务强化学习，使用大规模ResNets进行Atari的多任务强化学习，在Q-transformers中进行机器人操纵，无搜索下的国际象棋对局，以及使用高容量Transformer进行语言代理Wordle任务，在这些领域取得了最先进的结果。通过仔细分析，我们展示了分类交叉熵的好处主要源于其减轻了值为基础的强化学习固有的问题，如嘈杂的目标和非稳态性。总体而言，我们认为简单地将值函数训练与分类交叉熵相结合可以在几乎没有成本的情况下显著提高深度强化学习的可扩展性。
<br /><br />总结: 本文研究了通过使用分类交叉熵代替回归来训练值函数，从而改善深度强化学习的可扩展性。作者证明了这种方法在多个领域均取得了显著的性能提升，包括Atari游戏、机器人操纵和语言代理任务。通过分析，作者指出分类交叉熵的好处主要来自于其减轻值函数训练中的固有问题。因此，本文的贡献在于提出了一种简单且高效的方法来提高深度强化学习的可扩展性，为相关领域的研究和应用带来了新的启发。 <div>
Value functions are a central component of deep reinforcement learning (RL).
These functions, parameterized by neural networks, are trained using a mean
squared error regression objective to match bootstrapped target values.
However, scaling value-based RL methods that use regression to large networks,
such as high-capacity Transformers, has proven challenging. This difficulty is
in stark contrast to supervised learning: by leveraging a cross-entropy
classification loss, supervised methods have scaled reliably to massive
networks. Observing this discrepancy, in this paper, we investigate whether the
scalability of deep RL can also be improved simply by using classification in
place of regression for training value functions. We demonstrate that value
functions trained with categorical cross-entropy significantly improves
performance and scalability in a variety of domains. These include: single-task
RL on Atari 2600 games with SoftMoEs, multi-task RL on Atari with large-scale
ResNets, robotic manipulation with Q-transformers, playing Chess without
search, and a language-agent Wordle task with high-capacity Transformers,
achieving state-of-the-art results on these domains. Through careful analysis,
we show that the benefits of categorical cross-entropy primarily stem from its
ability to mitigate issues inherent to value-based RL, such as noisy targets
and non-stationarity. Overall, we argue that a simple shift to training value
functions with categorical cross-entropy can yield substantial improvements in
the scalability of deep RL at little-to-no cost.
]]></content:encoded>
<pubDate>2024-03-06T18:55:47Z</pubDate>
</item>
<item>
<title>Can Audio Reveal Music Performance Difficulty? Insights from the Piano
  Syllabus Dataset</title>
<link>http://arxiv.org/abs/2403.03947v1</link>
<guid>http://arxiv.org/abs/2403.03947v1</guid>
<content:encoded><![CDATA[
<div> 音乐教育, 音乐信息检索, 音频分析, 数据集, 难度估计
<br /><br />总结:
音乐教育中自动估计音乐作品难度的重要性，音乐信息检索领域已经有一些工作致力于这一任务。本文首次提出了一个基于音频录音的音乐作品难度估计数据集，并开发了一个识别框架来处理不同的输入表示。实验证明了该提议的有效性，并将数据集、代码和训练模型公开分享，以促进该领域的进一步研究。 <div>
Automatically estimating the performance difficulty of a music piece
represents a key process in music education to create tailored curricula
according to the individual needs of the students. Given its relevance, the
Music Information Retrieval (MIR) field depicts some proof-of-concept works
addressing this task that mainly focuses on high-level music abstractions such
as machine-readable scores or music sheet images. In this regard, the potential
of directly analyzing audio recordings has been generally neglected, which
prevents students from exploring diverse music pieces that may not have a
formal symbolic-level transcription. This work pioneers in the automatic
estimation of performance difficulty of music pieces on audio recordings with
two precise contributions: (i) the first audio-based difficulty estimation
dataset -- namely, Piano Syllabus (PSyllabus) dataset -- featuring 7,901 piano
pieces across 11 difficulty levels from 1,233 composers; and (ii) a recognition
framework capable of managing different input representations -- both unimodal
and multimodal manners -- directly derived from audio to perform the difficulty
estimation task. The comprehensive experimentation comprising different
pre-training schemes, input modalities, and multi-task scenarios prove the
validity of the proposal and establishes PSyllabus as a reference dataset for
audio-based difficulty estimation in the MIR field. The dataset as well as the
developed code and trained models are publicly shared to promote further
research in the field.
]]></content:encoded>
<pubDate>2024-03-06T18:54:13Z</pubDate>
</item>
<item>
<title>Scaling Rectified Flow Transformers for High-Resolution Image Synthesis</title>
<link>http://arxiv.org/abs/2403.03206v1</link>
<guid>http://arxiv.org/abs/2403.03206v1</guid>
<content:encoded><![CDATA[
<div> 高维数据，扩散模型，噪音采样，文本到图像生成，变压器架构
<br />
本文介绍了扩散模型和矫正流模型对高维感知数据的生成建模技术，矫正流模型为直线连接了数据和噪音，具有更好的理论特性和概念简单性，但尚未成为标准做法。作者提出了改进的噪音采样技术，通过偏向感知相关尺度的噪音，提高了对于高分辨率文本到图像合成的性能。此外，作者提出了一个基于变压器的架构，实现了文本到图像的双向信息流，提高了文本理解能力、排版效果和人类评价。最后，作者强调了他们的最大模型优于现有模型，并将实验数据、代码和模型权重公开发布。 
<br /><br />总结: 
<br />扩散模型和矫正流模型为高维感知数据的生成建模提供了有效手段，提出了改进的噪音采样技术，以提高对高分辨率文本到图像合成的性能。新的基于变压器的架构实现了文本到图像的双向信息流，提高了文本理解能力、排版效果和人类评价。作者的最大模型优于现有模型，并将实验数据、代码和模型权重公开发布。 <div>
Diffusion models create data from noise by inverting the forward paths of
data towards noise and have emerged as a powerful generative modeling technique
for high-dimensional, perceptual data such as images and videos. Rectified flow
is a recent generative model formulation that connects data and noise in a
straight line. Despite its better theoretical properties and conceptual
simplicity, it is not yet decisively established as standard practice. In this
work, we improve existing noise sampling techniques for training rectified flow
models by biasing them towards perceptually relevant scales. Through a
large-scale study, we demonstrate the superior performance of this approach
compared to established diffusion formulations for high-resolution
text-to-image synthesis. Additionally, we present a novel transformer-based
architecture for text-to-image generation that uses separate weights for the
two modalities and enables a bidirectional flow of information between image
and text tokens, improving text comprehension, typography, and human preference
ratings. We demonstrate that this architecture follows predictable scaling
trends and correlates lower validation loss to improved text-to-image synthesis
as measured by various metrics and human evaluations. Our largest models
outperform state-of-the-art models, and we will make our experimental data,
code, and model weights publicly available.
]]></content:encoded>
<pubDate>2024-03-05T18:45:39Z</pubDate>
</item>
<item>
<title>Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers</title>
<link>http://arxiv.org/abs/2402.19479v1</link>
<guid>http://arxiv.org/abs/2402.19479v1</guid>
<content:encoded><![CDATA[
<div> 数据质量、视频标注、Panda-70M数据集、下游任务、模型训练

数据质量和标注质量上限了下游模型的质量。视频文本数据难以收集，因为手动标注耗时多，需要观看整个视频，而视频包含多个场景和动作。为了建立高质量字幕视频数据集，研究人员提出了一种自动方法，利用文本视频描述、字幕和视频帧。他们从公开可用的HD-VILA-100M数据集中筛选出380万个高分辨率视频，并将它们分成语义一致的视频片段，然后利用跨模态教师模型为每个视频获取字幕。接下来，在少量视频子集上微调检索模型，然后在整个数据集上使用该模型选择最佳字幕作为标注。于是得到了7000万个与高质量文本字幕配对的视频数据，命名为Panda-70M数据集。研究人员展示了该数据集在视频字幕生成、视频和文本检索等三项下游任务上的价值，表明基于该数据集训练的模型在大多数指标上得分明显更好。<br /><br />总结: 该研究提出了一种自动方法，利用跨模态输入建立高质量字幕视频数据集Panda-70M，并展示了其在多项下游任务上训练模型得分明显更好的价值。 <div>
The quality of the data and annotation upper-bounds the quality of a
downstream model. While there exist large text corpora and image-text pairs,
high-quality video-text data is much harder to collect. First of all, manual
labeling is more time-consuming, as it requires an annotator to watch an entire
video. Second, videos have a temporal dimension, consisting of several scenes
stacked together, and showing multiple actions. Accordingly, to establish a
video dataset with high-quality captions, we propose an automatic approach
leveraging multimodal inputs, such as textual video description, subtitles, and
individual video frames. Specifically, we curate 3.8M high-resolution videos
from the publicly available HD-VILA-100M dataset. We then split them into
semantically consistent video clips, and apply multiple cross-modality teacher
models to obtain captions for each video. Next, we finetune a retrieval model
on a small subset where the best caption of each video is manually selected and
then employ the model in the whole dataset to select the best caption as the
annotation. In this way, we get 70M videos paired with high-quality text
captions. We dub the dataset as Panda-70M. We show the value of the proposed
dataset on three downstream tasks: video captioning, video and text retrieval,
and text-driven video generation. The models trained on the proposed data score
substantially better on the majority of metrics across all the tasks.
]]></content:encoded>
<pubDate>2024-02-29T18:59:50Z</pubDate>
</item>
<item>
<title>Trajectory Prediction for Autonomous Driving Using a Transformer Network</title>
<link>http://arxiv.org/abs/2402.16501v1</link>
<guid>http://arxiv.org/abs/2402.16501v1</guid>
<content:encoded><![CDATA[
<div> 关键词: autonomous driving, multi-modal trajectory prediction, transformer network, convolutional networks, Lyft l5kit dataset

这篇论文介绍了基于Transformer网络的多模态轨迹预测框架，以及利用每个代理的语义地图作为输入，通过卷积网络自动获取相关的上下文信息。另外，还提出了一种新的辅助损失函数，惩罚不可行的越野预测。实验证明，该模型在Lyft l5kit数据集上取得了最先进的性能，显著提高了预测结果的准确性和可行性。

总结:
1. 论文介绍了自动驾驶中预测周围代理的轨迹是一个具有挑战性的任务。
2. 提出了基于Transformer网络的多模态轨迹预测框架，并利用语义地图作为输入。
3. 使用卷积网络自动获取相关的上下文信息。
4. 提出了一种新的辅助损失函数，用于惩罚不可行的越野预测。
5. 实验证明所提出的模型在Lyft l5kit数据集上取得了最先进的性能，显著提高了预测结果的准确性和可行性。 <div>
Predicting the trajectories of surrounding agents is still considered one of
the most challenging tasks for autonomous driving. In this paper, we introduce
a multi-modal trajectory prediction framework based on the transformer network.
The semantic maps of each agent are used as inputs to convolutional networks to
automatically derive relevant contextual information. A novel auxiliary loss
that penalizes unfeasible off-road predictions is also proposed in this study.
Experiments on the Lyft l5kit dataset show that the proposed model achieves
state-of-the-art performance, substantially improving the accuracy and
feasibility of the prediction outcomes.
]]></content:encoded>
<pubDate>2024-02-26T11:35:23Z</pubDate>
</item>
<item>
<title>LLMArena: Assessing Capabilities of Large Language Models in Dynamic
  Multi-Agent Environments</title>
<link>http://arxiv.org/abs/2402.16499v1</link>
<guid>http://arxiv.org/abs/2402.16499v1</guid>
<content:encoded><![CDATA[
<div> LLMArena, 大型语言模型, 多智能体环境, 评估框架, 实验

LLMArena 是一个新颖且易于扩展的框架，用于评估大型语言模型在多智能体动态环境中的不同能力。该框架包括七个不同的游戏环境，并采用 Trueskill 计分来评估语言模型的关键能力，包括空间推理、战略规划、数值推理、风险评估、沟通、对手建模和团队协作。研究人员通过对不同大小和类型的大型语言模型进行大量实验和人类评估，发现大型语言模型在对手建模和团队协作方面仍有很大的发展空间。作者希望LLMArena可以引导未来的研究，增强大型语言模型的这些能力，最终实现在动态的多智能体环境中更复杂和实用的应用。  <br /><br />总结: <br />LLMArena 是一个新颖且易于扩展的框架，用于评估大型语言模型在多智能体动态环境中的不同能力。该框架包括七个不同的游戏环境，并采用 Trueskill 计分来评估语言模型的关键能力，研究发现大型语言模型在对手建模和团队协作方面仍有很大的发展空间。作者希望LLMArena可以引导未来的研究，增强大型语言模型的这些能力，最终实现在动态的多智能体环境中更复杂和实用的应用。 <div>
Recent advancements in large language models (LLMs) have revealed their
potential for achieving autonomous agents possessing human-level intelligence.
However, existing benchmarks for evaluating LLM Agents either use static
datasets, potentially leading to data leakage or focus only on single-agent
scenarios, overlooking the complexities of multi-agent interactions. There is a
lack of a benchmark that evaluates the diverse capabilities of LLM agents in
multi-agent, dynamic environments. To this end, we introduce LLMArena, a novel
and easily extensible framework for evaluating the diverse capabilities of LLM
in multi-agent dynamic environments. LLMArena encompasses seven distinct gaming
environments, employing Trueskill scoring to assess crucial abilities in LLM
agents, including spatial reasoning, strategic planning, numerical reasoning,
risk assessment, communication, opponent modeling, and team collaboration. We
conduct an extensive experiment and human evaluation among different sizes and
types of LLMs, showing that LLMs still have a significant journey ahead in
their development towards becoming fully autonomous agents, especially in
opponent modeling and team collaboration. We hope LLMArena could guide future
research towards enhancing these capabilities in LLMs, ultimately leading to
more sophisticated and practical applications in dynamic, multi-agent settings.
The code and data will be available.
]]></content:encoded>
<pubDate>2024-02-26T11:31:48Z</pubDate>
</item>
<item>
<title>Q-FOX Learning: Breaking Tradition in Reinforcement Learning</title>
<link>http://arxiv.org/abs/2402.16562v1</link>
<guid>http://arxiv.org/abs/2402.16562v1</guid>
<content:encoded><![CDATA[
<div> 强化学习, 人工智能, 超参数调优, Q-FOX, OpenAI Gym
总结:<br /><br />这篇文章介绍了强化学习中超参数调优的重要性，提出了一种新的自动超参数调优方法 Q-FOX，该方法利用了FOX优化器和Q-learning算法，通过优化新的目标函数，优化了超参数以获得更好的强化学习性能。实验结果表明，Q-FOX相比其他优化器在解决OpenAI Gym环境控制任务时获得了更高的累积奖励。但Q-FOX仍有局限性，不能直接用于真实环境，也需要在模拟环境中进行迭代优化，因此存在一定的时间成本。综合来看，Q-FOX在强化学习的超参数调优方面发挥了重要作用。 <div>
Reinforcement learning (RL) is a subset of artificial intelligence (AI) where
agents learn the best action by interacting with the environment, making it
suitable for tasks that do not require labeled data or direct supervision.
Hyperparameters (HP) tuning refers to choosing the best parameter that leads to
optimal solutions in RL algorithms. Manual or random tuning of the HP may be a
crucial process because variations in this parameter lead to changes in the
overall learning aspects and different rewards. In this paper, a novel and
automatic HP-tuning method called Q-FOX is proposed. This uses both the FOX
optimizer, a new optimization method inspired by nature that mimics red foxes'
hunting behavior, and the commonly used, easy-to-implement RL Q-learning
algorithm to solve the problem of HP tuning. Moreover, a new objective function
is proposed which prioritizes the reward over the mean squared error (MSE) and
learning time (steps). Q-FOX has been evaluated on two OpenAI Gym environment
control tasks: Cart Pole and Frozen Lake. It exposed greater cumulative rewards
than HP tuning with other optimizers, such as PSO, GA, Bee, or randomly
selected HP. The cumulative reward for the Cart Pole task was 32.08, and for
the Frozen Lake task was 0.95. Despite the robustness of Q-FOX, it has
limitations. It cannot be used directly in real-word problems before choosing
the HP in a simulation environment because its processes work iteratively,
making it time-consuming. The results indicate that Q-FOX has played an
essential role in HP tuning for RL algorithms to effectively solve different
control tasks.
]]></content:encoded>
<pubDate>2024-02-26T13:39:04Z</pubDate>
</item>
<item>
<title>Contracts with Inspections</title>
<link>http://arxiv.org/abs/2402.16553v1</link>
<guid>http://arxiv.org/abs/2402.16553v1</guid>
<content:encoded><![CDATA[
<div> hidden-action model, principal-agent, incentive, inspection, deterministic<br />
<br />
本文提出了一个新的模型，放松了隐性行为的假设，允许委托人在一定成本下检查特定的行为。如果委托人发现代理人没有选择协定的行为，他可以扣留支付。这个模型的放松引入了更广泛的策略空间，委托人需要在积极激励（增加支付）和负面激励（增加检查）之间进行权衡。作者展示了如何在所有单调检查成本函数中找到最佳的确定性激励兼容检查方案。然后，作者转向随机检查方案，展示了在检查成本函数为次模的情况下可以有效地找到最佳的随机激励兼容检查方案。作者补充说，对于更一般的XOS检查成本函数，不可能有效地找到最佳的随机检查方案。 <br /><br />总结: 本文提出了一个新的委托人-代理人模型，在此模型下，作者展示了如何找到最佳的确定性和随机激励兼容检查方案。 <div>
In the classical principal-agent hidden-action model, a principal delegates
the execution of a costly task to an agent for which he can choose among
actions with different costs and different success probabilities to accomplish
the task. To incentivize the agent to exert effort, the principal can commit to
a contract, which is the amount of payment based on the task's success. A
crucial assumption of this model is that the principal can only base the
payment on the outcome but not on the agent's chosen action.
  In this work, we relax the hidden-action assumption and introduce a new model
where the principal is allowed to inspect subsets of actions at some cost that
depends on the inspected subset. If the principal discovers that the agent did
not select the agreed-upon action through the inspection, the principal can
withhold payment. This relaxation of the model introduces a broader strategy
space for the principal, who now faces a tradeoff between positive incentives
(increasing payment) and negative incentives (increasing inspection).
  We show how to find the best deterministic incentive-compatible inspection
scheme for all monotone inspection cost functions. We then turn to randomized
inspection schemes and show that one can efficiently find the best randomized
incentive-compatible inspection scheme when the inspection cost function is
submodular. We complement this result by showing that it is impossible to
efficiently find the optimal randomized inspection scheme for the more general
case of XOS inspection cost functions.
]]></content:encoded>
<pubDate>2024-02-26T13:26:34Z</pubDate>
</item>
<item>
<title>AgentOhana: Design Unified Data and Training Pipeline for Effective
  Agent Learning</title>
<link>http://arxiv.org/abs/2402.15506v1</link>
<guid>http://arxiv.org/abs/2402.15506v1</guid>
<content:encoded><![CDATA[
<div> AgentOhana, LLMs, agent trajectories, data loader, xLAM-v0.1
总结:<br /><br />本文介绍了AgentOhana作为解决LLMs在agent-based任务中面临挑战的综合解决方案。AgentOhana聚合了不同环境中的agent轨迹，将它们标准化和统一格式，简化了用于agent训练的数据加载器的创建。利用数据的统一性，我们的训练流程在不同数据源之间保持平衡，并在数据集分区和模型训练过程中保持设备独立的随机性。此外，我们还介绍了xLAM-v0.1，这是一个针对AI Agent的大型动作模型，展现了在各种基准测试中的卓越性能。 <div>
Autonomous agents powered by large language models (LLMs) have garnered
significant research attention. However, fully harnessing the potential of LLMs
for agent-based tasks presents inherent challenges due to the heterogeneous
nature of diverse data sources featuring multi-turn trajectories. In this
paper, we introduce \textbf{AgentOhana} as a comprehensive solution to address
these challenges. \textit{AgentOhana} aggregates agent trajectories from
distinct environments, spanning a wide array of scenarios. It meticulously
standardizes and unifies these trajectories into a consistent format,
streamlining the creation of a generic data loader optimized for agent
training. Leveraging the data unification, our training pipeline maintains
equilibrium across different data sources and preserves independent randomness
across devices during dataset partitioning and model training. Additionally, we
present \textbf{xLAM-v0.1}, a large action model tailored for AI agents, which
demonstrates exceptional performance across various benchmarks.
]]></content:encoded>
<pubDate>2024-02-23T18:56:26Z</pubDate>
</item>
<item>
<title>PALO: A Polyglot Large Multimodal Model for 5B People</title>
<link>http://arxiv.org/abs/2402.14818v1</link>
<guid>http://arxiv.org/abs/2402.14818v1</guid>
<content:encoded><![CDATA[
<div> 多语言模型、视觉推理、Palo、跨语言性能、benchmark<br />
在这项研究中，我们介绍了一种名为Palo的大型多语言多模态模型，该模型涵盖了10种主要语言，包括英语、中文、印地语、西班牙语、法语、阿拉伯语、孟加拉语、俄语、乌尔都语和日语，覆盖了约50亿人口（全球人口的65%）。我们采用半自动化翻译方法，利用经过精细调节的大型语言模型，将多模态指令数据集从英语翻译成目标语言，确保高语言准确性的同时，最大程度地减少了手动工作量。我们的方法提高了跨多种语言的整体性能，特别是对一些较少代表的语言，如印地语、阿拉伯语、孟加拉语和乌尔都语。我们训练了三种规模（1.7B、7B和13B参数）的模型，展示了其泛化性和可扩展性，在与强基线模型相比取得了实质性的改进。此外，我们还提出了首个多语言多模态基准测试，用于评估未来方法在各种语言中的视觉与语言推理能力。 <div>
In pursuit of more inclusive Vision-Language Models (VLMs), this study
introduces a Large Multilingual Multimodal Model called \textsc{Palo}.
\textsc{Palo} offers visual reasoning capabilities in 10 major languages,
including English, Chinese, Hindi, Spanish, French, Arabic, Bengali, Russian,
Urdu, and Japanese, that span a total of $\sim$5B people (65\% of the world
population). Our approach involves a semi-automated translation approach to
adapt the multimodal instruction dataset from English to the target languages
using a fine-tuned Large Language Model, thereby ensuring high linguistic
fidelity while allowing scalability due to minimal manual effort. The
incorporation of diverse instruction sets helps us boost overall performance
across multiple languages especially those that are underrepresented like
Hindi, Arabic, Bengali, and Urdu. The resulting models are trained across three
scales (1.7B, 7B and 13B parameters) to show the generalization and scalability
where we observe substantial improvements compared to strong baselines. We also
propose the first multilingual multimodal benchmark for the forthcoming
approaches to evaluate their vision-language reasoning capabilities across
languages. Code: https://github.com/mbzuai-oryx/PALO.
]]></content:encoded>
<pubDate>2024-02-22T18:59:58Z</pubDate>
</item>
<item>
<title>A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit
  Tasks in Public Health</title>
<link>http://arxiv.org/abs/2402.14807v1</link>
<guid>http://arxiv.org/abs/2402.14807v1</guid>
<content:encoded><![CDATA[
<div> 决策语言模型，RMAB，健康资源分配，政策优先级，模拟研究<br />
决策语言模型（DLM）是一个结合了大型语言模型（LLM）和多臂老虎机算法（RMAB）的工具，旨在通过人类语言指令动态调整公共卫生政策。研究通过与印度的ARMMAN合作，展示了DLM能够通过人类语言指令动态塑造政策结果。文章通过提出DLM这一新方法，希望解决在公共卫生领域面临的挑战，包括资源有限、政策优先级不断变化等问题。这一研究对于提高孕产妇的预防保健水平，降低孕产妇死亡率具有重要意义。 <br /><br />总结: <br />决策语言模型（DLM）结合了大型语言模型和多臂老虎机算法，旨在通过人类语言指令动态调整公共卫生政策。研究展示了DLM能够通过人类语言指令动态塑造政策结果。文章提出了这一新方法，希望解决公共卫生领域面临的挑战，对于提高孕产妇的预防保健水平，降低孕产妇死亡率具有重要意义。 <div>
Efforts to reduce maternal mortality rate, a key UN Sustainable Development
target (SDG Target 3.1), rely largely on preventative care programs to spread
critical health information to high-risk populations. These programs face two
important challenges: efficiently allocating limited health resources to large
beneficiary populations, and adapting to evolving policy priorities. While
prior works in restless multi-armed bandit (RMAB) demonstrated success in
public health allocation tasks, they lack flexibility to adapt to evolving
policy priorities. Concurrently, Large Language Models (LLMs) have emerged as
adept, automated planners in various domains, including robotic control and
navigation. In this paper, we propose DLM: a Decision Language Model for RMABs.
To enable dynamic fine-tuning of RMAB policies for challenging public health
settings using human-language commands, we propose using LLMs as automated
planners to (1) interpret human policy preference prompts, (2) propose code
reward functions for a multi-agent RL environment for RMABs, and (3) iterate on
the generated reward using feedback from RMAB simulations to effectively adapt
policy outcomes. In collaboration with ARMMAN, an India-based public health
organization promoting preventative care for pregnant mothers, we conduct a
simulation study, showing DLM can dynamically shape policy outcomes using only
human language commands as input.
]]></content:encoded>
<pubDate>2024-02-22T18:58:27Z</pubDate>
</item>
<item>
<title>OlympiadBench: A Challenging Benchmark for Promoting AGI with
  Olympiad-Level Bilingual Multimodal Scientific Problems</title>
<link>http://arxiv.org/abs/2402.14008v1</link>
<guid>http://arxiv.org/abs/2402.14008v1</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Multimodal Models, OlympiadBench, GPT-4V, 评估方法

- Large Language Models（LLMs）和 Large Multimodal Models（LMMs）已经超过了一般人类在不同任务上的表现
- 文章介绍了一个新的多语言多模态科学竞赛基准（OlympiadBench），包括数学和物理竞赛问题
- 评估了顶尖模型在OlympiadBench上的表现，并发现最好的模型 GPT-4V 在物理方面仅得到了11.28% 的平均分数
- 评估指出 GPT-4V 模型存在幻觉、知识遗漏和逻辑错误的问题
- 希望这一挑战性的基准可以成为未来通用人工智能研究的宝贵资源

<br /><br />总结:
大型语言模型（LLMs）和多模态模型（LMMs）在多个任务上已经超过了一般人类的能力水平。该文章介绍了OlympiadBench，这是一个面向奥林匹克级别的双语多模态科学基准，包含了来自数学和物理竞赛以及中国高考的 8,952 个问题，并进行了专家级别的详细注释。通过在OlympiadBench上评估顶尖模型，文章实施了全面的评估方法来准确评估模型的响应。然而，最好的表现模型 GPT-4V 在OlympiadBench上的平均得分仅为 17.23%，在物理方面仅有 11.28% 的得分，凸显了基准的严谨性和物理推理的复杂性。评估发现 GPT-4V 模型存在幻觉、知识遗漏和逻辑错误等问题。希望这一具有挑战性的基准可以成为未来通用人工智能研究的宝贵资源。 <div>
Recent advancements have seen Large Language Models (LLMs) and Large
Multimodal Models (LMMs) surpassing general human capabilities in various
tasks, approaching the proficiency level of human experts across multiple
domains. With traditional benchmarks becoming less challenging for these
models, new rigorous challenges are essential to gauge their advanced
abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual
multimodal scientific benchmark, featuring 8,952 problems from Olympiad-level
mathematics and physics competitions, including the Chinese college entrance
exam. Each problem is detailed with expert-level annotations for step-by-step
reasoning. Evaluating top-tier models on OlympiadBench, we implement a
comprehensive assessment methodology to accurately evaluate model responses.
Notably, the best-performing model, GPT-4V, attains an average score of 17.23%
on OlympiadBench, with a mere 11.28% in physics, highlighting the benchmark
rigor and the intricacy of physical reasoning. Our analysis orienting GPT-4V
points out prevalent issues with hallucinations, knowledge omissions, and
logical fallacies. We hope that our challenging benchmark can serve as a
valuable resource for helping future AGI research endeavors.
]]></content:encoded>
<pubDate>2024-02-21T18:49:26Z</pubDate>
</item>
<item>
<title>Information Elicitation in Agency Games</title>
<link>http://arxiv.org/abs/2402.14005v1</link>
<guid>http://arxiv.org/abs/2402.14005v1</guid>
<content:encoded><![CDATA[
<div> 观测性问题，信息披露，代理关系，成本相关变量，市场效率<br />
总结：<br />
本文讨论了数据采集和处理工具的快速发展，提出了决定计算哪些评估指标的挑战。作者通过代理关系模型分析了代理何时有动机向委托人披露成本相关变量的可观测性。结果表明，代理倾向于披露能够揭示高低成本之间明显差异的信息，而在披露信息时倾向于进行信息扭曲。最后，作者分析了总体福利问题，指出信息扭曲可能导致更高的总体福利。 <div>
Rapid progress in scalable, commoditized tools for data collection and data
processing has made it possible for firms and policymakers to employ ever more
complex metrics as guides for decision-making. These developments have
highlighted a prevailing challenge -- deciding *which* metrics to compute. In
particular, a firm's ability to compute a wider range of existing metrics does
not address the problem of *unknown unknowns*, which reflects informational
limitations on the part of the firm. To guide the choice of metrics in the face
of this informational problem, we turn to the evaluated agents themselves, who
may have more information than a principal about how to measure outcomes
effectively. We model this interaction as a simple agency game, where we ask:
*When does an agent have an incentive to reveal the observability of a
cost-correlated variable to the principal?* There are two effects: better
information reduces the agent's information rents but also makes some projects
go forward that otherwise would fail. We show that the agent prefers to reveal
information that exposes a strong enough differentiation between high and low
costs. Expanding the agent's action space to include the ability to *garble*
their information, we show that the agent often prefers to garble over full
revelation. Still, giving the agent the ability to garble can lead to higher
total welfare. Our model has analogies with price discrimination, and we
leverage some of these synergies to analyze total welfare.
]]></content:encoded>
<pubDate>2024-02-21T18:44:38Z</pubDate>
</item>
<item>
<title>CounterCurate: Enhancing Physical and Semantic Visio-Linguistic
  Compositional Reasoning via Counterfactual Examples</title>
<link>http://arxiv.org/abs/2402.13254v1</link>
<guid>http://arxiv.org/abs/2402.13254v1</guid>
<content:encoded><![CDATA[
<div> 对应关键词: CounterCurate, visio-linguistic compositional reasoning, physically grounded reasoning, data augmentation, semantic counterfactuals

本研究提出了CounterCurate框架，旨在全面提高对比和生成式多模态模型的视觉-语言组合推理能力。首先，发现目前模型在与物理相关的组合推理方面表现不佳，然后利用GLIGEN模型进行简单的数据增强，显著提高了模型性能，尤其是在新创建的Flickr30k-Positions基准测试中，CLIP和LLaVA的性能分别提高了33%和37%。其次，利用高性能文本和图像生成模型（GPT-4V和DALLE-3）创建具有挑战性的语义反事实情况，从而进一步提高了组合推理能力，在SugarCrepe基准测试中，CounterCurate的表现超过了GPT-4V。<br /><br />总结: 本研究提出了CounterCurate框架，通过解决当前模型在物理相关推理和反事实情况处理上的不足，提高了多模态模型的组合推理能力。 <div>
We propose CounterCurate, a framework to comprehensively improve the
visio-linguistic compositional reasoning capability for both contrastive and
generative multimodal models. In particular, we identify two under-explored
critical problems: the neglect of the physically grounded reasoning (counting
and position understanding) and the potential of using highly capable text and
image generation models for semantic counterfactual fine-tuning. Our work
pioneers an approach that addresses these gaps. We first spotlight the
near-chance performance of multimodal models like CLIP and LLaVA in physically
grounded compositional reasoning. We then apply simple data augmentation using
a grounded image generation model, GLIGEN, to generate finetuning data,
resulting in significant performance improvements: +33% and +37% for CLIP and
LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark.
Moreover, we exploit the capabilities of high-performing text generation and
image generation models, specifically GPT-4V and DALLE-3, to curate challenging
semantic counterfactuals, thereby further enhancing compositional reasoning
capabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms
GPT-4V.
]]></content:encoded>
<pubDate>2024-02-20T18:59:55Z</pubDate>
</item>
<item>
<title>Fusion of Diffusion Weighted MRI and Clinical Data for Predicting
  Functional Outcome after Acute Ischemic Stroke with Deep Contrastive Learning</title>
<link>http://arxiv.org/abs/2402.10894v1</link>
<guid>http://arxiv.org/abs/2402.10894v1</guid>
<content:encoded><![CDATA[
<div> 病中风，扩散加权MRI，健康结构概要，预测功能结果，深度融合学习网络

总结:<br />
文章探讨了使用扩散加权MRI模态结合健康结构概要对预测中风患者功能结果的有效性，以促进早期干预。提出了两阶段训练的深度融合学习网络，采用监督对比学习来学习区分特征，预测患者在中风发作后3个月是否需要长期护理。研究发现，所提出的融合模型在AUC、F1分数和准确性方面均表现优异，优于现有模型，尤其在医疗领域的结合图像和结构数据模型中。此外，扩散加权MRI可以与其他临床变量结合以获得更好的普适性和准确性，甚至可以替代NIHSS以实现同等水平的准确性。 <div>
Stroke is a common disabling neurological condition that affects about
one-quarter of the adult population over age 25; more than half of patients
still have poor outcomes, such as permanent functional dependence or even
death, after the onset of acute stroke. The aim of this study is to investigate
the efficacy of diffusion-weighted MRI modalities combining with structured
health profile on predicting the functional outcome to facilitate early
intervention. A deep fusion learning network is proposed with two-stage
training: the first stage focuses on cross-modality representation learning and
the second stage on classification. Supervised contrastive learning is
exploited to learn discriminative features that separate the two classes of
patients from embeddings of individual modalities and from the fused multimodal
embedding. The network takes as the input DWI and ADC images, and structured
health profile data. The outcome is the prediction of the patient needing
long-term care at 3 months after the onset of stroke. Trained and evaluated
with a dataset of 3297 patients, our proposed fusion model achieves 0.87, 0.80
and 80.45% for AUC, F1-score and accuracy, respectively, outperforming existing
models that consolidate both imaging and structured data in the medical domain.
If trained with comprehensive clinical variables, including NIHSS and
comorbidities, the gain from images on making accurate prediction is not
considered substantial, but significant. However, diffusion-weighted MRI can
replace NIHSS to achieve comparable level of accuracy combining with other
readily available clinical variables for better generalization.
]]></content:encoded>
<pubDate>2024-02-16T18:51:42Z</pubDate>
</item>
<item>
<title>When is Tree Search Useful for LLM Planning? It Depends on the
  Discriminator</title>
<link>http://arxiv.org/abs/2402.10890v1</link>
<guid>http://arxiv.org/abs/2402.10890v1</guid>
<content:encoded><![CDATA[
<div> 大语言模型，语言代理，规划方法，迭代校正，树搜索<br />
这篇论文研究了大语言模型在多步问题下的解决方法，使用了语言代理框架的三个组件：生成器，鉴别器和规划方法。实验表明，使用迭代校正和树搜索这两种高级规划方法要求鉴别器至少有90%准确率才能取得明显的改进效果，而当前的大语言模型的鉴别能力并未达到这一要求。另外，使用基于大语言模型的鉴别器时，高级规划方法可能未能很好地平衡准确性和效率，例如，相较于其他两种方法，树搜索至少慢了10-20倍，但带来的性能提升微乎其微，这阻碍了其在实际应用中的使用。总结：<br /><br />这篇论文研究了大语言模型在多步问题下的解决方法，使用了语言代理框架的三个组件：生成器，鉴别器和规划方法。实验表明，使用迭代校正和树搜索这两种高级规划方法要求鉴别器至少有90%准确率才能取得明显的改进效果，而当前的大语言模型的鉴别能力并未达到这一要求。另外，使用基于大语言模型的鉴别器时，高级规划方法可能未能很好地平衡准确性和效率，例如，相较于其他两种方法，树搜索至少慢了10-20倍，但带来的性能提升微乎其微，这阻碍了其在实际应用中的使用。 <div>
In this paper, we examine how large language models (LLMs) solve multi-step
problems under a language agent framework with three components: a generator, a
discriminator, and a planning method. We investigate the practical utility of
two advanced planning methods, iterative correction and tree search. We present
a comprehensive analysis of how discrimination accuracy affects the overall
performance of agents when using these two methods or a simpler method,
re-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical
reasoning, show that: (1) advanced planning methods demand discriminators with
at least 90% accuracy to achieve significant improvements over re-ranking; (2)
current LLMs' discrimination abilities have not met the needs of advanced
planning methods to achieve such improvements; (3) with LLM-based
discriminators, advanced planning methods may not adequately balance accuracy
and efficiency. For example, compared to the other two methods, tree search is
at least 10--20 times slower but leads to negligible performance gains, which
hinders its real-world applications. Code and data will be released at
https://github.com/OSU-NLP-Group/llm-planning-eval.
]]></content:encoded>
<pubDate>2024-02-16T18:45:58Z</pubDate>
</item>
<item>
<title>A Trembling House of Cards? Mapping Adversarial Attacks against Language
  Agents</title>
<link>http://arxiv.org/abs/2402.10196v1</link>
<guid>http://arxiv.org/abs/2402.10196v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 语言代理、大型语言模型、自动化技术、安全风险、攻击场景
总结: 
语言代理基于大型语言模型的自动化技术发展迅猛，但其在安全风险方面存在着新的挑战。本文首次系统地探讨了针对语言代理的对抗性攻击，并提出了12种潜在的攻击场景，涵盖了不同的攻击策略。在三个主要组成部分（感知、大脑、行动）的框架下，对攻击场景进行了全面讨论，并与先前应用于大型语言模型的成功攻击策略进行了联系。强调了在广泛部署语言代理之前，我们迫切需要全面了解语言代理的风险。 <br /><br /> <div>
Language agents powered by large language models (LLMs) have seen exploding
development. Their capability of using language as a vehicle for thought and
communication lends an incredible level of flexibility and versatility. People
have quickly capitalized on this capability to connect LLMs to a wide range of
external components and environments: databases, tools, the Internet, robotic
embodiment, etc. Many believe an unprecedentedly powerful automation technology
is emerging. However, new automation technologies come with new safety risks,
especially for intricate systems like language agents. There is a surprisingly
large gap between the speed and scale of their development and deployment and
our understanding of their safety risks. Are we building a house of cards? In
this position paper, we present the first systematic effort in mapping
adversarial attacks against language agents. We first present a unified
conceptual framework for agents with three major components: Perception, Brain,
and Action. Under this framework, we present a comprehensive discussion and
propose 12 potential attack scenarios against different components of an agent,
covering different attack strategies (e.g., input manipulation, adversarial
demonstrations, jailbreaking, backdoors). We also draw connections to
successful attack strategies previously applied to LLMs. We emphasize the
urgency to gain a thorough understanding of language agent risks before their
widespread deployment.
]]></content:encoded>
<pubDate>2024-02-15T18:51:32Z</pubDate>
</item>
<item>
<title>Rec-GPT4V: Multimodal Recommendation with Large Vision-Language Models</title>
<link>http://arxiv.org/abs/2402.08670v1</link>
<guid>http://arxiv.org/abs/2402.08670v1</guid>
<content:encoded><![CDATA[
<div> 关键词: large vision-language models, multimodal recommendations, Rec-GPT4V, user preferences, image sequence dynamics

总结: <br /><br />这篇文章讨论了大型视觉语言模型在多模态推荐中的应用，指出了其存在的挑战和复杂性。为了克服这些问题，提出了一种名为Rec-GPT4V的新颖推理方案：Visual-Summary Thought（VST），并利用用户历史作为上下文用户偏好来解决LVLMs缺乏用户偏好知识的问题。接下来，文章介绍了如何利用LVLMs生成项目图像摘要，并结合项目标题在自然语言空间中查询用户对候选项目的偏好。最后，通过对四个数据集和三种LVLMs进行全面实验，结果表明了VST的有效性。 <div>
The development of large vision-language models (LVLMs) offers the potential
to address challenges faced by traditional multimodal recommendations thanks to
their proficient understanding of static images and textual dynamics. However,
the application of LVLMs in this field is still limited due to the following
complexities: First, LVLMs lack user preference knowledge as they are trained
from vast general datasets. Second, LVLMs suffer setbacks in addressing
multiple image dynamics in scenarios involving discrete, noisy, and redundant
image sequences. To overcome these issues, we propose the novel reasoning
scheme named Rec-GPT4V: Visual-Summary Thought (VST) of leveraging large
vision-language models for multimodal recommendation. We utilize user history
as in-context user preferences to address the first challenge. Next, we prompt
LVLMs to generate item image summaries and utilize image comprehension in
natural language space combined with item titles to query the user preferences
over candidate items. We conduct comprehensive experiments across four datasets
with three LVLMs: GPT4-V, LLaVa-7b, and LLaVa-13b. The numerical results
indicate the efficacy of VST.
]]></content:encoded>
<pubDate>2024-02-13T18:51:18Z</pubDate>
</item>
<item>
<title>MODIPHY: Multimodal Obscured Detection for IoT using PHantom
  Convolution-Enabled Faster YOLO</title>
<link>http://arxiv.org/abs/2402.07894v1</link>
<guid>http://arxiv.org/abs/2402.07894v1</guid>
<content:encoded><![CDATA[
<div> 模型压缩, YOLO Phantom, 低光条件, 多模态数据, 实时性能

总结:<br />
本研究介绍了一种名为"YOLO Phantom"的小型YOLO模型，利用全新的Phantom卷积块实现了可比较的准确性，同时减少了43%的参数和模型大小，减少了19%的GFLOPs。YOLO Phantom利用多模态RGB-红外数据进行迁移学习，解决了低光和遮挡问题，使其在恶劣条件下具有强大的视觉能力。在物联网平台上，与先进的低光和RGB摄像头无缝连接，连接到基于AWS的通知端点，实现了高效的实时目标检测。基准测试显示，与基线YOLOv8n模型相比，热成像和RGB检测的帧率提升了17%和14%。为了贡献于社区，代码和多模态数据集都在GitHub上可用。 <div>
Low-light conditions and occluded scenarios impede object detection in
real-world Internet of Things (IoT) applications like autonomous vehicles and
security systems. While advanced machine learning models strive for accuracy,
their computational demands clash with the limitations of resource-constrained
devices, hampering real-time performance. In our current research, we tackle
this challenge, by introducing "YOLO Phantom", one of the smallest YOLO models
ever conceived. YOLO Phantom utilizes the novel Phantom Convolution block,
achieving comparable accuracy to the latest YOLOv8n model while simultaneously
reducing both parameters and model size by 43%, resulting in a significant 19%
reduction in Giga Floating Point Operations (GFLOPs). YOLO Phantom leverages
transfer learning on our multimodal RGB-infrared dataset to address low-light
and occlusion issues, equipping it with robust vision under adverse conditions.
Its real-world efficacy is demonstrated on an IoT platform with advanced
low-light and RGB cameras, seamlessly connecting to an AWS-based notification
endpoint for efficient real-time object detection. Benchmarks reveal a
substantial boost of 17% and 14% in frames per second (FPS) for thermal and RGB
detection, respectively, compared to the baseline YOLOv8n model. For community
contribution, both the code and the multimodal dataset are available on GitHub.
]]></content:encoded>
<pubDate>2024-02-12T18:56:53Z</pubDate>
</item>
<item>
<title>MAIDCRL: Semi-centralized Multi-Agent Influence Dense-CNN Reinforcement
  Learning</title>
<link>http://arxiv.org/abs/2402.07890v1</link>
<guid>http://arxiv.org/abs/2402.07890v1</guid>
<content:encoded><![CDATA[
<div> Distributed decision-making, multi-agent systems, interactive behavior learning, Dense Reinforcement Learning, agent influence maps<br />
<br />
在多智能体系统中，分布式决策面临着巨大挑战，尤其是在合作和竞争系统中的交互式行为学习方面。为了缓解这种复杂性，本文介绍了一种半集中式的Dense强化学习算法，通过智能体影响图（AIMs）来增强对StarCraft多智能体挑战（SMAC）场景中有效的多智能体控制的学习。我们扩展了MAIDRL中的DenseNet，并引入了半集中式多智能体Dense-CNN强化学习（MAIDCRL），通过将卷积层结合到深度模型架构中，并在同质和异质场景中进行性能评估。结果显示，CNN使得MAIDCRL显著提高了学习性能，并在复杂的异质SMAC场景中取得了更快的学习速度。我们进一步调查了模型的稳定性和鲁棒性。统计数据表明，我们的模型不仅在所有给定场景中实现了更高的胜率，而且提升了智能体的细粒度决策过程的学习。 <br /><br />总结: <br />该研究介绍了一种半集中式Dense-CNN强化学习算法MAIDCRL，该算法通过智能体影响图（AIMs）在StarCraft多智能体挑战（SMAC）场景中实现了有效的多智能体控制。研究结果表明，在异质场景中，CNN-enabled MAIDCRL显著提高了学习性能并实现了更快的学习速度。该模型不仅在所有给定场景中实现了更高的胜率，而且提升了智能体的细粒度决策过程的学习。 <div>
Distributed decision-making in multi-agent systems presents difficult
challenges for interactive behavior learning in both cooperative and
competitive systems. To mitigate this complexity, MAIDRL presents a
semi-centralized Dense Reinforcement Learning algorithm enhanced by agent
influence maps (AIMs), for learning effective multi-agent control on StarCraft
Multi-Agent Challenge (SMAC) scenarios. In this paper, we extend the DenseNet
in MAIDRL and introduce semi-centralized Multi-Agent Dense-CNN Reinforcement
Learning, MAIDCRL, by incorporating convolutional layers into the deep model
architecture, and evaluate the performance on both homogeneous and
heterogeneous scenarios. The results show that the CNN-enabled MAIDCRL
significantly improved the learning performance and achieved a faster learning
rate compared to the existing MAIDRL, especially on more complicated
heterogeneous SMAC scenarios. We further investigate the stability and
robustness of our model. The statistics reflect that our model not only
achieves higher winning rate in all the given scenarios but also boosts the
agent's learning process in fine-grained decision-making.
]]></content:encoded>
<pubDate>2024-02-12T18:53:20Z</pubDate>
</item>
<item>
<title>Feedback Loops With Language Models Drive In-Context Reward Hacking</title>
<link>http://arxiv.org/abs/2402.06627v1</link>
<guid>http://arxiv.org/abs/2402.06627v1</guid>
<content:encoded><![CDATA[
<div> 反馈循环, 语言模型, 奖励欺骗, 输出精化, 策略精化
总结:
反馈循环会导致在上下文环境中发生奖励欺骗，语言模型在测试时会优化一个潜在的目标，但在过程中会产生负面影响。输出精化和策略精化是导致奖励欺骗的两个过程。在静态数据集上的评估不足以捕捉最有害的行为。因此，建议使用三种评估方法来捕捉更多奖励欺骗的情况。随着人工智能的发展加速，反馈循环的影响将会增加，这就增加了理解它们在塑造语言模型行为方面的重要性。 <div>
Language models influence the external world: they query APIs that read and
write to web pages, generate content that shapes human behavior, and run system
commands as autonomous agents. These interactions form feedback loops: LLM
outputs affect the world, which in turn affect subsequent LLM outputs. In this
work, we show that feedback loops can cause in-context reward hacking (ICRH),
where the LLM at test-time optimizes a (potentially implicit) objective but
creates negative side effects in the process. For example, consider an LLM
agent deployed to increase Twitter engagement; the LLM may retrieve its
previous tweets into the context window and make them more controversial,
increasing engagement but also toxicity. We identify and study two processes
that lead to ICRH: output-refinement and policy-refinement. For these
processes, evaluations on static datasets are insufficient -- they miss the
feedback effects and thus cannot capture the most harmful behavior. In
response, we provide three recommendations for evaluation to capture more
instances of ICRH. As AI development accelerates, the effects of feedback loops
will proliferate, increasing the need to understand their role in shaping LLM
behavior.
]]></content:encoded>
<pubDate>2024-02-09T18:59:29Z</pubDate>
</item>
<item>
<title>SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large
  Language Models</title>
<link>http://arxiv.org/abs/2402.05935v1</link>
<guid>http://arxiv.org/abs/2402.05935v1</guid>
<content:encoded><![CDATA[
<div> SPHINX-X, Multimodality, Large Language Model, dataset, training efficiency
<br /><br />
总结:SPHINX-X是在SPHINX基础上开发的一系列大型多模态语言模型（MLLM），通过修改框架并简化多阶段训练，提高了架构和训练效率。他们组建了一个包括语言、视觉和视觉-语言任务的多领域、多模态数据集，并且通过TinyLlama1.1B、InternLM2-7B、LLaMA2-13B和Mixtral8x7B等不同基础LLM的训练，获得了参数大小和多语言能力各异的MLLM。综合基准测试显示了多模态性能与数据和参数规模之间的强相关性。代码和模型已经在https://github.com/Alpha-VLLM/LLaMA2-Accessory上发布。 <div>
We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM)
series developed upon SPHINX. To improve the architecture and training
efficiency, we modify the SPHINX framework by removing redundant visual
encoders, bypassing fully-padded sub-images with skip tokens, and simplifying
multi-stage training into a one-stage all-in-one paradigm. To fully unleash the
potential of MLLMs, we assemble a comprehensive multi-domain and multimodal
dataset covering publicly available resources in language, vision, and
vision-language tasks. We further enrich this collection with our curated OCR
intensive and Set-of-Mark datasets, extending the diversity and generality. By
training over different base LLMs including TinyLlama1.1B, InternLM2-7B,
LLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in
parameter size and multilingual capabilities. Comprehensive benchmarking
reveals a strong correlation between the multi-modal performance with the data
and parameter scales. Code and models are released at
https://github.com/Alpha-VLLM/LLaMA2-Accessory
]]></content:encoded>
<pubDate>2024-02-08T18:59:48Z</pubDate>
</item>
<item>
<title>An Interactive Agent Foundation Model</title>
<link>http://arxiv.org/abs/2402.05929v1</link>
<guid>http://arxiv.org/abs/2402.05929v1</guid>
<content:encoded><![CDATA[
<div> Agent-based systems, multi-task agent training paradigm, versatile AI framework, Robotics, Gaming AI, Healthcare

发展中的人工智能系统正从创建静态的、特定任务的模型转变为能够在广泛应用中表现良好的动态的基于代理的系统。本文提出了一个交互式Agent Foundation模型，采用新颖的多任务代理训练范式，用于跨多个领域、数据集和任务训练AI代理。我们的训练范式统一了各种预训练策略，包括视觉蒙版自动编码器、语言建模和下一个动作预测，实现了多才多艺的、适应性强的AI框架。我们展示了我们的框架在三个不同领域--机器人技术、游戏人工智能和医疗保健领域的性能。我们的模型展示了它在每个领域产生有意义和上下文相关的输出的能力。我们的方法的优势在于其通用性，利用各种数据源，如机器人序列、游戏数据、大规模视频数据集和文字信息，进行有效的多模态和多任务学习。我们的方法为开发通用、行动取向、多模态系统提供了一个有前途的途径。<br /><br />总结: 人工智能系统的发展正在朝着能够适用于多领域的动态代理系统转变，该模型利用多任务训练范式和多种预训练策略，展示了在机器人技术、游戏人工智能和医疗保健领域的良好性能，为开发通用而多才多艺的AI系统提供了有前途的方向。 <div>
The development of artificial intelligence systems is transitioning from
creating static, task-specific models to dynamic, agent-based systems capable
of performing well in a wide range of applications. We propose an Interactive
Agent Foundation Model that uses a novel multi-task agent training paradigm for
training AI agents across a wide range of domains, datasets, and tasks. Our
training paradigm unifies diverse pre-training strategies, including visual
masked auto-encoders, language modeling, and next-action prediction, enabling a
versatile and adaptable AI framework. We demonstrate the performance of our
framework across three separate domains -- Robotics, Gaming AI, and Healthcare.
Our model demonstrates its ability to generate meaningful and contextually
relevant outputs in each area. The strength of our approach lies in its
generality, leveraging a variety of data sources such as robotics sequences,
gameplay data, large-scale video datasets, and textual information for
effective multimodal and multi-task learning. Our approach provides a promising
avenue for developing generalist, action-taking, multimodal systems.
]]></content:encoded>
<pubDate>2024-02-08T18:58:02Z</pubDate>
</item>
<item>
<title>WebLINX: Real-World Website Navigation with Multi-Turn Dialogue</title>
<link>http://arxiv.org/abs/2402.05930v1</link>
<guid>http://arxiv.org/abs/2402.05930v1</guid>
<content:encoded><![CDATA[
We propose the problem of conversational web navigation, where a digital
agent controls a web browser and follows user instructions to solve real-world
tasks in a multi-turn dialogue fashion. To support this problem, we introduce
WEBLINX - a large-scale benchmark of 100K interactions across 2300 expert
demonstrations of conversational web navigation. Our benchmark covers a broad
range of patterns on over 150 real-world websites and can be used to train and
evaluate agents in diverse scenarios. Due to the magnitude of information
present, Large Language Models (LLMs) cannot process entire web pages in
real-time. To solve this bottleneck, we design a retrieval-inspired model that
efficiently prunes HTML pages by ranking relevant elements. We use the selected
elements, along with screenshots and action history, to assess a variety of
models for their ability to replicate human behavior when navigating the web.
Our experiments span from small text-only to proprietary multimodal LLMs. We
find that smaller finetuned decoders surpass the best zero-shot LLMs (including
GPT-4V), but also larger finetuned multimodal models which were explicitly
pretrained on screenshots. However, all finetuned models struggle to generalize
to unseen websites. Our findings highlight the need for large multimodal models
that can generalize to novel settings. Our code, data and models are available
for research: https://mcgill-nlp.github.io/weblinx
]]></content:encoded>
<pubDate>2024-02-08T18:58:02Z</pubDate>
</item>
<item>
<title>Language-Based Augmentation to Address Shortcut Learning in Object Goal
  Navigation</title>
<link>http://arxiv.org/abs/2402.05090v1</link>
<guid>http://arxiv.org/abs/2402.05090v1</guid>
<content:encoded><![CDATA[
<div> DRL, Object-Goal Navigation, Shortcut learning, Language-Based (L-B) augmentation, Vision-Language Model (VLM)
<br /><br />总结:
本文研究了深度强化学习在目标导航中的应用，发现在训练环境中存在快捷学习的问题，即代理程序学习到了针对特定环境细节的策略。作者设计了一个实验，证明了快捷学习的存在，并提出了基于语言的增强方法来解决这一问题。他们发现，通过在视觉-语言模型的多模态特征空间中进行增强，可以降低代理程序在新环境中的成功率下降，从而解决了快捷学习问题。 <div>
Deep Reinforcement Learning (DRL) has shown great potential in enabling
robots to find certain objects (e.g., `find a fridge') in environments like
homes or schools. This task is known as Object-Goal Navigation (ObjectNav). DRL
methods are predominantly trained and evaluated using environment simulators.
Although DRL has shown impressive results, the simulators may be biased or
limited. This creates a risk of shortcut learning, i.e., learning a policy
tailored to specific visual details of training environments. We aim to deepen
our understanding of shortcut learning in ObjectNav, its implications and
propose a solution. We design an experiment for inserting a shortcut bias in
the appearance of training environments. As a proof-of-concept, we associate
room types to specific wall colors (e.g., bedrooms with green walls), and
observe poor generalization of a state-of-the-art (SOTA) ObjectNav method to
environments where this is not the case (e.g., bedrooms with blue walls). We
find that shortcut learning is the root cause: the agent learns to navigate to
target objects, by simply searching for the associated wall color of the target
object's room. To solve this, we propose Language-Based (L-B) augmentation. Our
key insight is that we can leverage the multimodal feature space of a
Vision-Language Model (VLM) to augment visual representations directly at the
feature-level, requiring no changes to the simulator, and only an addition of
one layer to the model. Where the SOTA ObjectNav method's success rate drops
69%, our proposal has only a drop of 23%.
]]></content:encoded>
<pubDate>2024-02-07T18:44:27Z</pubDate>
</item>
<item>
<title>AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls</title>
<link>http://arxiv.org/abs/2402.04253v1</link>
<guid>http://arxiv.org/abs/2402.04253v1</guid>
<content:encoded><![CDATA[
<div> APIs, AnyTool, GPT-4, evaluation protocol, benchmark<br />
<br />总结:
本文介绍了一个名为AnyTool的大型语言模型代理，旨在通过利用大量工具来解决用户查询的方式来彻底改革工具的利用。通过利用超过16,000个来自Rapid API的API，任何工具主要包括三个元素：具有分层结构的API检索器、旨在使用一组选择的API候选解决用户查询的求解器，以及一个自我反思机制，当初始解决方案不可行时重新激活AnyTool。AnyTool由GPT-4的函数调用功能驱动，消除了训练外部模块的需要。作者还重新访问了之前作品引入的评估协议，并确定了这一协议存在的限制，导致人为地高通过率。通过修改评估协议以更好地反映实际应用场景，他们引入了另一个基准，称为AnyToolBench。在各种数据集上进行的实验表明，AnyTool优于强基准，例如ToolLLM和专门用于工具利用的GPT-4变体。例如，在ToolBench的平均通过率上，AnyTool的表现优于ToolLLM 35.4%。代码将在https://github.com/dyabel/AnyTool 上提供。 <div>
We introduce AnyTool, a large language model agent designed to revolutionize
the utilization of a vast array of tools in addressing user queries. We utilize
over 16,000 APIs from Rapid API, operating under the assumption that a subset
of these APIs could potentially resolve the queries. AnyTool primarily
incorporates three elements: an API retriever with a hierarchical structure, a
solver aimed at resolving user queries using a selected set of API candidates,
and a self-reflection mechanism, which re-activates AnyTool if the initial
solution proves impracticable. AnyTool is powered by the function calling
feature of GPT-4, eliminating the need for training external modules. We also
revisit the evaluation protocol introduced by previous works and identify a
limitation in this protocol that leads to an artificially high pass rate. By
revising the evaluation protocol to better reflect practical application
scenarios, we introduce an additional benchmark, termed AnyToolBench.
Experiments across various datasets demonstrate the superiority of our AnyTool
over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool
utilization. For instance, AnyTool outperforms ToolLLM by +35.4% in terms of
average pass rate on ToolBench. Code will be available at
https://github.com/dyabel/AnyTool.
]]></content:encoded>
<pubDate>2024-02-06T18:59:57Z</pubDate>
</item>
<item>
<title>EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters</title>
<link>http://arxiv.org/abs/2402.04252v1</link>
<guid>http://arxiv.org/abs/2402.04252v1</guid>
<content:encoded><![CDATA[
<div> 关键词: CLIP, EVA-CLIP-18B, 18-billion参数, 图像分类基准, EVA-style弱到强视觉模型缩放<br />
<br />
这篇文章介绍了EVA-CLIP-18B，这是迄今为止最大、最强大的开源CLIP模型，拥有180亿个参数。该模型在27个广泛认可的图像分类基准中取得了异常出色的零样本top-1准确率达80.7%，表现优于其前身EVA-CLIP（50亿参数）和其他开源CLIP模型。令人惊讶的是，尽管保持对LAION-2B和COYO-700M的20亿图像文本配对的训练数据集不变，EVA-CLIP模型规模的扩大仍然能够保持一致的性能改进。EVA-CLIP-18B展示了EVA风格的弱到强视觉模型缩放的潜力。通过公开发布模型权重，希望能促进未来在视觉和多模型基础模型上的研究。<br /><br />总结: 本文介绍了EVA-CLIP-18B，这是迄今为止最大、最强大的开源CLIP模型，拥有180亿个参数。该模型表现出色，通过公开发布模型权重，希望促进未来研究。 <div>
Scaling up contrastive language-image pretraining (CLIP) is critical for
empowering both vision and multimodal models. We present EVA-CLIP-18B, the
largest and most powerful open-source CLIP model to date, with 18-billion
parameters. With only 6-billion training samples seen, EVA-CLIP-18B achieves an
exceptional 80.7% zero-shot top-1 accuracy averaged across 27 widely recognized
image classification benchmarks, outperforming its forerunner EVA-CLIP
(5-billion parameters) and other open-source CLIP models by a large margin.
Remarkably, we observe a consistent performance improvement with the model size
scaling of EVA-CLIP, despite maintaining a constant training dataset of
2-billion image-text pairs from LAION-2B and COYO-700M. This dataset is openly
available and much smaller than the in-house datasets (e.g., DFN-5B, WebLI-10B)
employed in other state-of-the-art CLIP models. EVA-CLIP-18B demonstrates the
potential of EVA-style weak-to-strong visual model scaling. With our model
weights made publicly available, we hope to facilitate future research in
vision and multimodal foundation models.
]]></content:encoded>
<pubDate>2024-02-06T18:59:48Z</pubDate>
</item>
<item>
<title>Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science</title>
<link>http://arxiv.org/abs/2402.04247v1</link>
<guid>http://arxiv.org/abs/2402.04247v1</guid>
<content:encoded><![CDATA[
Intelligent agents powered by large language models (LLMs) have demonstrated
substantial promise in autonomously conducting experiments and facilitating
scientific discoveries across various disciplines. While their capabilities are
promising, they also introduce novel vulnerabilities that demand careful
consideration for safety. However, there exists a notable gap in the
literature, as there has been no comprehensive exploration of these
vulnerabilities. This position paper fills this gap by conducting a thorough
examination of vulnerabilities in LLM-based agents within scientific domains,
shedding light on potential risks associated with their misuse and emphasizing
the need for safety measures. We begin by providing a comprehensive overview of
the potential risks inherent to scientific LLM agents, taking into account user
intent, the specific scientific domain, and their potential impact on the
external environment. Then, we delve into the origins of these vulnerabilities
and provide a scoping review of the limited existing works. Based on our
analysis, we propose a triadic framework involving human regulation, agent
alignment, and an understanding of environmental feedback (agent regulation) to
mitigate these identified risks. Furthermore, we highlight the limitations and
challenges associated with safeguarding scientific agents and advocate for the
development of improved models, robust benchmarks, and comprehensive
regulations to address these issues effectively.
]]></content:encoded>
<pubDate>2024-02-06T18:54:07Z</pubDate>
</item>
<item>
<title>V-IRL: Grounding Virtual Intelligence in Real Life</title>
<link>http://arxiv.org/abs/2402.03310v1</link>
<guid>http://arxiv.org/abs/2402.03310v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 感知, AI代理, 环境, 虚拟平台, 互动
总结:<br /><br />这篇文章介绍了一个名为V-IRL的平台，旨在通过虚拟环境让AI代理能够在真实世界中交互。该平台能够帮助开发能够完成各种实际任务的代理，并且可作为一个广阔的测试基地，用于测量在感知、决策和与全球各地真实世界数据交互等能力方面的进展。通过在虚拟而又真实的环境中实现代理的具体体现，可以弥合数字世界和真实世界之间的现实差距。 <div>
There is a sensory gulf between the Earth that humans inhabit and the digital
realms in which modern AI agents are created. To develop AI agents that can
sense, think, and act as flexibly as humans in real-world settings, it is
imperative to bridge the realism gap between the digital and physical worlds.
How can we embody agents in an environment as rich and diverse as the one we
inhabit, without the constraints imposed by real hardware and control? Towards
this end, we introduce V-IRL: a platform that enables agents to scalably
interact with the real world in a virtual yet realistic environment. Our
platform serves as a playground for developing agents that can accomplish
various practical tasks and as a vast testbed for measuring progress in
capabilities spanning perception, decision-making, and interaction with
real-world data across the entire globe.
]]></content:encoded>
<pubDate>2024-02-05T18:59:36Z</pubDate>
</item>
<item>
<title>AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion</title>
<link>http://arxiv.org/abs/2402.03309v1</link>
<guid>http://arxiv.org/abs/2402.03309v1</guid>
<content:encoded><![CDATA[
<div> acoustic-optical neural surface reconstruction, underwater perception, 3D surface reconstruction, baselines, multimodal fusion<br />
<br />
在水下感知和三维表面重建领域存在着诸多挑战，涉及到建筑、安全、海洋考古和环境监测等广泛领域的应用。由于恶劣的操作条件、脆弱的环境和有限的导航控制，潜水器通常需要限制其运动范围，因此也限制了其测量基线。我们的研究开发了一种基于物理的多模式声光神经表面重建框架（AONeuS），能够有效地将高分辨率的RGB测量与低分辨率的深度成像声纳测量相结合。通过融合这些互补的模态，我们的框架可以从受限基线上捕获的测量中重建准确的高分辨率三维表面。通过大量的模拟和实验，我们证明AONeuS显著优于最近的仅RGB和仅声纳的反差分渲染表面重建方法。我们的论文结果可通过以下网址查看：https://aoneus.github.io/ <br /><br />总结: 水下感知和三维表面重建面临诸多挑战，AONeuS框架成功融合了声光神经表面重建，能够在受限的基线下实现准确的高分辨率三维表面重建，并且优于之前的方法。 <div>
Underwater perception and 3D surface reconstruction are challenging problems
with broad applications in construction, security, marine archaeology, and
environmental monitoring. Treacherous operating conditions, fragile
surroundings, and limited navigation control often dictate that submersibles
restrict their range of motion and, thus, the baseline over which they can
capture measurements. In the context of 3D scene reconstruction, it is
well-known that smaller baselines make reconstruction more challenging. Our
work develops a physics-based multimodal acoustic-optical neural surface
reconstruction framework (AONeuS) capable of effectively integrating
high-resolution RGB measurements with low-resolution depth-resolved imaging
sonar measurements. By fusing these complementary modalities, our framework can
reconstruct accurate high-resolution 3D surfaces from measurements captured
over heavily-restricted baselines. Through extensive simulations and in-lab
experiments, we demonstrate that AONeuS dramatically outperforms recent
RGB-only and sonar-only inverse-differentiable-rendering--based surface
reconstruction methods. A website visualizing the results of our paper is
located at this address: https://aoneus.github.io/
]]></content:encoded>
<pubDate>2024-02-05T18:59:31Z</pubDate>
</item>
<item>
<title>Do Diffusion Models Learn Semantically Meaningful and Efficient
  Representations?</title>
<link>http://arxiv.org/abs/2402.03305v1</link>
<guid>http://arxiv.org/abs/2402.03305v1</guid>
<content:encoded><![CDATA[
Diffusion models are capable of impressive feats of image generation with
uncommon juxtapositions such as astronauts riding horses on the moon with
properly placed shadows. These outputs indicate the ability to perform
compositional generalization, but how do the models do so? We perform
controlled experiments on conditional DDPMs learning to generate 2D spherical
Gaussian bumps centered at specified $x$- and $y$-positions. Our results show
that the emergence of semantically meaningful latent representations is key to
achieving high performance. En route to successful performance over learning,
the model traverses three distinct phases of latent representations: (phase A)
no latent structure, (phase B) a 2D manifold of disordered states, and (phase
C) a 2D ordered manifold. Corresponding to each of these phases, we identify
qualitatively different generation behaviors: 1) multiple bumps are generated,
2) one bump is generated but at inaccurate $x$ and $y$ locations, 3) a bump is
generated at the correct $x$ and y location. Furthermore, we show that even
under imbalanced datasets where features ($x$- versus $y$-positions) are
represented with skewed frequencies, the learning process for $x$ and $y$ is
coupled rather than factorized, demonstrating that simple vanilla-flavored
diffusion models cannot learn efficient representations in which localization
in $x$ and $y$ are factorized into separate 1D tasks. These findings suggest
the need for future work to find inductive biases that will push generative
models to discover and exploit factorizable independent structures in their
inputs, which will be required to vault these models into more data-efficient
regimes.
]]></content:encoded>
<pubDate>2024-02-05T18:58:38Z</pubDate>
</item>
<item>
<title>TravelPlanner: A Benchmark for Real-World Planning with Language Agents</title>
<link>http://arxiv.org/abs/2402.01622v1</link>
<guid>http://arxiv.org/abs/2402.01622v1</guid>
<content:encoded><![CDATA[
<div> 语言代理，人工智能，旅行规划，规划基准，挑战性测试<br />
<br />
人工智能自诞生以来一直将规划作为核心追求，但早期的人工智能代理主要专注于受限环境，因为缺乏人类级别规划所需的许多认知基础。然而，最近由大型语言模型（LLM）驱动的语言代理展现出了有趣的能力，如工具使用和推理。但这些语言代理能否在超出以往人工智能代理能力范围的更复杂环境中进行规划呢？为了推动这一调查，提出了“TravelPlanner”规划基准，侧重于旅行规划，这是一个常见的真实世界规划场景。它提供了一个丰富的沙盒环境，各种工具用于访问近400万条数据记录，以及1225个精心策划的规划意图和参考计划。全面的评估显示，当前的语言代理尚不能处理此类复杂规划任务，即使GPT-4的成功率也只有0.6％。语言代理在保持任务、使用正确工具收集信息或跟踪多个约束方面很困难。但我们注意到，语言代理仅仅有可能解决这样一个复杂的问题本身已经是非平凡的进步。TravelPlanner为未来语言代理提供了一个具有挑战性但有意义的试验平台。 <br /><br />总结: <div>
Planning has been part of the core pursuit for artificial intelligence since
its conception, but earlier AI agents mostly focused on constrained settings
because many of the cognitive substrates necessary for human-level planning
have been lacking. Recently, language agents powered by large language models
(LLMs) have shown interesting capabilities such as tool use and reasoning. Are
these language agents capable of planning in more complex settings that are out
of the reach of prior AI agents? To advance this investigation, we propose
TravelPlanner, a new planning benchmark that focuses on travel planning, a
common real-world planning scenario. It provides a rich sandbox environment,
various tools for accessing nearly four million data records, and 1,225
meticulously curated planning intents and reference plans. Comprehensive
evaluations show that the current language agents are not yet capable of
handling such complex planning tasks-even GPT-4 only achieves a success rate of
0.6%. Language agents struggle to stay on task, use the right tools to collect
information, or keep track of multiple constraints. However, we note that the
mere possibility for language agents to tackle such a complex problem is in
itself non-trivial progress. TravelPlanner provides a challenging yet
meaningful testbed for future language agents.
]]></content:encoded>
<pubDate>2024-02-02T18:39:51Z</pubDate>
</item>
<item>
<title>MAGDi: Structured Distillation of Multi-Agent Interaction Graphs
  Improves Reasoning in Smaller Language Models</title>
<link>http://arxiv.org/abs/2402.01620v1</link>
<guid>http://arxiv.org/abs/2402.01620v1</guid>
<content:encoded><![CDATA[
<div> 多智能体交互; 大型语言模型; 知识蒸馏; 推理能力; 效率提升<br />
<br />
多智能体交互的研究展示了大型语言模型在各种推理任务上的重大改进。然而，这些方法需要多个模型进行长时间的生成，并且成本高昂。此外，这些多智能体方法无法提供一个最终的、用于高效推理的单一模型。为了解决这一问题，文章介绍了一种新方法MAGDi，用于将多个大型语言模型之间的推理交互进行结构化蒸馏，以训练较小的模型。MAGDi通过将多智能体交互表示为图形，利用图编码器增强基础学生模型，并使用三种目标函数进行知识蒸馏：下一个标记的预测、正确和错误推理之间的对比损失，以及基于图的目标函数来模拟交互结构。实验证明，MAGDi提高了较小模型的推理能力，在七个广泛使用的常识和数学推理基准测试上表现优异，超过了几种从单个老师和多个老师进行蒸馏的方法。此外，MAGDi的效率也比其老师高一个数量级。作者进行了广泛的分析，表明MAGDi能够增强对跨领域任务的泛化能力，与基础学生模型的大小和强度呈正比，以及在应用自洽性时（一种依赖于模型多样性的推理技术）能够获得更大的改进（通过我们的多老师训练）。<br /><br />总结: <br />多智能体交互的研究展示了大型语言模型在各种推理任务上的重大改进；MAGDi通过结构化蒸馏提高了较小模型的推理能力；MAGDi在常识和数学推理基准测试上表现出色，且效率明显优于其老师；MAGDi增强了对跨领域任务的泛化能力，并且与基础学生模型的大小和强度呈正比；MAGDi在应用自洽性时能够获得更大的改进。 <div>
Multi-agent interactions between Large Language Model (LLM) agents have shown
major improvements on diverse reasoning tasks. However, these involve long
generations from multiple models across several rounds, making them expensive.
Moreover, these multi-agent approaches fail to provide a final, single model
for efficient inference. To address this, we introduce MAGDi, a new method for
structured distillation of the reasoning interactions between multiple LLMs
into smaller LMs. MAGDi teaches smaller models by representing multi-agent
interactions as graphs, augmenting a base student model with a graph encoder,
and distilling knowledge using three objective functions: next-token
prediction, a contrastive loss between correct and incorrect reasoning, and a
graph-based objective to model the interaction structure. Experiments on seven
widely-used commonsense and math reasoning benchmarks show that MAGDi improves
the reasoning capabilities of smaller models, outperforming several methods
that distill from a single teacher and multiple teachers. Moreover, MAGDi also
demonstrates an order of magnitude higher efficiency over its teachers. We
conduct extensive analyses to show that MAGDi (1) enhances the generalizability
to out-of-domain tasks, (2) scales positively with the size and strength of the
base student model, and (3) obtains larger improvements (via our multi-teacher
training) when applying self-consistency - an inference technique that relies
on model diversity.
]]></content:encoded>
<pubDate>2024-02-02T18:35:14Z</pubDate>
</item>
<item>
<title>Binding Touch to Everything: Learning Unified Multimodal Tactile
  Representations</title>
<link>http://arxiv.org/abs/2401.18084v1</link>
<guid>http://arxiv.org/abs/2401.18084v1</guid>
<content:encoded><![CDATA[
<div> tactile, multimodal learning, UniTouch, vision-based touch sensors, zero-shot setting
<br /><br />
1. 该研究介绍了UniTouch，一个统一的触觉模型，用于连接多种传感器从而实现触觉与视觉、语言和声音等多模态学习。
2. 通过将UniTouch嵌入与预训练的图像嵌入相关联，并提出可学习的传感器特定标记，使模型能够同时学习从一组异构触觉传感器中获取信息。
3. UniTouch能够在零-shot设置下进行各种触觉感知任务，从机器人抓取预测到触觉图像问题回答等。
4. 该模型具有巨大的潜在应用价值，能够解决触觉与其他模态之间的关联问题，对人类和计算系统都具有重大意义。
5. UniTouch是首个展示了这些能力的模型，具有重大的研究意义和应用前景。
<br /><br />总结: <br />本研究介绍了UniTouch，一个能够联合不同传感器的触觉模型，实现触觉与其他模态的学习。该模型能够进行多种触觉感知任务，并具有广泛的应用前景。 <div>
The ability to associate touch with other modalities has huge implications
for humans and computational systems. However, multimodal learning with touch
remains challenging due to the expensive data collection process and
non-standardized sensor outputs. We introduce UniTouch, a unified tactile model
for vision-based touch sensors connected to multiple modalities, including
vision, language, and sound. We achieve this by aligning our UniTouch
embeddings to pretrained image embeddings already associated with a variety of
other modalities. We further propose learnable sensor-specific tokens, allowing
the model to learn from a set of heterogeneous tactile sensors, all at the same
time. UniTouch is capable of conducting various touch sensing tasks in the
zero-shot setting, from robot grasping prediction to touch image question
answering. To the best of our knowledge, UniTouch is the first to demonstrate
such capabilities. Project page: https://cfeng16.github.io/UniTouch/
]]></content:encoded>
<pubDate>2024-01-31T18:59:57Z</pubDate>
</item>
<item>
<title>CARFF: Conditional Auto-encoded Radiance Field for 3D Scene Forecasting</title>
<link>http://arxiv.org/abs/2401.18075v1</link>
<guid>http://arxiv.org/abs/2401.18075v1</guid>
<content:encoded><![CDATA[
<div> 关键词: CARFF, Conditional Auto-encoded Radiance Field, 3D Scene Forecasting, Neural Radiance Field, CARLA driving simulator

CARFF是一种用于预测未来3D场景的方法，可以根据过去的观测（如2D自我中心图像）将图像映射到潜在的3D场景配置分布，并预测假设场景随时间的演变。该方法利用概率编码器将潜在场景表示映射到全局神经辐射场（NeRF），以表示3D场景模型，从而实现可解释的预测和简单的下游应用。此方法通过考虑环境状态和动态的不确定性复杂情景，扩展了以往的神经渲染工作。我们使用姿势条件VAE和NeRF的两阶段训练来学习3D表示。此外，我们利用混合密度网络，自回归地预测潜在场景表示作为一种部分可观察的马尔可夫决策过程。我们在CARLA驾驶模拟器中展示了我们方法的实用性，CARFF可用于实现复杂多智能体自动驾驶场景的高效轨迹和应急规划，包括视觉遮挡。<br /><br />总结: CARFF是一种用于预测未来3D场景的方法，利用概率编码器和全局神经辐射场表示潜在的3D场景模型，扩展了以往的神经渲染工作。该方法通过两阶段训练学习3D表示，并利用混合密度网络自回归地预测潜在场景表示。在CARLA驾驶模拟器中展示了方法的实用性，可用于复杂多智能体自动驾驶场景的轨迹和应急规划。 <div>
We propose CARFF: Conditional Auto-encoded Radiance Field for 3D Scene
Forecasting, a method for predicting future 3D scenes given past observations,
such as 2D ego-centric images. Our method maps an image to a distribution over
plausible 3D latent scene configurations using a probabilistic encoder, and
predicts the evolution of the hypothesized scenes through time. Our latent
scene representation conditions a global Neural Radiance Field (NeRF) to
represent a 3D scene model, which enables explainable predictions and
straightforward downstream applications. This approach extends beyond previous
neural rendering work by considering complex scenarios of uncertainty in
environmental states and dynamics. We employ a two-stage training of
Pose-Conditional-VAE and NeRF to learn 3D representations. Additionally, we
auto-regressively predict latent scene representations as a partially
observable Markov decision process, utilizing a mixture density network. We
demonstrate the utility of our method in realistic scenarios using the CARLA
driving simulator, where CARFF can be used to enable efficient trajectory and
contingency planning in complex multi-agent autonomous driving scenarios
involving visual occlusions.
]]></content:encoded>
<pubDate>2024-01-31T18:56:09Z</pubDate>
</item>
<item>
<title>Weaver: Foundation Models for Creative Writing</title>
<link>http://arxiv.org/abs/2401.17268v1</link>
<guid>http://arxiv.org/abs/2401.17268v1</guid>
<content:encoded><![CDATA[
<div> 关键词: Weaver, 大型语言模型, 写作能力, 领域专用, 预训练<br />
总结: <br />
本文介绍了Weaver，它是专门用于内容创作的大型语言模型家族的首个成员。Weaver经过精心筛选的语料库预训练，专注于提高大型语言模型的写作能力。通过新颖的方法进行微调，使Weaver能够生成更接近人类的文本，并能够遵循更多样化的创作指令。Weaver家族包括不同规模的模型，适用于不同的应用场景，并可以根据查询的复杂性进行动态调度。研究表明，Weaver模型在评估语言模型写作能力的基准测试中表现优异，超过了比它们大数倍的通用语言模型。此外，Weaver还原生支持检索增强生成和函数调用，并展示了这些能力在改进AI辅助写作系统中的各种应用案例。最后，文章讨论并总结了预训练和微调领域特定语言模型的指南和最佳实践。 <div>
This work introduces Weaver, our first family of large language models (LLMs)
dedicated to content creation. Weaver is pre-trained on a carefully selected
corpus that focuses on improving the writing capabilities of large language
models. We then fine-tune Weaver for creative and professional writing purposes
and align it to the preference of professional writers using a suit of novel
methods for instruction data synthesis and LLM alignment, making it able to
produce more human-like texts and follow more diverse instructions for content
creation. The Weaver family consists of models of Weaver Mini (1.8B), Weaver
Base (6B), Weaver Pro (14B), and Weaver Ultra (34B) sizes, suitable for
different applications and can be dynamically dispatched by a routing agent
according to query complexity to balance response quality and computation cost.
Evaluation on a carefully curated benchmark for assessing the writing
capabilities of LLMs shows Weaver models of all sizes outperform generalist
LLMs several times larger than them. Notably, our most-capable Weaver Ultra
model surpasses GPT-4, a state-of-the-art generalist LLM, on various writing
scenarios, demonstrating the advantage of training specialized LLMs for writing
purposes. Moreover, Weaver natively supports retrieval-augmented generation
(RAG) and function calling (tool usage). We present various use cases of these
abilities for improving AI-assisted writing systems, including integration of
external knowledge bases, tools, or APIs, and providing personalized writing
assistance. Furthermore, we discuss and summarize a guideline and best
practices for pre-training and fine-tuning domain-specific LLMs.
]]></content:encoded>
<pubDate>2024-01-30T18:58:43Z</pubDate>
</item>
<item>
<title>ReacLLaMA: Merging chemical and textual information in chemical
  reactivity AI models</title>
<link>http://arxiv.org/abs/2401.17267v1</link>
<guid>http://arxiv.org/abs/2401.17267v1</guid>
<content:encoded><![CDATA[
<div> Graphormer, reactivity model, chemical information, procedural text, accuracy

总结:<br />
本文提出了利用过程文本来增强Graphormer反应性模型并提高其准确性的方法。两种主要方法分别是训练一个适配器Graphormer模型，该模型提供了一个由GPT-2衍生的文本过程的潜在表示（ReacLLaMA-Adapter），以及使用LLaMA 2模型对数据集的未标记部分进行标记，然后在扩展数据集上训练Graphormer模型（Zero-Shot Labeling ReacLLaMA）。这两种方法都增强了对不利反应的识别能力，从而提供了更准确的模型，并改善了特异性。 <div>
Chemical reactivity models are developed to predict chemical reaction
outcomes in the form of classification (success/failure) or regression (product
yield) tasks. The vast majority of the reported models are trained solely on
chemical information such as reactants, products, reagents, and solvents, but
not on the details of a synthetic protocol. Herein incorporation of procedural
text with the aim to augment the Graphormer reactivity model and improve its
accuracy is presented. Two major approaches are used: training an adapter
Graphormer model that is provided with a GPT-2-derived latent representation of
the text procedure (ReacLLaMA-Adapter) and labeling an unlabeled part of a
dataset with the LLaMA 2 model followed by training the Graphormer on an
extended dataset (Zero-Shot Labeling ReacLLaMA). Both methodologies enhance the
discernment of unpromising reactions, thereby providing more accurate models
with improved specificity.
]]></content:encoded>
<pubDate>2024-01-30T18:57:08Z</pubDate>
</item>
<item>
<title>InternLM-XComposer2: Mastering Free-form Text-Image Composition and
  Comprehension in Vision-Language Large Model</title>
<link>http://arxiv.org/abs/2401.16420v1</link>
<guid>http://arxiv.org/abs/2401.16420v1</guid>
<content:encoded><![CDATA[
<div> 关键词: InternLM-XComposer2, PLoRA, 多模态理解, 高质量内容, GitHub链接
总结:
InternLM-XComposer2是一种先进的视觉语言模型，擅长自由形式的文本-图像组合和理解。它采用了Partial LoRA (PLoRA)方法，通过额外的LoRA参数对图像标记进行处理，从而在保持预训练语言知识完整性的同时，实现了精确的视觉理解和具有文学才华的文本组成。在实验结果中，InternLM-XComposer2在高质量长文本多模态内容的生成和跨各种基准测试中表现出了优越性能，不仅明显优于现有的多模态模型，而且在某些评估中甚至能够与甚至超过了GPT-4V和Gemini Pro。这突显了它在多模态理解领域的出色能力。InternLM-XComposer2模型系列的7B参数现已在https://github.com/InternLM/InternLM-XComposer公开提供。 <div>
We introduce InternLM-XComposer2, a cutting-edge vision-language model
excelling in free-form text-image composition and comprehension. This model
goes beyond conventional vision-language understanding, adeptly crafting
interleaved text-image content from diverse inputs like outlines, detailed
textual specifications, and reference images, enabling highly customizable
content creation. InternLM-XComposer2 proposes a Partial LoRA (PLoRA) approach
that applies additional LoRA parameters exclusively to image tokens to preserve
the integrity of pre-trained language knowledge, striking a balance between
precise vision understanding and text composition with literary talent.
Experimental results demonstrate the superiority of InternLM-XComposer2 based
on InternLM2-7B in producing high-quality long-text multi-modal content and its
exceptional vision-language understanding performance across various
benchmarks, where it not only significantly outperforms existing multimodal
models but also matches or even surpasses GPT-4V and Gemini Pro in certain
assessments. This highlights its remarkable proficiency in the realm of
multimodal understanding. The InternLM-XComposer2 model series with 7B
parameters are publicly available at
https://github.com/InternLM/InternLM-XComposer.
]]></content:encoded>
<pubDate>2024-01-29T18:59:02Z</pubDate>
</item>
<item>
<title>Annotated Hands for Generative Models</title>
<link>http://arxiv.org/abs/2401.15075v1</link>
<guid>http://arxiv.org/abs/2401.15075v1</guid>
<content:encoded><![CDATA[
<div> 生成模型, 手图像, 训练框架, 语义标注, 图像质量
<br />
生成模型如GANs和扩散模型在图像生成方面取得了令人瞩目的成就，但在生成手部图像方面却表现不佳。本文提出了一种新颖的训练框架，通过在训练图像中增加三个额外的通道，提供手部的标注信息，从而改善生成模型生成手部图像的能力。通过在合成手部图像数据集和真实照片上进行实验，证明了该方法的有效性。最终通过使用现成的手部检测器，测量生成的手部图像的质量得到了提升。 <br /><br />总结: <br />生成模型在图像生成方面表现突出，但在生成手部图像方面表现不佳；本文提出了一种新的训练框架，通过增加手部标注信息改善生成模型的能力；在合成和真实数据集上验证了方法的有效性；最终通过手部检测器测量了生成手部图像的质量。 <div>
Generative models such as GANs and diffusion models have demonstrated
impressive image generation capabilities. Despite these successes, these
systems are surprisingly poor at creating images with hands. We propose a novel
training framework for generative models that substantially improves the
ability of such systems to create hand images. Our approach is to augment the
training images with three additional channels that provide annotations to
hands in the image. These annotations provide additional structure that coax
the generative model to produce higher quality hand images. We demonstrate this
approach on two different generative models: a generative adversarial network
and a diffusion model. We demonstrate our method both on a new synthetic
dataset of hand images and also on real photographs that contain hands. We
measure the improved quality of the generated hands through higher confidence
in finger joint identification using an off-the-shelf hand detector.
]]></content:encoded>
<pubDate>2024-01-26T18:57:54Z</pubDate>
</item>
<item>
<title>Fully Independent Communication in Multi-Agent Reinforcement Learning</title>
<link>http://arxiv.org/abs/2401.15059v1</link>
<guid>http://arxiv.org/abs/2401.15059v1</guid>
<content:encoded><![CDATA[
<div> 多智能体强化学习, 通信方法, 参数共享, 网络容量, 学习效率
<br />
本文研究了多智能体强化学习中独立学习者如何进行通信，提出了一种新的学习方案作为解决方案。研究结果表明，尽管存在挑战，独立智能体仍然可以根据我们的方法学习通信策略。此外，我们利用这种方法研究了MARL中的通信如何受到不同网络容量的影响，无论是共享参数还是不共享参数。我们观察到通信并不总是必要的，并且在使用通信时需要考虑选择的智能体网络大小，以实现高效的学习。
<br /><br />总结: 
本文研究了在多智能体强化学习中独立学习者如何进行通信，并提出了一种新的学习方案作为解决方案。实验结果表明，独立智能体可以学习通信策略，并且证明了在学习效率方面网络容量的选择对通信的影响。 <div>
Multi-Agent Reinforcement Learning (MARL) comprises a broad area of research
within the field of multi-agent systems. Several recent works have focused
specifically on the study of communication approaches in MARL. While multiple
communication methods have been proposed, these might still be too complex and
not easily transferable to more practical contexts. One of the reasons for that
is due to the use of the famous parameter sharing trick. In this paper, we
investigate how independent learners in MARL that do not share parameters can
communicate. We demonstrate that this setting might incur into some problems,
to which we propose a new learning scheme as a solution. Our results show that,
despite the challenges, independent agents can still learn communication
strategies following our method. Additionally, we use this method to
investigate how communication in MARL is affected by different network
capacities, both for sharing and not sharing parameters. We observe that
communication may not always be needed and that the chosen agent network sizes
need to be considered when used together with communication in order to achieve
efficient learning.
]]></content:encoded>
<pubDate>2024-01-26T18:42:01Z</pubDate>
</item>
<item>
<title>LongFin: A Multimodal Document Understanding Model for Long Financial
  Domain Documents</title>
<link>http://arxiv.org/abs/2401.15050v1</link>
<guid>http://arxiv.org/abs/2401.15050v1</guid>
<content:encoded><![CDATA[
Document AI is a growing research field that focuses on the comprehension and
extraction of information from scanned and digital documents to make everyday
business operations more efficient. Numerous downstream tasks and datasets have
been introduced to facilitate the training of AI models capable of parsing and
extracting information from various document types such as receipts and scanned
forms. Despite these advancements, both existing datasets and models fail to
address critical challenges that arise in industrial contexts. Existing
datasets primarily comprise short documents consisting of a single page, while
existing models are constrained by a limited maximum length, often set at 512
tokens. Consequently, the practical application of these methods in financial
services, where documents can span multiple pages, is severely impeded. To
overcome these challenges, we introduce LongFin, a multimodal document AI model
capable of encoding up to 4K tokens. We also propose the LongForms dataset, a
comprehensive financial dataset that encapsulates several industrial challenges
in financial documents. Through an extensive evaluation, we demonstrate the
effectiveness of the LongFin model on the LongForms dataset, surpassing the
performance of existing public models while maintaining comparable results on
existing single-page benchmarks.
]]></content:encoded>
<pubDate>2024-01-26T18:23:45Z</pubDate>
</item>
<item>
<title>Deconstructing Denoising Diffusion Models for Self-Supervised Learning</title>
<link>http://arxiv.org/abs/2401.14404v1</link>
<guid>http://arxiv.org/abs/2401.14404v1</guid>
<content:encoded><![CDATA[
<div> Denoising Diffusion Models, representation learning, self-supervised learning, deconstructive procedure, classical methods
<br />
本研究探讨了最初用于图像生成的去噪扩散模型（DDM）的表示学习能力。我们的方法是逐步解构DDM，逐渐将其转化为经典的去噪自编码器（DAE）。这种解构性过程使我们能够探索现代DDM的各种组件如何影响自监督表示学习。我们观察到，只有很少现代组件对于学习良好的表示是至关重要的，而许多其他组件则是非必要的。我们的研究最终得出了一个高度简化的方法，在很大程度上类似于经典的DAE。我们希望我们的研究能重新引起人们对现代自监督学习领域内一系列经典方法的兴趣。
<br /><br />总结: Denoising Diffusion Models的表示学习能力得到探讨；研究采用逐步解构DDM的方法，发现现代DDMs的很多组件对良好的自监督表示学习并不是必要的；研究最终得出了一个高度简化的方法，类似于经典的DAE；希望研究能重新引起人们对现代自监督学习领域内经典方法的兴趣。 <div>
In this study, we examine the representation learning abilities of Denoising
Diffusion Models (DDM) that were originally purposed for image generation. Our
philosophy is to deconstruct a DDM, gradually transforming it into a classical
Denoising Autoencoder (DAE). This deconstructive procedure allows us to explore
how various components of modern DDMs influence self-supervised representation
learning. We observe that only a very few modern components are critical for
learning good representations, while many others are nonessential. Our study
ultimately arrives at an approach that is highly simplified and to a large
extent resembles a classical DAE. We hope our study will rekindle interest in a
family of classical methods within the realm of modern self-supervised
learning.
]]></content:encoded>
<pubDate>2024-01-25T18:59:57Z</pubDate>
</item>
<item>
<title>VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web
  Tasks</title>
<link>http://arxiv.org/abs/2401.13649v1</link>
<guid>http://arxiv.org/abs/2401.13649v1</guid>
<content:encoded><![CDATA[
<div> benchmark, autonomous agents, multimodal, VisualWebArena, web

在现有的基准测试基本上主要集中在基于文本的代理上，忽略了许多需要视觉信息来有效解决的自然任务。因此，我们引入了VisualWebArena，这是一个旨在评估多模态网络代理在现实中视觉相关任务表现的基准。它包括一系列多样化和复杂的基于网络的任务，评估自主多模态代理的各种能力。通过广泛的定量和定性分析，我们确定了文本-只有LLM代理的几个局限性，并揭示了最先进的多模态语言代理的能力差距。VisualWebArena为评估多模态自主语言代理提供了一个框架，并为构建更强大的网络自主代理提供了见解。 <div>
Autonomous agents capable of planning, reasoning, and executing actions on
the web offer a promising avenue for automating computer tasks. However, the
majority of existing benchmarks primarily focus on text-based agents,
neglecting many natural tasks that require visual information to effectively
solve. Given that most computer interfaces cater to human perception, visual
information often augments textual data in ways that text-only models struggle
to harness effectively. To bridge this gap, we introduce VisualWebArena, a
benchmark designed to assess the performance of multimodal web agents on
realistic \textit{visually grounded tasks}. VisualWebArena comprises of a set
of diverse and complex web-based tasks that evaluate various capabilities of
autonomous multimodal agents. To perform on this benchmark, agents need to
accurately process image-text inputs, interpret natural language instructions,
and execute actions on websites to accomplish user-defined objectives. We
conduct an extensive evaluation of state-of-the-art LLM-based autonomous
agents, including several multimodal models. Through extensive quantitative and
qualitative analysis, we identify several limitations of text-only LLM agents,
and reveal gaps in the capabilities of state-of-the-art multimodal language
agents. VisualWebArena provides a framework for evaluating multimodal
autonomous language agents, and offers insights towards building stronger
autonomous agents for the web. Our code, baseline models, and data is publicly
available at https://jykoh.com/vwa.
]]></content:encoded>
<pubDate>2024-01-24T18:35:21Z</pubDate>
</item>
<item>
<title>HAZARD Challenge: Embodied Decision Making in Dynamically Changing
  Environments</title>
<link>http://arxiv.org/abs/2401.12975v1</link>
<guid>http://arxiv.org/abs/2401.12975v1</guid>
<content:encoded><![CDATA[
<div> 高保真虚拟环境, 智能体, HAZARD, 动态环境, 大语言模型

最近，高保真虚拟环境的技术进步成为促进建立能够感知、推理和与物理世界互动的智能体的主要推动力之一。然而，在真实世界的情景中，智能体可能面临突发事件导致的动态环境变化，需要迅速做出相应的行动。为了弥补这一差距，研究人员提出了一个新的模拟智能体基准测试，名为HAZARD，专门设计用于评估智能体在动态情况下的决策能力。HAZARD包括火灾、洪水和风三种突发灾害场景，并专门支持利用大语言模型（LLMs）来辅助常识推理和决策制定。这一基准测试使我们能够评估自主智能体在动态变化的环境中通过强化学习（RL）、基于规则的方法和搜索方法等各种管道的决策能力。作为利用大语言模型解决这些具有挑战性任务的一个起步，我们进一步开发了一个基于LLM的智能体，并对其解决这些任务的前景和挑战进行了深入分析。HAZARD的详细信息可在https://vis-www.cs.umass.edu/hazard/找到。<br /><br />总结：最近出现了高保真虚拟环境技术，为了评估智能体在动态环境中的决策能力，研究人员提出了一个名为HAZARD的新的模拟智能体基准测试。该基准测试包括三种突发灾害场景，并特别支持利用大语言模型来辅助决策制定。研究人员还开发了基于LLM的智能体，并分析了其解决这些任务的前景和挑战。 <div>
Recent advances in high-fidelity virtual environments serve as one of the
major driving forces for building intelligent embodied agents to perceive,
reason and interact with the physical world. Typically, these environments
remain unchanged unless agents interact with them. However, in real-world
scenarios, agents might also face dynamically changing environments
characterized by unexpected events and need to rapidly take action accordingly.
To remedy this gap, we propose a new simulated embodied benchmark, called
HAZARD, specifically designed to assess the decision-making abilities of
embodied agents in dynamic situations. HAZARD consists of three unexpected
disaster scenarios, including fire, flood, and wind, and specifically supports
the utilization of large language models (LLMs) to assist common sense
reasoning and decision-making. This benchmark enables us to evaluate autonomous
agents' decision-making capabilities across various pipelines, including
reinforcement learning (RL), rule-based, and search-based methods in
dynamically changing environments. As a first step toward addressing this
challenge using large language models, we further develop an LLM-based agent
and perform an in-depth analysis of its promise and challenge of solving these
challenging tasks. HAZARD is available at https://vis-www.cs.umass.edu/hazard/.
]]></content:encoded>
<pubDate>2024-01-23T18:59:43Z</pubDate>
</item>
<item>
<title>CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation</title>
<link>http://arxiv.org/abs/2401.12208v1</link>
<guid>http://arxiv.org/abs/2401.12208v1</guid>
<content:encoded><![CDATA[
<div> CheXinstruct, CheXagent, vision-language模型, CXR解释, CheXbench
<br />
本文介绍了针对胸部X光片（CXR）解释的自动化模型CheXagent的开发。作者首先介绍了CheXinstruct数据集，然后提出了CheXagent模型，该模型包括临床大型语言模型（LLM）、视觉编码器和用于桥接视觉和语言模态的网络。此外，引入了CheXbench评估框架，用于系统评估FMs在8个临床相关的CXR解释任务上的表现。研究结果表明，CheXagent在CheXbench任务上的表现优于先前开发的通用和医学领域的FMs。此外，作者还进行了模型公平性评估，以突出潜在的性别、种族和年龄方面的性能差异。这项工作的详细信息可以在https://stanford-aimi.github.io/chexagent.html找到。
<br /><br />总结: 本文介绍了针对CXR解释的自动化模型CheXagent的开发，包括数据集、模型架构和评估框架。研究结果显示CheXagent在临床相关任务上表现优异，而且还进行了公平性评估。 <div>
Chest X-rays (CXRs) are the most frequently performed imaging test in
clinical practice. Recent advances in the development of vision-language
foundation models (FMs) give rise to the possibility of performing automated
CXR interpretation, which can assist physicians with clinical decision-making
and improve patient outcomes. However, developing FMs that can accurately
interpret CXRs is challenging due to the (1) limited availability of
large-scale vision-language datasets in the medical image domain, (2) lack of
vision and language encoders that can capture the complexities of medical data,
and (3) absence of evaluation frameworks for benchmarking the abilities of FMs
on CXR interpretation. In this work, we address these challenges by first
introducing \emph{CheXinstruct} - a large-scale instruction-tuning dataset
curated from 28 publicly-available datasets. We then present \emph{CheXagent} -
an instruction-tuned FM capable of analyzing and summarizing CXRs. To build
CheXagent, we design a clinical large language model (LLM) for parsing
radiology reports, a vision encoder for representing CXR images, and a network
to bridge the vision and language modalities. Finally, we introduce
\emph{CheXbench} - a novel benchmark designed to systematically evaluate FMs
across 8 clinically-relevant CXR interpretation tasks. Extensive quantitative
evaluations and qualitative reviews with five expert radiologists demonstrate
that CheXagent outperforms previously-developed general- and medical-domain FMs
on CheXbench tasks. Furthermore, in an effort to improve model transparency, we
perform a fairness evaluation across factors of sex, race and age to highlight
potential performance disparities. Our project is at
\url{https://stanford-aimi.github.io/chexagent.html}.
]]></content:encoded>
<pubDate>2024-01-22T18:51:07Z</pubDate>
</item>
<item>
<title>Retrieval-Guided Reinforcement Learning for Boolean Circuit Minimization</title>
<link>http://arxiv.org/abs/2401.12205v1</link>
<guid>http://arxiv.org/abs/2401.12205v1</guid>
<content:encoded><![CDATA[
<div> logic synthesis, chip design, hardware description languages, ABC-RL, quality-of-result

逻辑综合是芯片设计中的关键阶段，涉及将硬件描述语言（如Verilog）中编码的芯片规格优化为使用布尔逻辑门的高效实现。研究发现，预先训练的代理在面对全新设计时可能会偏离轨迹，对搜索轨迹产生不利影响。为解决这一挑战，提出了ABC-RL，一种根据训练数据集中的相似度分数计算出的精心调整建议的技术。实验结果显示，ABC-RL能够显著提高合成电路的结果质量（QoR），与当前最先进的技术相比，提高了最高达24.8%。此外，ABC-RL在运行时间上也取得了惊人的成就，与当前最先进的方法相比，实现了高达9倍的减少（等质量结果的情况下）。<br /><br />总结: 逻辑综合是芯片设计过程中的关键阶段，需要细致调整合成方法以适应各种硬件设计。ABC-RL技术通过精心调整预训练代理的建议，实现了合成电路质量的显著提高（最高24.8%）。与此同时，ABC-RL还取得了显著的运行时间减少（高达9倍）。 <div>
Logic synthesis, a pivotal stage in chip design, entails optimizing chip
specifications encoded in hardware description languages like Verilog into
highly efficient implementations using Boolean logic gates. The process
involves a sequential application of logic minimization heuristics (``synthesis
recipe"), with their arrangement significantly impacting crucial metrics such
as area and delay. Addressing the challenge posed by the broad spectrum of
design complexities - from variations of past designs (e.g., adders and
multipliers) to entirely novel configurations (e.g., innovative processor
instructions) - requires a nuanced `synthesis recipe` guided by human expertise
and intuition. This study conducts a thorough examination of learning and
search techniques for logic synthesis, unearthing a surprising revelation:
pre-trained agents, when confronted with entirely novel designs, may veer off
course, detrimentally affecting the search trajectory. We present ABC-RL, a
meticulously tuned $\alpha$ parameter that adeptly adjusts recommendations from
pre-trained agents during the search process. Computed based on similarity
scores through nearest neighbor retrieval from the training dataset, ABC-RL
yields superior synthesis recipes tailored for a wide array of hardware
designs. Our findings showcase substantial enhancements in the
Quality-of-result (QoR) of synthesized circuits, boasting improvements of up to
24.8% compared to state-of-the-art techniques. Furthermore, ABC-RL achieves an
impressive up to 9x reduction in runtime (iso-QoR) when compared to current
state-of-the-art methodologies.
]]></content:encoded>
<pubDate>2024-01-22T18:46:30Z</pubDate>
</item>
<item>
<title>Applications of flow models to the generation of correlated lattice QCD
  ensembles</title>
<link>http://arxiv.org/abs/2401.10874v1</link>
<guid>http://arxiv.org/abs/2401.10874v1</guid>
<content:encoded><![CDATA[
<div> 关键词: normalizing flows, lattice quantum field theory, variance reduction, gauge theories, QCD observables
总结: 
本研究利用机器学习的正规化流在晶格量子场论的背景下生成不同作用参数下统计相关的晶格规范场集合。这项工作展示了如何利用这些相关性来减少计算观测量时的方差。通过一种新颖的残余流架构，演示了三种概念验证应用：规范理论的连续极限、QCD观测量的质量依赖性，以及基于费曼-赫尔曼方法的强子矩阵元素。在所有三种情况下，实验证明当机器学习流被纳入时，与使用无关集合或直接重加权进行的相同计算相比，统计不确定性显著减少。 <br /><br /> <div>
Machine-learned normalizing flows can be used in the context of lattice
quantum field theory to generate statistically correlated ensembles of lattice
gauge fields at different action parameters. This work demonstrates how these
correlations can be exploited for variance reduction in the computation of
observables. Three different proof-of-concept applications are demonstrated
using a novel residual flow architecture: continuum limits of gauge theories,
the mass dependence of QCD observables, and hadronic matrix elements based on
the Feynman-Hellmann approach. In all three cases, it is shown that statistical
uncertainties are significantly reduced when machine-learned flows are
incorporated as compared with the same calculations performed with uncorrelated
ensembles or direct reweighting.
]]></content:encoded>
<pubDate>2024-01-19T18:33:52Z</pubDate>
</item>
<item>
<title>Vlogger: Make Your Dream A Vlog</title>
<link>http://arxiv.org/abs/2401.09414v1</link>
<guid>http://arxiv.org/abs/2401.09414v1</guid>
<content:encoded><![CDATA[
<div> Vlogger, AI, video blog, Large Language Model, 生成<br />
Script, Actor, ShowMaker, Voicer, 模型合作生成vlog的四个关键角色<br />

总结:<br />
本文介绍了Vlogger，这是一个用于生成用户描述的分钟级视频博客的通用AI系统。与几秒钟的短视频不同，vlog通常包含一个复杂的故事情节和多样化的场景，这对大多数现有的视频生成方法来说是一个挑战。为了突破这一瓶颈，Vlogger巧妙地利用大型语言模型（LLM）作为导演，并将vlog的长视频生成任务分解为四个关键阶段。在这些阶段中，我们调用各种基础模型来扮演vlog专业人员的关键角色，包括（1）脚本，（2）演员，（3）ShowMaker和（4）Voicer。通过这种模仿人类的设计，我们的Vlogger可以通过自上而下的规划和自下而上的拍摄，以可解释的合作方式生成vlog。此外，我们还引入了一个新颖的视频扩散模型ShowMaker，它在我们的Vlogger中充当了一个摄影师的角色，用于生成每个拍摄场景的视频片段。通过巧妙地结合脚本和演员作为文本和视觉提示，它可以有效地增强片段的时空连贯性。此外，我们为ShowMaker设计了一个简洁的混合训练范式，提升了其T2V生成和预测能力。最后，广泛的实验证明，我们的方法在零样本T2V生成和预测任务上取得了最先进的性能。更重要的是，Vlogger可以从开放世界描述中生成超过5分钟的vlog，而且在脚本和演员上不会丢失视频的连贯性。所有代码和模型都可以在https://github.com/zhuangshaobin/Vlogger上找到。 <div>
In this work, we present Vlogger, a generic AI system for generating a
minute-level video blog (i.e., vlog) of user descriptions. Different from short
videos with a few seconds, vlog often contains a complex storyline with
diversified scenes, which is challenging for most existing video generation
approaches. To break through this bottleneck, our Vlogger smartly leverages
Large Language Model (LLM) as Director and decomposes a long video generation
task of vlog into four key stages, where we invoke various foundation models to
play the critical roles of vlog professionals, including (1) Script, (2) Actor,
(3) ShowMaker, and (4) Voicer. With such a design of mimicking human beings,
our Vlogger can generate vlogs through explainable cooperation of top-down
planning and bottom-up shooting. Moreover, we introduce a novel video diffusion
model, ShowMaker, which serves as a videographer in our Vlogger for generating
the video snippet of each shooting scene. By incorporating Script and Actor
attentively as textual and visual prompts, it can effectively enhance
spatial-temporal coherence in the snippet. Besides, we design a concise mixed
training paradigm for ShowMaker, boosting its capacity for both T2V generation
and prediction. Finally, the extensive experiments show that our method
achieves state-of-the-art performance on zero-shot T2V generation and
prediction tasks. More importantly, Vlogger can generate over 5-minute vlogs
from open-world descriptions, without loss of video coherence on script and
actor. The code and model is all available at
https://github.com/zhuangshaobin/Vlogger.
]]></content:encoded>
<pubDate>2024-01-17T18:55:12Z</pubDate>
</item>
<item>
<title>Multimodal assessment of best possible self as a self-regulatory
  activity for the classroom</title>
<link>http://arxiv.org/abs/2401.08424v1</link>
<guid>http://arxiv.org/abs/2401.08424v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 最佳可能自我，积极心理干预，心身效应，自我调节资源，大学生

最佳可能自我（BPS）是一种被证明可以增强幸福感的积极心理干预，包括描述理想未来情景的写作活动。这篇论文比较了为教室环境改编的BPS活动和与之时间匹配的对照活动（NA）对心理生理效应的影响。三十三名本科生参与了这项研究，评估了三个时间段（之前，期间，之后）的状态焦虑（状态-特质焦虑量表，STAI）、情感（情感滑块，AS）和心脏迷走神经活动（心率变异性，HRV）作为自我调节资源使用的指标。结果显示，与NA相比，BPS导致了积极情绪价值（期间）的显著增加，并且整体上更高水平的心脏迷走神经活动（HRV）。这些发现表明，BPS作为一种自我调节技术具有潜在的特性，旨在培养积极情绪，对自我调节资源产生积极影响。由于BPS不需要专业知识或专门技术来进行管理，教育者在教学和学生实践自我调节时可能会选择这种适当的活动。这项研究呈现了对大学生进行的一个简短BPS活动的自我调节效应的可复制的多模态方法的证据。<br /><br />总结:本研究展示了BPS活动对大学生的自我调节效应的证据，表明BPS在提高积极情感和积极影响自我调节资源方面具有潜力。 BPS活动可以在教学中用于教育者教学和学生实践自我调节。 <div>
Best possible self (BPS) is a positive psychological intervention shown to
enhance well-being which involves writing a description of an ideal future
scenario. This paper presents a comparison of psychophysiological effects of a
BPS activity that has been adapted for classroom settings and a time-matched
control activity (NA). Thirty-three undergraduate students participated in the
study that assessed state anxiety (State-Trait Anxiety Inventory, STAI), affect
(Affective Slider, AS), and cardiac vagal activity (heart-rate variability,
HRV) as an indicator of self-regulatory resource usage, at three time periods
(PRE, DURING, POST). Results show that BPS led to a significantly greater
increase in positive valence (DURING) and overall higher levels of cardiac
vagal activity (HRV) compared to NA. These findings suggest that BPS has
promising characteristics as a self-regulatory technique aimed at fostering
positive affect and positively impacting self-regulatory resources. As BPS does
not require expert knowledge nor specialized technology to administer, it may
be a suitable activity for educators to use when teaching and having students
practice self-regulation. This study presents evidence collected in a
replicable multimodal approach of the self-regulatory effects of a brief BPS
activity on undergraduate students.
]]></content:encoded>
<pubDate>2024-01-16T15:11:12Z</pubDate>
</item>
<item>
<title>AGG: Amortized Generative 3D Gaussians for Single Image to 3D</title>
<link>http://arxiv.org/abs/2401.04099v1</link>
<guid>http://arxiv.org/abs/2401.04099v1</guid>
<content:encoded><![CDATA[
<div> 3D content creation, 3D Gaussian splatting, Amortized Generative 3D Gaussian framework, optimization-based, super-resolution

3D内容创建需求增长，研究了各种3D表示以从单个图像生成3D对象。最近，基于3D高斯光斑的模型在3D重建和生成方面取得了优异的渲染效果。然而，基于3D高斯光斑的方法通常是基于优化的，需要许多计算昂贵的分数蒸馏步骤。为了克服这些挑战，引入了一种摊销生成3D高斯框架（AGG），它可以立即从单个图像生成3D高斯，无需每个实例进行优化。AGG利用中间混合表示，将生成3D高斯位置和其他外观属性进行联合优化。此外，还提出了一个级联管道，首先生成3D数据的粗表示，然后利用3D高斯超分辨率模块进行上采样。我们的方法与现有的基于优化的3D高斯框架和利用其他3D表示的基于采样的管线进行了评估，结果表明AGG在生成能力上具有竞争优势，无论是定性还是定量，同时速度快几个数量级。项目页面：https://ir1d.github.io/AGG/ <br /><br />总结: 3D内容创建需求增长，研究了各种3D表示以从单个图像生成3D对象。基于3D高斯光斑的模型在3D重建和生成方面表现出色，但通常是基于优化的，需要许多计算昂贵的分数蒸馏步骤。为了克服这些挑战，引入了一种摊销生成3D高斯框架（AGG），它可以立即从单个图像生成3D高斯，无需每个实例进行优化。除此之外，提出了一个级联管道，首先生成3D数据的粗表示，然后利用3D高斯超分辨率模块进行上采样。我们的方法在生成能力上具有竞争优势，无论是定性还是定量，同时速度快几个数量级。 <div>
Given the growing need for automatic 3D content creation pipelines, various
3D representations have been studied to generate 3D objects from a single
image. Due to its superior rendering efficiency, 3D Gaussian splatting-based
models have recently excelled in both 3D reconstruction and generation. 3D
Gaussian splatting approaches for image to 3D generation are often
optimization-based, requiring many computationally expensive score-distillation
steps. To overcome these challenges, we introduce an Amortized Generative 3D
Gaussian framework (AGG) that instantly produces 3D Gaussians from a single
image, eliminating the need for per-instance optimization. Utilizing an
intermediate hybrid representation, AGG decomposes the generation of 3D
Gaussian locations and other appearance attributes for joint optimization.
Moreover, we propose a cascaded pipeline that first generates a coarse
representation of the 3D data and later upsamples it with a 3D Gaussian
super-resolution module. Our method is evaluated against existing
optimization-based 3D Gaussian frameworks and sampling-based pipelines
utilizing other 3D representations, where AGG showcases competitive generation
abilities both qualitatively and quantitatively while being several orders of
magnitude faster. Project page: https://ir1d.github.io/AGG/
]]></content:encoded>
<pubDate>2024-01-08T18:56:33Z</pubDate>
</item>
<item>
<title>The Tactician's Web of Large-Scale Formal Knowledge</title>
<link>http://arxiv.org/abs/2401.02950v1</link>
<guid>http://arxiv.org/abs/2401.02950v1</guid>
<content:encoded><![CDATA[
<div> Coq proof assistant, formal mathematical knowledge, machine learning, interconnected web, proof engineering
<br /><br />总结:
Tactician's Web是一个基于Coq证明助手构建的平台，提供了一个庞大而强大的机器检查的数学知识网络，方便机器学习、分析和证明工程。该平台导出一个数据集，其中包含广泛的形式化理论，呈现为定义、定理、证明术语、策略和证明状态的网络。紧密集成的Coq提供了使代理商可用于证明工程师作为实用工具的独特可能性。理论被编码为语义图和人类可读的文本，各自具有独特的优点和缺点。证明代理可以通过相同丰富的数据表示与Coq交互，并且可以自动在一组定理上进行基准测试。 <div>
The Tactician's Web is a platform offering a large web of strongly
interconnected, machine-checked, formal mathematical knowledge conveniently
packaged for machine learning, analytics, and proof engineering. Built on top
of the Coq proof assistant, the platform exports a dataset containing a wide
variety of formal theories, presented as a web of definitions, theorems, proof
terms, tactics, and proof states. Theories are encoded both as a semantic graph
(rendered below) and as human-readable text, each with a unique set of
advantages and disadvantages. Proving agents may interact with Coq through the
same rich data representation and can be automatically benchmarked on a set of
theorems. Tight integration with Coq provides the unique possibility to make
agents available to proof engineers as practical tools.
]]></content:encoded>
<pubDate>2024-01-05T18:52:35Z</pubDate>
</item>
<item>
<title>Graph2Tac: Learning Hierarchical Representations of Math Concepts in
  Theorem proving</title>
<link>http://arxiv.org/abs/2401.02949v1</link>
<guid>http://arxiv.org/abs/2401.02949v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 数学, AI代理, Coq证明助手, 图神经网络, 定义嵌入

总结: <br /><br />这篇文章讨论了数学和其应用中的概念，提到了在AI代理进行新定理证明时，需要实时将新信息融入其知识库，特别是在Coq证明助手中。作者使用了基于图的数据集，构建了图神经网络Graph2Tac(G2T)，能够考虑到导致当前目标的整个定义层次。同时，他们还提出了一项新的定义嵌入任务，用于计算训练中未见的数学概念的表示。这些方法使神经网络的性能能够与最先进的k最近邻预测器相媲美。 <div>
Concepts abound in mathematics and its applications. They vary greatly
between subject areas, and new ones are introduced in each mathematical paper
or application. A formal theory builds a hierarchy of definitions, theorems and
proofs that reference each other. When an AI agent is proving a new theorem,
most of the mathematical concepts and lemmas relevant to that theorem may have
never been seen during training. This is especially true in the Coq proof
assistant, which has a diverse library of Coq projects, each with its own
definitions, lemmas, and even custom tactic procedures used to prove those
lemmas. It is essential for agents to incorporate such new information into
their knowledge base on the fly. We work towards this goal by utilizing a new,
large-scale, graph-based dataset for machine learning in Coq. We leverage a
faithful graph-representation of Coq terms that induces a directed graph of
dependencies between definitions to create a novel graph neural network,
Graph2Tac (G2T), that takes into account not only the current goal, but also
the entire hierarchy of definitions that led to the current goal. G2T is an
online model that is deeply integrated into the users' workflow and can adapt
in real time to new Coq projects and their definitions. It complements well
with other online models that learn in real time from new proof scripts. Our
novel definition embedding task, which is trained to compute representations of
mathematical concepts not seen during training, boosts the performance of the
neural network to rival state-of-the-art k-nearest neighbor predictors.
]]></content:encoded>
<pubDate>2024-01-05T18:52:09Z</pubDate>
</item>
<item>
<title>ODIN: A Single Model for 2D and 3D Perception</title>
<link>http://arxiv.org/abs/2401.02416v1</link>
<guid>http://arxiv.org/abs/2401.02416v1</guid>
<content:encoded><![CDATA[
<div> 3D perception benchmarks, ScanNet, point clouds, transformer architecture, ODIN
<br />
这篇论文介绍了一个名为ODIN的模型，它能够同时处理2D RGB图像和3D点云数据，并使用了一种交替融合2D和3D信息的transformer架构。模型通过区分处理的token的位置编码来区分2D和3D特征操作。ODIN在ScanNet200、Matterport3D和AI2THOR 3D实例分割基准测试中实现了最先进的性能，并在ScanNet、S3DIS和COCO基准测试上实现了有竞争力的表现。当使用实际感知的3D点云代替从3D网格采样的点云时，它比所有先前的工作表现出更高的性能。在可指导的具身代理架构中用作3D感知引擎时，在TEACh对话动作基准测试中创造了新的最先进技术。 <div>
State-of-the-art models on contemporary 3D perception benchmarks like ScanNet
consume and label dataset-provided 3D point clouds, obtained through post
processing of sensed multiview RGB-D images. They are typically trained
in-domain, forego large-scale 2D pre-training and outperform alternatives that
featurize the posed RGB-D multiview images instead. The gap in performance
between methods that consume posed images versus post-processed 3D point clouds
has fueled the belief that 2D and 3D perception require distinct model
architectures. In this paper, we challenge this view and propose ODIN
(Omni-Dimensional INstance segmentation), a model that can segment and label
both 2D RGB images and 3D point clouds, using a transformer architecture that
alternates between 2D within-view and 3D cross-view information fusion. Our
model differentiates 2D and 3D feature operations through the positional
encodings of the tokens involved, which capture pixel coordinates for 2D patch
tokens and 3D coordinates for 3D feature tokens. ODIN achieves state-of-the-art
performance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation
benchmarks, and competitive performance on ScanNet, S3DIS and COCO. It
outperforms all previous works by a wide margin when the sensed 3D point cloud
is used in place of the point cloud sampled from 3D mesh. When used as the 3D
perception engine in an instructable embodied agent architecture, it sets a new
state-of-the-art on the TEACh action-from-dialogue benchmark. Our code and
checkpoints can be found at the project website: https://odin-seg.github.io.
]]></content:encoded>
<pubDate>2024-01-04T18:59:25Z</pubDate>
</item>
<item>
<title>LLaMA Pro: Progressive LLaMA with Block Expansion</title>
<link>http://arxiv.org/abs/2401.02415v1</link>
<guid>http://arxiv.org/abs/2401.02415v1</guid>
<content:encoded><![CDATA[
<div> LLaMA, post-pretraining, Transformer blocks, programming, mathematics
<br />
Large Language Models（LLMs）通过扩展Transformer模块的方法进行新的后预训练，从而在编程和数学领域取得了显著的进展。新模型LLaMA Pro-8.3B在通用任务、编程和数学领域表现出色，并在各种基准测试中取得了先进的性能。该研究为整合自然语言和编程语言提供了宝贵的见解，并为在各种环境中有效运行的先进语言代理的发展奠定了坚实的基础。 
<br /><br />总结: 
<br />LLaMA Pro-8.3B是通过扩展Transformer模块的后预训练方法得到的新模型，在通用任务、编程和数学领域获得了出色的性能。该研究为整合自然语言和编程语言提供了宝贵的见解，并为在各种环境中有效运行的先进语言代理的发展奠定了坚实的基础。 <div>
Humans generally acquire new skills without compromising the old; however,
the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to
CodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with
an expansion of Transformer blocks. We tune the expanded blocks using only new
corpus, efficiently and effectively improving the model's knowledge without
catastrophic forgetting. In this paper, we experiment on the corpus of code and
math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from
LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro
and its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced
performance among various benchmarks, demonstrating superiority over existing
open models in the LLaMA family and the immense potential of reasoning and
addressing diverse tasks as an intelligent agent. Our findings provide valuable
insights into integrating natural and programming languages, laying a solid
foundation for developing advanced language agents that operate effectively in
various environments.
]]></content:encoded>
<pubDate>2024-01-04T18:59:12Z</pubDate>
</item>
<item>
<title>TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview</title>
<link>http://arxiv.org/abs/2401.01330v1</link>
<guid>http://arxiv.org/abs/2401.01330v1</guid>
<content:encoded><![CDATA[
<div> TREC iKAT, Conversational Search Agents, personalized context, decisional search tasks, information operators <br />
<br />
总结: 本文介绍了TREC iKAT对话式检索的研究领域，强调了对话搜索代理的个性化适应能力和决策性搜索任务的重要性。文章描述了任务、主题、数据收集和评估框架，并总结了提交的研究成果。文章强调了不同用户角色和其信息需求的多样性，以及对话式检索的个性化上下文对于提高搜索效率的重要性。 <div>
Conversational Information Seeking stands as a pivotal research area with
significant contributions from previous works. The TREC Interactive Knowledge
Assistance Track (iKAT) builds on the foundational work of the TREC
Conversational Assistance Track (CAsT). However, iKAT distinctively emphasizes
the creation and research of conversational search agents that adapt responses
based on user's prior interactions and present context. The challenge lies in
enabling Conversational Search Agents (CSA) to incorporate this personalized
context to efficiency and effectively guide users through the relevant
information to them. iKAT also emphasizes decisional search tasks, where users
sift through data and information to weigh up options in order to reach a
conclusion or perform an action. These tasks, prevalent in everyday
information-seeking decisions -- be it related to travel, health, or shopping
-- often revolve around a subset of high-level information operators where
queries or questions about the information space include: finding options,
comparing options, identifying the pros and cons of options, etc. Given the
different personas and their information need (expressed through the sequence
of questions), diverse conversation trajectories will arise -- because the
answers to these similar queries will be very different. In this paper, we
report on the first year of TREC iKAT, describing the task, topics, data
collection, and evaluation framework. We further review the submissions and
summarize the findings.
]]></content:encoded>
<pubDate>2024-01-02T18:40:03Z</pubDate>
</item>
<item>
<title>K-PERM: Personalized Response Generation Using Dynamic Knowledge
  Retrieval and Persona-Adaptive Queries</title>
<link>http://arxiv.org/abs/2312.17748v1</link>
<guid>http://arxiv.org/abs/2312.17748v1</guid>
<content:encoded><![CDATA[
<div> 个性化、对话代理、知识、K-PERM、FoCus数据集
<br /><br />
对话代理的个性化可以提高对话质量和用户参与度，但缺乏外部知识以适当地满足用户的个性特点。为了增强个性化响应的相关性和全面性，文章提出了一个两步方法，包括选择性地整合用户个性和用补充信息来情景化回应。作者们开发了K-PERM（知识引导的个性化与奖励调节），这是一个动态对话代理，结合了这些元素。K-PERM在流行的FoCus数据集上取得了最先进的性能，该数据集包含有关全球地标的真实个性化对话。作者还表明，使用K-PERM的回应可以提高最先进的LLMs（例如GPT 3.5）的性能10.5％，突显了K-PERM对个性化聊天机器人的影响。 <div>
Personalizing conversational agents can enhance the quality of conversations
and increase user engagement. However, they often lack external knowledge to
appropriately tend to a user's persona. This is particularly crucial for
practical applications like mental health support, nutrition planning,
culturally sensitive conversations, or reducing toxic behavior in
conversational agents. To enhance the relevance and comprehensiveness of
personalized responses, we propose using a two-step approach that involves (1)
selectively integrating user personas and (2) contextualizing the response with
supplementing information from a background knowledge source. We develop K-PERM
(Knowledge-guided PErsonalization with Reward Modulation), a dynamic
conversational agent that combines these elements. K-PERM achieves
state-of-the-art performance on the popular FoCus dataset, containing
real-world personalized conversations concerning global landmarks. We show that
using responses from K-PERM can improve performance in state-of-the-art LLMs
(GPT 3.5) by 10.5%, highlighting the impact of K-PERM for personalizing
chatbots.
]]></content:encoded>
<pubDate>2023-12-29T18:59:58Z</pubDate>
</item>
<item>
<title>MURP: Multi-Agent Ultra-Wideband Relative Pose Estimation with
  Constrained Communications in 3D Environments</title>
<link>http://arxiv.org/abs/2312.17731v1</link>
<guid>http://arxiv.org/abs/2312.17731v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 多机器人系统, 相对定位, 3D姿态估计, 超宽带测距标签, 误差校正
总结:
本文提出了一种新颖的多机器人系统中的相对定位方法，使用超宽带(UWB)测距标签进行3D姿态估计。相比先前的方法，本文的方法通过使用本地收集到的UWB测距数据以及先验状态约束，并在违反约束时进行检测，可以避免通信网络能力不足和团队规模增加时可能出现的问题。通过利用已学习的平均测距偏差校正，我们实现了19%的定位误差改进，实验结果显示平均绝对位置误差为0.24米，航向误差为9.5度。与其他最新方法相比，本文的方法表现得更好，同时在通信成本显著较高的方法中也具有竞争力。此外，我们还提供了数据集供他人使用。 <br /><br />总结: 本文提出了一种新的多机器人系统的相对定位方法，通过使用UWB测距标签和误差校正来提高定位精度，并在保持竞争力的同时降低了通信成本。 <div>
Inter-agent relative localization is critical for many multi-robot systems
operating in the absence of external positioning infrastructure or prior
environmental knowledge. We propose a novel inter-agent relative 3D pose
estimation system where each participating agent is equipped with several
ultra-wideband (UWB) ranging tags. Prior work typically supplements noisy UWB
range measurements with additional continuously transmitted data, such as
odometry, leading to potential scaling issues with increased team size and/or
decreased communication network capability. By equipping each agent with
multiple UWB antennas, our approach addresses these concerns by using only
locally collected UWB range measurements, a priori state constraints, and
detections of when said constraints are violated. Leveraging our learned mean
ranging bias correction, we gain a 19% positional error improvement giving us
experimental mean absolute position and heading errors of 0.24m and 9.5 degrees
respectively. When compared to other state-of-the-art approaches, our work
demonstrates improved performance over similar systems, while remaining
competitive with methods that have significantly higher communication costs.
Additionally, we make our datasets available.
]]></content:encoded>
<pubDate>2023-12-29T18:40:05Z</pubDate>
</item>
<item>
<title>EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards
  Embodied AI</title>
<link>http://arxiv.org/abs/2312.16170v1</link>
<guid>http://arxiv.org/abs/2312.16170v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 计算机视觉, 机器人, 3D场景理解, 多模态感知数据集, Embodied Perceptron

总结:<br /><br />本文介绍了一种新的多模态、自我中心的3D感知数据集和基准测试，名为EmbodiedScan，以及一个基于此数据集的基线框架Embodied Perceptron。该数据集包含了超过5k个扫描，包括100万个自我中心的RGB-D视图，160k个涵盖760个类别的3D定向框和80个常见类别的密集语义占用。Embodied Perceptron能够处理任意数量的多模态输入，并展示出卓越的3D感知能力，不仅在基本3D感知任务和语言相关任务方面，在实际环境中也能取得显著成果。该研究填补了传统研究在3D场景理解方面的空白，为计算机视觉和机器人领域的进一步研究提供了有益的参考。GitHub链接提供了代码、数据集和基准测试。 <div>
In the realm of computer vision and robotics, embodied agents are expected to
explore their environment and carry out human instructions. This necessitates
the ability to fully understand 3D scenes given their first-person observations
and contextualize them into language for interaction. However, traditional
research focuses more on scene-level input and output setups from a global
view. To address the gap, we introduce EmbodiedScan, a multi-modal, ego-centric
3D perception dataset and benchmark for holistic 3D scene understanding. It
encompasses over 5k scans encapsulating 1M ego-centric RGB-D views, 1M language
prompts, 160k 3D-oriented boxes spanning over 760 categories, some of which
partially align with LVIS, and dense semantic occupancy with 80 common
categories. Building upon this database, we introduce a baseline framework
named Embodied Perceptron. It is capable of processing an arbitrary number of
multi-modal inputs and demonstrates remarkable 3D perception capabilities, both
within the two series of benchmarks we set up, i.e., fundamental 3D perception
tasks and language-grounded tasks, and in the wild. Codes, datasets, and
benchmarks will be available at https://github.com/OpenRobotLab/EmbodiedScan.
]]></content:encoded>
<pubDate>2023-12-26T18:59:11Z</pubDate>
</item>
<item>
<title>From Text to Multimodal: A Comprehensive Survey of Adversarial Example
  Generation in Question Answering Systems</title>
<link>http://arxiv.org/abs/2312.16156v1</link>
<guid>http://arxiv.org/abs/2312.16156v1</guid>
<content:encoded><![CDATA[
<div> 关键词: 对抗性机器学习, 问答系统, 文本, 多模态, 模型漏洞

总结: 
本文综合评述了在问答系统领域中对抗性示例生成技术，包括文本和多模态情景。首先概述了传统问答模型，然后通过对基于规则的扰动和先进的生成模型的探讨，研究了对抗性示例的生成技术。之后，扩展到多模态问答系统，分析了各种方法，并对生成模型、seq2seq架构和混合方法进行了研究。研究覆盖了不同的防御策略、对抗性数据集和评估指标，并展示了对抗性问答方面的全面文献。最后，考虑了对抗性问题生成的未来发展方向，突出了可以推进文本和多模态问答系统在对抗性挑战方面的潜在研究方向。 <div>
Integrating adversarial machine learning with Question Answering (QA) systems
has emerged as a critical area for understanding the vulnerabilities and
robustness of these systems. This article aims to comprehensively review
adversarial example-generation techniques in the QA field, including textual
and multimodal contexts. We examine the techniques employed through systematic
categorization, providing a comprehensive, structured review. Beginning with an
overview of traditional QA models, we traverse the adversarial example
generation by exploring rule-based perturbations and advanced generative
models. We then extend our research to include multimodal QA systems, analyze
them across various methods, and examine generative models, seq2seq
architectures, and hybrid methodologies. Our research grows to different
defense strategies, adversarial datasets, and evaluation metrics and
illustrates the comprehensive literature on adversarial QA. Finally, the paper
considers the future landscape of adversarial question generation, highlighting
potential research directions that can advance textual and multimodal QA
systems in the context of adversarial challenges.
]]></content:encoded>
<pubDate>2023-12-26T18:30:29Z</pubDate>
</item>
</channel>
</rss>